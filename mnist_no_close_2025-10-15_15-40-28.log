WARNING:root:Defaulting to PJRT_DEVICE=CPU
/localdev/jameszianxu/clean/tt-xla/examples/pytorch/mnist.py:29: DeprecationWarning: Use torch_xla.device instead
  device = xm.xla_device()
2025-10-15 15:35:14.730 (   0.000s) [        90B6F000]      dylib_platform.cc:47       1| DylibPlatform::SubclassInitialize
2025-10-15 15:35:14.738 (   0.007s) [        90B6F000]     client_instance.cc:44       1| ClientInstance::ClientInstance
2025-10-15 15:35:14.738 (   0.007s) [        90B6F000]              client.cc:18       1| TTClientInstance::TTClientInstance
2025-10-15 15:35:14.738 (   0.007s) [        90B6F000]     client_instance.cc:74       1| ClientInstance::Initialize
2025-10-15 15:35:25.332 (  10.601s) [        90B6F000]     client_instance.cc:366      1| ClientInstance::getOrCreateMeshDevice - creating new mesh device
2025-10-15 15:35:25.332 (  10.601s) [        90B6F000]     client_instance.cc:419      1| [JAMES] Opening mesh device
2025-10-15 15:35:30.050 (  15.320s) [        90B6F000]              stubs.inc:106   WARN| STUB: PJRT_Client_TopologyDescription
2025-10-15 15:35:30.051 (  15.320s) [        90B6F000]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-10-15 15:35:30.051 (  15.320s) [        90B6F000]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-10-15 15:35:30.051 (  15.320s) [        90B6F000]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-10-15 15:35:30.051 (  15.320s) [        90B6F000]     client_instance.cc:518      1| ClientInstance::PJRT_Client_PlatformVersion
2025-10-15 15:35:30.051 (  15.320s) [        90B6F000]     client_instance.cc:498      1| ClientInstance::PJRT_Client_PlatformName
2025-10-15 15:35:30.051 (  15.320s) [        90B6F000]     client_instance.cc:530      1| ClientInstance::PJRT_Client_Devices
2025-10-15 15:35:30.051 (  15.320s) [        90B6F000]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-10-15 15:35:30.051 (  15.320s) [        90B6F000]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-10-15 15:35:30.051 (  15.320s) [        90B6F000]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-10-15 15:35:30.051 (  15.320s) [        90B6F000]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-10-15 15:35:30.051 (  15.320s) [        90B6F000]     client_instance.cc:543      1| ClientInstance::PJRT_Client_AddressableDevices
2025-10-15 15:35:30.051 (  15.320s) [        90B6F000]     client_instance.cc:593      1| ClientInstance::PJRT_Client_AddressableMemories
2025-10-15 15:35:30.051 (  15.320s) [        90B6F000]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-10-15 15:35:30.051 (  15.320s) [        90B6F000]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-10-15 15:35:30.051 (  15.320s) [        90B6F000]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-15 15:35:30.051 (  15.320s) [        90B6F000]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-15 15:35:30.051 (  15.320s) [        90B6F000]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-15 15:35:30.051 (  15.320s) [        90B6F000]        api_bindings.cc:76       1| PJRT_Plugin_Attributes
2025-10-15 15:35:30.051358: W torch_xla/csrc/runtime/profiler.cpp:88] Profiler API not found for PJRT plugin
2025-10-15 15:35:30.051 (  15.320s) [        90B6F000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-15 15:35:30.051 (  15.320s) [        90B6F000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-15 15:35:30.051 (  15.320s) [        90B6F000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-15 15:35:30.051 (  15.320s) [        90B6F000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-15 15:35:30.051 (  15.320s) [        90B6F000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-15 15:35:30.051 (  15.320s) [        90B6F000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-15 15:35:30.051 (  15.320s) [        90B6F000]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-10-15 15:35:30.051 (  15.320s) [        90B6F000]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-10-15 15:35:30.051 (  15.320s) [        90B6F000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-15 15:35:30.051 (  15.320s) [        90B6F000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-15 15:35:30.051 (  15.320s) [        90B6F000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-15 15:35:30.051 (  15.320s) [        90B6F000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-15 15:35:30.051 (  15.320s) [        90B6F000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-15 15:35:30.052 (  15.322s) [        90B6F000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-15 15:35:30.052 (  15.322s) [        90B6F000]     client_instance.cc:649      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-15 15:35:30.052 (  15.322s) [        90B6F000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-15 15:35:30.053 (  15.323s) [        90B6F000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-15 15:35:30.053 (  15.323s) [        90B6F000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-15 15:35:30.053 (  15.323s) [        90B6F000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-15 15:35:30.053 (  15.323s) [        90B6F000]     client_instance.cc:649      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-15 15:35:30.053 (  15.323s) [        90B6F000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-15 15:35:30.054 (  15.323s) [        90B6F000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-15 15:35:30.054 (  15.323s) [        90B6F000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-15 15:35:30.054 (  15.323s) [        90B6F000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-15 15:35:30.054 (  15.323s) [        90B6F000]     client_instance.cc:649      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-15 15:35:30.054 (  15.323s) [        90B6F000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-15 15:35:30.054 (  15.323s) [        90B6F000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-15 15:35:30.054 (  15.323s) [        90B6F000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-15 15:35:30.054 (  15.323s) [        90B6F000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-15 15:35:30.054 (  15.323s) [        90B6F000]     client_instance.cc:649      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-15 15:35:30.054 (  15.323s) [        90B6F000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-15 15:35:30.054 (  15.324s) [        90B6F000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-15 15:35:30.054 (  15.324s) [        90B6F000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-15 15:35:30.054 (  15.324s) [        90B6F000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-15 15:35:30.054 (  15.324s) [        90B6F000]     client_instance.cc:649      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-15 15:35:30.054 (  15.324s) [        90B6F000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-15 15:35:30.054 (  15.324s) [        90B6F000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-15 15:35:30.054 (  15.324s) [        90B6F000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-15 15:35:30.055 (  15.324s) [        90B6F000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-15 15:35:30.055 (  15.324s) [        90B6F000]     client_instance.cc:649      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-15 15:35:30.055 (  15.324s) [        90B6F000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-15 15:35:30.055 (  15.324s) [        90B6F000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-15 15:35:30.055 (  15.324s) [        90B6F000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-15 15:35:30.055 (  15.325s) [        90B6F000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-15 15:35:30.055 (  15.325s) [        90B6F000]     client_instance.cc:649      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-15 15:35:30.055 (  15.325s) [        90B6F000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-15 15:35:30.056 (  15.325s) [        90B6F000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-15 15:35:30.056 (  15.325s) [        90B6F000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-15 15:35:30.056 (  15.325s) [        90B6F000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-15 15:35:30.056 (  15.325s) [        90B6F000]     client_instance.cc:649      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-15 15:35:30.056 (  15.325s) [        90B6F000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-15 15:35:30.056 (  15.325s) [        90B6F000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-15 15:35:30.056 (  15.325s) [        90B6F000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-15 15:35:30.056 (  15.325s) [        90B6F000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-15 15:35:30.056 (  15.325s) [        90B6F000]     client_instance.cc:649      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-15 15:35:30.056 (  15.325s) [        90B6F000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-15 15:35:30.056 (  15.325s) [        90B6F000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-15 15:35:30.056 (  15.325s) [        90B6F000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-15 15:35:30.446 (  15.716s) [        90B6F000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-15 15:35:30.447 (  15.716s) [        90B6F000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-15 15:35:30.447 (  15.716s) [        90B6F000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-15 15:35:30.447 (  15.716s) [        90B6F000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-15 15:35:30.447 (  15.716s) [        90B6F000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-15 15:35:30.447 (  15.716s) [        90B6F000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-15 15:35:30.447 (  15.716s) [        90B6F000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-15 15:35:30.447 (  15.716s) [        90B6F000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-15 15:35:30.447 (  15.716s) [        90B6F000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-15 15:35:30.447 (  15.716s) [        90B6F000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-15 15:35:30.447 (  15.716s) [        90B6F000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-15 15:35:30.447 (  15.716s) [        90B6F000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-15 15:35:30.447 (  15.716s) [        90B6F000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-15 15:35:30.447 (  15.716s) [        90B6F000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-15 15:35:30.447 (  15.716s) [        90B6F000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-15 15:35:30.447 (  15.716s) [        90B6F000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-15 15:35:30.447 (  15.716s) [        90B6F000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-15 15:35:30.447 (  15.716s) [        90B6F000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-15 15:35:30.447 (  15.716s) [        90B6F000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-15 15:35:30.447 (  15.716s) [        90B6F000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-15 15:35:30.447 (  15.716s) [        90B6F000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-15 15:35:30.447 (  15.716s) [        90B6F000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-15 15:35:30.447 (  15.716s) [        90B6F000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-15 15:35:30.447 (  15.716s) [        90B6F000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-15 15:35:30.447 (  15.716s) [        90B6F000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-15 15:35:30.447 (  15.716s) [        90B6F000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-15 15:35:30.447 (  15.716s) [        90B6F000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-15 15:35:30.462 (  15.732s) [        90B6F000]     client_instance.cc:606      1| ClientInstance::PJRT_Client_Compile
2025-10-15 15:35:30.463 (  15.732s) [        90B6F000]      module_builder.cc:221      1| ModuleBuilder::buildModule
2025-10-15 15:35:30.465 (  15.734s) [        90B6F000]      module_builder.cc:334      1| VHLO Module:
#loc1 = loc("xla__device_data")
#loc6 = loc("aten__max_pool2d")
#loc11 = loc("aten__amax")
#loc15 = loc("aten__sum")
module @SyncTensorsGraph.162 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<10x!vhlo.bf16_v1> loc("xla__device_data"), %arg1: !vhlo.tensor_v1<10x128x!vhlo.bf16_v1> loc("xla__device_data"), %arg2: !vhlo.tensor_v1<128x!vhlo.bf16_v1> loc("xla__device_data"), %arg3: !vhlo.tensor_v1<128x9216x!vhlo.bf16_v1> loc("xla__device_data"), %arg4: !vhlo.tensor_v1<64x!vhlo.bf16_v1> loc("xla__device_data"), %arg5: !vhlo.tensor_v1<64x32x3x3x!vhlo.bf16_v1> loc("xla__device_data"), %arg6: !vhlo.tensor_v1<32x!vhlo.bf16_v1> loc("xla__device_data"), %arg7: !vhlo.tensor_v1<32x1x3x3x!vhlo.bf16_v1> loc("xla__device_data"), %arg8: !vhlo.tensor_v1<4x1x28x28x!vhlo.bf16_v1> loc("xla__device_data")) -> (!vhlo.tensor_v1<4x10x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF80> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.broadcast_in_dim_v1"(%1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x128x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.broadcast_in_dim_v1"(%1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x64x24x24x!vhlo.bf16_v1> loc(#loc)
    %4 = "vhlo.broadcast_in_dim_v1"(%1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x26x26x!vhlo.bf16_v1> loc(#loc)
    %5 = "vhlo.custom_call_v1"(%arg8) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_0">}>} : (!vhlo.tensor_v1<4x1x28x28x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x1x28x28x!vhlo.bf16_v1> loc(#loc2)
    %6 = "vhlo.custom_call_v1"(%arg7) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___conv1_weight">}>} : (!vhlo.tensor_v1<32x1x3x3x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x3x3x!vhlo.bf16_v1> loc(#loc2)
    %7 = "vhlo.convolution_v1"(%5, %6) <{batch_group_count = #vhlo.integer_v1<1 : i64>, feature_group_count = #vhlo.integer_v1<1 : i64>, input_batch_dimension = #vhlo.integer_v1<0 : i64>, input_feature_dimension = #vhlo.integer_v1<1 : i64>, input_spatial_dimensions = #vhlo.tensor_v1<dense<[2, 3]> : tensor<2xi64>>, kernel_input_feature_dimension = #vhlo.integer_v1<1 : i64>, kernel_output_feature_dimension = #vhlo.integer_v1<0 : i64>, kernel_spatial_dimensions = #vhlo.tensor_v1<dense<[2, 3]> : tensor<2xi64>>, lhs_dilation = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>, output_batch_dimension = #vhlo.integer_v1<0 : i64>, output_feature_dimension = #vhlo.integer_v1<1 : i64>, output_spatial_dimensions = #vhlo.tensor_v1<dense<[2, 3]> : tensor<2xi64>>, padding = #vhlo.tensor_v1<dense<0> : tensor<2x2xi64>>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_dilation = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>, window_reversal = #vhlo.tensor_v1<dense<false> : tensor<2xi1>>, window_strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>}> : (!vhlo.tensor_v1<4x1x28x28x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x3x3x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x26x26x!vhlo.bf16_v1> loc(#loc3)
    %8 = "vhlo.reshape_v1"(%arg6) : (!vhlo.tensor_v1<32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x32x!vhlo.bf16_v1> loc(#loc4)
    %9 = "vhlo.custom_call_v1"(%8) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___conv1_bias">}>} : (!vhlo.tensor_v1<1x1x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x32x!vhlo.bf16_v1> loc(#loc2)
    %10 = "vhlo.reshape_v1"(%9) : (!vhlo.tensor_v1<1x1x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x!vhlo.bf16_v1> loc(#loc4)
    %11 = "vhlo.broadcast_in_dim_v1"(%10) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x26x26x!vhlo.bf16_v1> loc(#loc3)
    %12 = "vhlo.add_v1"(%7, %11) : (!vhlo.tensor_v1<4x32x26x26x!vhlo.bf16_v1>, !vhlo.tensor_v1<4x32x26x26x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x26x26x!vhlo.bf16_v1> loc(#loc3)
    %13 = "vhlo.maximum_v1"(%12, %4) : (!vhlo.tensor_v1<4x32x26x26x!vhlo.bf16_v1>, !vhlo.tensor_v1<4x32x26x26x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x26x26x!vhlo.bf16_v1> loc(#loc5)
    %14 = "vhlo.custom_call_v1"(%arg5) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___conv2_weight">}>} : (!vhlo.tensor_v1<64x32x3x3x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<64x32x3x3x!vhlo.bf16_v1> loc(#loc2)
    %15 = "vhlo.convolution_v1"(%13, %14) <{batch_group_count = #vhlo.integer_v1<1 : i64>, feature_group_count = #vhlo.integer_v1<1 : i64>, input_batch_dimension = #vhlo.integer_v1<0 : i64>, input_feature_dimension = #vhlo.integer_v1<1 : i64>, input_spatial_dimensions = #vhlo.tensor_v1<dense<[2, 3]> : tensor<2xi64>>, kernel_input_feature_dimension = #vhlo.integer_v1<1 : i64>, kernel_output_feature_dimension = #vhlo.integer_v1<0 : i64>, kernel_spatial_dimensions = #vhlo.tensor_v1<dense<[2, 3]> : tensor<2xi64>>, lhs_dilation = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>, output_batch_dimension = #vhlo.integer_v1<0 : i64>, output_feature_dimension = #vhlo.integer_v1<1 : i64>, output_spatial_dimensions = #vhlo.tensor_v1<dense<[2, 3]> : tensor<2xi64>>, padding = #vhlo.tensor_v1<dense<0> : tensor<2x2xi64>>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_dilation = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>, window_reversal = #vhlo.tensor_v1<dense<false> : tensor<2xi1>>, window_strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>}> : (!vhlo.tensor_v1<4x32x26x26x!vhlo.bf16_v1>, !vhlo.tensor_v1<64x32x3x3x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x64x24x24x!vhlo.bf16_v1> loc(#loc3)
    %16 = "vhlo.reshape_v1"(%arg4) : (!vhlo.tensor_v1<64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x64x!vhlo.bf16_v1> loc(#loc4)
    %17 = "vhlo.custom_call_v1"(%16) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___conv2_bias">}>} : (!vhlo.tensor_v1<1x1x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x64x!vhlo.bf16_v1> loc(#loc2)
    %18 = "vhlo.reshape_v1"(%17) : (!vhlo.tensor_v1<1x1x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<64x!vhlo.bf16_v1> loc(#loc4)
    %19 = "vhlo.broadcast_in_dim_v1"(%18) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x64x24x24x!vhlo.bf16_v1> loc(#loc3)
    %20 = "vhlo.add_v1"(%15, %19) : (!vhlo.tensor_v1<4x64x24x24x!vhlo.bf16_v1>, !vhlo.tensor_v1<4x64x24x24x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x64x24x24x!vhlo.bf16_v1> loc(#loc3)
    %21 = "vhlo.maximum_v1"(%20, %3) : (!vhlo.tensor_v1<4x64x24x24x!vhlo.bf16_v1>, !vhlo.tensor_v1<4x64x24x24x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x64x24x24x!vhlo.bf16_v1> loc(#loc5)
    %22 = "vhlo.reduce_window_v1"(%21, %0) <{base_dilations = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>, padding = #vhlo.tensor_v1<dense<0> : tensor<4x2xi64>>, window_dilations = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>, window_dimensions = #vhlo.tensor_v1<dense<[1, 1, 2, 2]> : tensor<4xi64>>, window_strides = #vhlo.tensor_v1<dense<[1, 1, 2, 2]> : tensor<4xi64>>}> ({
    ^bb0(%arg9: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("aten__max_pool2d"), %arg10: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("aten__max_pool2d")):
      %55 = "vhlo.maximum_v1"(%arg9, %arg10) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc7)
      "vhlo.return_v1"(%55) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<4x64x24x24x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x64x12x12x!vhlo.bf16_v1> loc(#loc6)
    %23 = "vhlo.reshape_v1"(%22) : (!vhlo.tensor_v1<4x64x12x12x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x9216x!vhlo.bf16_v1> loc(#loc4)
    %24 = "vhlo.reshape_v1"(%arg3) : (!vhlo.tensor_v1<128x9216x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128x9216x!vhlo.bf16_v1> loc(#loc4)
    %25 = "vhlo.custom_call_v1"(%24) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___fc1_weight">}>} : (!vhlo.tensor_v1<1x128x9216x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128x9216x!vhlo.bf16_v1> loc(#loc2)
    %26 = "vhlo.reshape_v1"(%25) : (!vhlo.tensor_v1<1x128x9216x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x9216x!vhlo.bf16_v1> loc(#loc4)
    %27 = "vhlo.transpose_v1"(%26) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[9216,128]{0,1}">} : (!vhlo.tensor_v1<128x9216x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<9216x128x!vhlo.bf16_v1> loc(#loc8)
    %28 = "vhlo.dot_general_v2"(%23, %27) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<4x9216x!vhlo.bf16_v1>, !vhlo.tensor_v1<9216x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x128x!vhlo.bf16_v1> loc(#loc9)
    %29 = "vhlo.reshape_v1"(%arg2) : (!vhlo.tensor_v1<128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1> loc(#loc4)
    %30 = "vhlo.custom_call_v1"(%29) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___fc1_bias">}>} : (!vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1> loc(#loc2)
    %31 = "vhlo.reshape_v1"(%30) : (!vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x!vhlo.bf16_v1> loc(#loc4)
    %32 = "vhlo.broadcast_in_dim_v1"(%31) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x128x!vhlo.bf16_v1> loc(#loc10)
    %33 = "vhlo.add_v1"(%28, %32) : (!vhlo.tensor_v1<4x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<4x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x128x!vhlo.bf16_v1> loc(#loc10)
    %34 = "vhlo.maximum_v1"(%33, %2) : (!vhlo.tensor_v1<4x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<4x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x128x!vhlo.bf16_v1> loc(#loc5)
    %35 = "vhlo.reshape_v1"(%arg1) : (!vhlo.tensor_v1<10x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x10x128x!vhlo.bf16_v1> loc(#loc4)
    %36 = "vhlo.custom_call_v1"(%35) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___fc2_weight">}>} : (!vhlo.tensor_v1<1x10x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x10x128x!vhlo.bf16_v1> loc(#loc2)
    %37 = "vhlo.reshape_v1"(%36) : (!vhlo.tensor_v1<1x10x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<10x128x!vhlo.bf16_v1> loc(#loc4)
    %38 = "vhlo.transpose_v1"(%37) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[128,10]{0,1}">} : (!vhlo.tensor_v1<10x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x10x!vhlo.bf16_v1> loc(#loc8)
    %39 = "vhlo.dot_general_v2"(%34, %38) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<4x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x10x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x10x!vhlo.bf16_v1> loc(#loc9)
    %40 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<10x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x10x!vhlo.bf16_v1> loc(#loc4)
    %41 = "vhlo.custom_call_v1"(%40) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___fc2_bias">}>} : (!vhlo.tensor_v1<1x1x10x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x10x!vhlo.bf16_v1> loc(#loc2)
    %42 = "vhlo.reshape_v1"(%41) : (!vhlo.tensor_v1<1x1x10x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<10x!vhlo.bf16_v1> loc(#loc4)
    %43 = "vhlo.broadcast_in_dim_v1"(%42) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<10x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x10x!vhlo.bf16_v1> loc(#loc10)
    %44 = "vhlo.add_v1"(%39, %43) : (!vhlo.tensor_v1<4x10x!vhlo.bf16_v1>, !vhlo.tensor_v1<4x10x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x10x!vhlo.bf16_v1> loc(#loc10)
    %45 = "vhlo.reduce_v1"(%44, %0) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg9: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("aten__amax"), %arg10: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("aten__amax")):
      %55 = "vhlo.maximum_v1"(%arg9, %arg10) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc12)
      "vhlo.return_v1"(%55) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<4x10x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x!vhlo.bf16_v1> loc(#loc11)
    %46 = "vhlo.broadcast_in_dim_v1"(%45) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x10x!vhlo.bf16_v1> loc(#loc13)
    %47 = "vhlo.subtract_v1"(%44, %46) : (!vhlo.tensor_v1<4x10x!vhlo.bf16_v1>, !vhlo.tensor_v1<4x10x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x10x!vhlo.bf16_v1> loc(#loc13)
    %48 = "vhlo.exponential_v2"(%47) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<4x10x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x10x!vhlo.bf16_v1> loc(#loc14)
    %49 = "vhlo.reduce_v1"(%48, %1) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg9: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("aten__sum"), %arg10: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("aten__sum")):
      %55 = "vhlo.add_v1"(%arg9, %arg10) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc16)
      "vhlo.return_v1"(%55) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<4x10x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x!vhlo.bf16_v1> loc(#loc15)
    %50 = "vhlo.reshape_v1"(%49) : (!vhlo.tensor_v1<4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x1x!vhlo.bf16_v1> loc(#loc15)
    %51 = "vhlo.log_v2"(%50) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<4x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x1x!vhlo.bf16_v1> loc(#loc17)
    %52 = "vhlo.reshape_v1"(%51) : (!vhlo.tensor_v1<4x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x!vhlo.bf16_v1> loc(#loc13)
    %53 = "vhlo.broadcast_in_dim_v1"(%52) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x10x!vhlo.bf16_v1> loc(#loc13)
    %54 = "vhlo.subtract_v1"(%47, %53) : (!vhlo.tensor_v1<4x10x!vhlo.bf16_v1>, !vhlo.tensor_v1<4x10x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x10x!vhlo.bf16_v1> loc(#loc13)
    "vhlo.return_v1"(%54) : (!vhlo.tensor_v1<4x10x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">} loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("xla__custom_call")
#loc3 = loc("aten__convolution_overrideable")
#loc4 = loc("aten__view")
#loc5 = loc("aten__relu")
#loc7 = loc("maximum.73")
#loc8 = loc("aten__permute")
#loc9 = loc("aten__mm")
#loc10 = loc("aten__add")
#loc12 = loc("maximum.138")
#loc13 = loc("aten__sub")
#loc14 = loc("aten__exp")
#loc16 = loc("add.152")
#loc17 = loc("aten__log")
------------------ END OF MLIR MODULE ------------------
2025-10-15 15:35:30.473 (  15.743s) [        90B6F000]      module_builder.cc:353      1| SHLO Module:
#loc1 = loc("xla__device_data")
#loc6 = loc("aten__max_pool2d")
module @SyncTensorsGraph.162 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<10xbf16> loc("xla__device_data"), %arg1: tensor<10x128xbf16> loc("xla__device_data"), %arg2: tensor<128xbf16> loc("xla__device_data"), %arg3: tensor<128x9216xbf16> loc("xla__device_data"), %arg4: tensor<64xbf16> loc("xla__device_data"), %arg5: tensor<64x32x3x3xbf16> loc("xla__device_data"), %arg6: tensor<32xbf16> loc("xla__device_data"), %arg7: tensor<32x1x3x3xbf16> loc("xla__device_data"), %arg8: tensor<4x1x28x28xbf16> loc("xla__device_data")) -> tensor<4x10xbf16> {
    %cst = stablehlo.constant dense<0xFF80> : tensor<bf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<4x128xbf16> loc(#loc)
    %1 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<4x64x24x24xbf16> loc(#loc)
    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<4x32x26x26xbf16> loc(#loc)
    %3 = stablehlo.custom_call @tt.mark_argument(%arg8) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_0"}} : (tensor<4x1x28x28xbf16>) -> tensor<4x1x28x28xbf16> loc(#loc2)
    %4 = stablehlo.custom_call @tt.mark_argument(%arg7) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___conv1_weight"}} : (tensor<32x1x3x3xbf16>) -> tensor<32x1x3x3xbf16> loc(#loc2)
    %5 = stablehlo.convolution(%3, %4) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<4x1x28x28xbf16>, tensor<32x1x3x3xbf16>) -> tensor<4x32x26x26xbf16> loc(#loc3)
    %6 = stablehlo.reshape %arg6 : (tensor<32xbf16>) -> tensor<1x1x32xbf16> loc(#loc4)
    %7 = stablehlo.custom_call @tt.mark_argument(%6) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___conv1_bias"}} : (tensor<1x1x32xbf16>) -> tensor<1x1x32xbf16> loc(#loc2)
    %8 = stablehlo.reshape %7 : (tensor<1x1x32xbf16>) -> tensor<32xbf16> loc(#loc4)
    %9 = stablehlo.broadcast_in_dim %8, dims = [1] : (tensor<32xbf16>) -> tensor<4x32x26x26xbf16> loc(#loc3)
    %10 = stablehlo.add %5, %9 : tensor<4x32x26x26xbf16> loc(#loc3)
    %11 = stablehlo.maximum %10, %2 : tensor<4x32x26x26xbf16> loc(#loc5)
    %12 = stablehlo.custom_call @tt.mark_argument(%arg5) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___conv2_weight"}} : (tensor<64x32x3x3xbf16>) -> tensor<64x32x3x3xbf16> loc(#loc2)
    %13 = stablehlo.convolution(%11, %12) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<4x32x26x26xbf16>, tensor<64x32x3x3xbf16>) -> tensor<4x64x24x24xbf16> loc(#loc3)
    %14 = stablehlo.reshape %arg4 : (tensor<64xbf16>) -> tensor<1x1x64xbf16> loc(#loc4)
    %15 = stablehlo.custom_call @tt.mark_argument(%14) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___conv2_bias"}} : (tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc2)
    %16 = stablehlo.reshape %15 : (tensor<1x1x64xbf16>) -> tensor<64xbf16> loc(#loc4)
    %17 = stablehlo.broadcast_in_dim %16, dims = [1] : (tensor<64xbf16>) -> tensor<4x64x24x24xbf16> loc(#loc3)
    %18 = stablehlo.add %13, %17 : tensor<4x64x24x24xbf16> loc(#loc3)
    %19 = stablehlo.maximum %18, %1 : tensor<4x64x24x24xbf16> loc(#loc5)
    %20 = "stablehlo.reduce_window"(%19, %cst) <{window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> ({
    ^bb0(%arg9: tensor<bf16> loc("aten__max_pool2d"), %arg10: tensor<bf16> loc("aten__max_pool2d")):
      %53 = stablehlo.maximum %arg9, %arg10 : tensor<bf16> loc(#loc7)
      stablehlo.return %53 : tensor<bf16> loc(#loc)
    }) : (tensor<4x64x24x24xbf16>, tensor<bf16>) -> tensor<4x64x12x12xbf16> loc(#loc6)
    %21 = stablehlo.reshape %20 : (tensor<4x64x12x12xbf16>) -> tensor<4x9216xbf16> loc(#loc4)
    %22 = stablehlo.reshape %arg3 : (tensor<128x9216xbf16>) -> tensor<1x128x9216xbf16> loc(#loc4)
    %23 = stablehlo.custom_call @tt.mark_argument(%22) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___fc1_weight"}} : (tensor<1x128x9216xbf16>) -> tensor<1x128x9216xbf16> loc(#loc2)
    %24 = stablehlo.reshape %23 : (tensor<1x128x9216xbf16>) -> tensor<128x9216xbf16> loc(#loc4)
    %25 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[9216,128]{0,1}"} : (tensor<128x9216xbf16>) -> tensor<9216x128xbf16> loc(#loc8)
    %26 = stablehlo.dot_general %21, %25, contracting_dims = [1] x [0] : (tensor<4x9216xbf16>, tensor<9216x128xbf16>) -> tensor<4x128xbf16> loc(#loc9)
    %27 = stablehlo.reshape %arg2 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4)
    %28 = stablehlo.custom_call @tt.mark_argument(%27) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___fc1_bias"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2)
    %29 = stablehlo.reshape %28 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4)
    %30 = stablehlo.broadcast_in_dim %29, dims = [1] : (tensor<128xbf16>) -> tensor<4x128xbf16> loc(#loc10)
    %31 = stablehlo.add %26, %30 : tensor<4x128xbf16> loc(#loc10)
    %32 = stablehlo.maximum %31, %0 : tensor<4x128xbf16> loc(#loc5)
    %33 = stablehlo.reshape %arg1 : (tensor<10x128xbf16>) -> tensor<1x10x128xbf16> loc(#loc4)
    %34 = stablehlo.custom_call @tt.mark_argument(%33) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___fc2_weight"}} : (tensor<1x10x128xbf16>) -> tensor<1x10x128xbf16> loc(#loc2)
    %35 = stablehlo.reshape %34 : (tensor<1x10x128xbf16>) -> tensor<10x128xbf16> loc(#loc4)
    %36 = stablehlo.transpose %35, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,10]{0,1}"} : (tensor<10x128xbf16>) -> tensor<128x10xbf16> loc(#loc8)
    %37 = stablehlo.dot_general %32, %36, contracting_dims = [1] x [0] : (tensor<4x128xbf16>, tensor<128x10xbf16>) -> tensor<4x10xbf16> loc(#loc9)
    %38 = stablehlo.reshape %arg0 : (tensor<10xbf16>) -> tensor<1x1x10xbf16> loc(#loc4)
    %39 = stablehlo.custom_call @tt.mark_argument(%38) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___fc2_bias"}} : (tensor<1x1x10xbf16>) -> tensor<1x1x10xbf16> loc(#loc2)
    %40 = stablehlo.reshape %39 : (tensor<1x1x10xbf16>) -> tensor<10xbf16> loc(#loc4)
    %41 = stablehlo.broadcast_in_dim %40, dims = [1] : (tensor<10xbf16>) -> tensor<4x10xbf16> loc(#loc10)
    %42 = stablehlo.add %37, %41 : tensor<4x10xbf16> loc(#loc10)
    %43 = stablehlo.reduce(%42 init: %cst) applies stablehlo.maximum across dimensions = [1] : (tensor<4x10xbf16>, tensor<bf16>) -> tensor<4xbf16> loc(#loc11)
    %44 = stablehlo.broadcast_in_dim %43, dims = [0] : (tensor<4xbf16>) -> tensor<4x10xbf16> loc(#loc12)
    %45 = stablehlo.subtract %42, %44 : tensor<4x10xbf16> loc(#loc12)
    %46 = stablehlo.exponential %45 : tensor<4x10xbf16> loc(#loc13)
    %47 = stablehlo.reduce(%46 init: %cst_0) applies stablehlo.add across dimensions = [1] : (tensor<4x10xbf16>, tensor<bf16>) -> tensor<4xbf16> loc(#loc14)
    %48 = stablehlo.reshape %47 : (tensor<4xbf16>) -> tensor<4x1xbf16> loc(#loc14)
    %49 = stablehlo.log %48 : tensor<4x1xbf16> loc(#loc15)
    %50 = stablehlo.reshape %49 : (tensor<4x1xbf16>) -> tensor<4xbf16> loc(#loc12)
    %51 = stablehlo.broadcast_in_dim %50, dims = [0] : (tensor<4xbf16>) -> tensor<4x10xbf16> loc(#loc12)
    %52 = stablehlo.subtract %45, %51 : tensor<4x10xbf16> loc(#loc12)
    return %52 : tensor<4x10xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("xla__custom_call")
#loc3 = loc("aten__convolution_overrideable")
#loc4 = loc("aten__view")
#loc5 = loc("aten__relu")
#loc7 = loc("maximum.73")
#loc8 = loc("aten__permute")
#loc9 = loc("aten__mm")
#loc10 = loc("aten__add")
#loc11 = loc("aten__amax")
#loc12 = loc("aten__sub")
#loc13 = loc("aten__exp")
#loc14 = loc("aten__sum")
#loc15 = loc("aten__log")
------------------ END OF MLIR MODULE ------------------
2025-10-15 15:35:30.477 (  15.746s) [        90B6F000]      module_builder.cc:365      1| SHLO Module after frontend StableHLO pipeline:
#loc1 = loc("xla__device_data")
#loc5 = loc("aten__max_pool2d")
module @SyncTensorsGraph.162 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<10xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___fc2_bias"} loc("xla__device_data"), %arg1: tensor<10x128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___fc2_weight"} loc("xla__device_data"), %arg2: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___fc1_bias"} loc("xla__device_data"), %arg3: tensor<128x9216xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___fc1_weight"} loc("xla__device_data"), %arg4: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___conv2_bias"} loc("xla__device_data"), %arg5: tensor<64x32x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___conv2_weight"} loc("xla__device_data"), %arg6: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___conv1_bias"} loc("xla__device_data"), %arg7: tensor<32x1x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___conv1_weight"} loc("xla__device_data"), %arg8: tensor<4x1x28x28xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"} loc("xla__device_data")) -> tensor<4x10xbf16> {
    %cst = stablehlo.constant dense<0xFF80> : tensor<bf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<4x128xbf16> loc(#loc)
    %1 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<4x64x24x24xbf16> loc(#loc)
    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<4x32x26x26xbf16> loc(#loc)
    %3 = stablehlo.convolution(%arg8, %arg7) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<4x1x28x28xbf16>, tensor<32x1x3x3xbf16>) -> tensor<4x32x26x26xbf16> loc(#loc2)
    %4 = stablehlo.reshape %arg6 : (tensor<32xbf16>) -> tensor<1x1x32xbf16> loc(#loc3)
    %5 = stablehlo.reshape %4 : (tensor<1x1x32xbf16>) -> tensor<32xbf16> loc(#loc3)
    %6 = stablehlo.broadcast_in_dim %5, dims = [1] : (tensor<32xbf16>) -> tensor<4x32x26x26xbf16> loc(#loc2)
    %7 = stablehlo.add %3, %6 : tensor<4x32x26x26xbf16> loc(#loc2)
    %8 = stablehlo.maximum %7, %2 : tensor<4x32x26x26xbf16> loc(#loc4)
    %9 = stablehlo.convolution(%8, %arg5) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<4x32x26x26xbf16>, tensor<64x32x3x3xbf16>) -> tensor<4x64x24x24xbf16> loc(#loc2)
    %10 = stablehlo.reshape %arg4 : (tensor<64xbf16>) -> tensor<1x1x64xbf16> loc(#loc3)
    %11 = stablehlo.reshape %10 : (tensor<1x1x64xbf16>) -> tensor<64xbf16> loc(#loc3)
    %12 = stablehlo.broadcast_in_dim %11, dims = [1] : (tensor<64xbf16>) -> tensor<4x64x24x24xbf16> loc(#loc2)
    %13 = stablehlo.add %9, %12 : tensor<4x64x24x24xbf16> loc(#loc2)
    %14 = stablehlo.maximum %13, %1 : tensor<4x64x24x24xbf16> loc(#loc4)
    %15 = "stablehlo.reduce_window"(%14, %cst) <{window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> ({
    ^bb0(%arg9: tensor<bf16> loc("aten__max_pool2d"), %arg10: tensor<bf16> loc("aten__max_pool2d")):
      %44 = stablehlo.maximum %arg9, %arg10 : tensor<bf16> loc(#loc6)
      stablehlo.return %44 : tensor<bf16> loc(#loc)
    }) : (tensor<4x64x24x24xbf16>, tensor<bf16>) -> tensor<4x64x12x12xbf16> loc(#loc5)
    %16 = stablehlo.reshape %15 : (tensor<4x64x12x12xbf16>) -> tensor<4x9216xbf16> loc(#loc3)
    %17 = stablehlo.reshape %arg3 : (tensor<128x9216xbf16>) -> tensor<1x128x9216xbf16> loc(#loc3)
    %18 = stablehlo.reshape %17 : (tensor<1x128x9216xbf16>) -> tensor<128x9216xbf16> loc(#loc3)
    %19 = stablehlo.transpose %18, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[9216,128]{0,1}"} : (tensor<128x9216xbf16>) -> tensor<9216x128xbf16> loc(#loc7)
    %20 = stablehlo.dot_general %16, %19, contracting_dims = [1] x [0] : (tensor<4x9216xbf16>, tensor<9216x128xbf16>) -> tensor<4x128xbf16> loc(#loc8)
    %21 = stablehlo.reshape %arg2 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3)
    %22 = stablehlo.reshape %21 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc3)
    %23 = stablehlo.broadcast_in_dim %22, dims = [1] : (tensor<128xbf16>) -> tensor<4x128xbf16> loc(#loc9)
    %24 = stablehlo.add %20, %23 : tensor<4x128xbf16> loc(#loc9)
    %25 = stablehlo.maximum %24, %0 : tensor<4x128xbf16> loc(#loc4)
    %26 = stablehlo.reshape %arg1 : (tensor<10x128xbf16>) -> tensor<1x10x128xbf16> loc(#loc3)
    %27 = stablehlo.reshape %26 : (tensor<1x10x128xbf16>) -> tensor<10x128xbf16> loc(#loc3)
    %28 = stablehlo.transpose %27, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,10]{0,1}"} : (tensor<10x128xbf16>) -> tensor<128x10xbf16> loc(#loc7)
    %29 = stablehlo.dot_general %25, %28, contracting_dims = [1] x [0] : (tensor<4x128xbf16>, tensor<128x10xbf16>) -> tensor<4x10xbf16> loc(#loc8)
    %30 = stablehlo.reshape %arg0 : (tensor<10xbf16>) -> tensor<1x1x10xbf16> loc(#loc3)
    %31 = stablehlo.reshape %30 : (tensor<1x1x10xbf16>) -> tensor<10xbf16> loc(#loc3)
    %32 = stablehlo.broadcast_in_dim %31, dims = [1] : (tensor<10xbf16>) -> tensor<4x10xbf16> loc(#loc9)
    %33 = stablehlo.add %29, %32 : tensor<4x10xbf16> loc(#loc9)
    %34 = stablehlo.reduce(%33 init: %cst) applies stablehlo.maximum across dimensions = [1] : (tensor<4x10xbf16>, tensor<bf16>) -> tensor<4xbf16> loc(#loc10)
    %35 = stablehlo.broadcast_in_dim %34, dims = [0] : (tensor<4xbf16>) -> tensor<4x10xbf16> loc(#loc11)
    %36 = stablehlo.subtract %33, %35 : tensor<4x10xbf16> loc(#loc11)
    %37 = stablehlo.exponential %36 : tensor<4x10xbf16> loc(#loc12)
    %38 = stablehlo.reduce(%37 init: %cst_0) applies stablehlo.add across dimensions = [1] : (tensor<4x10xbf16>, tensor<bf16>) -> tensor<4xbf16> loc(#loc13)
    %39 = stablehlo.reshape %38 : (tensor<4xbf16>) -> tensor<4x1xbf16> loc(#loc13)
    %40 = stablehlo.log %39 : tensor<4x1xbf16> loc(#loc14)
    %41 = stablehlo.reshape %40 : (tensor<4x1xbf16>) -> tensor<4xbf16> loc(#loc11)
    %42 = stablehlo.broadcast_in_dim %41, dims = [0] : (tensor<4xbf16>) -> tensor<4x10xbf16> loc(#loc11)
    %43 = stablehlo.subtract %36, %42 : tensor<4x10xbf16> loc(#loc11)
    return %43 : tensor<4x10xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("aten__convolution_overrideable")
#loc3 = loc("aten__view")
#loc4 = loc("aten__relu")
#loc6 = loc("maximum.73")
#loc7 = loc("aten__permute")
#loc8 = loc("aten__mm")
#loc9 = loc("aten__add")
#loc10 = loc("aten__amax")
#loc11 = loc("aten__sub")
#loc12 = loc("aten__exp")
#loc13 = loc("aten__sum")
#loc14 = loc("aten__log")
------------------ END OF MLIR MODULE ------------------
2025-10-15 15:35:30.489 (  15.758s) [        90B6F000]      module_builder.cc:682      1| SHLO Module after compiler StableHLO pipeline:
#loc1 = loc("xla__device_data")
#loc5 = loc("aten__max_pool2d")
module @SyncTensorsGraph.162 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=1]> loc(#loc)
  func.func @main(%arg0: tensor<10xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___fc2_bias"} loc("xla__device_data"), %arg1: tensor<10x128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___fc2_weight"} loc("xla__device_data"), %arg2: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___fc1_bias"} loc("xla__device_data"), %arg3: tensor<128x9216xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___fc1_weight"} loc("xla__device_data"), %arg4: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___conv2_bias"} loc("xla__device_data"), %arg5: tensor<64x32x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___conv2_weight"} loc("xla__device_data"), %arg6: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___conv1_bias"} loc("xla__device_data"), %arg7: tensor<32x1x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___conv1_weight"} loc("xla__device_data"), %arg8: tensor<4x1x28x28xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_0"} loc("xla__device_data")) -> (tensor<4x10xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0xFF80> : tensor<bf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<4x128xbf16> loc(#loc)
    %1 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<4x64x24x24xbf16> loc(#loc)
    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<4x32x26x26xbf16> loc(#loc)
    %3 = stablehlo.convolution(%arg8, %arg7) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<4x1x28x28xbf16>, tensor<32x1x3x3xbf16>) -> tensor<4x32x26x26xbf16> loc(#loc2)
    %4 = stablehlo.reshape %arg6 : (tensor<32xbf16>) -> tensor<1x1x32xbf16> loc(#loc3)
    %5 = stablehlo.reshape %4 : (tensor<1x1x32xbf16>) -> tensor<32xbf16> loc(#loc3)
    %6 = stablehlo.broadcast_in_dim %5, dims = [1] : (tensor<32xbf16>) -> tensor<4x32x26x26xbf16> loc(#loc2)
    %7 = stablehlo.add %3, %6 : tensor<4x32x26x26xbf16> loc(#loc2)
    %8 = stablehlo.maximum %7, %2 : tensor<4x32x26x26xbf16> loc(#loc4)
    %9 = stablehlo.convolution(%8, %arg5) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<4x32x26x26xbf16>, tensor<64x32x3x3xbf16>) -> tensor<4x64x24x24xbf16> loc(#loc2)
    %10 = stablehlo.reshape %arg4 : (tensor<64xbf16>) -> tensor<1x1x64xbf16> loc(#loc3)
    %11 = stablehlo.reshape %10 : (tensor<1x1x64xbf16>) -> tensor<64xbf16> loc(#loc3)
    %12 = stablehlo.broadcast_in_dim %11, dims = [1] : (tensor<64xbf16>) -> tensor<4x64x24x24xbf16> loc(#loc2)
    %13 = stablehlo.add %9, %12 : tensor<4x64x24x24xbf16> loc(#loc2)
    %14 = stablehlo.maximum %13, %1 : tensor<4x64x24x24xbf16> loc(#loc4)
    %15 = "stablehlo.reduce_window"(%14, %cst) <{window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> ({
    ^bb0(%arg9: tensor<bf16> loc("aten__max_pool2d"), %arg10: tensor<bf16> loc("aten__max_pool2d")):
      %44 = stablehlo.maximum %arg9, %arg10 : tensor<bf16> loc(#loc6)
      stablehlo.return %44 : tensor<bf16> loc(#loc)
    }) : (tensor<4x64x24x24xbf16>, tensor<bf16>) -> tensor<4x64x12x12xbf16> loc(#loc5)
    %16 = stablehlo.reshape %15 : (tensor<4x64x12x12xbf16>) -> tensor<4x9216xbf16> loc(#loc3)
    %17 = stablehlo.reshape %arg3 : (tensor<128x9216xbf16>) -> tensor<1x128x9216xbf16> loc(#loc3)
    %18 = stablehlo.reshape %17 : (tensor<1x128x9216xbf16>) -> tensor<128x9216xbf16> loc(#loc3)
    %19 = stablehlo.transpose %18, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[9216,128]{0,1}"} : (tensor<128x9216xbf16>) -> tensor<9216x128xbf16> loc(#loc7)
    %20 = stablehlo.dot_general %16, %19, contracting_dims = [1] x [0] : (tensor<4x9216xbf16>, tensor<9216x128xbf16>) -> tensor<4x128xbf16> loc(#loc8)
    %21 = stablehlo.reshape %arg2 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3)
    %22 = stablehlo.reshape %21 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc3)
    %23 = stablehlo.broadcast_in_dim %22, dims = [1] : (tensor<128xbf16>) -> tensor<4x128xbf16> loc(#loc9)
    %24 = stablehlo.add %20, %23 : tensor<4x128xbf16> loc(#loc9)
    %25 = stablehlo.maximum %24, %0 : tensor<4x128xbf16> loc(#loc4)
    %26 = stablehlo.reshape %arg1 : (tensor<10x128xbf16>) -> tensor<1x10x128xbf16> loc(#loc3)
    %27 = stablehlo.reshape %26 : (tensor<1x10x128xbf16>) -> tensor<10x128xbf16> loc(#loc3)
    %28 = stablehlo.transpose %27, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,10]{0,1}"} : (tensor<10x128xbf16>) -> tensor<128x10xbf16> loc(#loc7)
    %29 = stablehlo.dot_general %25, %28, contracting_dims = [1] x [0] : (tensor<4x128xbf16>, tensor<128x10xbf16>) -> tensor<4x10xbf16> loc(#loc8)
    %30 = stablehlo.reshape %arg0 : (tensor<10xbf16>) -> tensor<1x1x10xbf16> loc(#loc3)
    %31 = stablehlo.reshape %30 : (tensor<1x1x10xbf16>) -> tensor<10xbf16> loc(#loc3)
    %32 = stablehlo.broadcast_in_dim %31, dims = [1] : (tensor<10xbf16>) -> tensor<4x10xbf16> loc(#loc9)
    %33 = stablehlo.add %29, %32 : tensor<4x10xbf16> loc(#loc9)
    %34 = stablehlo.reduce(%33 init: %cst) applies stablehlo.maximum across dimensions = [1] : (tensor<4x10xbf16>, tensor<bf16>) -> tensor<4xbf16> loc(#loc10)
    %35 = stablehlo.broadcast_in_dim %34, dims = [0] : (tensor<4xbf16>) -> tensor<4x10xbf16> loc(#loc11)
    %36 = stablehlo.subtract %33, %35 : tensor<4x10xbf16> loc(#loc11)
    %37 = stablehlo.exponential %36 : tensor<4x10xbf16> loc(#loc12)
    %38 = stablehlo.reduce(%37 init: %cst_0) applies stablehlo.add across dimensions = [1] : (tensor<4x10xbf16>, tensor<bf16>) -> tensor<4xbf16> loc(#loc13)
    %39 = stablehlo.reshape %38 : (tensor<4xbf16>) -> tensor<4x1xbf16> loc(#loc13)
    %40 = stablehlo.log %39 : tensor<4x1xbf16> loc(#loc14)
    %41 = stablehlo.reshape %40 : (tensor<4x1xbf16>) -> tensor<4xbf16> loc(#loc11)
    %42 = stablehlo.broadcast_in_dim %41, dims = [0] : (tensor<4xbf16>) -> tensor<4x10xbf16> loc(#loc11)
    %43 = stablehlo.subtract %36, %42 : tensor<4x10xbf16> loc(#loc11)
    return %43 : tensor<4x10xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("aten__convolution_overrideable")
#loc3 = loc("aten__view")
#loc4 = loc("aten__relu")
#loc6 = loc("maximum.73")
#loc7 = loc("aten__permute")
#loc8 = loc("aten__mm")
#loc9 = loc("aten__add")
#loc10 = loc("aten__amax")
#loc11 = loc("aten__sub")
#loc12 = loc("aten__exp")
#loc13 = loc("aten__sum")
#loc14 = loc("aten__log")
------------------ END OF MLIR MODULE ------------------
2025-10-15 15:35:30.493 (  15.763s) [        90B6F000]      module_builder.cc:715      1| TTIR Module:
#loc1 = loc("xla__device_data")
module @SyncTensorsGraph.162 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  func.func @main(%arg0: tensor<10xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___fc2_bias"} loc("xla__device_data"), %arg1: tensor<10x128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___fc2_weight"} loc("xla__device_data"), %arg2: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___fc1_bias"} loc("xla__device_data"), %arg3: tensor<128x9216xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___fc1_weight"} loc("xla__device_data"), %arg4: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___conv2_bias"} loc("xla__device_data"), %arg5: tensor<64x32x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___conv2_weight"} loc("xla__device_data"), %arg6: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___conv1_bias"} loc("xla__device_data"), %arg7: tensor<32x1x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___conv1_weight"} loc("xla__device_data"), %arg8: tensor<4x1x28x28xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_0"} loc("xla__device_data")) -> (tensor<4x10xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<0xFF80> : tensor<bf16>}> : () -> tensor<bf16> loc(#loc)
    %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<bf16>}> : () -> tensor<bf16> loc(#loc)
    %2 = ttir.empty() : tensor<1x1xbf16> loc(#loc)
    %3 = "ttir.reshape"(%1, %2) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16> loc(#loc)
    %4 = ttir.empty() : tensor<4x128xbf16> loc(#loc)
    %5 = "ttir.broadcast"(%3, %4) <{broadcast_dimensions = array<i64: 4, 128>}> : (tensor<1x1xbf16>, tensor<4x128xbf16>) -> tensor<4x128xbf16> loc(#loc)
    %6 = ttir.empty() : tensor<1x1x1x1xbf16> loc(#loc)
    %7 = "ttir.reshape"(%1, %6) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16> loc(#loc)
    %8 = ttir.empty() : tensor<4x64x24x24xbf16> loc(#loc)
    %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 4, 64, 24, 24>}> : (tensor<1x1x1x1xbf16>, tensor<4x64x24x24xbf16>) -> tensor<4x64x24x24xbf16> loc(#loc)
    %10 = ttir.empty() : tensor<1x1x1x1xbf16> loc(#loc)
    %11 = "ttir.reshape"(%1, %10) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16> loc(#loc)
    %12 = ttir.empty() : tensor<4x32x26x26xbf16> loc(#loc)
    %13 = "ttir.broadcast"(%11, %12) <{broadcast_dimensions = array<i64: 4, 32, 26, 26>}> : (tensor<1x1x1x1xbf16>, tensor<4x32x26x26xbf16>) -> tensor<4x32x26x26xbf16> loc(#loc)
    %14 = ttir.empty() : tensor<4x32x26x26xbf16> loc(#loc2)
    %15 = "ttir.convolution"(%arg8, %arg7, %14) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<4x1x28x28xbf16>, tensor<32x1x3x3xbf16>, tensor<4x32x26x26xbf16>) -> tensor<4x32x26x26xbf16> loc(#loc2)
    %16 = ttir.empty() : tensor<1x1x32xbf16> loc(#loc3)
    %17 = "ttir.reshape"(%arg6, %16) <{shape = [1 : i32, 1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x1x32xbf16>) -> tensor<1x1x32xbf16> loc(#loc3)
    %18 = ttir.empty() : tensor<32xbf16> loc(#loc3)
    %19 = "ttir.reshape"(%17, %18) <{shape = [32 : i32]}> : (tensor<1x1x32xbf16>, tensor<32xbf16>) -> tensor<32xbf16> loc(#loc3)
    %20 = ttir.empty() : tensor<1x32x1x1xbf16> loc(#loc2)
    %21 = "ttir.reshape"(%19, %20) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16> loc(#loc2)
    %22 = ttir.empty() : tensor<4x32x26x26xbf16> loc(#loc2)
    %23 = "ttir.broadcast"(%21, %22) <{broadcast_dimensions = array<i64: 4, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<4x32x26x26xbf16>) -> tensor<4x32x26x26xbf16> loc(#loc2)
    %24 = ttir.empty() : tensor<4x32x26x26xbf16> loc(#loc2)
    %25 = "ttir.add"(%15, %23, %24) : (tensor<4x32x26x26xbf16>, tensor<4x32x26x26xbf16>, tensor<4x32x26x26xbf16>) -> tensor<4x32x26x26xbf16> loc(#loc2)
    %26 = ttir.empty() : tensor<4x32x26x26xbf16> loc(#loc4)
    %27 = "ttir.maximum"(%25, %13, %26) : (tensor<4x32x26x26xbf16>, tensor<4x32x26x26xbf16>, tensor<4x32x26x26xbf16>) -> tensor<4x32x26x26xbf16> loc(#loc4)
    %28 = ttir.empty() : tensor<4x64x24x24xbf16> loc(#loc2)
    %29 = "ttir.convolution"(%27, %arg5, %28) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<4x32x26x26xbf16>, tensor<64x32x3x3xbf16>, tensor<4x64x24x24xbf16>) -> tensor<4x64x24x24xbf16> loc(#loc2)
    %30 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc3)
    %31 = "ttir.reshape"(%arg4, %30) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc3)
    %32 = ttir.empty() : tensor<64xbf16> loc(#loc3)
    %33 = "ttir.reshape"(%31, %32) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc3)
    %34 = ttir.empty() : tensor<1x64x1x1xbf16> loc(#loc2)
    %35 = "ttir.reshape"(%33, %34) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16> loc(#loc2)
    %36 = ttir.empty() : tensor<4x64x24x24xbf16> loc(#loc2)
    %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 4, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<4x64x24x24xbf16>) -> tensor<4x64x24x24xbf16> loc(#loc2)
    %38 = ttir.empty() : tensor<4x64x24x24xbf16> loc(#loc2)
    %39 = "ttir.add"(%29, %37, %38) : (tensor<4x64x24x24xbf16>, tensor<4x64x24x24xbf16>, tensor<4x64x24x24xbf16>) -> tensor<4x64x24x24xbf16> loc(#loc2)
    %40 = ttir.empty() : tensor<4x64x24x24xbf16> loc(#loc4)
    %41 = "ttir.maximum"(%39, %9, %40) : (tensor<4x64x24x24xbf16>, tensor<4x64x24x24xbf16>, tensor<4x64x24x24xbf16>) -> tensor<4x64x24x24xbf16> loc(#loc4)
    %42 = ttir.empty() : tensor<4x64x12x12xbf16> loc(#loc5)
    %43 = "ttir.pooling"(%41, %42) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<4x64x24x24xbf16>, tensor<4x64x12x12xbf16>) -> tensor<4x64x12x12xbf16> loc(#loc5)
    %44 = ttir.empty() : tensor<4x9216xbf16> loc(#loc3)
    %45 = "ttir.reshape"(%43, %44) <{shape = [4 : i32, 9216 : i32]}> : (tensor<4x64x12x12xbf16>, tensor<4x9216xbf16>) -> tensor<4x9216xbf16> loc(#loc3)
    %46 = ttir.empty() : tensor<1x128x9216xbf16> loc(#loc3)
    %47 = "ttir.reshape"(%arg3, %46) <{shape = [1 : i32, 128 : i32, 9216 : i32]}> : (tensor<128x9216xbf16>, tensor<1x128x9216xbf16>) -> tensor<1x128x9216xbf16> loc(#loc3)
    %48 = ttir.empty() : tensor<128x9216xbf16> loc(#loc3)
    %49 = "ttir.reshape"(%47, %48) <{shape = [128 : i32, 9216 : i32]}> : (tensor<1x128x9216xbf16>, tensor<128x9216xbf16>) -> tensor<128x9216xbf16> loc(#loc3)
    %50 = ttir.empty() : tensor<9216x128xbf16> loc(#loc6)
    %51 = "ttir.permute"(%49, %50) <{permutation = array<i64: 1, 0>}> : (tensor<128x9216xbf16>, tensor<9216x128xbf16>) -> tensor<9216x128xbf16> loc(#loc6)
    %52 = "ttir.dot_general"(%45, %51) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<4x9216xbf16>, tensor<9216x128xbf16>) -> tensor<4x128xbf16> loc(#loc7)
    %53 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc3)
    %54 = "ttir.reshape"(%arg2, %53) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3)
    %55 = ttir.empty() : tensor<128xbf16> loc(#loc3)
    %56 = "ttir.reshape"(%54, %55) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc3)
    %57 = ttir.empty() : tensor<1x128xbf16> loc(#loc8)
    %58 = "ttir.reshape"(%56, %57) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc8)
    %59 = ttir.empty() : tensor<4x128xbf16> loc(#loc8)
    %60 = "ttir.broadcast"(%58, %59) <{broadcast_dimensions = array<i64: 4, 1>}> : (tensor<1x128xbf16>, tensor<4x128xbf16>) -> tensor<4x128xbf16> loc(#loc8)
    %61 = ttir.empty() : tensor<4x128xbf16> loc(#loc8)
    %62 = "ttir.add"(%52, %60, %61) : (tensor<4x128xbf16>, tensor<4x128xbf16>, tensor<4x128xbf16>) -> tensor<4x128xbf16> loc(#loc8)
    %63 = ttir.empty() : tensor<4x128xbf16> loc(#loc4)
    %64 = "ttir.maximum"(%62, %5, %63) : (tensor<4x128xbf16>, tensor<4x128xbf16>, tensor<4x128xbf16>) -> tensor<4x128xbf16> loc(#loc4)
    %65 = ttir.empty() : tensor<1x10x128xbf16> loc(#loc3)
    %66 = "ttir.reshape"(%arg1, %65) <{shape = [1 : i32, 10 : i32, 128 : i32]}> : (tensor<10x128xbf16>, tensor<1x10x128xbf16>) -> tensor<1x10x128xbf16> loc(#loc3)
    %67 = ttir.empty() : tensor<10x128xbf16> loc(#loc3)
    %68 = "ttir.reshape"(%66, %67) <{shape = [10 : i32, 128 : i32]}> : (tensor<1x10x128xbf16>, tensor<10x128xbf16>) -> tensor<10x128xbf16> loc(#loc3)
    %69 = ttir.empty() : tensor<128x10xbf16> loc(#loc6)
    %70 = "ttir.permute"(%68, %69) <{permutation = array<i64: 1, 0>}> : (tensor<10x128xbf16>, tensor<128x10xbf16>) -> tensor<128x10xbf16> loc(#loc6)
    %71 = "ttir.dot_general"(%64, %70) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<4x128xbf16>, tensor<128x10xbf16>) -> tensor<4x10xbf16> loc(#loc7)
    %72 = ttir.empty() : tensor<1x1x10xbf16> loc(#loc3)
    %73 = "ttir.reshape"(%arg0, %72) <{shape = [1 : i32, 1 : i32, 10 : i32]}> : (tensor<10xbf16>, tensor<1x1x10xbf16>) -> tensor<1x1x10xbf16> loc(#loc3)
    %74 = ttir.empty() : tensor<10xbf16> loc(#loc3)
    %75 = "ttir.reshape"(%73, %74) <{shape = [10 : i32]}> : (tensor<1x1x10xbf16>, tensor<10xbf16>) -> tensor<10xbf16> loc(#loc3)
    %76 = ttir.empty() : tensor<1x10xbf16> loc(#loc8)
    %77 = "ttir.reshape"(%75, %76) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xbf16>, tensor<1x10xbf16>) -> tensor<1x10xbf16> loc(#loc8)
    %78 = ttir.empty() : tensor<4x10xbf16> loc(#loc8)
    %79 = "ttir.broadcast"(%77, %78) <{broadcast_dimensions = array<i64: 4, 1>}> : (tensor<1x10xbf16>, tensor<4x10xbf16>) -> tensor<4x10xbf16> loc(#loc8)
    %80 = ttir.empty() : tensor<4x10xbf16> loc(#loc8)
    %81 = "ttir.add"(%71, %79, %80) : (tensor<4x10xbf16>, tensor<4x10xbf16>, tensor<4x10xbf16>) -> tensor<4x10xbf16> loc(#loc8)
    %82 = ttir.empty() : tensor<4xbf16> loc(#loc9)
    %83 = "ttir.max"(%81, %82) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<4x10xbf16>, tensor<4xbf16>) -> tensor<4xbf16> loc(#loc9)
    %84 = ttir.empty() : tensor<4x1xbf16> loc(#loc10)
    %85 = "ttir.reshape"(%83, %84) <{shape = [4 : i32, 1 : i32]}> : (tensor<4xbf16>, tensor<4x1xbf16>) -> tensor<4x1xbf16> loc(#loc10)
    %86 = ttir.empty() : tensor<4x10xbf16> loc(#loc10)
    %87 = "ttir.broadcast"(%85, %86) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<4x1xbf16>, tensor<4x10xbf16>) -> tensor<4x10xbf16> loc(#loc10)
    %88 = ttir.empty() : tensor<4x10xbf16> loc(#loc10)
    %89 = "ttir.subtract"(%81, %87, %88) : (tensor<4x10xbf16>, tensor<4x10xbf16>, tensor<4x10xbf16>) -> tensor<4x10xbf16> loc(#loc10)
    %90 = ttir.empty() : tensor<4x10xbf16> loc(#loc11)
    %91 = "ttir.exp"(%89, %90) : (tensor<4x10xbf16>, tensor<4x10xbf16>) -> tensor<4x10xbf16> loc(#loc11)
    %92 = ttir.empty() : tensor<4xbf16> loc(#loc12)
    %93 = "ttir.sum"(%91, %92) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<4x10xbf16>, tensor<4xbf16>) -> tensor<4xbf16> loc(#loc12)
    %94 = ttir.empty() : tensor<4x1xbf16> loc(#loc12)
    %95 = "ttir.reshape"(%93, %94) <{shape = [4 : i32, 1 : i32]}> : (tensor<4xbf16>, tensor<4x1xbf16>) -> tensor<4x1xbf16> loc(#loc12)
    %96 = ttir.empty() : tensor<4x1xbf16> loc(#loc13)
    %97 = "ttir.log"(%95, %96) : (tensor<4x1xbf16>, tensor<4x1xbf16>) -> tensor<4x1xbf16> loc(#loc13)
    %98 = ttir.empty() : tensor<4xbf16> loc(#loc10)
    %99 = "ttir.reshape"(%97, %98) <{shape = [4 : i32]}> : (tensor<4x1xbf16>, tensor<4xbf16>) -> tensor<4xbf16> loc(#loc10)
    %100 = ttir.empty() : tensor<4x1xbf16> loc(#loc10)
    %101 = "ttir.reshape"(%99, %100) <{shape = [4 : i32, 1 : i32]}> : (tensor<4xbf16>, tensor<4x1xbf16>) -> tensor<4x1xbf16> loc(#loc10)
    %102 = ttir.empty() : tensor<4x10xbf16> loc(#loc10)
    %103 = "ttir.broadcast"(%101, %102) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<4x1xbf16>, tensor<4x10xbf16>) -> tensor<4x10xbf16> loc(#loc10)
    %104 = ttir.empty() : tensor<4x10xbf16> loc(#loc10)
    %105 = "ttir.subtract"(%89, %103, %104) : (tensor<4x10xbf16>, tensor<4x10xbf16>, tensor<4x10xbf16>) -> tensor<4x10xbf16> loc(#loc10)
    return %105 : tensor<4x10xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("aten__convolution_overrideable")
#loc3 = loc("aten__view")
#loc4 = loc("aten__relu")
#loc5 = loc("aten__max_pool2d")
#loc6 = loc("aten__permute")
#loc7 = loc("aten__mm")
#loc8 = loc("aten__add")
#loc9 = loc("aten__amax")
#loc10 = loc("aten__sub")
#loc11 = loc("aten__exp")
#loc12 = loc("aten__sum")
#loc13 = loc("aten__log")
------------------ END OF MLIR MODULE ------------------
2025-10-15 15:35:30.496 (  15.766s) [        90B6F000]      module_builder.cc:769   WARN| `mhlo.num_partitions` attribute not found, assuming default number of partitions: 1
2025-10-15 15:35:30.496 (  15.766s) [        90B6F000]      module_builder.cc:783   WARN| `mhlo.num_replicas` attribute not found, assuming default number of replicas: 1
2025-10-15 15:35:30.496 (  15.766s) [        90B6F000]      module_builder.cc:795   WARN| Num replicas and num partitions are not set, inferring the number of devices from mesh shape
2025-10-15 15:35:30.497 (  15.767s) [        90B6F000]     client_instance.cc:383      1| ClientInstance::getOrCreateMeshDevice - reshaping mesh device - [1, 2] -> [1, 1]
2025-10-15 15:35:30.497 (  15.767s) [        90B6F000]     client_instance.cc:403      1| Closing parent mesh device in getOrCreateMeshDevice before opening a new one
2025-10-15 15:35:30.546 (  15.815s) [        90B6F000]     client_instance.cc:406      1| Opening new parent mesh device in getOrCreateMeshDevice for reshape
2025-10-15 15:35:30.546 (  15.815s) [        90B6F000]     client_instance.cc:419      1| [JAMES] Opening mesh device
2025-10-15 15:35:32.249 (  17.518s) [        90B6F000]     client_instance.cc:479      1| ClientInstance::getOrCreateOptimizerSubmesh - creating optimizer submesh
2025-10-15 15:35:32.282 (  17.552s) [        90B6F000]      module_builder.cc:859      1| TTNN Module:
#dram = #ttnn.buffer_type<dram>
#loc = loc(unknown)
#loc4 = loc("xla__device_data")
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073177056, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073185472, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 + d1 + d2, d3), <1x1>, memref<1x64xbf16, #system_memory>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 + d1 + d2, d3), <1x1>, memref<1x64xbf16, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 + d1 + d2, d3), <1x1>, memref<1x32xbf16, #system_memory>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 + d1 + d2, d3), <1x1>, memref<1x32xbf16, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x288x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 96 + d1 * 3 + d2, d3), <1x1>, memref<6144x3xbf16, #system_memory>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3 + d1 * 3 + d2, d3), <1x1>, memref<96x3xbf16, #system_memory>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 896 + d1 * 32 + d2, d3), <1x1>, memref<112x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3136 + d1 * 3136 + d2, d3), <1x1>, memref<98x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3136 + d1 * 3136 + d2, d3), <1x1>, memref<3136x1xbf16, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2720 + d1 * 2720 + d2, d3), <1x1>, memref<85x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2704 + d1 * 2704 + d2, d3), <1x1>, memref<2704x32xbf16, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2304 + d1 * 2304 + d2, d3), <1x1>, memref<72x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2304 + d1 * 2304 + d2, d3), <1x1>, memref<2304x64xbf16, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<576x64xbf16, #dram>, <interleaved>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<48x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<256x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
module @SyncTensorsGraph.162 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.162 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x1, chipIds = [0]> loc(#loc)
      func.func @main_const_eval_0() -> (tensor<1x1xbf16, #ttnn_layout>, tensor<1x1x1x1xbf16, #ttnn_layout1>, tensor<1x1x1x1xbf16, #ttnn_layout1>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout2> loc(#loc)
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout2>) -> tensor<1x1xbf16, #ttnn_layout> loc(#loc)
        %3 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout2>) -> tensor<1x1x1x1xbf16, #ttnn_layout1> loc(#loc)
        %4 = "ttnn.permute"(%3) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x1x1xbf16, #ttnn_layout1>) -> tensor<1x1x1x1xbf16, #ttnn_layout1> loc(#loc20)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout1>) -> () loc(#loc20)
        %5 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout2>) -> tensor<1x1x1x1xbf16, #ttnn_layout1> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<bf16, #ttnn_layout2>) -> () loc(#loc)
        %6 = "ttnn.permute"(%5) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x1x1xbf16, #ttnn_layout1>) -> tensor<1x1x1x1xbf16, #ttnn_layout1> loc(#loc21)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout1>) -> () loc(#loc21)
        return %2, %4, %6 : tensor<1x1xbf16, #ttnn_layout>, tensor<1x1x1x1xbf16, #ttnn_layout1>, tensor<1x1x1x1xbf16, #ttnn_layout1> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_1(%arg0: tensor<10xbf16, #ttnn_layout3> loc(unknown)) -> tensor<4x10xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xbf16, #ttnn_layout3>) -> tensor<1x10xbf16, #ttnn_layout> loc(#loc3)
        %1 = "ttnn.repeat"(%0) <{repeat_dims = #ttnn.shape<4x1>}> : (tensor<1x10xbf16, #ttnn_layout>) -> tensor<4x10xbf16, #ttnn_layout> loc(#loc3)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x10xbf16, #ttnn_layout>) -> () loc(#loc3)
        return %1 : tensor<4x10xbf16, #ttnn_layout> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_2(%arg0: tensor<128xbf16, #ttnn_layout4> loc(unknown)) -> tensor<4x128xbf16, #ttnn_layout5> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128xbf16, #ttnn_layout5> loc(#loc3)
        %1 = "ttnn.repeat"(%0) <{repeat_dims = #ttnn.shape<4x1>}> : (tensor<1x128xbf16, #ttnn_layout5>) -> tensor<4x128xbf16, #ttnn_layout5> loc(#loc3)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x128xbf16, #ttnn_layout5>) -> () loc(#loc3)
        return %1 : tensor<4x128xbf16, #ttnn_layout5> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_3(%arg0: tensor<64xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x1x1x64xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16, #ttnn_layout6>) -> tensor<1x1x1x64xbf16, #ttnn_layout8> loc(#loc14)
        %1 = "ttnn.to_layout"(%0) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x64xbf16, #ttnn_layout8>) -> tensor<1x1x1x64xbf16, #ttnn_layout9> loc(#loc14)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout8>) -> () loc(#loc14)
        %2 = "ttnn.from_device"(%1) : (tensor<1x1x1x64xbf16, #ttnn_layout9>) -> tensor<1x1x1x64xbf16, #ttnn_layout7> loc(#loc14)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout9>) -> () loc(#loc14)
        return %2 : tensor<1x1x1x64xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_4(%arg0: tensor<32xbf16, #ttnn_layout3> loc(unknown)) -> tensor<1x1x1x32xbf16, #ttnn_layout10> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 1 : i32, 32 : i32]}> : (tensor<32xbf16, #ttnn_layout3>) -> tensor<1x1x1x32xbf16, #ttnn_layout1> loc(#loc14)
        %1 = "ttnn.to_layout"(%0) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x32xbf16, #ttnn_layout1>) -> tensor<1x1x1x32xbf16, #ttnn_layout11> loc(#loc14)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x1x32xbf16, #ttnn_layout1>) -> () loc(#loc14)
        %2 = "ttnn.from_device"(%1) : (tensor<1x1x1x32xbf16, #ttnn_layout11>) -> tensor<1x1x1x32xbf16, #ttnn_layout10> loc(#loc14)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x1x1x32xbf16, #ttnn_layout11>) -> () loc(#loc14)
        return %2 : tensor<1x1x1x32xbf16, #ttnn_layout10> loc(#loc)
      } loc(#loc)
      func.func @main(%arg0: tensor<10xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___fc2_bias"} loc("xla__device_data"), %arg1: tensor<10x128xbf16, #ttnn_layout5> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___fc2_weight"} loc("xla__device_data"), %arg2: tensor<128xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___fc1_bias"} loc("xla__device_data"), %arg3: tensor<128x9216xbf16, #ttnn_layout12> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___fc1_weight"} loc("xla__device_data"), %arg4: tensor<64xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___conv2_bias"} loc("xla__device_data"), %arg5: tensor<64x32x3x3xbf16, #ttnn_layout13> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___conv2_weight"} loc("xla__device_data"), %arg6: tensor<32xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___conv1_bias"} loc("xla__device_data"), %arg7: tensor<32x1x3x3xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___conv1_weight"} loc("xla__device_data"), %arg8: tensor<4x1x28x28xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_0"} loc("xla__device_data")) -> (tensor<4x10xbf16, #ttnn_layout> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0:3 = ttcore.load_cached(@main_const_eval_0, []) : () -> (tensor<1x1xbf16, #ttnn_layout>, tensor<1x1x1x1xbf16, #ttnn_layout1>, tensor<1x1x1x1xbf16, #ttnn_layout1>) loc(#loc)
        %1 = ttcore.load_cached(@main_const_eval_1, [%arg0]) : (tensor<10xbf16, #ttnn_layout3>) -> tensor<4x10xbf16, #ttnn_layout> loc(#loc)
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<10xbf16, #ttnn_layout3>) -> () loc(#loc)
        %2 = ttcore.load_cached(@main_const_eval_2, [%arg2]) : (tensor<128xbf16, #ttnn_layout4>) -> tensor<4x128xbf16, #ttnn_layout5> loc(#loc)
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<128xbf16, #ttnn_layout4>) -> () loc(#loc)
        %3 = ttcore.load_cached(@main_const_eval_3, [%arg4]) : (tensor<64xbf16, #ttnn_layout6>) -> tensor<1x1x1x64xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<64xbf16, #ttnn_layout6>) -> () loc(#loc)
        %4 = ttcore.load_cached(@main_const_eval_4, [%arg6]) : (tensor<32xbf16, #ttnn_layout3>) -> tensor<1x1x1x32xbf16, #ttnn_layout10> loc(#loc)
        "ttnn.deallocate"(%arg6) <{force = false}> : (tensor<32xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %6 = "ttnn.permute"(%arg8) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<4x1x28x28xbf16, #ttnn_layout15>) -> tensor<4x28x28x1xbf16, #ttnn_layout16> loc(#loc13)
        "ttnn.deallocate"(%arg8) <{force = false}> : (tensor<4x1x28x28xbf16, #ttnn_layout15>) -> () loc(#loc13)
        %7 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 3136 : i32, 1 : i32]}> : (tensor<4x28x28x1xbf16, #ttnn_layout16>) -> tensor<1x1x3136x1xbf16, #ttnn_layout17> loc(#loc22)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<4x28x28x1xbf16, #ttnn_layout16>) -> () loc(#loc22)
        %8 = "ttnn.to_layout"(%7) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x3136x1xbf16, #ttnn_layout17>) -> tensor<1x1x3136x1xbf16, #ttnn_layout18> loc(#loc15)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x3136x1xbf16, #ttnn_layout17>) -> () loc(#loc15)
        %9 = "ttnn.conv2d"(%8, %arg7, %4, %5) <{batch_size = 4 : i32, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 1 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 32 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x3136x1xbf16, #ttnn_layout18>, tensor<32x1x3x3xbf16, #ttnn_layout14>, tensor<1x1x1x32xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<1x1x2704x32xbf16, #ttnn_layout19> loc(#loc2)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x1x3136x1xbf16, #ttnn_layout18>) -> () loc(#loc2)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x32xbf16, #ttnn_layout10>) -> () loc(#loc2)
        "ttnn.deallocate"(%arg7) <{force = false}> : (tensor<32x1x3x3xbf16, #ttnn_layout14>) -> () loc(#loc2)
        %10 = "ttnn.maximum"(%9, %0#2) : (tensor<1x1x2704x32xbf16, #ttnn_layout19>, tensor<1x1x1x1xbf16, #ttnn_layout1>) -> tensor<1x1x2704x32xbf16, #ttnn_layout19> loc(#loc5)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x2704x32xbf16, #ttnn_layout19>) -> () loc(#loc5)
        "ttnn.deallocate"(%0#2) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout1>) -> () loc(#loc5)
        %11 = "ttnn.to_layout"(%10) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x2704x32xbf16, #ttnn_layout19>) -> tensor<1x1x2704x32xbf16, #ttnn_layout20> loc(#loc15)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x1x2704x32xbf16, #ttnn_layout19>) -> () loc(#loc15)
        %12 = "ttnn.conv2d"(%11, %arg5, %3, %5) <{batch_size = 4 : i32, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 32 : i32, input_height = 26 : i32, input_width = 26 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x2704x32xbf16, #ttnn_layout20>, tensor<64x32x3x3xbf16, #ttnn_layout13>, tensor<1x1x1x64xbf16, #ttnn_layout7>, !ttnn.device) -> tensor<1x1x2304x64xbf16, #ttnn_layout21> loc(#loc2)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x2704x32xbf16, #ttnn_layout20>) -> () loc(#loc2)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout7>) -> () loc(#loc2)
        "ttnn.deallocate"(%arg5) <{force = false}> : (tensor<64x32x3x3xbf16, #ttnn_layout13>) -> () loc(#loc2)
        %13 = "ttnn.maximum"(%12, %0#1) : (tensor<1x1x2304x64xbf16, #ttnn_layout21>, tensor<1x1x1x1xbf16, #ttnn_layout1>) -> tensor<1x1x2304x64xbf16, #ttnn_layout21> loc(#loc5)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x2304x64xbf16, #ttnn_layout21>) -> () loc(#loc5)
        "ttnn.deallocate"(%0#1) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout1>) -> () loc(#loc5)
        %14 = "ttnn.to_layout"(%13) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x2304x64xbf16, #ttnn_layout21>) -> tensor<1x1x2304x64xbf16, #ttnn_layout22> loc(#loc16)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x2304x64xbf16, #ttnn_layout21>) -> () loc(#loc16)
        %15 = "ttnn.max_pool2d"(%14) <{batch_size = 4 : si32, ceil_mode = false, channels = 64 : si32, dilation = array<i32: 1, 1>, in_place_halo = false, input_height = 24 : si32, input_width = 24 : si32, kernel_size = array<i32: 2, 2>, padding = array<i32: 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x2304x64xbf16, #ttnn_layout22>) -> tensor<1x1x576x64xbf16, #ttnn_layout23> loc(#loc1)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x2304x64xbf16, #ttnn_layout22>) -> () loc(#loc1)
        %16 = "ttnn.to_layout"(%15) <{layout = #ttnn.layout<tile>}> : (tensor<1x1x576x64xbf16, #ttnn_layout23>) -> tensor<1x1x576x64xbf16, #ttnn_layout24> loc(#loc16)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout23>) -> () loc(#loc16)
        %17 = "ttnn.reshape"(%16) <{shape = [4 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x576x64xbf16, #ttnn_layout24>) -> tensor<4x12x12x64xbf16, #ttnn_layout25> loc(#loc17)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout24>) -> () loc(#loc17)
        %18 = "ttnn.permute"(%17) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<4x12x12x64xbf16, #ttnn_layout25>) -> tensor<4x64x12x12xbf16, #ttnn_layout26> loc(#loc18)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<4x12x12x64xbf16, #ttnn_layout25>) -> () loc(#loc18)
        %19 = "ttnn.reshape"(%18) <{shape = [4 : i32, 9216 : i32]}> : (tensor<4x64x12x12xbf16, #ttnn_layout26>) -> tensor<4x9216xbf16, #ttnn_layout27> loc(#loc6)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<4x64x12x12xbf16, #ttnn_layout26>) -> () loc(#loc6)
        %20 = "ttnn.linear"(%19, %arg3, %2) <{transpose_a = false, transpose_b = true}> : (tensor<4x9216xbf16, #ttnn_layout27>, tensor<128x9216xbf16, #ttnn_layout12>, tensor<4x128xbf16, #ttnn_layout5>) -> tensor<4x128xbf16, #ttnn_layout5> loc(#loc3)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<4x9216xbf16, #ttnn_layout27>) -> () loc(#loc3)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<4x128xbf16, #ttnn_layout5>) -> () loc(#loc3)
        "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<128x9216xbf16, #ttnn_layout12>) -> () loc(#loc3)
        %21 = "ttnn.maximum"(%20, %0#0) : (tensor<4x128xbf16, #ttnn_layout5>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<4x128xbf16, #ttnn_layout5> loc(#loc5)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<4x128xbf16, #ttnn_layout5>) -> () loc(#loc5)
        "ttnn.deallocate"(%0#0) <{force = false}> : (tensor<1x1xbf16, #ttnn_layout>) -> () loc(#loc5)
        %22 = "ttnn.linear"(%21, %arg1, %1) <{transpose_a = false, transpose_b = true}> : (tensor<4x128xbf16, #ttnn_layout5>, tensor<10x128xbf16, #ttnn_layout5>, tensor<4x10xbf16, #ttnn_layout>) -> tensor<4x10xbf16, #ttnn_layout> loc(#loc3)
        "ttnn.deallocate"(%21) <{force = false}> : (tensor<4x128xbf16, #ttnn_layout5>) -> () loc(#loc3)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<4x10xbf16, #ttnn_layout>) -> () loc(#loc3)
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<10x128xbf16, #ttnn_layout5>) -> () loc(#loc3)
        %23 = "ttnn.max"(%22) <{dim_arg = [1 : i32], keep_dim = true}> : (tensor<4x10xbf16, #ttnn_layout>) -> tensor<4x1xbf16, #ttnn_layout> loc(#loc7)
        %24 = "ttnn.neg"(%23) : (tensor<4x1xbf16, #ttnn_layout>) -> tensor<4x1xbf16, #ttnn_layout> loc(#loc19)
        "ttnn.deallocate"(%23) <{force = false}> : (tensor<4x1xbf16, #ttnn_layout>) -> () loc(#loc19)
        %25 = "ttnn.add"(%22, %24) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<4x10xbf16, #ttnn_layout>, tensor<4x1xbf16, #ttnn_layout>) -> tensor<4x10xbf16, #ttnn_layout> loc(#loc8)
        "ttnn.deallocate"(%24) <{force = false}> : (tensor<4x1xbf16, #ttnn_layout>) -> () loc(#loc8)
        "ttnn.deallocate"(%22) <{force = false}> : (tensor<4x10xbf16, #ttnn_layout>) -> () loc(#loc8)
        %26 = "ttnn.exp"(%25) : (tensor<4x10xbf16, #ttnn_layout>) -> tensor<4x10xbf16, #ttnn_layout> loc(#loc9)
        %27 = "ttnn.sum"(%26) <{dim_arg = [1 : i32], keep_dim = true}> : (tensor<4x10xbf16, #ttnn_layout>) -> tensor<4x1xbf16, #ttnn_layout> loc(#loc10)
        "ttnn.deallocate"(%26) <{force = false}> : (tensor<4x10xbf16, #ttnn_layout>) -> () loc(#loc10)
        %28 = "ttnn.log"(%27) : (tensor<4x1xbf16, #ttnn_layout>) -> tensor<4x1xbf16, #ttnn_layout> loc(#loc11)
        "ttnn.deallocate"(%27) <{force = false}> : (tensor<4x1xbf16, #ttnn_layout>) -> () loc(#loc11)
        %29 = "ttnn.neg"(%28) : (tensor<4x1xbf16, #ttnn_layout>) -> tensor<4x1xbf16, #ttnn_layout> loc(#loc19)
        "ttnn.deallocate"(%28) <{force = false}> : (tensor<4x1xbf16, #ttnn_layout>) -> () loc(#loc19)
        %30 = "ttnn.add"(%25, %29) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<4x10xbf16, #ttnn_layout>, tensor<4x1xbf16, #ttnn_layout>) -> tensor<4x10xbf16, #ttnn_layout> loc(#loc8)
        "ttnn.deallocate"(%29) <{force = false}> : (tensor<4x1xbf16, #ttnn_layout>) -> () loc(#loc8)
        "ttnn.deallocate"(%25) <{force = false}> : (tensor<4x10xbf16, #ttnn_layout>) -> () loc(#loc8)
        return %30 : tensor<4x10xbf16, #ttnn_layout> loc(#loc)
      } loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc1 = loc("aten__max_pool2d")
#loc2 = loc("aten__convolution_overrideable")
#loc3 = loc("aten__add")
#loc5 = loc("aten__relu")
#loc6 = loc("aten__view")
#loc7 = loc("aten__amax")
#loc8 = loc("aten__sub")
#loc9 = loc("aten__exp")
#loc10 = loc("aten__sum")
#loc11 = loc("aten__log")
#loc12 = loc("aten__max_pool2d_permuteInput"(#loc1))
#loc13 = loc("aten__convolution_overrideable_input"(#loc2))
#loc14 = loc("aten__convolution_overrideable_bias"(#loc2))
#loc15 = loc("aten__convolution_overrideable_workaround"(#loc2))
#loc16 = loc("aten__max_pool2d_workaround"(#loc1))
#loc17 = loc("aten__max_pool2d_reshape"(#loc1))
#loc18 = loc("aten__max_pool2d_permuteOutput"(#loc1))
#loc19 = loc("aten__sub_neg"(#loc8))
#loc20 = loc("aten__max_pool2d_permuteInput_tm1"(#loc12))
#loc21 = loc("aten__convolution_overrideable_input_tm1"(#loc13))
#loc22 = loc("aten__convolution_overrideable_input_reshape"(#loc13))
------------------ END OF MLIR MODULE ------------------
2025-10-15 15:35:32.297 (  17.566s) [        90B6F000]loaded_executable_insta:69       1| LoadedExecutableInstance::PJRT_LoadedExecutable_GetExecutable
2025-10-15 15:35:32.297 (  17.567s) [        90B6F000]loaded_executable_insta:88       1| LoadedExecutableInstance::PJRT_LoadedExecutable_AddressableDevices
2025-10-15 15:35:32.298 (  17.567s) [        90B6F000]              stubs.inc:70    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2025-10-15 15:35:32.298 (  17.567s) [        90B6F000]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-10-15 15:35:32.298 (  17.567s) [        90B6F000]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-10-15 15:35:32.298 (  17.567s) [        90B6F000]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-10-15 15:35:32.298 (  17.567s) [        90B6F000] executable_instance.cc:107      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-10-15 15:35:32.298 (  17.567s) [        90B6F000] executable_instance.cc:107      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-10-15 15:35:32.305 (  17.574s) [        90B6F000] executable_instance.cc:107      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-10-15 15:35:32.305 (  17.574s) [        90B6F000] executable_instance.cc:107      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-10-15 15:35:32.315 (  17.584s) [        62FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-15 15:35:32.315 (  17.584s) [        62FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-15 15:35:32.315 (  17.584s) [        62FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-15 15:35:32.315 (  17.584s) [        62FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-15 15:35:32.315 (  17.584s) [        62FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-15 15:35:32.315 (  17.584s) [        62FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-15 15:35:32.315 (  17.584s) [        62FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-15 15:35:32.315 (  17.584s) [        62FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-15 15:35:32.315 (  17.584s) [        62FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-15 15:35:32.315 (  17.584s) [        62FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-15 15:35:32.315 (  17.584s) [        62FFD640] executable_instance.cc:139      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-10-15 15:35:32.315 (  17.584s) [        62FFD640]loaded_executable_insta:125      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-10-15 15:35:32.315 (  17.584s) [        62FFD640]flatbuffer_loaded_execu:411      1| FlatbufferLoadedExecutableInstance::Execute
2025-10-15 15:35:32.315 (  17.585s) [        62FFD640]     client_instance.cc:376      1| ClientInstance::getOrCreateMeshDevice - reusing already opened mesh device [1, 1]
2025-10-15 15:36:08.275 (  53.544s) [        62FFD640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-15 15:36:08.275 (  53.544s) [        62FFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-15 15:36:08.275 (  53.544s) [        62FFD640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-15 15:36:08.275 (  53.544s) [        62FFD640]     buffer_instance.cc:440      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-10-15 15:36:08.275 (  53.544s) [        62FFD640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-15 15:36:08.275 (  53.544s) [        62FFD640]     buffer_instance.cc:409      1| BufferInstance::PJRT_Buffer_ElementType
2025-10-15 15:36:08.275 (  53.544s) [        90B6F000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-15 15:36:08.275 (  53.544s) [        90B6F000]     buffer_instance.cc:428      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-10-15 15:36:08.275 (  53.544s) [        90B6F000]     buffer_instance.cc:409      1| BufferInstance::PJRT_Buffer_ElementType
2025-10-15 15:36:08.275 (  53.545s) [        90B6F000]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-15 15:36:08.275 (  53.545s) [        90B6F000]     buffer_instance.cc:450      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-10-15 15:36:08.275 (  53.545s) [        90B6F000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-15 15:36:08.276 (  53.545s) [        4EFFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-15 15:36:08.277 (  53.547s) [        90B6F000]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-15 15:36:08.278 (  53.547s) [        90B6F000]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
Using TT-Metal from the source tree: /localdev/jameszianxu/clean/tt-xla/third_party/tt-mlir/src/tt-mlir/third_party/tt-metal/src/tt-metal
WARNING: TT plugin is setting XLA_STABLEHLO_COMPILE to 1. This is required for TT PJRT plugin to work correctly.
tensor([[-2.3750, -2.2969, -2.3750, -2.2812, -2.3125, -2.2031, -2.2969, -2.2344,
         -2.3281, -2.2500],
        [-2.3750, -2.2969, -2.3750, -2.2812, -2.3125, -2.2031, -2.2969, -2.2344,
         -2.3281, -2.2500],
        [-2.3750, -2.2969, -2.3750, -2.2812, -2.3125, -2.2031, -2.2969, -2.2344,
         -2.3281, -2.2500],
        [-2.3750, -2.2969, -2.3750, -2.2812, -2.3125, -2.2031, -2.2969, -2.2344,
         -2.3281, -2.2500]], device='xla:0', dtype=torch.bfloat16)
2025-10-15 15:36:08.279 (  53.548s) [        90B6F000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-15 15:36:08.279 (  53.548s) [        90B6F000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-15 15:36:08.279 (  53.548s) [        90B6F000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-15 15:36:08.279 (  53.548s) [        90B6F000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-15 15:36:08.523 (  53.792s) [        90B6F000]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-15 15:36:08.523 (  53.792s) [        90B6F000]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-15 15:36:08.523 (  53.793s) [        90B6F000]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-15 15:36:08.523 (  53.793s) [        90B6F000]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-15 15:36:08.524 (  53.793s) [        90B6F000]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-15 15:36:08.524 (  53.793s) [        90B6F000]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-15 15:36:08.524 (  53.794s) [        90B6F000]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-15 15:36:08.524 (  53.794s) [        90B6F000]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
