WARNING:root:Defaulting to PJRT_DEVICE=CPU
Read unexpected run_mailbox value: 0x40 (expected 0x80 or 0x0)
2025-09-04 19:32:36.216 | critical |          Always | Read unexpected run_mailbox value from core (x=25,y=17) (assert.hpp:107)
Read unexpected run_mailbox value: 0x40 (expected 0x80 or 0x0)
2025-09-04 19:32:36.258 | critical |          Always | Read unexpected run_mailbox value from core (x=25,y=16) (assert.hpp:107)
2025-09-04 19:32:39.197841: W torch_xla/csrc/runtime/profiler.cc:88] Profiler API not found for PJRT plugin
Using TT-Metal from the source tree: /localdev/jameszianxu/tt-xla/third_party/tt-mlir/src/tt-mlir/third_party/tt-metal/src/tt-metal
WARNING: TT plugin is setting XLA_STABLEHLO_COMPILE to 1. This is required for TT PJRT plugin to work correctly.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 52.30it/s]
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
module @SyncTensorsGraph.456 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, %arg1: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg2: !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>, %arg4: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg5: !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>, %arg6: !vhlo.tensor_v1<1x7x!vhlo.i64_v1>, %arg7: !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, %arg8: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg9: !vhlo.tensor_v1<7x!vhlo.i64_v1>, %arg10: !vhlo.tensor_v1<!vhlo.i64_v1>, %arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg13: !vhlo.tensor_v1<64x!vhlo.f32_v1>, %arg14: !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>, %arg15: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg16: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg17: !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>, %arg18: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<7x1024xbf16>>}> : () -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1> : tensor<7x1024xi64>>}> : () -> !vhlo.tensor_v1<7x1024x!vhlo.i64_v1>
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0> : tensor<7xi64>>}> : () -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %3 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<1x8x1024x128xbf16>>}> : () -> !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>
    %4 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<3.25520843E-4> : tensor<1x7xf32>>}> : () -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %5 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2.000000e+00> : tensor<1x7x3072xf32>>}> : () -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %6 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF800000> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %7 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>>}> : () -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %8 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<"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tensor<1024xi64>>}> : () -> !vhlo.tensor_v1<1024x!vhlo.i64_v1>
    %9 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %10 = "vhlo.reshape_v1"(%arg18) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %11 = "vhlo.custom_call_v1"(%10) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_norm_weight">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %12 = "vhlo.reshape_v1"(%11) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1>
    %13 = "vhlo.convert_v1"(%12) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %14 = "vhlo.broadcast_in_dim_v1"(%13) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %15 = "vhlo.reshape_v1"(%arg7) : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>
    %16 = "vhlo.custom_call_v1"(%15) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_embed_tokens_weight">}>} : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>
    %17 = "vhlo.reshape_v1"(%16) : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>
    %18 = "vhlo.reshape_v1"(%arg6) : (!vhlo.tensor_v1<1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>
    %19 = "vhlo.custom_call_v1"(%18) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_0">}>} : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>
    %20 = "vhlo.reshape_v1"(%19) : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %21 = "vhlo.convert_v1"(%20) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.ui32_v1>
    %22 = "vhlo.gather_v2"(%17, %21) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 3072]> : tensor<2xi64>>, start_index_map = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x!vhlo.ui32_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %23 = "vhlo.reshape_v1"(%22) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %24 = "vhlo.reshape_v1"(%arg8) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %25 = "vhlo.custom_call_v1"(%24) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___input_layernorm_weight">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %26 = "vhlo.reshape_v1"(%25) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1>
    %27 = "vhlo.convert_v1"(%26) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %28 = "vhlo.broadcast_in_dim_v1"(%27) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %29 = "vhlo.convert_v1"(%23) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %30 = "vhlo.power_v1"(%29, %5) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %31 = "vhlo.reduce_v1"(%30, %9) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg19: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg20: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %240 = "vhlo.add_v1"(%arg19, %arg20) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%240) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %32 = "vhlo.multiply_v1"(%31, %4) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %33 = "vhlo.reshape_v1"(%32) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %34 = "vhlo.broadcast_in_dim_v1"(%arg1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %35 = "vhlo.add_v1"(%33, %34) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %36 = "vhlo.rsqrt_v1"(%35) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %37 = "vhlo.reshape_v1"(%36) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %38 = "vhlo.broadcast_in_dim_v1"(%37) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %39 = "vhlo.multiply_v1"(%29, %38) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %40 = "vhlo.convert_v1"(%39) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %41 = "vhlo.convert_v1"(%40) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %42 = "vhlo.multiply_v1"(%28, %41) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %43 = "vhlo.convert_v1"(%42) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %44 = "vhlo.reshape_v1"(%43) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %45 = "vhlo.reshape_v1"(%arg15) : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>
    %46 = "vhlo.custom_call_v1"(%45) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>
    %47 = "vhlo.reshape_v1"(%46) : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %48 = "vhlo.transpose_v1"(%47) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,3072]{0,1}">} : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %49 = "vhlo.dot_general_v2"(%44, %48) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %50 = "vhlo.reshape_v1"(%49) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>
    %51 = "vhlo.transpose_v1"(%50) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,24,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %52 = "vhlo.convert_v1"(%51) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,24,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %53 = "vhlo.reshape_v1"(%arg13) : (!vhlo.tensor_v1<64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>
    %54 = "vhlo.custom_call_v1"(%53) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"constant">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_rotary_emb_inv_freq">}>} : (!vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>
    %55 = "vhlo.reshape_v1"(%54) : (!vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>
    %56 = "vhlo.reshape_v1"(%arg9) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>
    %57 = "vhlo.custom_call_v1"(%56) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_1">}>} : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>
    %58 = "vhlo.reshape_v1"(%57) : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %59 = "vhlo.convert_v1"(%57) : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.f32_v1>
    %60 = "vhlo.dot_general_v2"(%55, %59) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>
    %61 = "vhlo.transpose_v1"(%60) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1]> : tensor<3xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[1, 2, 0]> : tensor<3xindex>>, xla_shape = #vhlo.string_v1<"f32[1,7,64]{1,2,0}">} : (!vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>
    %62 = "vhlo.concatenate_v1"(%61, %61) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %63 = "vhlo.cosine_v1"(%62) : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %64 = "vhlo.convert_v1"(%63) : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>
    %65 = "vhlo.reshape_v1"(%64) : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>
    %66 = "vhlo.convert_v1"(%65) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>
    %67 = "vhlo.reshape_v1"(%66) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %68 = "vhlo.broadcast_in_dim_v1"(%67) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %69 = "vhlo.multiply_v1"(%52, %68) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %70 = "vhlo.convert_v1"(%69) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %71 = "vhlo.slice_v1"(%51) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 7, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>
    %72 = "vhlo.negate_v1"(%71) : (!vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>
    %73 = "vhlo.slice_v1"(%51) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 7, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>
    %74 = "vhlo.concatenate_v1"(%72, %73) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %75 = "vhlo.convert_v1"(%74) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %76 = "vhlo.sine_v1"(%62) : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %77 = "vhlo.convert_v1"(%76) : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>
    %78 = "vhlo.reshape_v1"(%77) : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>
    %79 = "vhlo.convert_v1"(%78) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>
    %80 = "vhlo.reshape_v1"(%79) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %81 = "vhlo.broadcast_in_dim_v1"(%80) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %82 = "vhlo.multiply_v1"(%75, %81) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %83 = "vhlo.convert_v1"(%82) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %84 = "vhlo.add_v1"(%70, %83) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %85 = "vhlo.reshape_v1"(%84) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>
    %86 = "vhlo.custom_call_v1"(%3) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"constant">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"kwargs____past_key_values___key_cache_0">}>} : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>
    %87 = "vhlo.compare_v1"(%58, %2) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 LT>}> : (!vhlo.tensor_v1<7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.bool_v1>
    %88 = "vhlo.broadcast_in_dim_v1"(%arg10) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %89 = "vhlo.add_v1"(%58, %88) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %90 = "vhlo.select_v1"(%87, %89, %58) : (!vhlo.tensor_v1<7x!vhlo.bool_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %91 = "vhlo.reshape_v1"(%90) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x1x!vhlo.i64_v1>
    %92 = "vhlo.reshape_v1"(%arg14) : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>
    %93 = "vhlo.custom_call_v1"(%92) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>
    %94 = "vhlo.reshape_v1"(%93) : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>
    %95 = "vhlo.transpose_v1"(%94) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,1024]{0,1}">} : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>
    %96 = "vhlo.dot_general_v2"(%44, %95) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>
    %97 = "vhlo.reshape_v1"(%96) : (!vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>
    %98 = "vhlo.transpose_v1"(%97) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %99 = "vhlo.convert_v1"(%98) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %100 = "vhlo.broadcast_in_dim_v1"(%67) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %101 = "vhlo.multiply_v1"(%99, %100) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %102 = "vhlo.convert_v1"(%101) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %103 = "vhlo.slice_v1"(%98) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 7, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %104 = "vhlo.negate_v1"(%103) : (!vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %105 = "vhlo.slice_v1"(%98) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 7, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %106 = "vhlo.concatenate_v1"(%104, %105) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %107 = "vhlo.convert_v1"(%106) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %108 = "vhlo.broadcast_in_dim_v1"(%80) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %109 = "vhlo.multiply_v1"(%107, %108) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %110 = "vhlo.convert_v1"(%109) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %111 = "vhlo.add_v1"(%102, %110) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %112 = "vhlo.scatter_v2"(%86, %91, %111) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg19: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg20: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg20) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>
    %113 = "vhlo.broadcast_in_dim_v1"(%112) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x1024x128x!vhlo.bf16_v1>
    %114 = "vhlo.reshape_v1"(%113) : (!vhlo.tensor_v1<1x8x3x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1024x128x!vhlo.bf16_v1>
    %115 = "vhlo.transpose_v1"(%114) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 3, 1, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,24,128,1024]{2,3,1,0}">} : (!vhlo.tensor_v1<1x24x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x128x1024x!vhlo.bf16_v1>
    %116 = "vhlo.reshape_v1"(%115) : (!vhlo.tensor_v1<1x24x128x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x128x1024x!vhlo.bf16_v1>
    %117 = "vhlo.dot_general_v2"(%85, %116) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x128x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x1024x!vhlo.bf16_v1>
    %118 = "vhlo.reshape_v1"(%117) : (!vhlo.tensor_v1<24x7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x1024x!vhlo.bf16_v1>
    %119 = "vhlo.convert_v1"(%118) : (!vhlo.tensor_v1<1x24x7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>
    %120 = "vhlo.broadcast_in_dim_v1"(%arg12) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>
    %121 = "vhlo.multiply_v1"(%119, %120) : (!vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>
    %122 = "vhlo.convert_v1"(%121) : (!vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x1024x!vhlo.bf16_v1>
    %123 = "vhlo.broadcast_in_dim_v1"(%8) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1024x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.i64_v1>
    %124 = "vhlo.broadcast_in_dim_v1"(%7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.i64_v1>
    %125 = "vhlo.subtract_v1"(%123, %124) : (!vhlo.tensor_v1<7x1024x!vhlo.i64_v1>, !vhlo.tensor_v1<7x1024x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.i64_v1>
    %126 = "vhlo.compare_v1"(%125, %1) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GE>}> : (!vhlo.tensor_v1<7x1024x!vhlo.i64_v1>, !vhlo.tensor_v1<7x1024x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bool_v1>
    %127 = "vhlo.broadcast_in_dim_v1"(%arg11) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>
    %128 = "vhlo.select_v1"(%126, %127, %0) : (!vhlo.tensor_v1<7x1024x!vhlo.bool_v1>, !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>
    %129 = "vhlo.convert_v1"(%128) : (!vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.f32_v1>
    %130 = "vhlo.broadcast_in_dim_v1"(%58) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.i64_v1>
    %131 = "vhlo.compare_v1"(%123, %130) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<7x1024x!vhlo.i64_v1>, !vhlo.tensor_v1<7x1024x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bool_v1>
    %132 = "vhlo.convert_v1"(%131) : (!vhlo.tensor_v1<7x1024x!vhlo.bool_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.f32_v1>
    %133 = "vhlo.multiply_v1"(%129, %132) : (!vhlo.tensor_v1<7x1024x!vhlo.f32_v1>, !vhlo.tensor_v1<7x1024x!vhlo.f32_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.f32_v1>
    %134 = "vhlo.convert_v1"(%133) : (!vhlo.tensor_v1<7x1024x!vhlo.f32_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>
    %135 = "vhlo.reshape_v1"(%134) : (!vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x1024x!vhlo.bf16_v1>
    %136 = "vhlo.broadcast_in_dim_v1"(%135) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x1024x!vhlo.bf16_v1>
    %137 = "vhlo.add_v1"(%122, %136) : (!vhlo.tensor_v1<1x24x7x1024x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x1024x!vhlo.bf16_v1>
    %138 = "vhlo.convert_v1"(%137) : (!vhlo.tensor_v1<1x24x7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>
    %139 = "vhlo.reduce_v1"(%138, %6) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg19: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg20: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %240 = "vhlo.maximum_v1"(%arg19, %arg20) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%240) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>
    %140 = "vhlo.broadcast_in_dim_v1"(%139) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>
    %141 = "vhlo.subtract_v1"(%138, %140) : (!vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>
    %142 = "vhlo.exponential_v2"(%141) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>
    %143 = "vhlo.reduce_v1"(%142, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg19: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg20: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %240 = "vhlo.add_v1"(%arg19, %arg20) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%240) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>
    %144 = "vhlo.broadcast_in_dim_v1"(%143) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>
    %145 = "vhlo.divide_v1"(%142, %144) : (!vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>
    %146 = "vhlo.convert_v1"(%145) : (!vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x1024x!vhlo.bf16_v1>
    %147 = "vhlo.reshape_v1"(%146) : (!vhlo.tensor_v1<1x24x7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x1024x!vhlo.bf16_v1>
    %148 = "vhlo.custom_call_v1"(%3) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"constant">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"kwargs____past_key_values___value_cache_0">}>} : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>
    %149 = "vhlo.reshape_v1"(%arg5) : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>
    %150 = "vhlo.custom_call_v1"(%149) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>
    %151 = "vhlo.reshape_v1"(%150) : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>
    %152 = "vhlo.transpose_v1"(%151) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,1024]{0,1}">} : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>
    %153 = "vhlo.dot_general_v2"(%44, %152) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>
    %154 = "vhlo.reshape_v1"(%153) : (!vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>
    %155 = "vhlo.transpose_v1"(%154) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %156 = "vhlo.scatter_v2"(%148, %91, %155) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg19: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg20: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg20) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>
    %157 = "vhlo.broadcast_in_dim_v1"(%156) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x1024x128x!vhlo.bf16_v1>
    %158 = "vhlo.reshape_v1"(%157) : (!vhlo.tensor_v1<1x8x3x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x1024x128x!vhlo.bf16_v1>
    %159 = "vhlo.dot_general_v2"(%147, %158) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x7x1024x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>
    %160 = "vhlo.reshape_v1"(%159) : (!vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %161 = "vhlo.transpose_v1"(%160) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,7,24,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>
    %162 = "vhlo.reshape_v1"(%161) : (!vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %163 = "vhlo.reshape_v1"(%arg4) : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>
    %164 = "vhlo.custom_call_v1"(%163) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_o_proj_weight">}>} : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>
    %165 = "vhlo.reshape_v1"(%164) : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %166 = "vhlo.transpose_v1"(%165) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,3072]{0,1}">} : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %167 = "vhlo.dot_general_v2"(%162, %166) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %168 = "vhlo.reshape_v1"(%167) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %169 = "vhlo.add_v1"(%23, %168) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %170 = "vhlo.reshape_v1"(%arg16) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %171 = "vhlo.custom_call_v1"(%170) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___post_attention_layernorm_weight">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %172 = "vhlo.reshape_v1"(%171) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1>
    %173 = "vhlo.convert_v1"(%172) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %174 = "vhlo.broadcast_in_dim_v1"(%173) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %175 = "vhlo.convert_v1"(%169) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %176 = "vhlo.power_v1"(%175, %5) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %177 = "vhlo.reduce_v1"(%176, %9) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg19: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg20: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %240 = "vhlo.add_v1"(%arg19, %arg20) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%240) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %178 = "vhlo.multiply_v1"(%177, %4) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %179 = "vhlo.reshape_v1"(%178) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %180 = "vhlo.add_v1"(%179, %34) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %181 = "vhlo.rsqrt_v1"(%180) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %182 = "vhlo.reshape_v1"(%181) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %183 = "vhlo.broadcast_in_dim_v1"(%182) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %184 = "vhlo.multiply_v1"(%175, %183) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %185 = "vhlo.convert_v1"(%184) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %186 = "vhlo.convert_v1"(%185) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %187 = "vhlo.multiply_v1"(%174, %186) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %188 = "vhlo.convert_v1"(%187) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %189 = "vhlo.reshape_v1"(%188) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %190 = "vhlo.reshape_v1"(%arg17) : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>
    %191 = "vhlo.custom_call_v1"(%190) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_gate_proj_weight">}>} : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>
    %192 = "vhlo.reshape_v1"(%191) : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>
    %193 = "vhlo.transpose_v1"(%192) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,8192]{0,1}">} : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>
    %194 = "vhlo.dot_general_v2"(%189, %193) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %195 = "vhlo.reshape_v1"(%194) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %196 = "vhlo.convert_v1"(%195) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %197 = "vhlo.logistic_v1"(%195) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %198 = "vhlo.convert_v1"(%197) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %199 = "vhlo.multiply_v1"(%196, %198) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %200 = "vhlo.convert_v1"(%199) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %201 = "vhlo.convert_v1"(%200) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %202 = "vhlo.reshape_v1"(%arg3) : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>
    %203 = "vhlo.custom_call_v1"(%202) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_up_proj_weight">}>} : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>
    %204 = "vhlo.reshape_v1"(%203) : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>
    %205 = "vhlo.transpose_v1"(%204) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,8192]{0,1}">} : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>
    %206 = "vhlo.dot_general_v2"(%189, %205) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %207 = "vhlo.reshape_v1"(%206) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %208 = "vhlo.convert_v1"(%207) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %209 = "vhlo.multiply_v1"(%201, %208) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %210 = "vhlo.convert_v1"(%209) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %211 = "vhlo.reshape_v1"(%210) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %212 = "vhlo.reshape_v1"(%arg2) : (!vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1>
    %213 = "vhlo.custom_call_v1"(%212) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_down_proj_weight">}>} : (!vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1>
    %214 = "vhlo.reshape_v1"(%213) : (!vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>
    %215 = "vhlo.transpose_v1"(%214) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,3072]{0,1}">} : (!vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>
    %216 = "vhlo.dot_general_v2"(%211, %215) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %217 = "vhlo.reshape_v1"(%216) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %218 = "vhlo.add_v1"(%169, %217) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %219 = "vhlo.convert_v1"(%218) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %220 = "vhlo.power_v1"(%219, %5) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %221 = "vhlo.reduce_v1"(%220, %9) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg19: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg20: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %240 = "vhlo.add_v1"(%arg19, %arg20) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%240) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %222 = "vhlo.multiply_v1"(%221, %4) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %223 = "vhlo.reshape_v1"(%222) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %224 = "vhlo.add_v1"(%223, %34) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %225 = "vhlo.rsqrt_v1"(%224) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %226 = "vhlo.reshape_v1"(%225) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %227 = "vhlo.broadcast_in_dim_v1"(%226) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %228 = "vhlo.multiply_v1"(%219, %227) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %229 = "vhlo.convert_v1"(%228) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %230 = "vhlo.convert_v1"(%229) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %231 = "vhlo.multiply_v1"(%14, %230) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %232 = "vhlo.convert_v1"(%231) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %233 = "vhlo.reshape_v1"(%232) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %234 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>
    %235 = "vhlo.custom_call_v1"(%234) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___lm_head_weight">}>} : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>
    %236 = "vhlo.reshape_v1"(%235) : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>
    %237 = "vhlo.transpose_v1"(%236) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,128256]{0,1}">} : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x128256x!vhlo.bf16_v1>
    %238 = "vhlo.dot_general_v2"(%233, %237) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>
    %239 = "vhlo.reshape_v1"(%238) : (!vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>
    "vhlo.return_v1"(%239) : (!vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}
module @SyncTensorsGraph.456 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<128256x3072xbf16>, %arg1: tensor<f32>, %arg2: tensor<3072x8192xbf16>, %arg3: tensor<8192x3072xbf16>, %arg4: tensor<3072x3072xbf16>, %arg5: tensor<1024x3072xbf16>, %arg6: tensor<1x7xi64>, %arg7: tensor<128256x3072xbf16>, %arg8: tensor<3072xbf16>, %arg9: tensor<7xi64>, %arg10: tensor<i64>, %arg11: tensor<bf16>, %arg12: tensor<f32>, %arg13: tensor<64xf32>, %arg14: tensor<1024x3072xbf16>, %arg15: tensor<3072x3072xbf16>, %arg16: tensor<3072xbf16>, %arg17: tensor<8192x3072xbf16>, %arg18: tensor<3072xbf16>) -> tensor<1x7x128256xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<7x1024xbf16>
    %c = stablehlo.constant dense<1> : tensor<7x1024xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<1x8x1024x128xbf16>
    %cst_2 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<1x7x3072xf32>
    %cst_4 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_5 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %c_6 = stablehlo.constant dense<"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tensor<1024xi64>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %0 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %1 = stablehlo.custom_call @tt.mark_argument(%0) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_norm_weight"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %2 = stablehlo.reshape %1 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %3 = stablehlo.convert %2 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %4 = stablehlo.broadcast_in_dim %3, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %5 = stablehlo.reshape %arg7 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %6 = stablehlo.custom_call @tt.mark_argument(%5) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_embed_tokens_weight"}} : (tensor<1x128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %7 = stablehlo.reshape %6 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %8 = stablehlo.reshape %arg6 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %9 = stablehlo.custom_call @tt.mark_argument(%8) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_0"}} : (tensor<1x1x7xi64>) -> tensor<1x1x7xi64>
    %10 = stablehlo.reshape %9 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %11 = stablehlo.convert %10 : (tensor<7xi64>) -> tensor<7xui32>
    %12 = "stablehlo.gather"(%7, %11) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %13 = stablehlo.reshape %12 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %14 = stablehlo.reshape %arg8 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %15 = stablehlo.custom_call @tt.mark_argument(%14) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %16 = stablehlo.reshape %15 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %17 = stablehlo.convert %16 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %19 = stablehlo.convert %13 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %20 = stablehlo.power %19, %cst_3 : tensor<1x7x3072xf32>
    %21 = stablehlo.reduce(%20 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %22 = stablehlo.multiply %21, %cst_2 : tensor<1x7xf32>
    %23 = stablehlo.reshape %22 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %24 = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %25 = stablehlo.add %23, %24 : tensor<1x7x1xf32>
    %26 = stablehlo.rsqrt %25 : tensor<1x7x1xf32>
    %27 = stablehlo.reshape %26 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %28 = stablehlo.broadcast_in_dim %27, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %29 = stablehlo.multiply %19, %28 : tensor<1x7x3072xf32>
    %30 = stablehlo.convert %29 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %31 = stablehlo.convert %30 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %32 = stablehlo.multiply %18, %31 : tensor<1x7x3072xf32>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %34 = stablehlo.reshape %33 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %35 = stablehlo.reshape %arg15 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %36 = stablehlo.custom_call @tt.mark_argument(%35) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}} : (tensor<1x3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %37 = stablehlo.reshape %36 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %38 = stablehlo.transpose %37, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %39 = stablehlo.dot_general %34, %38, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %40 = stablehlo.reshape %39 : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %41 = stablehlo.transpose %40, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %42 = stablehlo.convert %41 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %43 = stablehlo.reshape %arg13 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %44 = stablehlo.custom_call @tt.mark_argument(%43) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___model_rotary_emb_inv_freq"}} : (tensor<1x1x64xf32>) -> tensor<1x1x64xf32>
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %46 = stablehlo.reshape %arg9 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %47 = stablehlo.custom_call @tt.mark_argument(%46) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_1"}} : (tensor<1x1x7xi64>) -> tensor<1x1x7xi64>
    %48 = stablehlo.reshape %47 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %49 = stablehlo.convert %47 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %50 = stablehlo.dot_general %45, %49, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %51 = stablehlo.transpose %50, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %52 = stablehlo.concatenate %51, %51, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %53 = stablehlo.cosine %52 : tensor<1x7x128xf32>
    %54 = stablehlo.convert %53 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %55 = stablehlo.reshape %54 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %56 = stablehlo.convert %55 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %57 = stablehlo.reshape %56 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %58 = stablehlo.broadcast_in_dim %57, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %59 = stablehlo.multiply %42, %58 : tensor<1x24x7x128xf32>
    %60 = stablehlo.convert %59 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %61 = stablehlo.slice %41 [0:1, 0:24, 0:7, 64:128] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %62 = stablehlo.negate %61 : tensor<1x24x7x64xbf16>
    %63 = stablehlo.slice %41 [0:1, 0:24, 0:7, 0:64] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %64 = stablehlo.concatenate %62, %63, dim = 3 : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %65 = stablehlo.convert %64 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %66 = stablehlo.sine %52 : tensor<1x7x128xf32>
    %67 = stablehlo.convert %66 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %68 = stablehlo.reshape %67 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %69 = stablehlo.convert %68 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %70 = stablehlo.reshape %69 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %71 = stablehlo.broadcast_in_dim %70, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %72 = stablehlo.multiply %65, %71 : tensor<1x24x7x128xf32>
    %73 = stablehlo.convert %72 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %74 = stablehlo.add %60, %73 : tensor<1x24x7x128xbf16>
    %75 = stablehlo.reshape %74 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %76 = stablehlo.custom_call @tt.mark_argument(%cst_1) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "kwargs____past_key_values___key_cache_0"}} : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %77 = stablehlo.compare  LT, %48, %c_0 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %78 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %79 = stablehlo.add %48, %78 : tensor<7xi64>
    %80 = stablehlo.select %77, %79, %48 : tensor<7xi1>, tensor<7xi64>
    %81 = stablehlo.reshape %80 : (tensor<7xi64>) -> tensor<7x1xi64>
    %82 = stablehlo.reshape %arg14 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %83 = stablehlo.custom_call @tt.mark_argument(%82) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}} : (tensor<1x1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %84 = stablehlo.reshape %83 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %85 = stablehlo.transpose %84, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %86 = stablehlo.dot_general %34, %85, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %87 = stablehlo.reshape %86 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %88 = stablehlo.transpose %87, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %89 = stablehlo.convert %88 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %90 = stablehlo.broadcast_in_dim %57, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %91 = stablehlo.multiply %89, %90 : tensor<1x8x7x128xf32>
    %92 = stablehlo.convert %91 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %93 = stablehlo.slice %88 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %94 = stablehlo.negate %93 : tensor<1x8x7x64xbf16>
    %95 = stablehlo.slice %88 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %96 = stablehlo.concatenate %94, %95, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %97 = stablehlo.convert %96 : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %98 = stablehlo.broadcast_in_dim %70, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %99 = stablehlo.multiply %97, %98 : tensor<1x8x7x128xf32>
    %100 = stablehlo.convert %99 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %101 = stablehlo.add %92, %100 : tensor<1x8x7x128xbf16>
    %102 = "stablehlo.scatter"(%76, %81, %101) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg19: tensor<bf16>, %arg20: tensor<bf16>):
      stablehlo.return %arg20 : tensor<bf16>
    }) : (tensor<1x8x1024x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %103 = stablehlo.broadcast_in_dim %102, dims = [0, 1, 3, 4] : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x3x1024x128xbf16>
    %104 = stablehlo.reshape %103 : (tensor<1x8x3x1024x128xbf16>) -> tensor<1x24x1024x128xbf16>
    %105 = stablehlo.transpose %104, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,1024]{2,3,1,0}"} : (tensor<1x24x1024x128xbf16>) -> tensor<1x24x128x1024xbf16>
    %106 = stablehlo.reshape %105 : (tensor<1x24x128x1024xbf16>) -> tensor<24x128x1024xbf16>
    %107 = stablehlo.dot_general %75, %106, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x1024xbf16>) -> tensor<24x7x1024xbf16>
    %108 = stablehlo.reshape %107 : (tensor<24x7x1024xbf16>) -> tensor<1x24x7x1024xbf16>
    %109 = stablehlo.convert %108 : (tensor<1x24x7x1024xbf16>) -> tensor<1x24x7x1024xf32>
    %110 = stablehlo.broadcast_in_dim %arg12, dims = [] : (tensor<f32>) -> tensor<1x24x7x1024xf32>
    %111 = stablehlo.multiply %109, %110 : tensor<1x24x7x1024xf32>
    %112 = stablehlo.convert %111 : (tensor<1x24x7x1024xf32>) -> tensor<1x24x7x1024xbf16>
    %113 = stablehlo.broadcast_in_dim %c_6, dims = [1] : (tensor<1024xi64>) -> tensor<7x1024xi64>
    %114 = stablehlo.broadcast_in_dim %c_5, dims = [0] : (tensor<7xi64>) -> tensor<7x1024xi64>
    %115 = stablehlo.subtract %113, %114 : tensor<7x1024xi64>
    %116 = stablehlo.compare  GE, %115, %c : (tensor<7x1024xi64>, tensor<7x1024xi64>) -> tensor<7x1024xi1>
    %117 = stablehlo.broadcast_in_dim %arg11, dims = [] : (tensor<bf16>) -> tensor<7x1024xbf16>
    %118 = stablehlo.select %116, %117, %cst : tensor<7x1024xi1>, tensor<7x1024xbf16>
    %119 = stablehlo.convert %118 : (tensor<7x1024xbf16>) -> tensor<7x1024xf32>
    %120 = stablehlo.broadcast_in_dim %48, dims = [0] : (tensor<7xi64>) -> tensor<7x1024xi64>
    %121 = stablehlo.compare  GT, %113, %120 : (tensor<7x1024xi64>, tensor<7x1024xi64>) -> tensor<7x1024xi1>
    %122 = stablehlo.convert %121 : (tensor<7x1024xi1>) -> tensor<7x1024xf32>
    %123 = stablehlo.multiply %119, %122 : tensor<7x1024xf32>
    %124 = stablehlo.convert %123 : (tensor<7x1024xf32>) -> tensor<7x1024xbf16>
    %125 = stablehlo.reshape %124 : (tensor<7x1024xbf16>) -> tensor<1x7x1024xbf16>
    %126 = stablehlo.broadcast_in_dim %125, dims = [0, 2, 3] : (tensor<1x7x1024xbf16>) -> tensor<1x24x7x1024xbf16>
    %127 = stablehlo.add %112, %126 : tensor<1x24x7x1024xbf16>
    %128 = stablehlo.convert %127 : (tensor<1x24x7x1024xbf16>) -> tensor<1x24x7x1024xf32>
    %129 = stablehlo.reduce(%128 init: %cst_4) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x7x1024xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %130 = stablehlo.broadcast_in_dim %129, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x1024xf32>
    %131 = stablehlo.subtract %128, %130 : tensor<1x24x7x1024xf32>
    %132 = stablehlo.exponential %131 : tensor<1x24x7x1024xf32>
    %133 = stablehlo.reduce(%132 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x24x7x1024xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x1024xf32>
    %135 = stablehlo.divide %132, %134 : tensor<1x24x7x1024xf32>
    %136 = stablehlo.convert %135 : (tensor<1x24x7x1024xf32>) -> tensor<1x24x7x1024xbf16>
    %137 = stablehlo.reshape %136 : (tensor<1x24x7x1024xbf16>) -> tensor<24x7x1024xbf16>
    %138 = stablehlo.custom_call @tt.mark_argument(%cst_1) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "kwargs____past_key_values___value_cache_0"}} : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %139 = stablehlo.reshape %arg5 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %140 = stablehlo.custom_call @tt.mark_argument(%139) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}} : (tensor<1x1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %141 = stablehlo.reshape %140 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %142 = stablehlo.transpose %141, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %143 = stablehlo.dot_general %34, %142, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %144 = stablehlo.reshape %143 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %146 = "stablehlo.scatter"(%138, %81, %145) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg19: tensor<bf16>, %arg20: tensor<bf16>):
      stablehlo.return %arg20 : tensor<bf16>
    }) : (tensor<1x8x1024x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %147 = stablehlo.broadcast_in_dim %146, dims = [0, 1, 3, 4] : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x3x1024x128xbf16>
    %148 = stablehlo.reshape %147 : (tensor<1x8x3x1024x128xbf16>) -> tensor<24x1024x128xbf16>
    %149 = stablehlo.dot_general %137, %148, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x1024xbf16>, tensor<24x1024x128xbf16>) -> tensor<24x7x128xbf16>
    %150 = stablehlo.reshape %149 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %151 = stablehlo.transpose %150, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %152 = stablehlo.reshape %151 : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %153 = stablehlo.reshape %arg4 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %154 = stablehlo.custom_call @tt.mark_argument(%153) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}} : (tensor<1x3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %155 = stablehlo.reshape %154 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %156 = stablehlo.transpose %155, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %157 = stablehlo.dot_general %152, %156, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %158 = stablehlo.reshape %157 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %159 = stablehlo.add %13, %158 : tensor<1x7x3072xbf16>
    %160 = stablehlo.reshape %arg16 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %161 = stablehlo.custom_call @tt.mark_argument(%160) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %162 = stablehlo.reshape %161 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %163 = stablehlo.convert %162 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %164 = stablehlo.broadcast_in_dim %163, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %165 = stablehlo.convert %159 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %166 = stablehlo.power %165, %cst_3 : tensor<1x7x3072xf32>
    %167 = stablehlo.reduce(%166 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %168 = stablehlo.multiply %167, %cst_2 : tensor<1x7xf32>
    %169 = stablehlo.reshape %168 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %170 = stablehlo.add %169, %24 : tensor<1x7x1xf32>
    %171 = stablehlo.rsqrt %170 : tensor<1x7x1xf32>
    %172 = stablehlo.reshape %171 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %173 = stablehlo.broadcast_in_dim %172, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %174 = stablehlo.multiply %165, %173 : tensor<1x7x3072xf32>
    %175 = stablehlo.convert %174 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %176 = stablehlo.convert %175 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %177 = stablehlo.multiply %164, %176 : tensor<1x7x3072xf32>
    %178 = stablehlo.convert %177 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %179 = stablehlo.reshape %178 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %180 = stablehlo.reshape %arg17 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %181 = stablehlo.custom_call @tt.mark_argument(%180) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}} : (tensor<1x8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %182 = stablehlo.reshape %181 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %183 = stablehlo.transpose %182, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %184 = stablehlo.dot_general %179, %183, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %185 = stablehlo.reshape %184 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %186 = stablehlo.convert %185 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %187 = stablehlo.logistic %185 : tensor<1x7x8192xbf16>
    %188 = stablehlo.convert %187 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %189 = stablehlo.multiply %186, %188 : tensor<1x7x8192xf32>
    %190 = stablehlo.convert %189 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %191 = stablehlo.convert %190 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %192 = stablehlo.reshape %arg3 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %193 = stablehlo.custom_call @tt.mark_argument(%192) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}} : (tensor<1x8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %194 = stablehlo.reshape %193 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %195 = stablehlo.transpose %194, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %196 = stablehlo.dot_general %179, %195, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %197 = stablehlo.reshape %196 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %198 = stablehlo.convert %197 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %199 = stablehlo.multiply %191, %198 : tensor<1x7x8192xf32>
    %200 = stablehlo.convert %199 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %201 = stablehlo.reshape %200 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %202 = stablehlo.reshape %arg2 : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %203 = stablehlo.custom_call @tt.mark_argument(%202) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}} : (tensor<1x3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %204 = stablehlo.reshape %203 : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %205 = stablehlo.transpose %204, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %206 = stablehlo.dot_general %201, %205, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %207 = stablehlo.reshape %206 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %208 = stablehlo.add %159, %207 : tensor<1x7x3072xbf16>
    %209 = stablehlo.convert %208 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %210 = stablehlo.power %209, %cst_3 : tensor<1x7x3072xf32>
    %211 = stablehlo.reduce(%210 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %212 = stablehlo.multiply %211, %cst_2 : tensor<1x7xf32>
    %213 = stablehlo.reshape %212 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %214 = stablehlo.add %213, %24 : tensor<1x7x1xf32>
    %215 = stablehlo.rsqrt %214 : tensor<1x7x1xf32>
    %216 = stablehlo.reshape %215 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %217 = stablehlo.broadcast_in_dim %216, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %218 = stablehlo.multiply %209, %217 : tensor<1x7x3072xf32>
    %219 = stablehlo.convert %218 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %220 = stablehlo.convert %219 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %221 = stablehlo.multiply %4, %220 : tensor<1x7x3072xf32>
    %222 = stablehlo.convert %221 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %223 = stablehlo.reshape %222 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %224 = stablehlo.reshape %arg0 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %225 = stablehlo.custom_call @tt.mark_argument(%224) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___lm_head_weight"}} : (tensor<1x128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %226 = stablehlo.reshape %225 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %227 = stablehlo.transpose %226, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %228 = stablehlo.dot_general %223, %227, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %229 = stablehlo.reshape %228 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %229 : tensor<1x7x128256xbf16>
  }
}
module @SyncTensorsGraph.456 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<128256x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___lm_head_weight"}, %arg1: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_0"}, %arg2: tensor<3072x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg3: tensor<8192x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg4: tensor<3072x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg5: tensor<1024x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg6: tensor<1x7xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"}, %arg7: tensor<128256x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg8: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg9: tensor<7xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_1"}, %arg10: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_1"}, %arg11: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_2"}, %arg12: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_3"}, %arg13: tensor<64xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg14: tensor<1024x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg15: tensor<3072x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg16: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg17: tensor<8192x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg18: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_norm_weight"}) -> tensor<1x7x128256xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<7x1024xbf16>
    %c = stablehlo.constant dense<1> : tensor<7x1024xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<1x8x1024x128xbf16>
    %cst_2 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<1x7x3072xf32>
    %cst_4 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_5 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %c_6 = stablehlo.constant dense<"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tensor<1024xi64>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %0 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %1 = stablehlo.reshape %0 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %2 = stablehlo.convert %1 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %3 = stablehlo.broadcast_in_dim %2, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %4 = stablehlo.reshape %arg7 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %5 = stablehlo.reshape %4 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %6 = stablehlo.reshape %arg6 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %7 = stablehlo.reshape %6 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %8 = stablehlo.convert %7 : (tensor<7xi64>) -> tensor<7xui32>
    %9 = "stablehlo.gather"(%5, %8) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %10 = stablehlo.reshape %9 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %11 = stablehlo.reshape %arg8 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %12 = stablehlo.reshape %11 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %13 = stablehlo.convert %12 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %14 = stablehlo.broadcast_in_dim %13, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %15 = stablehlo.convert %10 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %16 = stablehlo.power %15, %cst_3 : tensor<1x7x3072xf32>
    %17 = stablehlo.reduce(%16 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %18 = stablehlo.multiply %17, %cst_2 : tensor<1x7xf32>
    %19 = stablehlo.reshape %18 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %20 = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %21 = stablehlo.add %19, %20 : tensor<1x7x1xf32>
    %22 = stablehlo.rsqrt %21 : tensor<1x7x1xf32>
    %23 = stablehlo.reshape %22 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %24 = stablehlo.broadcast_in_dim %23, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %25 = stablehlo.multiply %15, %24 : tensor<1x7x3072xf32>
    %26 = stablehlo.convert %25 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %27 = stablehlo.convert %26 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %28 = stablehlo.multiply %14, %27 : tensor<1x7x3072xf32>
    %29 = stablehlo.convert %28 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %30 = stablehlo.reshape %29 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %31 = stablehlo.reshape %arg15 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %32 = stablehlo.reshape %31 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %33 = stablehlo.transpose %32, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %34 = stablehlo.dot_general %30, %33, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %35 = stablehlo.reshape %34 : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %36 = stablehlo.transpose %35, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %37 = stablehlo.convert %36 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %38 = stablehlo.reshape %arg13 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %39 = stablehlo.reshape %38 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %40 = stablehlo.reshape %arg9 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %41 = stablehlo.reshape %40 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %42 = stablehlo.convert %40 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %43 = stablehlo.dot_general %39, %42, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %44 = stablehlo.transpose %43, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %45 = stablehlo.concatenate %44, %44, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %46 = stablehlo.cosine %45 : tensor<1x7x128xf32>
    %47 = stablehlo.convert %46 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %48 = stablehlo.reshape %47 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %50 = stablehlo.reshape %49 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %51 = stablehlo.broadcast_in_dim %50, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %52 = stablehlo.multiply %37, %51 : tensor<1x24x7x128xf32>
    %53 = stablehlo.convert %52 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %54 = stablehlo.slice %36 [0:1, 0:24, 0:7, 64:128] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %55 = stablehlo.negate %54 : tensor<1x24x7x64xbf16>
    %56 = stablehlo.slice %36 [0:1, 0:24, 0:7, 0:64] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %57 = stablehlo.concatenate %55, %56, dim = 3 : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %58 = stablehlo.convert %57 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %59 = stablehlo.sine %45 : tensor<1x7x128xf32>
    %60 = stablehlo.convert %59 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %61 = stablehlo.reshape %60 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %62 = stablehlo.convert %61 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %63 = stablehlo.reshape %62 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %64 = stablehlo.broadcast_in_dim %63, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %65 = stablehlo.multiply %58, %64 : tensor<1x24x7x128xf32>
    %66 = stablehlo.convert %65 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %67 = stablehlo.add %53, %66 : tensor<1x24x7x128xbf16>
    %68 = stablehlo.reshape %67 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %69 = stablehlo.compare  LT, %41, %c_0 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %70 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %71 = stablehlo.add %41, %70 : tensor<7xi64>
    %72 = stablehlo.select %69, %71, %41 : tensor<7xi1>, tensor<7xi64>
    %73 = stablehlo.reshape %72 : (tensor<7xi64>) -> tensor<7x1xi64>
    %74 = stablehlo.reshape %arg14 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %30, %76, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = stablehlo.convert %79 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %81 = stablehlo.broadcast_in_dim %50, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %82 = stablehlo.multiply %80, %81 : tensor<1x8x7x128xf32>
    %83 = stablehlo.convert %82 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %84 = stablehlo.slice %79 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %85 = stablehlo.negate %84 : tensor<1x8x7x64xbf16>
    %86 = stablehlo.slice %79 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %87 = stablehlo.concatenate %85, %86, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %88 = stablehlo.convert %87 : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %89 = stablehlo.broadcast_in_dim %63, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %90 = stablehlo.multiply %88, %89 : tensor<1x8x7x128xf32>
    %91 = stablehlo.convert %90 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %92 = stablehlo.add %83, %91 : tensor<1x8x7x128xbf16>
    %93 = "stablehlo.scatter"(%cst_1, %73, %92) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg19: tensor<bf16>, %arg20: tensor<bf16>):
      stablehlo.return %arg20 : tensor<bf16>
    }) : (tensor<1x8x1024x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %94 = stablehlo.broadcast_in_dim %93, dims = [0, 1, 3, 4] : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x3x1024x128xbf16>
    %95 = stablehlo.reshape %94 : (tensor<1x8x3x1024x128xbf16>) -> tensor<1x24x1024x128xbf16>
    %96 = stablehlo.transpose %95, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,1024]{2,3,1,0}"} : (tensor<1x24x1024x128xbf16>) -> tensor<1x24x128x1024xbf16>
    %97 = stablehlo.reshape %96 : (tensor<1x24x128x1024xbf16>) -> tensor<24x128x1024xbf16>
    %98 = stablehlo.dot_general %68, %97, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x1024xbf16>) -> tensor<24x7x1024xbf16>
    %99 = stablehlo.reshape %98 : (tensor<24x7x1024xbf16>) -> tensor<1x24x7x1024xbf16>
    %100 = stablehlo.convert %99 : (tensor<1x24x7x1024xbf16>) -> tensor<1x24x7x1024xf32>
    %101 = stablehlo.broadcast_in_dim %arg12, dims = [] : (tensor<f32>) -> tensor<1x24x7x1024xf32>
    %102 = stablehlo.multiply %100, %101 : tensor<1x24x7x1024xf32>
    %103 = stablehlo.convert %102 : (tensor<1x24x7x1024xf32>) -> tensor<1x24x7x1024xbf16>
    %104 = stablehlo.broadcast_in_dim %c_6, dims = [1] : (tensor<1024xi64>) -> tensor<7x1024xi64>
    %105 = stablehlo.broadcast_in_dim %c_5, dims = [0] : (tensor<7xi64>) -> tensor<7x1024xi64>
    %106 = stablehlo.subtract %104, %105 : tensor<7x1024xi64>
    %107 = stablehlo.compare  GE, %106, %c : (tensor<7x1024xi64>, tensor<7x1024xi64>) -> tensor<7x1024xi1>
    %108 = stablehlo.broadcast_in_dim %arg11, dims = [] : (tensor<bf16>) -> tensor<7x1024xbf16>
    %109 = stablehlo.select %107, %108, %cst : tensor<7x1024xi1>, tensor<7x1024xbf16>
    %110 = stablehlo.convert %109 : (tensor<7x1024xbf16>) -> tensor<7x1024xf32>
    %111 = stablehlo.broadcast_in_dim %41, dims = [0] : (tensor<7xi64>) -> tensor<7x1024xi64>
    %112 = stablehlo.compare  GT, %104, %111 : (tensor<7x1024xi64>, tensor<7x1024xi64>) -> tensor<7x1024xi1>
    %113 = stablehlo.convert %112 : (tensor<7x1024xi1>) -> tensor<7x1024xf32>
    %114 = stablehlo.multiply %110, %113 : tensor<7x1024xf32>
    %115 = stablehlo.convert %114 : (tensor<7x1024xf32>) -> tensor<7x1024xbf16>
    %116 = stablehlo.reshape %115 : (tensor<7x1024xbf16>) -> tensor<1x7x1024xbf16>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 2, 3] : (tensor<1x7x1024xbf16>) -> tensor<1x24x7x1024xbf16>
    %118 = stablehlo.add %103, %117 : tensor<1x24x7x1024xbf16>
    %119 = stablehlo.convert %118 : (tensor<1x24x7x1024xbf16>) -> tensor<1x24x7x1024xf32>
    %120 = stablehlo.reduce(%119 init: %cst_4) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x7x1024xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %121 = stablehlo.broadcast_in_dim %120, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x1024xf32>
    %122 = stablehlo.subtract %119, %121 : tensor<1x24x7x1024xf32>
    %123 = stablehlo.exponential %122 : tensor<1x24x7x1024xf32>
    %124 = stablehlo.reduce(%123 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x24x7x1024xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %125 = stablehlo.broadcast_in_dim %124, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x1024xf32>
    %126 = stablehlo.divide %123, %125 : tensor<1x24x7x1024xf32>
    %127 = stablehlo.convert %126 : (tensor<1x24x7x1024xf32>) -> tensor<1x24x7x1024xbf16>
    %128 = stablehlo.reshape %127 : (tensor<1x24x7x1024xbf16>) -> tensor<24x7x1024xbf16>
    %129 = stablehlo.reshape %arg5 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %131 = stablehlo.transpose %130, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %132 = stablehlo.dot_general %30, %131, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %133 = stablehlo.reshape %132 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %134 = stablehlo.transpose %133, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %135 = "stablehlo.scatter"(%cst_1, %73, %134) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg19: tensor<bf16>, %arg20: tensor<bf16>):
      stablehlo.return %arg20 : tensor<bf16>
    }) : (tensor<1x8x1024x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %136 = stablehlo.broadcast_in_dim %135, dims = [0, 1, 3, 4] : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x3x1024x128xbf16>
    %137 = stablehlo.reshape %136 : (tensor<1x8x3x1024x128xbf16>) -> tensor<24x1024x128xbf16>
    %138 = stablehlo.dot_general %128, %137, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x1024xbf16>, tensor<24x1024x128xbf16>) -> tensor<24x7x128xbf16>
    %139 = stablehlo.reshape %138 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %140 = stablehlo.transpose %139, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %141 = stablehlo.reshape %140 : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %142 = stablehlo.reshape %arg4 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %143 = stablehlo.reshape %142 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %144 = stablehlo.transpose %143, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %145 = stablehlo.dot_general %141, %144, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %146 = stablehlo.reshape %145 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %147 = stablehlo.add %10, %146 : tensor<1x7x3072xbf16>
    %148 = stablehlo.reshape %arg16 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %149 = stablehlo.reshape %148 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %150 = stablehlo.convert %149 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %151 = stablehlo.broadcast_in_dim %150, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %152 = stablehlo.convert %147 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %153 = stablehlo.power %152, %cst_3 : tensor<1x7x3072xf32>
    %154 = stablehlo.reduce(%153 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %155 = stablehlo.multiply %154, %cst_2 : tensor<1x7xf32>
    %156 = stablehlo.reshape %155 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %157 = stablehlo.add %156, %20 : tensor<1x7x1xf32>
    %158 = stablehlo.rsqrt %157 : tensor<1x7x1xf32>
    %159 = stablehlo.reshape %158 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %160 = stablehlo.broadcast_in_dim %159, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %161 = stablehlo.multiply %152, %160 : tensor<1x7x3072xf32>
    %162 = stablehlo.convert %161 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %163 = stablehlo.convert %162 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %164 = stablehlo.multiply %151, %163 : tensor<1x7x3072xf32>
    %165 = stablehlo.convert %164 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %166 = stablehlo.reshape %165 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %167 = stablehlo.reshape %arg17 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %168 = stablehlo.reshape %167 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %169 = stablehlo.transpose %168, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %170 = stablehlo.dot_general %166, %169, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %171 = stablehlo.reshape %170 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %172 = stablehlo.convert %171 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %173 = stablehlo.logistic %171 : tensor<1x7x8192xbf16>
    %174 = stablehlo.convert %173 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %175 = stablehlo.multiply %172, %174 : tensor<1x7x8192xf32>
    %176 = stablehlo.convert %175 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %177 = stablehlo.convert %176 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %178 = stablehlo.reshape %arg3 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %179 = stablehlo.reshape %178 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %180 = stablehlo.transpose %179, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %181 = stablehlo.dot_general %166, %180, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %182 = stablehlo.reshape %181 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %183 = stablehlo.convert %182 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %184 = stablehlo.multiply %177, %183 : tensor<1x7x8192xf32>
    %185 = stablehlo.convert %184 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %186 = stablehlo.reshape %185 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %187 = stablehlo.reshape %arg2 : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %188 = stablehlo.reshape %187 : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %189 = stablehlo.transpose %188, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %190 = stablehlo.dot_general %186, %189, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %191 = stablehlo.reshape %190 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %192 = stablehlo.add %147, %191 : tensor<1x7x3072xbf16>
    %193 = stablehlo.convert %192 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %194 = stablehlo.power %193, %cst_3 : tensor<1x7x3072xf32>
    %195 = stablehlo.reduce(%194 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %196 = stablehlo.multiply %195, %cst_2 : tensor<1x7xf32>
    %197 = stablehlo.reshape %196 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %198 = stablehlo.add %197, %20 : tensor<1x7x1xf32>
    %199 = stablehlo.rsqrt %198 : tensor<1x7x1xf32>
    %200 = stablehlo.reshape %199 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %201 = stablehlo.broadcast_in_dim %200, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %202 = stablehlo.multiply %193, %201 : tensor<1x7x3072xf32>
    %203 = stablehlo.convert %202 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %204 = stablehlo.convert %203 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %205 = stablehlo.multiply %3, %204 : tensor<1x7x3072xf32>
    %206 = stablehlo.convert %205 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %207 = stablehlo.reshape %206 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %208 = stablehlo.reshape %arg0 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %209 = stablehlo.reshape %208 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %210 = stablehlo.transpose %209, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %211 = stablehlo.dot_general %207, %210, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %212 = stablehlo.reshape %211 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %212 : tensor<1x7x128256xbf16>
  }
}
module @SyncTensorsGraph.456 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=1]>
  func.func @main(%arg0: tensor<128256x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___lm_head_weight"}, %arg1: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "auto_annotated_const_0"}, %arg2: tensor<3072x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg3: tensor<8192x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg4: tensor<3072x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg5: tensor<1024x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg6: tensor<1x7xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_0"}, %arg7: tensor<128256x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg8: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg9: tensor<7xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_1"}, %arg10: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "auto_annotated_const_1"}, %arg11: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "auto_annotated_const_2"}, %arg12: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "auto_annotated_const_3"}, %arg13: tensor<64xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg14: tensor<1024x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg15: tensor<3072x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg16: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg17: tensor<8192x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg18: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<7x1024xbf16>
    %c = stablehlo.constant dense<1> : tensor<7x1024xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<1x8x1024x128xbf16>
    %cst_2 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<1x7x3072xf32>
    %cst_4 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_5 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %c_6 = stablehlo.constant dense<"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tensor<1024xi64>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %0 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %1 = stablehlo.reshape %0 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %2 = stablehlo.convert %1 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %3 = stablehlo.broadcast_in_dim %2, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %4 = stablehlo.reshape %arg7 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %5 = stablehlo.reshape %4 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %6 = stablehlo.reshape %arg6 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %7 = stablehlo.reshape %6 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %8 = stablehlo.convert %7 : (tensor<7xi64>) -> tensor<7xui32>
    %9 = "stablehlo.gather"(%5, %8) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %10 = stablehlo.reshape %9 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %11 = stablehlo.reshape %arg8 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %12 = stablehlo.reshape %11 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %13 = stablehlo.convert %12 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %14 = stablehlo.broadcast_in_dim %13, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %15 = stablehlo.convert %10 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %16 = stablehlo.power %15, %cst_3 : tensor<1x7x3072xf32>
    %17 = stablehlo.reduce(%16 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %18 = stablehlo.multiply %17, %cst_2 : tensor<1x7xf32>
    %19 = stablehlo.reshape %18 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %20 = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %21 = stablehlo.add %19, %20 : tensor<1x7x1xf32>
    %22 = stablehlo.rsqrt %21 : tensor<1x7x1xf32>
    %23 = stablehlo.reshape %22 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %24 = stablehlo.broadcast_in_dim %23, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %25 = stablehlo.multiply %15, %24 : tensor<1x7x3072xf32>
    %26 = stablehlo.convert %25 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %27 = stablehlo.convert %26 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %28 = stablehlo.multiply %14, %27 : tensor<1x7x3072xf32>
    %29 = stablehlo.convert %28 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %30 = stablehlo.reshape %29 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %31 = stablehlo.reshape %arg15 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %32 = stablehlo.reshape %31 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %33 = stablehlo.transpose %32, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %34 = stablehlo.dot_general %30, %33, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %35 = stablehlo.reshape %34 : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %36 = stablehlo.transpose %35, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %37 = stablehlo.convert %36 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %38 = stablehlo.reshape %arg13 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %39 = stablehlo.reshape %38 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %40 = stablehlo.reshape %arg9 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %41 = stablehlo.reshape %40 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %42 = stablehlo.convert %40 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %43 = stablehlo.dot_general %39, %42, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %44 = stablehlo.transpose %43, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %45 = stablehlo.concatenate %44, %44, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %46 = stablehlo.cosine %45 : tensor<1x7x128xf32>
    %47 = stablehlo.convert %46 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %48 = stablehlo.reshape %47 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %50 = stablehlo.reshape %49 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %51 = stablehlo.broadcast_in_dim %50, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %52 = stablehlo.multiply %37, %51 : tensor<1x24x7x128xf32>
    %53 = stablehlo.convert %52 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %54 = stablehlo.slice %36 [0:1, 0:24, 0:7, 64:128] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %55 = stablehlo.negate %54 : tensor<1x24x7x64xbf16>
    %56 = stablehlo.slice %36 [0:1, 0:24, 0:7, 0:64] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %57 = stablehlo.concatenate %55, %56, dim = 3 : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %58 = stablehlo.convert %57 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %59 = stablehlo.sine %45 : tensor<1x7x128xf32>
    %60 = stablehlo.convert %59 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %61 = stablehlo.reshape %60 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %62 = stablehlo.convert %61 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %63 = stablehlo.reshape %62 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %64 = stablehlo.broadcast_in_dim %63, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %65 = stablehlo.multiply %58, %64 : tensor<1x24x7x128xf32>
    %66 = stablehlo.convert %65 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %67 = stablehlo.add %53, %66 : tensor<1x24x7x128xbf16>
    %68 = stablehlo.reshape %67 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %69 = stablehlo.compare  LT, %41, %c_0 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %70 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %71 = stablehlo.add %41, %70 : tensor<7xi64>
    %72 = stablehlo.select %69, %71, %41 : tensor<7xi1>, tensor<7xi64>
    %73 = stablehlo.reshape %72 : (tensor<7xi64>) -> tensor<7x1xi64>
    %74 = stablehlo.reshape %arg14 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %30, %76, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = stablehlo.convert %79 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %81 = stablehlo.broadcast_in_dim %50, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %82 = stablehlo.multiply %80, %81 : tensor<1x8x7x128xf32>
    %83 = stablehlo.convert %82 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %84 = stablehlo.slice %79 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %85 = stablehlo.negate %84 : tensor<1x8x7x64xbf16>
    %86 = stablehlo.slice %79 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %87 = stablehlo.concatenate %85, %86, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %88 = stablehlo.convert %87 : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %89 = stablehlo.broadcast_in_dim %63, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %90 = stablehlo.multiply %88, %89 : tensor<1x8x7x128xf32>
    %91 = stablehlo.convert %90 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %92 = stablehlo.add %83, %91 : tensor<1x8x7x128xbf16>
    %93 = "stablehlo.scatter"(%cst_1, %73, %92) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg19: tensor<bf16>, %arg20: tensor<bf16>):
      stablehlo.return %arg20 : tensor<bf16>
    }) : (tensor<1x8x1024x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %94 = stablehlo.broadcast_in_dim %93, dims = [0, 1, 3, 4] : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x3x1024x128xbf16>
    %95 = stablehlo.reshape %94 : (tensor<1x8x3x1024x128xbf16>) -> tensor<1x24x1024x128xbf16>
    %96 = stablehlo.transpose %95, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,1024]{2,3,1,0}"} : (tensor<1x24x1024x128xbf16>) -> tensor<1x24x128x1024xbf16>
    %97 = stablehlo.reshape %96 : (tensor<1x24x128x1024xbf16>) -> tensor<24x128x1024xbf16>
    %98 = stablehlo.dot_general %68, %97, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x1024xbf16>) -> tensor<24x7x1024xbf16>
    %99 = stablehlo.reshape %98 : (tensor<24x7x1024xbf16>) -> tensor<1x24x7x1024xbf16>
    %100 = stablehlo.convert %99 : (tensor<1x24x7x1024xbf16>) -> tensor<1x24x7x1024xf32>
    %101 = stablehlo.broadcast_in_dim %arg12, dims = [] : (tensor<f32>) -> tensor<1x24x7x1024xf32>
    %102 = stablehlo.multiply %100, %101 : tensor<1x24x7x1024xf32>
    %103 = stablehlo.convert %102 : (tensor<1x24x7x1024xf32>) -> tensor<1x24x7x1024xbf16>
    %104 = stablehlo.broadcast_in_dim %c_6, dims = [1] : (tensor<1024xi64>) -> tensor<7x1024xi64>
    %105 = stablehlo.broadcast_in_dim %c_5, dims = [0] : (tensor<7xi64>) -> tensor<7x1024xi64>
    %106 = stablehlo.subtract %104, %105 : tensor<7x1024xi64>
    %107 = stablehlo.compare  GE, %106, %c : (tensor<7x1024xi64>, tensor<7x1024xi64>) -> tensor<7x1024xi1>
    %108 = stablehlo.broadcast_in_dim %arg11, dims = [] : (tensor<bf16>) -> tensor<7x1024xbf16>
    %109 = stablehlo.select %107, %108, %cst : tensor<7x1024xi1>, tensor<7x1024xbf16>
    %110 = stablehlo.convert %109 : (tensor<7x1024xbf16>) -> tensor<7x1024xf32>
    %111 = stablehlo.broadcast_in_dim %41, dims = [0] : (tensor<7xi64>) -> tensor<7x1024xi64>
    %112 = stablehlo.compare  GT, %104, %111 : (tensor<7x1024xi64>, tensor<7x1024xi64>) -> tensor<7x1024xi1>
    %113 = stablehlo.convert %112 : (tensor<7x1024xi1>) -> tensor<7x1024xf32>
    %114 = stablehlo.multiply %110, %113 : tensor<7x1024xf32>
    %115 = stablehlo.convert %114 : (tensor<7x1024xf32>) -> tensor<7x1024xbf16>
    %116 = stablehlo.reshape %115 : (tensor<7x1024xbf16>) -> tensor<1x7x1024xbf16>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 2, 3] : (tensor<1x7x1024xbf16>) -> tensor<1x24x7x1024xbf16>
    %118 = stablehlo.add %103, %117 : tensor<1x24x7x1024xbf16>
    %119 = stablehlo.convert %118 : (tensor<1x24x7x1024xbf16>) -> tensor<1x24x7x1024xf32>
    %120 = stablehlo.reduce(%119 init: %cst_4) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x7x1024xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %121 = stablehlo.broadcast_in_dim %120, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x1024xf32>
    %122 = stablehlo.subtract %119, %121 : tensor<1x24x7x1024xf32>
    %123 = stablehlo.exponential %122 : tensor<1x24x7x1024xf32>
    %124 = stablehlo.reduce(%123 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x24x7x1024xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %125 = stablehlo.broadcast_in_dim %124, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x1024xf32>
    %126 = stablehlo.divide %123, %125 : tensor<1x24x7x1024xf32>
    %127 = stablehlo.convert %126 : (tensor<1x24x7x1024xf32>) -> tensor<1x24x7x1024xbf16>
    %128 = stablehlo.reshape %127 : (tensor<1x24x7x1024xbf16>) -> tensor<24x7x1024xbf16>
    %129 = stablehlo.reshape %arg5 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %131 = stablehlo.transpose %130, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %132 = stablehlo.dot_general %30, %131, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %133 = stablehlo.reshape %132 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %134 = stablehlo.transpose %133, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %135 = "stablehlo.scatter"(%cst_1, %73, %134) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg19: tensor<bf16>, %arg20: tensor<bf16>):
      stablehlo.return %arg20 : tensor<bf16>
    }) : (tensor<1x8x1024x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %136 = stablehlo.broadcast_in_dim %135, dims = [0, 1, 3, 4] : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x3x1024x128xbf16>
    %137 = stablehlo.reshape %136 : (tensor<1x8x3x1024x128xbf16>) -> tensor<24x1024x128xbf16>
    %138 = stablehlo.dot_general %128, %137, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x1024xbf16>, tensor<24x1024x128xbf16>) -> tensor<24x7x128xbf16>
    %139 = stablehlo.reshape %138 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %140 = stablehlo.transpose %139, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %141 = stablehlo.reshape %140 : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %142 = stablehlo.reshape %arg4 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %143 = stablehlo.reshape %142 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %144 = stablehlo.transpose %143, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %145 = stablehlo.dot_general %141, %144, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %146 = stablehlo.reshape %145 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %147 = stablehlo.add %10, %146 : tensor<1x7x3072xbf16>
    %148 = stablehlo.reshape %arg16 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %149 = stablehlo.reshape %148 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %150 = stablehlo.convert %149 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %151 = stablehlo.broadcast_in_dim %150, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %152 = stablehlo.convert %147 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %153 = stablehlo.power %152, %cst_3 : tensor<1x7x3072xf32>
    %154 = stablehlo.reduce(%153 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %155 = stablehlo.multiply %154, %cst_2 : tensor<1x7xf32>
    %156 = stablehlo.reshape %155 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %157 = stablehlo.add %156, %20 : tensor<1x7x1xf32>
    %158 = stablehlo.rsqrt %157 : tensor<1x7x1xf32>
    %159 = stablehlo.reshape %158 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %160 = stablehlo.broadcast_in_dim %159, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %161 = stablehlo.multiply %152, %160 : tensor<1x7x3072xf32>
    %162 = stablehlo.convert %161 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %163 = stablehlo.convert %162 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %164 = stablehlo.multiply %151, %163 : tensor<1x7x3072xf32>
    %165 = stablehlo.convert %164 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %166 = stablehlo.reshape %165 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %167 = stablehlo.reshape %arg17 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %168 = stablehlo.reshape %167 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %169 = stablehlo.transpose %168, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %170 = stablehlo.dot_general %166, %169, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %171 = stablehlo.reshape %170 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %172 = stablehlo.convert %171 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %173 = stablehlo.logistic %171 : tensor<1x7x8192xbf16>
    %174 = stablehlo.convert %173 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %175 = stablehlo.multiply %172, %174 : tensor<1x7x8192xf32>
    %176 = stablehlo.convert %175 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %177 = stablehlo.convert %176 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %178 = stablehlo.reshape %arg3 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %179 = stablehlo.reshape %178 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %180 = stablehlo.transpose %179, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %181 = stablehlo.dot_general %166, %180, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %182 = stablehlo.reshape %181 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %183 = stablehlo.convert %182 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %184 = stablehlo.multiply %177, %183 : tensor<1x7x8192xf32>
    %185 = stablehlo.convert %184 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %186 = stablehlo.reshape %185 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %187 = stablehlo.reshape %arg2 : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %188 = stablehlo.reshape %187 : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %189 = stablehlo.transpose %188, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %190 = stablehlo.dot_general %186, %189, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %191 = stablehlo.reshape %190 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %192 = stablehlo.add %147, %191 : tensor<1x7x3072xbf16>
    %193 = stablehlo.convert %192 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %194 = stablehlo.power %193, %cst_3 : tensor<1x7x3072xf32>
    %195 = stablehlo.reduce(%194 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %196 = stablehlo.multiply %195, %cst_2 : tensor<1x7xf32>
    %197 = stablehlo.reshape %196 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %198 = stablehlo.add %197, %20 : tensor<1x7x1xf32>
    %199 = stablehlo.rsqrt %198 : tensor<1x7x1xf32>
    %200 = stablehlo.reshape %199 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %201 = stablehlo.broadcast_in_dim %200, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %202 = stablehlo.multiply %193, %201 : tensor<1x7x3072xf32>
    %203 = stablehlo.convert %202 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %204 = stablehlo.convert %203 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %205 = stablehlo.multiply %3, %204 : tensor<1x7x3072xf32>
    %206 = stablehlo.convert %205 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %207 = stablehlo.reshape %206 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %208 = stablehlo.reshape %arg0 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %209 = stablehlo.reshape %208 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %210 = stablehlo.transpose %209, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %211 = stablehlo.dot_general %207, %210, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %212 = stablehlo.reshape %211 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %212 : tensor<1x7x128256xbf16>
  }
}
module @SyncTensorsGraph.456 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  func.func @main(%arg0: tensor<128256x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___lm_head_weight"}, %arg1: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "auto_annotated_const_0"}, %arg2: tensor<3072x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg3: tensor<8192x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg4: tensor<3072x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg5: tensor<1024x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg6: tensor<1x7xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_0"}, %arg7: tensor<128256x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg8: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg9: tensor<7xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_1"}, %arg10: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "auto_annotated_const_1"}, %arg11: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "auto_annotated_const_2"}, %arg12: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "auto_annotated_const_3"}, %arg13: tensor<64xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg14: tensor<1024x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg15: tensor<3072x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg16: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg17: tensor<8192x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg18: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<7x1024xbf16>}> : () -> tensor<7x1024xbf16>
    %1 = "ttir.constant"() <{value = dense<1> : tensor<7x1024xi64>}> : () -> tensor<7x1024xi64>
    %2 = "ttir.constant"() <{value = dense<0> : tensor<7xi64>}> : () -> tensor<7xi64>
    %3 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x8x1024x128xbf16>}> : () -> tensor<1x8x1024x128xbf16>
    %4 = "ttir.constant"() <{value = dense<3.25520843E-4> : tensor<1x7xf32>}> : () -> tensor<1x7xf32>
    %5 = "ttir.constant"() <{value = dense<2.000000e+00> : tensor<1x7x3072xf32>}> : () -> tensor<1x7x3072xf32>
    %6 = "ttir.constant"() <{value = dense<0xFF800000> : tensor<f32>}> : () -> tensor<f32>
    %7 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>}> : () -> tensor<7xi64>
    %8 = "ttir.constant"() <{value = dense<"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tensor<1024xi64>}> : () -> tensor<1024xi64>
    %9 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<f32>}> : () -> tensor<f32>
    %10 = ttir.empty() : tensor<1x1x3072xbf16>
    %11 = "ttir.reshape"(%arg18, %10) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %12 = ttir.empty() : tensor<3072xbf16>
    %13 = "ttir.reshape"(%11, %12) <{shape = [3072 : i32]}> : (tensor<1x1x3072xbf16>, tensor<3072xbf16>) -> tensor<3072xbf16>
    %14 = ttir.empty() : tensor<3072xf32>
    %15 = "ttir.typecast"(%13, %14) <{conservative_folding = false}> : (tensor<3072xbf16>, tensor<3072xf32>) -> tensor<3072xf32>
    %16 = ttir.empty() : tensor<1x1x3072xf32>
    %17 = "ttir.reshape"(%15, %16) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %18 = ttir.empty() : tensor<1x7x3072xf32>
    %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 7, 1>}> : (tensor<1x1x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %20 = ttir.empty() : tensor<1x128256x3072xbf16>
    %21 = "ttir.reshape"(%arg7, %20) <{shape = [1 : i32, 128256 : i32, 3072 : i32]}> : (tensor<128256x3072xbf16>, tensor<1x128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %22 = ttir.empty() : tensor<128256x3072xbf16>
    %23 = "ttir.reshape"(%21, %22) <{shape = [128256 : i32, 3072 : i32]}> : (tensor<1x128256x3072xbf16>, tensor<128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %24 = ttir.empty() : tensor<1x1x7xi64>
    %25 = "ttir.reshape"(%arg6, %24) <{shape = [1 : i32, 1 : i32, 7 : i32]}> : (tensor<1x7xi64>, tensor<1x1x7xi64>) -> tensor<1x1x7xi64>
    %26 = ttir.empty() : tensor<7xi64>
    %27 = "ttir.reshape"(%25, %26) <{shape = [7 : i32]}> : (tensor<1x1x7xi64>, tensor<7xi64>) -> tensor<7xi64>
    %28 = ttir.empty() : tensor<7xui32>
    %29 = "ttir.typecast"(%27, %28) <{conservative_folding = false}> : (tensor<7xi64>, tensor<7xui32>) -> tensor<7xui32>
    %30 = ttir.empty() : tensor<7x3072xbf16>
    %31 = "ttir.gather"(%23, %29, %30) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 3072>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<128256x3072xbf16>, tensor<7xui32>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %32 = ttir.empty() : tensor<1x7x3072xbf16>
    %33 = "ttir.reshape"(%31, %32) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %34 = ttir.empty() : tensor<1x1x3072xbf16>
    %35 = "ttir.reshape"(%arg8, %34) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %36 = ttir.empty() : tensor<3072xbf16>
    %37 = "ttir.reshape"(%35, %36) <{shape = [3072 : i32]}> : (tensor<1x1x3072xbf16>, tensor<3072xbf16>) -> tensor<3072xbf16>
    %38 = ttir.empty() : tensor<3072xf32>
    %39 = "ttir.typecast"(%37, %38) <{conservative_folding = false}> : (tensor<3072xbf16>, tensor<3072xf32>) -> tensor<3072xf32>
    %40 = ttir.empty() : tensor<1x1x3072xf32>
    %41 = "ttir.reshape"(%39, %40) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %42 = ttir.empty() : tensor<1x7x3072xf32>
    %43 = "ttir.broadcast"(%41, %42) <{broadcast_dimensions = array<i64: 1, 7, 1>}> : (tensor<1x1x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %44 = ttir.empty() : tensor<1x7x3072xf32>
    %45 = "ttir.typecast"(%33, %44) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %46 = ttir.empty() : tensor<1x7x3072xf32>
    %47 = "ttir.pow"(%45, %5, %46) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %48 = ttir.empty() : tensor<1x7xf32>
    %49 = "ttir.sum"(%47, %48) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %50 = ttir.empty() : tensor<1x7xf32>
    %51 = "ttir.multiply"(%49, %4, %50) : (tensor<1x7xf32>, tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %52 = ttir.empty() : tensor<1x7x1xf32>
    %53 = "ttir.reshape"(%51, %52) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %54 = ttir.empty() : tensor<1x1x1xf32>
    %55 = "ttir.reshape"(%arg1, %54) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %56 = ttir.empty() : tensor<1x7x1xf32>
    %57 = "ttir.broadcast"(%55, %56) <{broadcast_dimensions = array<i64: 1, 7, 1>}> : (tensor<1x1x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %58 = ttir.empty() : tensor<1x7x1xf32>
    %59 = "ttir.add"(%53, %57, %58) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %60 = ttir.empty() : tensor<1x7x1xf32>
    %61 = "ttir.rsqrt"(%59, %60) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %62 = ttir.empty() : tensor<1x7xf32>
    %63 = "ttir.reshape"(%61, %62) <{shape = [1 : i32, 7 : i32]}> : (tensor<1x7x1xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %64 = ttir.empty() : tensor<1x7x1xf32>
    %65 = "ttir.reshape"(%63, %64) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %66 = ttir.empty() : tensor<1x7x3072xf32>
    %67 = "ttir.broadcast"(%65, %66) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x7x1xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %68 = ttir.empty() : tensor<1x7x3072xf32>
    %69 = "ttir.multiply"(%45, %67, %68) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %70 = ttir.empty() : tensor<1x7x3072xbf16>
    %71 = "ttir.typecast"(%69, %70) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %72 = ttir.empty() : tensor<1x7x3072xf32>
    %73 = "ttir.typecast"(%71, %72) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %74 = ttir.empty() : tensor<1x7x3072xf32>
    %75 = "ttir.multiply"(%43, %73, %74) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %76 = ttir.empty() : tensor<1x7x3072xbf16>
    %77 = "ttir.typecast"(%75, %76) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %78 = ttir.empty() : tensor<7x3072xbf16>
    %79 = "ttir.reshape"(%77, %78) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x7x3072xbf16>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %80 = ttir.empty() : tensor<1x3072x3072xbf16>
    %81 = "ttir.reshape"(%arg15, %80) <{shape = [1 : i32, 3072 : i32, 3072 : i32]}> : (tensor<3072x3072xbf16>, tensor<1x3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %82 = ttir.empty() : tensor<3072x3072xbf16>
    %83 = "ttir.reshape"(%81, %82) <{shape = [3072 : i32, 3072 : i32]}> : (tensor<1x3072x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %84 = ttir.empty() : tensor<3072x3072xbf16>
    %85 = "ttir.permute"(%83, %84) <{permutation = array<i64: 1, 0>}> : (tensor<3072x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %86 = "ttir.dot_general"(%79, %85) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %87 = ttir.empty() : tensor<1x7x24x128xbf16>
    %88 = "ttir.reshape"(%86, %87) <{shape = [1 : i32, 7 : i32, 24 : i32, 128 : i32]}> : (tensor<7x3072xbf16>, tensor<1x7x24x128xbf16>) -> tensor<1x7x24x128xbf16>
    %89 = ttir.empty() : tensor<1x24x7x128xbf16>
    %90 = "ttir.permute"(%88, %89) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x24x128xbf16>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %91 = ttir.empty() : tensor<1x24x7x128xf32>
    %92 = "ttir.typecast"(%90, %91) <{conservative_folding = false}> : (tensor<1x24x7x128xbf16>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %93 = ttir.empty() : tensor<1x1x64xf32>
    %94 = "ttir.reshape"(%arg13, %93) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xf32>, tensor<1x1x64xf32>) -> tensor<1x1x64xf32>
    %95 = ttir.empty() : tensor<1x64x1xf32>
    %96 = "ttir.reshape"(%94, %95) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<1x1x64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
    %97 = ttir.empty() : tensor<1x1x7xi64>
    %98 = "ttir.reshape"(%arg9, %97) <{shape = [1 : i32, 1 : i32, 7 : i32]}> : (tensor<7xi64>, tensor<1x1x7xi64>) -> tensor<1x1x7xi64>
    %99 = ttir.empty() : tensor<7xi64>
    %100 = "ttir.reshape"(%98, %99) <{shape = [7 : i32]}> : (tensor<1x1x7xi64>, tensor<7xi64>) -> tensor<7xi64>
    %101 = ttir.empty() : tensor<1x1x7xf32>
    %102 = "ttir.typecast"(%98, %101) <{conservative_folding = false}> : (tensor<1x1x7xi64>, tensor<1x1x7xf32>) -> tensor<1x1x7xf32>
    %103 = "ttir.dot_general"(%96, %102) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %104 = ttir.empty() : tensor<1x7x64xf32>
    %105 = "ttir.permute"(%103, %104) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x7xf32>, tensor<1x7x64xf32>) -> tensor<1x7x64xf32>
    %106 = ttir.empty() : tensor<1x7x128xf32>
    %107 = "ttir.concat"(%105, %105, %106) <{dim = 2 : si32}> : (tensor<1x7x64xf32>, tensor<1x7x64xf32>, tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
    %108 = ttir.empty() : tensor<1x7x128xf32>
    %109 = "ttir.cos"(%107, %108) : (tensor<1x7x128xf32>, tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
    %110 = ttir.empty() : tensor<1x7x128xbf16>
    %111 = "ttir.typecast"(%109, %110) <{conservative_folding = false}> : (tensor<1x7x128xf32>, tensor<1x7x128xbf16>) -> tensor<1x7x128xbf16>
    %112 = ttir.empty() : tensor<1x1x7x128xbf16>
    %113 = "ttir.reshape"(%111, %112) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xbf16>, tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %114 = ttir.empty() : tensor<1x1x7x128xf32>
    %115 = "ttir.typecast"(%113, %114) <{conservative_folding = false}> : (tensor<1x1x7x128xbf16>, tensor<1x1x7x128xf32>) -> tensor<1x1x7x128xf32>
    %116 = ttir.empty() : tensor<1x7x128xf32>
    %117 = "ttir.reshape"(%115, %116) <{shape = [1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x1x7x128xf32>, tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
    %118 = ttir.empty() : tensor<1x1x7x128xf32>
    %119 = "ttir.reshape"(%117, %118) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32>, tensor<1x1x7x128xf32>) -> tensor<1x1x7x128xf32>
    %120 = ttir.empty() : tensor<1x24x7x128xf32>
    %121 = "ttir.broadcast"(%119, %120) <{broadcast_dimensions = array<i64: 1, 24, 1, 1>}> : (tensor<1x1x7x128xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %122 = ttir.empty() : tensor<1x24x7x128xf32>
    %123 = "ttir.multiply"(%92, %121, %122) : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %124 = ttir.empty() : tensor<1x24x7x128xbf16>
    %125 = "ttir.typecast"(%123, %124) <{conservative_folding = false}> : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %126 = ttir.empty() : tensor<1x24x7x64xbf16>
    %127 = "ttir.slice_static"(%90, %126) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 24 : i32, 7 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x7x128xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x64xbf16>
    %128 = ttir.empty() : tensor<1x24x7x64xbf16>
    %129 = "ttir.neg"(%127, %128) : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x64xbf16>
    %130 = ttir.empty() : tensor<1x24x7x64xbf16>
    %131 = "ttir.slice_static"(%90, %130) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 24 : i32, 7 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x7x128xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x64xbf16>
    %132 = ttir.empty() : tensor<1x24x7x128xbf16>
    %133 = "ttir.concat"(%129, %131, %132) <{dim = 3 : si32}> : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %134 = ttir.empty() : tensor<1x24x7x128xf32>
    %135 = "ttir.typecast"(%133, %134) <{conservative_folding = false}> : (tensor<1x24x7x128xbf16>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %136 = ttir.empty() : tensor<1x7x128xf32>
    %137 = "ttir.sin"(%107, %136) : (tensor<1x7x128xf32>, tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
    %138 = ttir.empty() : tensor<1x7x128xbf16>
    %139 = "ttir.typecast"(%137, %138) <{conservative_folding = false}> : (tensor<1x7x128xf32>, tensor<1x7x128xbf16>) -> tensor<1x7x128xbf16>
    %140 = ttir.empty() : tensor<1x1x7x128xbf16>
    %141 = "ttir.reshape"(%139, %140) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xbf16>, tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %142 = ttir.empty() : tensor<1x1x7x128xf32>
    %143 = "ttir.typecast"(%141, %142) <{conservative_folding = false}> : (tensor<1x1x7x128xbf16>, tensor<1x1x7x128xf32>) -> tensor<1x1x7x128xf32>
    %144 = ttir.empty() : tensor<1x7x128xf32>
    %145 = "ttir.reshape"(%143, %144) <{shape = [1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x1x7x128xf32>, tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
    %146 = ttir.empty() : tensor<1x1x7x128xf32>
    %147 = "ttir.reshape"(%145, %146) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32>, tensor<1x1x7x128xf32>) -> tensor<1x1x7x128xf32>
    %148 = ttir.empty() : tensor<1x24x7x128xf32>
    %149 = "ttir.broadcast"(%147, %148) <{broadcast_dimensions = array<i64: 1, 24, 1, 1>}> : (tensor<1x1x7x128xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %150 = ttir.empty() : tensor<1x24x7x128xf32>
    %151 = "ttir.multiply"(%135, %149, %150) : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %152 = ttir.empty() : tensor<1x24x7x128xbf16>
    %153 = "ttir.typecast"(%151, %152) <{conservative_folding = false}> : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %154 = ttir.empty() : tensor<1x24x7x128xbf16>
    %155 = "ttir.add"(%125, %153, %154) : (tensor<1x24x7x128xbf16>, tensor<1x24x7x128xbf16>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %156 = ttir.empty() : tensor<24x7x128xbf16>
    %157 = "ttir.reshape"(%155, %156) <{shape = [24 : i32, 7 : i32, 128 : i32]}> : (tensor<1x24x7x128xbf16>, tensor<24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %158 = ttir.empty() : tensor<7xi1>
    %159 = "ttir.lt"(%100, %2, %158) : (tensor<7xi64>, tensor<7xi64>, tensor<7xi1>) -> tensor<7xi1>
    %160 = ttir.empty() : tensor<1xi64>
    %161 = "ttir.reshape"(%arg10, %160) <{shape = [1 : i32]}> : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %162 = ttir.empty() : tensor<7xi64>
    %163 = "ttir.broadcast"(%161, %162) <{broadcast_dimensions = array<i64: 7>}> : (tensor<1xi64>, tensor<7xi64>) -> tensor<7xi64>
    %164 = ttir.empty() : tensor<7xi64>
    %165 = "ttir.add"(%100, %163, %164) : (tensor<7xi64>, tensor<7xi64>, tensor<7xi64>) -> tensor<7xi64>
    %166 = ttir.empty() : tensor<7xi64>
    %167 = "ttir.where"(%159, %165, %100, %166) : (tensor<7xi1>, tensor<7xi64>, tensor<7xi64>, tensor<7xi64>) -> tensor<7xi64>
    %168 = ttir.empty() : tensor<7x1xi64>
    %169 = "ttir.reshape"(%167, %168) <{shape = [7 : i32, 1 : i32]}> : (tensor<7xi64>, tensor<7x1xi64>) -> tensor<7x1xi64>
    %170 = ttir.empty() : tensor<1x1024x3072xbf16>
    %171 = "ttir.reshape"(%arg14, %170) <{shape = [1 : i32, 1024 : i32, 3072 : i32]}> : (tensor<1024x3072xbf16>, tensor<1x1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %172 = ttir.empty() : tensor<1024x3072xbf16>
    %173 = "ttir.reshape"(%171, %172) <{shape = [1024 : i32, 3072 : i32]}> : (tensor<1x1024x3072xbf16>, tensor<1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %174 = ttir.empty() : tensor<3072x1024xbf16>
    %175 = "ttir.permute"(%173, %174) <{permutation = array<i64: 1, 0>}> : (tensor<1024x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<3072x1024xbf16>
    %176 = "ttir.dot_general"(%79, %175) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %177 = ttir.empty() : tensor<1x7x8x128xbf16>
    %178 = "ttir.reshape"(%176, %177) <{shape = [1 : i32, 7 : i32, 8 : i32, 128 : i32]}> : (tensor<7x1024xbf16>, tensor<1x7x8x128xbf16>) -> tensor<1x7x8x128xbf16>
    %179 = ttir.empty() : tensor<1x8x7x128xbf16>
    %180 = "ttir.permute"(%178, %179) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x8x128xbf16>, tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %181 = ttir.empty() : tensor<1x8x7x128xf32>
    %182 = "ttir.typecast"(%180, %181) <{conservative_folding = false}> : (tensor<1x8x7x128xbf16>, tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xf32>
    %183 = ttir.empty() : tensor<1x1x7x128xf32>
    %184 = "ttir.reshape"(%117, %183) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32>, tensor<1x1x7x128xf32>) -> tensor<1x1x7x128xf32>
    %185 = ttir.empty() : tensor<1x8x7x128xf32>
    %186 = "ttir.broadcast"(%184, %185) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x7x128xf32>, tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xf32>
    %187 = ttir.empty() : tensor<1x8x7x128xf32>
    %188 = "ttir.multiply"(%182, %186, %187) : (tensor<1x8x7x128xf32>, tensor<1x8x7x128xf32>, tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xf32>
    %189 = ttir.empty() : tensor<1x8x7x128xbf16>
    %190 = "ttir.typecast"(%188, %189) <{conservative_folding = false}> : (tensor<1x8x7x128xf32>, tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %191 = ttir.empty() : tensor<1x8x7x64xbf16>
    %192 = "ttir.slice_static"(%180, %191) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 8 : i32, 7 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x7x128xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x64xbf16>
    %193 = ttir.empty() : tensor<1x8x7x64xbf16>
    %194 = "ttir.neg"(%192, %193) : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x64xbf16>
    %195 = ttir.empty() : tensor<1x8x7x64xbf16>
    %196 = "ttir.slice_static"(%180, %195) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 7 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x7x128xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x64xbf16>
    %197 = ttir.empty() : tensor<1x8x7x128xbf16>
    %198 = "ttir.concat"(%194, %196, %197) <{dim = 3 : si32}> : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>, tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %199 = ttir.empty() : tensor<1x8x7x128xf32>
    %200 = "ttir.typecast"(%198, %199) <{conservative_folding = false}> : (tensor<1x8x7x128xbf16>, tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xf32>
    %201 = ttir.empty() : tensor<1x1x7x128xf32>
    %202 = "ttir.reshape"(%145, %201) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32>, tensor<1x1x7x128xf32>) -> tensor<1x1x7x128xf32>
    %203 = ttir.empty() : tensor<1x8x7x128xf32>
    %204 = "ttir.broadcast"(%202, %203) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x7x128xf32>, tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xf32>
    %205 = ttir.empty() : tensor<1x8x7x128xf32>
    %206 = "ttir.multiply"(%200, %204, %205) : (tensor<1x8x7x128xf32>, tensor<1x8x7x128xf32>, tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xf32>
    %207 = ttir.empty() : tensor<1x8x7x128xbf16>
    %208 = "ttir.typecast"(%206, %207) <{conservative_folding = false}> : (tensor<1x8x7x128xf32>, tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %209 = ttir.empty() : tensor<1x8x7x128xbf16>
    %210 = "ttir.add"(%190, %208, %209) : (tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %211 = ttir.empty() : tensor<1x8x1024x128xbf16>
    %212 = "ttir.scatter"(%3, %169, %210, %211) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x8x1024x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>, tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %213 = ttir.empty() : tensor<1x8x1x1024x128xbf16>
    %214 = "ttir.reshape"(%212, %213) <{shape = [1 : i32, 8 : i32, 1 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x8x1024x128xbf16>, tensor<1x8x1x1024x128xbf16>) -> tensor<1x8x1x1024x128xbf16>
    %215 = ttir.empty() : tensor<1x8x3x1024x128xbf16>
    %216 = "ttir.broadcast"(%214, %215) <{broadcast_dimensions = array<i64: 1, 1, 3, 1, 1>}> : (tensor<1x8x1x1024x128xbf16>, tensor<1x8x3x1024x128xbf16>) -> tensor<1x8x3x1024x128xbf16>
    %217 = ttir.empty() : tensor<1x24x1024x128xbf16>
    %218 = "ttir.reshape"(%216, %217) <{shape = [1 : i32, 24 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x8x3x1024x128xbf16>, tensor<1x24x1024x128xbf16>) -> tensor<1x24x1024x128xbf16>
    %219 = ttir.empty() : tensor<1x24x128x1024xbf16>
    %220 = "ttir.permute"(%218, %219) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x24x1024x128xbf16>, tensor<1x24x128x1024xbf16>) -> tensor<1x24x128x1024xbf16>
    %221 = ttir.empty() : tensor<24x128x1024xbf16>
    %222 = "ttir.reshape"(%220, %221) <{shape = [24 : i32, 128 : i32, 1024 : i32]}> : (tensor<1x24x128x1024xbf16>, tensor<24x128x1024xbf16>) -> tensor<24x128x1024xbf16>
    %223 = "ttir.dot_general"(%157, %222) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<24x7x128xbf16>, tensor<24x128x1024xbf16>) -> tensor<24x7x1024xbf16>
    %224 = ttir.empty() : tensor<1x24x7x1024xbf16>
    %225 = "ttir.reshape"(%223, %224) <{shape = [1 : i32, 24 : i32, 7 : i32, 1024 : i32]}> : (tensor<24x7x1024xbf16>, tensor<1x24x7x1024xbf16>) -> tensor<1x24x7x1024xbf16>
    %226 = ttir.empty() : tensor<1x24x7x1024xf32>
    %227 = "ttir.typecast"(%225, %226) <{conservative_folding = false}> : (tensor<1x24x7x1024xbf16>, tensor<1x24x7x1024xf32>) -> tensor<1x24x7x1024xf32>
    %228 = ttir.empty() : tensor<1x1x1x1xf32>
    %229 = "ttir.reshape"(%arg12, %228) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %230 = ttir.empty() : tensor<1x24x7x1024xf32>
    %231 = "ttir.broadcast"(%229, %230) <{broadcast_dimensions = array<i64: 1, 24, 7, 1024>}> : (tensor<1x1x1x1xf32>, tensor<1x24x7x1024xf32>) -> tensor<1x24x7x1024xf32>
    %232 = ttir.empty() : tensor<1x24x7x1024xf32>
    %233 = "ttir.multiply"(%227, %231, %232) : (tensor<1x24x7x1024xf32>, tensor<1x24x7x1024xf32>, tensor<1x24x7x1024xf32>) -> tensor<1x24x7x1024xf32>
    %234 = ttir.empty() : tensor<1x24x7x1024xbf16>
    %235 = "ttir.typecast"(%233, %234) <{conservative_folding = false}> : (tensor<1x24x7x1024xf32>, tensor<1x24x7x1024xbf16>) -> tensor<1x24x7x1024xbf16>
    %236 = ttir.empty() : tensor<1x1024xi64>
    %237 = "ttir.reshape"(%8, %236) <{shape = [1 : i32, 1024 : i32]}> : (tensor<1024xi64>, tensor<1x1024xi64>) -> tensor<1x1024xi64>
    %238 = ttir.empty() : tensor<7x1024xi64>
    %239 = "ttir.broadcast"(%237, %238) <{broadcast_dimensions = array<i64: 7, 1>}> : (tensor<1x1024xi64>, tensor<7x1024xi64>) -> tensor<7x1024xi64>
    %240 = ttir.empty() : tensor<7x1xi64>
    %241 = "ttir.reshape"(%7, %240) <{shape = [7 : i32, 1 : i32]}> : (tensor<7xi64>, tensor<7x1xi64>) -> tensor<7x1xi64>
    %242 = ttir.empty() : tensor<7x1024xi64>
    %243 = "ttir.broadcast"(%241, %242) <{broadcast_dimensions = array<i64: 1, 1024>}> : (tensor<7x1xi64>, tensor<7x1024xi64>) -> tensor<7x1024xi64>
    %244 = ttir.empty() : tensor<7x1024xi64>
    %245 = "ttir.subtract"(%239, %243, %244) : (tensor<7x1024xi64>, tensor<7x1024xi64>, tensor<7x1024xi64>) -> tensor<7x1024xi64>
    %246 = ttir.empty() : tensor<7x1024xi1>
    %247 = "ttir.ge"(%245, %1, %246) : (tensor<7x1024xi64>, tensor<7x1024xi64>, tensor<7x1024xi1>) -> tensor<7x1024xi1>
    %248 = ttir.empty() : tensor<1x1xbf16>
    %249 = "ttir.reshape"(%arg11, %248) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %250 = ttir.empty() : tensor<7x1024xbf16>
    %251 = "ttir.broadcast"(%249, %250) <{broadcast_dimensions = array<i64: 7, 1024>}> : (tensor<1x1xbf16>, tensor<7x1024xbf16>) -> tensor<7x1024xbf16>
    %252 = ttir.empty() : tensor<7x1024xbf16>
    %253 = "ttir.where"(%247, %251, %0, %252) : (tensor<7x1024xi1>, tensor<7x1024xbf16>, tensor<7x1024xbf16>, tensor<7x1024xbf16>) -> tensor<7x1024xbf16>
    %254 = ttir.empty() : tensor<7x1024xf32>
    %255 = "ttir.typecast"(%253, %254) <{conservative_folding = false}> : (tensor<7x1024xbf16>, tensor<7x1024xf32>) -> tensor<7x1024xf32>
    %256 = ttir.empty() : tensor<7x1xi64>
    %257 = "ttir.reshape"(%100, %256) <{shape = [7 : i32, 1 : i32]}> : (tensor<7xi64>, tensor<7x1xi64>) -> tensor<7x1xi64>
    %258 = ttir.empty() : tensor<7x1024xi64>
    %259 = "ttir.broadcast"(%257, %258) <{broadcast_dimensions = array<i64: 1, 1024>}> : (tensor<7x1xi64>, tensor<7x1024xi64>) -> tensor<7x1024xi64>
    %260 = ttir.empty() : tensor<7x1024xi1>
    %261 = "ttir.gt"(%239, %259, %260) : (tensor<7x1024xi64>, tensor<7x1024xi64>, tensor<7x1024xi1>) -> tensor<7x1024xi1>
    %262 = ttir.empty() : tensor<7x1024xf32>
    %263 = "ttir.typecast"(%261, %262) <{conservative_folding = false}> : (tensor<7x1024xi1>, tensor<7x1024xf32>) -> tensor<7x1024xf32>
    %264 = ttir.empty() : tensor<7x1024xf32>
    %265 = "ttir.multiply"(%255, %263, %264) : (tensor<7x1024xf32>, tensor<7x1024xf32>, tensor<7x1024xf32>) -> tensor<7x1024xf32>
    %266 = ttir.empty() : tensor<7x1024xbf16>
    %267 = "ttir.typecast"(%265, %266) <{conservative_folding = false}> : (tensor<7x1024xf32>, tensor<7x1024xbf16>) -> tensor<7x1024xbf16>
    %268 = ttir.empty() : tensor<1x7x1024xbf16>
    %269 = "ttir.reshape"(%267, %268) <{shape = [1 : i32, 7 : i32, 1024 : i32]}> : (tensor<7x1024xbf16>, tensor<1x7x1024xbf16>) -> tensor<1x7x1024xbf16>
    %270 = ttir.empty() : tensor<1x1x7x1024xbf16>
    %271 = "ttir.reshape"(%269, %270) <{shape = [1 : i32, 1 : i32, 7 : i32, 1024 : i32]}> : (tensor<1x7x1024xbf16>, tensor<1x1x7x1024xbf16>) -> tensor<1x1x7x1024xbf16>
    %272 = ttir.empty() : tensor<1x24x7x1024xbf16>
    %273 = "ttir.broadcast"(%271, %272) <{broadcast_dimensions = array<i64: 1, 24, 1, 1>}> : (tensor<1x1x7x1024xbf16>, tensor<1x24x7x1024xbf16>) -> tensor<1x24x7x1024xbf16>
    %274 = ttir.empty() : tensor<1x24x7x1024xbf16>
    %275 = "ttir.add"(%235, %273, %274) : (tensor<1x24x7x1024xbf16>, tensor<1x24x7x1024xbf16>, tensor<1x24x7x1024xbf16>) -> tensor<1x24x7x1024xbf16>
    %276 = ttir.empty() : tensor<1x24x7x1024xf32>
    %277 = "ttir.typecast"(%275, %276) <{conservative_folding = false}> : (tensor<1x24x7x1024xbf16>, tensor<1x24x7x1024xf32>) -> tensor<1x24x7x1024xf32>
    %278 = ttir.empty() : tensor<1x24x7xf32>
    %279 = "ttir.max"(%277, %278) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x24x7x1024xf32>, tensor<1x24x7xf32>) -> tensor<1x24x7xf32>
    %280 = ttir.empty() : tensor<1x24x7x1xf32>
    %281 = "ttir.reshape"(%279, %280) <{shape = [1 : i32, 24 : i32, 7 : i32, 1 : i32]}> : (tensor<1x24x7xf32>, tensor<1x24x7x1xf32>) -> tensor<1x24x7x1xf32>
    %282 = ttir.empty() : tensor<1x24x7x1024xf32>
    %283 = "ttir.broadcast"(%281, %282) <{broadcast_dimensions = array<i64: 1, 1, 1, 1024>}> : (tensor<1x24x7x1xf32>, tensor<1x24x7x1024xf32>) -> tensor<1x24x7x1024xf32>
    %284 = ttir.empty() : tensor<1x24x7x1024xf32>
    %285 = "ttir.subtract"(%277, %283, %284) : (tensor<1x24x7x1024xf32>, tensor<1x24x7x1024xf32>, tensor<1x24x7x1024xf32>) -> tensor<1x24x7x1024xf32>
    %286 = ttir.empty() : tensor<1x24x7x1024xf32>
    %287 = "ttir.exp"(%285, %286) : (tensor<1x24x7x1024xf32>, tensor<1x24x7x1024xf32>) -> tensor<1x24x7x1024xf32>
    %288 = ttir.empty() : tensor<1x24x7xf32>
    %289 = "ttir.sum"(%287, %288) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x24x7x1024xf32>, tensor<1x24x7xf32>) -> tensor<1x24x7xf32>
    %290 = ttir.empty() : tensor<1x24x7x1xf32>
    %291 = "ttir.reshape"(%289, %290) <{shape = [1 : i32, 24 : i32, 7 : i32, 1 : i32]}> : (tensor<1x24x7xf32>, tensor<1x24x7x1xf32>) -> tensor<1x24x7x1xf32>
    %292 = ttir.empty() : tensor<1x24x7x1024xf32>
    %293 = "ttir.broadcast"(%291, %292) <{broadcast_dimensions = array<i64: 1, 1, 1, 1024>}> : (tensor<1x24x7x1xf32>, tensor<1x24x7x1024xf32>) -> tensor<1x24x7x1024xf32>
    %294 = ttir.empty() : tensor<1x24x7x1024xf32>
    %295 = "ttir.div"(%287, %293, %294) : (tensor<1x24x7x1024xf32>, tensor<1x24x7x1024xf32>, tensor<1x24x7x1024xf32>) -> tensor<1x24x7x1024xf32>
    %296 = ttir.empty() : tensor<1x24x7x1024xbf16>
    %297 = "ttir.typecast"(%295, %296) <{conservative_folding = false}> : (tensor<1x24x7x1024xf32>, tensor<1x24x7x1024xbf16>) -> tensor<1x24x7x1024xbf16>
    %298 = ttir.empty() : tensor<24x7x1024xbf16>
    %299 = "ttir.reshape"(%297, %298) <{shape = [24 : i32, 7 : i32, 1024 : i32]}> : (tensor<1x24x7x1024xbf16>, tensor<24x7x1024xbf16>) -> tensor<24x7x1024xbf16>
    %300 = ttir.empty() : tensor<1x1024x3072xbf16>
    %301 = "ttir.reshape"(%arg5, %300) <{shape = [1 : i32, 1024 : i32, 3072 : i32]}> : (tensor<1024x3072xbf16>, tensor<1x1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %302 = ttir.empty() : tensor<1024x3072xbf16>
    %303 = "ttir.reshape"(%301, %302) <{shape = [1024 : i32, 3072 : i32]}> : (tensor<1x1024x3072xbf16>, tensor<1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %304 = ttir.empty() : tensor<3072x1024xbf16>
    %305 = "ttir.permute"(%303, %304) <{permutation = array<i64: 1, 0>}> : (tensor<1024x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<3072x1024xbf16>
    %306 = "ttir.dot_general"(%79, %305) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %307 = ttir.empty() : tensor<1x7x8x128xbf16>
    %308 = "ttir.reshape"(%306, %307) <{shape = [1 : i32, 7 : i32, 8 : i32, 128 : i32]}> : (tensor<7x1024xbf16>, tensor<1x7x8x128xbf16>) -> tensor<1x7x8x128xbf16>
    %309 = ttir.empty() : tensor<1x8x7x128xbf16>
    %310 = "ttir.permute"(%308, %309) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x8x128xbf16>, tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %311 = ttir.empty() : tensor<1x8x1024x128xbf16>
    %312 = "ttir.scatter"(%3, %169, %310, %311) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x8x1024x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>, tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %313 = ttir.empty() : tensor<1x8x1x1024x128xbf16>
    %314 = "ttir.reshape"(%312, %313) <{shape = [1 : i32, 8 : i32, 1 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x8x1024x128xbf16>, tensor<1x8x1x1024x128xbf16>) -> tensor<1x8x1x1024x128xbf16>
    %315 = ttir.empty() : tensor<1x8x3x1024x128xbf16>
    %316 = "ttir.broadcast"(%314, %315) <{broadcast_dimensions = array<i64: 1, 1, 3, 1, 1>}> : (tensor<1x8x1x1024x128xbf16>, tensor<1x8x3x1024x128xbf16>) -> tensor<1x8x3x1024x128xbf16>
    %317 = ttir.empty() : tensor<24x1024x128xbf16>
    %318 = "ttir.reshape"(%316, %317) <{shape = [24 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x8x3x1024x128xbf16>, tensor<24x1024x128xbf16>) -> tensor<24x1024x128xbf16>
    %319 = "ttir.dot_general"(%299, %318) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<24x7x1024xbf16>, tensor<24x1024x128xbf16>) -> tensor<24x7x128xbf16>
    %320 = ttir.empty() : tensor<1x24x7x128xbf16>
    %321 = "ttir.reshape"(%319, %320) <{shape = [1 : i32, 24 : i32, 7 : i32, 128 : i32]}> : (tensor<24x7x128xbf16>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %322 = ttir.empty() : tensor<1x7x24x128xbf16>
    %323 = "ttir.permute"(%321, %322) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x24x7x128xbf16>, tensor<1x7x24x128xbf16>) -> tensor<1x7x24x128xbf16>
    %324 = ttir.empty() : tensor<7x3072xbf16>
    %325 = "ttir.reshape"(%323, %324) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x7x24x128xbf16>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %326 = ttir.empty() : tensor<1x3072x3072xbf16>
    %327 = "ttir.reshape"(%arg4, %326) <{shape = [1 : i32, 3072 : i32, 3072 : i32]}> : (tensor<3072x3072xbf16>, tensor<1x3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %328 = ttir.empty() : tensor<3072x3072xbf16>
    %329 = "ttir.reshape"(%327, %328) <{shape = [3072 : i32, 3072 : i32]}> : (tensor<1x3072x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %330 = ttir.empty() : tensor<3072x3072xbf16>
    %331 = "ttir.permute"(%329, %330) <{permutation = array<i64: 1, 0>}> : (tensor<3072x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %332 = "ttir.dot_general"(%325, %331) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %333 = ttir.empty() : tensor<1x7x3072xbf16>
    %334 = "ttir.reshape"(%332, %333) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %335 = ttir.empty() : tensor<1x7x3072xbf16>
    %336 = "ttir.add"(%33, %334, %335) : (tensor<1x7x3072xbf16>, tensor<1x7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %337 = ttir.empty() : tensor<1x1x3072xbf16>
    %338 = "ttir.reshape"(%arg16, %337) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %339 = ttir.empty() : tensor<3072xbf16>
    %340 = "ttir.reshape"(%338, %339) <{shape = [3072 : i32]}> : (tensor<1x1x3072xbf16>, tensor<3072xbf16>) -> tensor<3072xbf16>
    %341 = ttir.empty() : tensor<3072xf32>
    %342 = "ttir.typecast"(%340, %341) <{conservative_folding = false}> : (tensor<3072xbf16>, tensor<3072xf32>) -> tensor<3072xf32>
    %343 = ttir.empty() : tensor<1x1x3072xf32>
    %344 = "ttir.reshape"(%342, %343) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %345 = ttir.empty() : tensor<1x7x3072xf32>
    %346 = "ttir.broadcast"(%344, %345) <{broadcast_dimensions = array<i64: 1, 7, 1>}> : (tensor<1x1x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %347 = ttir.empty() : tensor<1x7x3072xf32>
    %348 = "ttir.typecast"(%336, %347) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %349 = ttir.empty() : tensor<1x7x3072xf32>
    %350 = "ttir.pow"(%348, %5, %349) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %351 = ttir.empty() : tensor<1x7xf32>
    %352 = "ttir.sum"(%350, %351) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %353 = ttir.empty() : tensor<1x7xf32>
    %354 = "ttir.multiply"(%352, %4, %353) : (tensor<1x7xf32>, tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %355 = ttir.empty() : tensor<1x7x1xf32>
    %356 = "ttir.reshape"(%354, %355) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %357 = ttir.empty() : tensor<1x7x1xf32>
    %358 = "ttir.add"(%356, %57, %357) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %359 = ttir.empty() : tensor<1x7x1xf32>
    %360 = "ttir.rsqrt"(%358, %359) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %361 = ttir.empty() : tensor<1x7xf32>
    %362 = "ttir.reshape"(%360, %361) <{shape = [1 : i32, 7 : i32]}> : (tensor<1x7x1xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %363 = ttir.empty() : tensor<1x7x1xf32>
    %364 = "ttir.reshape"(%362, %363) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %365 = ttir.empty() : tensor<1x7x3072xf32>
    %366 = "ttir.broadcast"(%364, %365) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x7x1xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %367 = ttir.empty() : tensor<1x7x3072xf32>
    %368 = "ttir.multiply"(%348, %366, %367) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %369 = ttir.empty() : tensor<1x7x3072xbf16>
    %370 = "ttir.typecast"(%368, %369) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %371 = ttir.empty() : tensor<1x7x3072xf32>
    %372 = "ttir.typecast"(%370, %371) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %373 = ttir.empty() : tensor<1x7x3072xf32>
    %374 = "ttir.multiply"(%346, %372, %373) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %375 = ttir.empty() : tensor<1x7x3072xbf16>
    %376 = "ttir.typecast"(%374, %375) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %377 = ttir.empty() : tensor<7x3072xbf16>
    %378 = "ttir.reshape"(%376, %377) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x7x3072xbf16>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %379 = ttir.empty() : tensor<1x8192x3072xbf16>
    %380 = "ttir.reshape"(%arg17, %379) <{shape = [1 : i32, 8192 : i32, 3072 : i32]}> : (tensor<8192x3072xbf16>, tensor<1x8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %381 = ttir.empty() : tensor<8192x3072xbf16>
    %382 = "ttir.reshape"(%380, %381) <{shape = [8192 : i32, 3072 : i32]}> : (tensor<1x8192x3072xbf16>, tensor<8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %383 = ttir.empty() : tensor<3072x8192xbf16>
    %384 = "ttir.permute"(%382, %383) <{permutation = array<i64: 1, 0>}> : (tensor<8192x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %385 = "ttir.dot_general"(%378, %384) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %386 = ttir.empty() : tensor<1x7x8192xbf16>
    %387 = "ttir.reshape"(%385, %386) <{shape = [1 : i32, 7 : i32, 8192 : i32]}> : (tensor<7x8192xbf16>, tensor<1x7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %388 = ttir.empty() : tensor<1x7x8192xf32>
    %389 = "ttir.typecast"(%387, %388) <{conservative_folding = false}> : (tensor<1x7x8192xbf16>, tensor<1x7x8192xf32>) -> tensor<1x7x8192xf32>
    %390 = ttir.empty() : tensor<1x7x8192xbf16>
    %391 = "ttir.sigmoid"(%387, %390) : (tensor<1x7x8192xbf16>, tensor<1x7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %392 = ttir.empty() : tensor<1x7x8192xf32>
    %393 = "ttir.typecast"(%391, %392) <{conservative_folding = false}> : (tensor<1x7x8192xbf16>, tensor<1x7x8192xf32>) -> tensor<1x7x8192xf32>
    %394 = ttir.empty() : tensor<1x7x8192xf32>
    %395 = "ttir.multiply"(%389, %393, %394) : (tensor<1x7x8192xf32>, tensor<1x7x8192xf32>, tensor<1x7x8192xf32>) -> tensor<1x7x8192xf32>
    %396 = ttir.empty() : tensor<1x7x8192xbf16>
    %397 = "ttir.typecast"(%395, %396) <{conservative_folding = false}> : (tensor<1x7x8192xf32>, tensor<1x7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %398 = ttir.empty() : tensor<1x7x8192xf32>
    %399 = "ttir.typecast"(%397, %398) <{conservative_folding = false}> : (tensor<1x7x8192xbf16>, tensor<1x7x8192xf32>) -> tensor<1x7x8192xf32>
    %400 = ttir.empty() : tensor<1x8192x3072xbf16>
    %401 = "ttir.reshape"(%arg3, %400) <{shape = [1 : i32, 8192 : i32, 3072 : i32]}> : (tensor<8192x3072xbf16>, tensor<1x8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %402 = ttir.empty() : tensor<8192x3072xbf16>
    %403 = "ttir.reshape"(%401, %402) <{shape = [8192 : i32, 3072 : i32]}> : (tensor<1x8192x3072xbf16>, tensor<8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %404 = ttir.empty() : tensor<3072x8192xbf16>
    %405 = "ttir.permute"(%403, %404) <{permutation = array<i64: 1, 0>}> : (tensor<8192x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %406 = "ttir.dot_general"(%378, %405) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %407 = ttir.empty() : tensor<1x7x8192xbf16>
    %408 = "ttir.reshape"(%406, %407) <{shape = [1 : i32, 7 : i32, 8192 : i32]}> : (tensor<7x8192xbf16>, tensor<1x7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %409 = ttir.empty() : tensor<1x7x8192xf32>
    %410 = "ttir.typecast"(%408, %409) <{conservative_folding = false}> : (tensor<1x7x8192xbf16>, tensor<1x7x8192xf32>) -> tensor<1x7x8192xf32>
    %411 = ttir.empty() : tensor<1x7x8192xf32>
    %412 = "ttir.multiply"(%399, %410, %411) : (tensor<1x7x8192xf32>, tensor<1x7x8192xf32>, tensor<1x7x8192xf32>) -> tensor<1x7x8192xf32>
    %413 = ttir.empty() : tensor<1x7x8192xbf16>
    %414 = "ttir.typecast"(%412, %413) <{conservative_folding = false}> : (tensor<1x7x8192xf32>, tensor<1x7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %415 = ttir.empty() : tensor<7x8192xbf16>
    %416 = "ttir.reshape"(%414, %415) <{shape = [7 : i32, 8192 : i32]}> : (tensor<1x7x8192xbf16>, tensor<7x8192xbf16>) -> tensor<7x8192xbf16>
    %417 = ttir.empty() : tensor<1x3072x8192xbf16>
    %418 = "ttir.reshape"(%arg2, %417) <{shape = [1 : i32, 3072 : i32, 8192 : i32]}> : (tensor<3072x8192xbf16>, tensor<1x3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %419 = ttir.empty() : tensor<3072x8192xbf16>
    %420 = "ttir.reshape"(%418, %419) <{shape = [3072 : i32, 8192 : i32]}> : (tensor<1x3072x8192xbf16>, tensor<3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %421 = ttir.empty() : tensor<8192x3072xbf16>
    %422 = "ttir.permute"(%420, %421) <{permutation = array<i64: 1, 0>}> : (tensor<3072x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %423 = "ttir.dot_general"(%416, %422) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %424 = ttir.empty() : tensor<1x7x3072xbf16>
    %425 = "ttir.reshape"(%423, %424) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %426 = ttir.empty() : tensor<1x7x3072xbf16>
    %427 = "ttir.add"(%336, %425, %426) : (tensor<1x7x3072xbf16>, tensor<1x7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %428 = ttir.empty() : tensor<1x7x3072xf32>
    %429 = "ttir.typecast"(%427, %428) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %430 = ttir.empty() : tensor<1x7x3072xf32>
    %431 = "ttir.pow"(%429, %5, %430) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %432 = ttir.empty() : tensor<1x7xf32>
    %433 = "ttir.sum"(%431, %432) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %434 = ttir.empty() : tensor<1x7xf32>
    %435 = "ttir.multiply"(%433, %4, %434) : (tensor<1x7xf32>, tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %436 = ttir.empty() : tensor<1x7x1xf32>
    %437 = "ttir.reshape"(%435, %436) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %438 = ttir.empty() : tensor<1x7x1xf32>
    %439 = "ttir.add"(%437, %57, %438) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %440 = ttir.empty() : tensor<1x7x1xf32>
    %441 = "ttir.rsqrt"(%439, %440) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %442 = ttir.empty() : tensor<1x7xf32>
    %443 = "ttir.reshape"(%441, %442) <{shape = [1 : i32, 7 : i32]}> : (tensor<1x7x1xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %444 = ttir.empty() : tensor<1x7x1xf32>
    %445 = "ttir.reshape"(%443, %444) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %446 = ttir.empty() : tensor<1x7x3072xf32>
    %447 = "ttir.broadcast"(%445, %446) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x7x1xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %448 = ttir.empty() : tensor<1x7x3072xf32>
    %449 = "ttir.multiply"(%429, %447, %448) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %450 = ttir.empty() : tensor<1x7x3072xbf16>
    %451 = "ttir.typecast"(%449, %450) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %452 = ttir.empty() : tensor<1x7x3072xf32>
    %453 = "ttir.typecast"(%451, %452) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %454 = ttir.empty() : tensor<1x7x3072xf32>
    %455 = "ttir.multiply"(%19, %453, %454) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %456 = ttir.empty() : tensor<1x7x3072xbf16>
    %457 = "ttir.typecast"(%455, %456) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %458 = ttir.empty() : tensor<7x3072xbf16>
    %459 = "ttir.reshape"(%457, %458) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x7x3072xbf16>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %460 = ttir.empty() : tensor<1x128256x3072xbf16>
    %461 = "ttir.reshape"(%arg0, %460) <{shape = [1 : i32, 128256 : i32, 3072 : i32]}> : (tensor<128256x3072xbf16>, tensor<1x128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %462 = ttir.empty() : tensor<128256x3072xbf16>
    %463 = "ttir.reshape"(%461, %462) <{shape = [128256 : i32, 3072 : i32]}> : (tensor<1x128256x3072xbf16>, tensor<128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %464 = ttir.empty() : tensor<3072x128256xbf16>
    %465 = "ttir.permute"(%463, %464) <{permutation = array<i64: 1, 0>}> : (tensor<128256x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<3072x128256xbf16>
    %466 = "ttir.dot_general"(%459, %465) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %467 = ttir.empty() : tensor<1x7x128256xbf16>
    %468 = "ttir.reshape"(%466, %467) <{shape = [1 : i32, 7 : i32, 128256 : i32]}> : (tensor<7x128256xbf16>, tensor<1x7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %468 : tensor<1x7x128256xbf16>
  }
}
loc("scatter.259"): error: failed to legalize operation 'ttir.fill_cache'
parameter devices before movement to xla {device(type='cpu')}
parameter devices after movement to xla {device(type='xla', index=0)}
Traceback (most recent call last):
  File "/localdev/jameszianxu/tt-xla/examples/pytorch/llama.py", line 93, in <module>
    llama()
  File "/localdev/jameszianxu/tt-xla/examples/pytorch/llama.py", line 79, in llama
    output:CausalLMOutputWithPast = model(**input_args)
                                    ^^^^^^^^^^^^^^^^^^^
  File "/localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1749, in _wrapped_call_impl
    return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 655, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/utils/generic.py", line 953, in wrapper
    @wraps(func)
  File "/localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/localdev/jameszianxu/tt-xla/python_package/tt_torch/backend/backend.py", line 87, in __call__
    torch_xla._XLAC._xla_sync_multi(list(output), self.devices, wait=False)
RuntimeError: Bad StatusOr access: INTERNAL: Error code: 13
