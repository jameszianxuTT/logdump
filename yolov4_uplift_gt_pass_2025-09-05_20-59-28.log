// -----// IR Dump Before UpdateGlobalToLocalShapesPass (update-global-to-local-shapes) ('builtin.module' operation) //----- //
module {
  sdy.mesh @mesh = <["default"=1, "batch"=2]>
  func.func @main(%arg0: tensor<32x3x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<64x32x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg2: tensor<64x64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg3: tensor<64x64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg4: tensor<32x64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg5: tensor<64x32x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg6: tensor<64x64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg7: tensor<64x128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg8: tensor<128x64x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg9: tensor<64x128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg10: tensor<64x128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg11: tensor<64x64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg12: tensor<64x64x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg13: tensor<64x64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg14: tensor<64x64x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg15: tensor<64x64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg16: tensor<128x128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg17: tensor<256x128x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg18: tensor<128x256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg19: tensor<128x256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg20: tensor<128x128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg21: tensor<128x128x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg22: tensor<128x128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg23: tensor<128x128x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg24: tensor<128x128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg25: tensor<128x128x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg26: tensor<128x128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg27: tensor<128x128x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg28: tensor<128x128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg29: tensor<128x128x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg30: tensor<128x128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg31: tensor<128x128x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg32: tensor<128x128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg33: tensor<128x128x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg34: tensor<128x128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg35: tensor<128x128x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg36: tensor<128x128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg37: tensor<256x256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg38: tensor<512x256x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg39: tensor<256x512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg40: tensor<256x512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg41: tensor<256x256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg42: tensor<256x256x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg43: tensor<256x256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg44: tensor<256x256x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg45: tensor<256x256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg46: tensor<256x256x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg47: tensor<256x256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg48: tensor<256x256x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg49: tensor<256x256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg50: tensor<256x256x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg51: tensor<256x256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg52: tensor<256x256x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg53: tensor<256x256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg54: tensor<256x256x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg55: tensor<256x256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg56: tensor<256x256x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg57: tensor<256x256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg58: tensor<512x512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg59: tensor<1024x512x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg60: tensor<512x1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg61: tensor<512x1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg62: tensor<512x512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg63: tensor<512x512x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg64: tensor<512x512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg65: tensor<512x512x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg66: tensor<512x512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg67: tensor<512x512x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg68: tensor<512x512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg69: tensor<512x512x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg70: tensor<512x512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg71: tensor<1024x1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg72: tensor<512x1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg73: tensor<1024x512x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg74: tensor<512x1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg75: tensor<512x2048x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg76: tensor<1024x512x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg77: tensor<512x1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg78: tensor<256x512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg79: tensor<256x512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg80: tensor<256x512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg81: tensor<512x256x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg82: tensor<256x512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg83: tensor<512x256x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg84: tensor<256x512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg85: tensor<128x256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg86: tensor<128x256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg87: tensor<128x256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg88: tensor<256x128x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg89: tensor<128x256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg90: tensor<256x128x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg91: tensor<128x256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg92: tensor<256x128x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg93: tensor<255x256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg94: tensor<255xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg95: tensor<256x128x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg96: tensor<256x512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg97: tensor<512x256x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg98: tensor<256x512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg99: tensor<512x256x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg100: tensor<256x512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg101: tensor<512x256x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg102: tensor<255x512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg103: tensor<255xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg104: tensor<512x256x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg105: tensor<512x1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg106: tensor<1024x512x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg107: tensor<512x1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg108: tensor<1024x512x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg109: tensor<512x1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg110: tensor<1024x512x3x3xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg111: tensor<255x1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg112: tensor<255xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg113: tensor<32x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg114: tensor<32x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg115: tensor<32x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg116: tensor<32x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg117: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg118: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg119: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg120: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg121: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg122: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg123: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg124: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg125: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg126: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg127: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg128: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg129: tensor<32x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg130: tensor<32x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg131: tensor<32x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg132: tensor<32x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg133: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg134: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg135: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg136: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg137: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg138: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg139: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg140: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg141: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg142: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg143: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg144: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg145: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg146: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg147: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg148: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg149: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg150: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg151: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg152: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg153: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg154: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg155: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg156: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg157: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg158: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg159: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg160: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg161: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg162: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg163: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg164: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg165: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg166: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg167: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg168: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg169: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg170: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg171: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg172: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg173: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg174: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg175: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg176: tensor<64x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg177: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg178: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg179: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg180: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg181: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg182: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg183: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg184: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg185: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg186: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg187: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg188: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg189: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg190: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg191: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg192: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg193: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg194: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg195: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg196: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg197: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg198: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg199: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg200: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg201: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg202: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg203: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg204: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg205: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg206: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg207: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg208: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg209: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg210: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg211: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg212: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg213: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg214: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg215: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg216: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg217: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg218: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg219: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg220: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg221: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg222: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg223: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg224: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg225: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg226: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg227: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg228: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg229: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg230: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg231: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg232: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg233: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg234: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg235: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg236: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg237: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg238: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg239: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg240: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg241: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg242: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg243: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg244: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg245: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg246: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg247: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg248: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg249: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg250: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg251: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg252: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg253: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg254: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg255: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg256: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg257: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg258: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg259: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg260: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg261: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg262: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg263: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg264: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg265: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg266: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg267: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg268: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg269: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg270: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg271: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg272: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg273: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg274: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg275: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg276: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg277: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg278: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg279: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg280: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg281: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg282: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg283: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg284: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg285: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg286: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg287: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg288: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg289: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg290: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg291: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg292: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg293: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg294: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg295: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg296: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg297: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg298: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg299: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg300: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg301: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg302: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg303: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg304: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg305: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg306: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg307: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg308: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg309: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg310: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg311: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg312: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg313: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg314: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg315: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg316: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg317: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg318: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg319: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg320: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg321: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg322: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg323: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg324: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg325: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg326: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg327: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg328: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg329: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg330: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg331: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg332: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg333: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg334: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg335: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg336: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg337: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg338: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg339: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg340: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg341: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg342: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg343: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg344: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg345: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg346: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg347: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg348: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg349: tensor<1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg350: tensor<1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg351: tensor<1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg352: tensor<1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg353: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg354: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg355: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg356: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg357: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg358: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg359: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg360: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg361: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg362: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg363: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg364: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg365: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg366: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg367: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg368: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg369: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg370: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg371: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg372: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg373: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg374: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg375: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg376: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg377: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg378: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg379: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg380: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg381: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg382: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg383: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg384: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg385: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg386: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg387: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg388: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg389: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg390: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg391: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg392: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg393: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg394: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg395: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg396: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg397: tensor<1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg398: tensor<1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg399: tensor<1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg400: tensor<1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg401: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg402: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg403: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg404: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg405: tensor<1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg406: tensor<1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg407: tensor<1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg408: tensor<1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg409: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg410: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg411: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg412: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg413: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg414: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg415: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg416: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg417: tensor<1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg418: tensor<1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg419: tensor<1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg420: tensor<1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg421: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg422: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg423: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg424: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg425: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg426: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg427: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg428: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg429: tensor<30xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg430: tensor<40xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg431: tensor<1024x15x30xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"batch", ?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg432: tensor<1024x20x40xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"batch", ?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg433: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg434: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg435: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg436: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg437: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg438: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg439: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg440: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg441: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg442: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg443: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg444: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg445: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg446: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg447: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg448: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg449: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg450: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg451: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg452: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg453: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg454: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg455: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg456: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg457: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg458: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg459: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg460: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg461: tensor<60xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg462: tensor<80xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg463: tensor<512x30x60xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"batch", ?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg464: tensor<512x40x80xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"batch", ?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg465: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg466: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg467: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg468: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg469: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg470: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg471: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg472: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg473: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg474: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg475: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg476: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg477: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg478: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg479: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg480: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg481: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg482: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg483: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg484: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg485: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg486: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg487: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg488: tensor<128x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg489: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg490: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg491: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg492: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg493: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg494: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg495: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg496: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg497: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg498: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg499: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg500: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg501: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg502: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg503: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg504: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg505: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg506: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg507: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg508: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg509: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg510: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg511: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg512: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg513: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg514: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg515: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg516: tensor<256x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg517: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg518: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg519: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg520: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg521: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg522: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg523: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg524: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg525: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg526: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg527: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg528: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg529: tensor<1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg530: tensor<1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg531: tensor<1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg532: tensor<1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg533: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg534: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg535: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg536: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg537: tensor<1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg538: tensor<1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg539: tensor<1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg540: tensor<1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg541: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg542: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg543: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg544: tensor<512x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg545: tensor<1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg546: tensor<1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg547: tensor<1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg548: tensor<1024x1x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg549: tensor<4x3x480x640xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"batch", ?}, {?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) -> (tensor<4x255x60x80xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"batch", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<4x255x30x40xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"batch", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<4x255x15x20xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"batch", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:3 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %arg16, %arg17, %arg18, %arg19, %arg20, %arg21, %arg22, %arg23, %arg24, %arg25, %arg26, %arg27, %arg28, %arg29, %arg30, %arg31, %arg32, %arg33, %arg34, %arg35, %arg36, %arg37, %arg38, %arg39, %arg40, %arg41, %arg42, %arg43, %arg44, %arg45, %arg46, %arg47, %arg48, %arg49, %arg50, %arg51, %arg52, %arg53, %arg54, %arg55, %arg56, %arg57, %arg58, %arg59, %arg60, %arg61, %arg62, %arg63, %arg64, %arg65, %arg66, %arg67, %arg68, %arg69, %arg70, %arg71, %arg72, %arg73, %arg74, %arg75, %arg76, %arg77, %arg78, %arg79, %arg80, %arg81, %arg82, %arg83, %arg84, %arg85, %arg86, %arg87, %arg88, %arg89, %arg90, %arg91, %arg92, %arg93, %arg94, %arg95, %arg96, %arg97, %arg98, %arg99, %arg100, %arg101, %arg102, %arg103, %arg104, %arg105, %arg106, %arg107, %arg108, %arg109, %arg110, %arg111, %arg112, %arg113, %arg114, %arg115, %arg116, %arg117, %arg118, %arg119, %arg120, %arg121, %arg122, %arg123, %arg124, %arg125, %arg126, %arg127, %arg128, %arg129, %arg130, %arg131, %arg132, %arg133, %arg134, %arg135, %arg136, %arg137, %arg138, %arg139, %arg140, %arg141, %arg142, %arg143, %arg144, %arg145, %arg146, %arg147, %arg148, %arg149, %arg150, %arg151, %arg152, %arg153, %arg154, %arg155, %arg156, %arg157, %arg158, %arg159, %arg160, %arg161, %arg162, %arg163, %arg164, %arg165, %arg166, %arg167, %arg168, %arg169, %arg170, %arg171, %arg172, %arg173, %arg174, %arg175, %arg176, %arg177, %arg178, %arg179, %arg180, %arg181, %arg182, %arg183, %arg184, %arg185, %arg186, %arg187, %arg188, %arg189, %arg190, %arg191, %arg192, %arg193, %arg194, %arg195, %arg196, %arg197, %arg198, %arg199, %arg200, %arg201, %arg202, %arg203, %arg204, %arg205, %arg206, %arg207, %arg208, %arg209, %arg210, %arg211, %arg212, %arg213, %arg214, %arg215, %arg216, %arg217, %arg218, %arg219, %arg220, %arg221, %arg222, %arg223, %arg224, %arg225, %arg226, %arg227, %arg228, %arg229, %arg230, %arg231, %arg232, %arg233, %arg234, %arg235, %arg236, %arg237, %arg238, %arg239, %arg240, %arg241, %arg242, %arg243, %arg244, %arg245, %arg246, %arg247, %arg248, %arg249, %arg250, %arg251, %arg252, %arg253, %arg254, %arg255, %arg256, %arg257, %arg258, %arg259, %arg260, %arg261, %arg262, %arg263, %arg264, %arg265, %arg266, %arg267, %arg268, %arg269, %arg270, %arg271, %arg272, %arg273, %arg274, %arg275, %arg276, %arg277, %arg278, %arg279, %arg280, %arg281, %arg282, %arg283, %arg284, %arg285, %arg286, %arg287, %arg288, %arg289, %arg290, %arg291, %arg292, %arg293, %arg294, %arg295, %arg296, %arg297, %arg298, %arg299, %arg300, %arg301, %arg302, %arg303, %arg304, %arg305, %arg306, %arg307, %arg308, %arg309, %arg310, %arg311, %arg312, %arg313, %arg314, %arg315, %arg316, %arg317, %arg318, %arg319, %arg320, %arg321, %arg322, %arg323, %arg324, %arg325, %arg326, %arg327, %arg328, %arg329, %arg330, %arg331, %arg332, %arg333, %arg334, %arg335, %arg336, %arg337, %arg338, %arg339, %arg340, %arg341, %arg342, %arg343, %arg344, %arg345, %arg346, %arg347, %arg348, %arg349, %arg350, %arg351, %arg352, %arg353, %arg354, %arg355, %arg356, %arg357, %arg358, %arg359, %arg360, %arg361, %arg362, %arg363, %arg364, %arg365, %arg366, %arg367, %arg368, %arg369, %arg370, %arg371, %arg372, %arg373, %arg374, %arg375, %arg376, %arg377, %arg378, %arg379, %arg380, %arg381, %arg382, %arg383, %arg384, %arg385, %arg386, %arg387, %arg388, %arg389, %arg390, %arg391, %arg392, %arg393, %arg394, %arg395, %arg396, %arg397, %arg398, %arg399, %arg400, %arg401, %arg402, %arg403, %arg404, %arg405, %arg406, %arg407, %arg408, %arg409, %arg410, %arg411, %arg412, %arg413, %arg414, %arg415, %arg416, %arg417, %arg418, %arg419, %arg420, %arg421, %arg422, %arg423, %arg424, %arg425, %arg426, %arg427, %arg428, %arg429, %arg430, %arg431, %arg432, %arg433, %arg434, %arg435, %arg436, %arg437, %arg438, %arg439, %arg440, %arg441, %arg442, %arg443, %arg444, %arg445, %arg446, %arg447, %arg448, %arg449, %arg450, %arg451, %arg452, %arg453, %arg454, %arg455, %arg456, %arg457, %arg458, %arg459, %arg460, %arg461, %arg462, %arg463, %arg464, %arg465, %arg466, %arg467, %arg468, %arg469, %arg470, %arg471, %arg472, %arg473, %arg474, %arg475, %arg476, %arg477, %arg478, %arg479, %arg480, %arg481, %arg482, %arg483, %arg484, %arg485, %arg486, %arg487, %arg488, %arg489, %arg490, %arg491, %arg492, %arg493, %arg494, %arg495, %arg496, %arg497, %arg498, %arg499, %arg500, %arg501, %arg502, %arg503, %arg504, %arg505, %arg506, %arg507, %arg508, %arg509, %arg510, %arg511, %arg512, %arg513, %arg514, %arg515, %arg516, %arg517, %arg518, %arg519, %arg520, %arg521, %arg522, %arg523, %arg524, %arg525, %arg526, %arg527, %arg528, %arg529, %arg530, %arg531, %arg532, %arg533, %arg534, %arg535, %arg536, %arg537, %arg538, %arg539, %arg540, %arg541, %arg542, %arg543, %arg544, %arg545, %arg546, %arg547, %arg548, %arg549) in_shardings=[<@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}]>, <@mesh, [{?}]>, <@mesh, [{"batch", ?}, {?}, {?}]>, <@mesh, [{"batch", ?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}]>, <@mesh, [{?}]>, <@mesh, [{"batch", ?}, {?}, {?}]>, <@mesh, [{"batch", ?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{"batch", ?}, {?}, {?}, {?}]>] out_shardings=[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>, <@mesh, [{"batch", ?}, {?}, {?}, {?}]>, <@mesh, [{"batch", ?}, {?}, {?}, {?}]>] manual_axes={} (%arg550: tensor<32x3x3x3xbf16>, %arg551: tensor<64x32x3x3xbf16>, %arg552: tensor<64x64x1x1xbf16>, %arg553: tensor<64x64x1x1xbf16>, %arg554: tensor<32x64x1x1xbf16>, %arg555: tensor<64x32x3x3xbf16>, %arg556: tensor<64x64x1x1xbf16>, %arg557: tensor<64x128x1x1xbf16>, %arg558: tensor<128x64x3x3xbf16>, %arg559: tensor<64x128x1x1xbf16>, %arg560: tensor<64x128x1x1xbf16>, %arg561: tensor<64x64x1x1xbf16>, %arg562: tensor<64x64x3x3xbf16>, %arg563: tensor<64x64x1x1xbf16>, %arg564: tensor<64x64x3x3xbf16>, %arg565: tensor<64x64x1x1xbf16>, %arg566: tensor<128x128x1x1xbf16>, %arg567: tensor<256x128x3x3xbf16>, %arg568: tensor<128x256x1x1xbf16>, %arg569: tensor<128x256x1x1xbf16>, %arg570: tensor<128x128x1x1xbf16>, %arg571: tensor<128x128x3x3xbf16>, %arg572: tensor<128x128x1x1xbf16>, %arg573: tensor<128x128x3x3xbf16>, %arg574: tensor<128x128x1x1xbf16>, %arg575: tensor<128x128x3x3xbf16>, %arg576: tensor<128x128x1x1xbf16>, %arg577: tensor<128x128x3x3xbf16>, %arg578: tensor<128x128x1x1xbf16>, %arg579: tensor<128x128x3x3xbf16>, %arg580: tensor<128x128x1x1xbf16>, %arg581: tensor<128x128x3x3xbf16>, %arg582: tensor<128x128x1x1xbf16>, %arg583: tensor<128x128x3x3xbf16>, %arg584: tensor<128x128x1x1xbf16>, %arg585: tensor<128x128x3x3xbf16>, %arg586: tensor<128x128x1x1xbf16>, %arg587: tensor<256x256x1x1xbf16>, %arg588: tensor<512x256x3x3xbf16>, %arg589: tensor<256x512x1x1xbf16>, %arg590: tensor<256x512x1x1xbf16>, %arg591: tensor<256x256x1x1xbf16>, %arg592: tensor<256x256x3x3xbf16>, %arg593: tensor<256x256x1x1xbf16>, %arg594: tensor<256x256x3x3xbf16>, %arg595: tensor<256x256x1x1xbf16>, %arg596: tensor<256x256x3x3xbf16>, %arg597: tensor<256x256x1x1xbf16>, %arg598: tensor<256x256x3x3xbf16>, %arg599: tensor<256x256x1x1xbf16>, %arg600: tensor<256x256x3x3xbf16>, %arg601: tensor<256x256x1x1xbf16>, %arg602: tensor<256x256x3x3xbf16>, %arg603: tensor<256x256x1x1xbf16>, %arg604: tensor<256x256x3x3xbf16>, %arg605: tensor<256x256x1x1xbf16>, %arg606: tensor<256x256x3x3xbf16>, %arg607: tensor<256x256x1x1xbf16>, %arg608: tensor<512x512x1x1xbf16>, %arg609: tensor<1024x512x3x3xbf16>, %arg610: tensor<512x1024x1x1xbf16>, %arg611: tensor<512x1024x1x1xbf16>, %arg612: tensor<512x512x1x1xbf16>, %arg613: tensor<512x512x3x3xbf16>, %arg614: tensor<512x512x1x1xbf16>, %arg615: tensor<512x512x3x3xbf16>, %arg616: tensor<512x512x1x1xbf16>, %arg617: tensor<512x512x3x3xbf16>, %arg618: tensor<512x512x1x1xbf16>, %arg619: tensor<512x512x3x3xbf16>, %arg620: tensor<512x512x1x1xbf16>, %arg621: tensor<1024x1024x1x1xbf16>, %arg622: tensor<512x1024x1x1xbf16>, %arg623: tensor<1024x512x3x3xbf16>, %arg624: tensor<512x1024x1x1xbf16>, %arg625: tensor<512x2048x1x1xbf16>, %arg626: tensor<1024x512x3x3xbf16>, %arg627: tensor<512x1024x1x1xbf16>, %arg628: tensor<256x512x1x1xbf16>, %arg629: tensor<256x512x1x1xbf16>, %arg630: tensor<256x512x1x1xbf16>, %arg631: tensor<512x256x3x3xbf16>, %arg632: tensor<256x512x1x1xbf16>, %arg633: tensor<512x256x3x3xbf16>, %arg634: tensor<256x512x1x1xbf16>, %arg635: tensor<128x256x1x1xbf16>, %arg636: tensor<128x256x1x1xbf16>, %arg637: tensor<128x256x1x1xbf16>, %arg638: tensor<256x128x3x3xbf16>, %arg639: tensor<128x256x1x1xbf16>, %arg640: tensor<256x128x3x3xbf16>, %arg641: tensor<128x256x1x1xbf16>, %arg642: tensor<256x128x3x3xbf16>, %arg643: tensor<255x256x1x1xbf16>, %arg644: tensor<255xbf16>, %arg645: tensor<256x128x3x3xbf16>, %arg646: tensor<256x512x1x1xbf16>, %arg647: tensor<512x256x3x3xbf16>, %arg648: tensor<256x512x1x1xbf16>, %arg649: tensor<512x256x3x3xbf16>, %arg650: tensor<256x512x1x1xbf16>, %arg651: tensor<512x256x3x3xbf16>, %arg652: tensor<255x512x1x1xbf16>, %arg653: tensor<255xbf16>, %arg654: tensor<512x256x3x3xbf16>, %arg655: tensor<512x1024x1x1xbf16>, %arg656: tensor<1024x512x3x3xbf16>, %arg657: tensor<512x1024x1x1xbf16>, %arg658: tensor<1024x512x3x3xbf16>, %arg659: tensor<512x1024x1x1xbf16>, %arg660: tensor<1024x512x3x3xbf16>, %arg661: tensor<255x1024x1x1xbf16>, %arg662: tensor<255xbf16>, %arg663: tensor<32x1x1xbf16>, %arg664: tensor<32x1x1xbf16>, %arg665: tensor<32x1x1xbf16>, %arg666: tensor<32x1x1xbf16>, %arg667: tensor<64x1x1xbf16>, %arg668: tensor<64x1x1xbf16>, %arg669: tensor<64x1x1xbf16>, %arg670: tensor<64x1x1xbf16>, %arg671: tensor<64x1x1xbf16>, %arg672: tensor<64x1x1xbf16>, %arg673: tensor<64x1x1xbf16>, %arg674: tensor<64x1x1xbf16>, %arg675: tensor<64x1x1xbf16>, %arg676: tensor<64x1x1xbf16>, %arg677: tensor<64x1x1xbf16>, %arg678: tensor<64x1x1xbf16>, %arg679: tensor<32x1x1xbf16>, %arg680: tensor<32x1x1xbf16>, %arg681: tensor<32x1x1xbf16>, %arg682: tensor<32x1x1xbf16>, %arg683: tensor<64x1x1xbf16>, %arg684: tensor<64x1x1xbf16>, %arg685: tensor<64x1x1xbf16>, %arg686: tensor<64x1x1xbf16>, %arg687: tensor<64x1x1xbf16>, %arg688: tensor<64x1x1xbf16>, %arg689: tensor<64x1x1xbf16>, %arg690: tensor<64x1x1xbf16>, %arg691: tensor<64x1x1xbf16>, %arg692: tensor<64x1x1xbf16>, %arg693: tensor<64x1x1xbf16>, %arg694: tensor<64x1x1xbf16>, %arg695: tensor<128x1x1xbf16>, %arg696: tensor<128x1x1xbf16>, %arg697: tensor<128x1x1xbf16>, %arg698: tensor<128x1x1xbf16>, %arg699: tensor<64x1x1xbf16>, %arg700: tensor<64x1x1xbf16>, %arg701: tensor<64x1x1xbf16>, %arg702: tensor<64x1x1xbf16>, %arg703: tensor<64x1x1xbf16>, %arg704: tensor<64x1x1xbf16>, %arg705: tensor<64x1x1xbf16>, %arg706: tensor<64x1x1xbf16>, %arg707: tensor<64x1x1xbf16>, %arg708: tensor<64x1x1xbf16>, %arg709: tensor<64x1x1xbf16>, %arg710: tensor<64x1x1xbf16>, %arg711: tensor<64x1x1xbf16>, %arg712: tensor<64x1x1xbf16>, %arg713: tensor<64x1x1xbf16>, %arg714: tensor<64x1x1xbf16>, %arg715: tensor<64x1x1xbf16>, %arg716: tensor<64x1x1xbf16>, %arg717: tensor<64x1x1xbf16>, %arg718: tensor<64x1x1xbf16>, %arg719: tensor<64x1x1xbf16>, %arg720: tensor<64x1x1xbf16>, %arg721: tensor<64x1x1xbf16>, %arg722: tensor<64x1x1xbf16>, %arg723: tensor<64x1x1xbf16>, %arg724: tensor<64x1x1xbf16>, %arg725: tensor<64x1x1xbf16>, %arg726: tensor<64x1x1xbf16>, %arg727: tensor<128x1x1xbf16>, %arg728: tensor<128x1x1xbf16>, %arg729: tensor<128x1x1xbf16>, %arg730: tensor<128x1x1xbf16>, %arg731: tensor<256x1x1xbf16>, %arg732: tensor<256x1x1xbf16>, %arg733: tensor<256x1x1xbf16>, %arg734: tensor<256x1x1xbf16>, %arg735: tensor<128x1x1xbf16>, %arg736: tensor<128x1x1xbf16>, %arg737: tensor<128x1x1xbf16>, %arg738: tensor<128x1x1xbf16>, %arg739: tensor<128x1x1xbf16>, %arg740: tensor<128x1x1xbf16>, %arg741: tensor<128x1x1xbf16>, %arg742: tensor<128x1x1xbf16>, %arg743: tensor<128x1x1xbf16>, %arg744: tensor<128x1x1xbf16>, %arg745: tensor<128x1x1xbf16>, %arg746: tensor<128x1x1xbf16>, %arg747: tensor<128x1x1xbf16>, %arg748: tensor<128x1x1xbf16>, %arg749: tensor<128x1x1xbf16>, %arg750: tensor<128x1x1xbf16>, %arg751: tensor<128x1x1xbf16>, %arg752: tensor<128x1x1xbf16>, %arg753: tensor<128x1x1xbf16>, %arg754: tensor<128x1x1xbf16>, %arg755: tensor<128x1x1xbf16>, %arg756: tensor<128x1x1xbf16>, %arg757: tensor<128x1x1xbf16>, %arg758: tensor<128x1x1xbf16>, %arg759: tensor<128x1x1xbf16>, %arg760: tensor<128x1x1xbf16>, %arg761: tensor<128x1x1xbf16>, %arg762: tensor<128x1x1xbf16>, %arg763: tensor<128x1x1xbf16>, %arg764: tensor<128x1x1xbf16>, %arg765: tensor<128x1x1xbf16>, %arg766: tensor<128x1x1xbf16>, %arg767: tensor<128x1x1xbf16>, %arg768: tensor<128x1x1xbf16>, %arg769: tensor<128x1x1xbf16>, %arg770: tensor<128x1x1xbf16>, %arg771: tensor<128x1x1xbf16>, %arg772: tensor<128x1x1xbf16>, %arg773: tensor<128x1x1xbf16>, %arg774: tensor<128x1x1xbf16>, %arg775: tensor<128x1x1xbf16>, %arg776: tensor<128x1x1xbf16>, %arg777: tensor<128x1x1xbf16>, %arg778: tensor<128x1x1xbf16>, %arg779: tensor<128x1x1xbf16>, %arg780: tensor<128x1x1xbf16>, %arg781: tensor<128x1x1xbf16>, %arg782: tensor<128x1x1xbf16>, %arg783: tensor<128x1x1xbf16>, %arg784: tensor<128x1x1xbf16>, %arg785: tensor<128x1x1xbf16>, %arg786: tensor<128x1x1xbf16>, %arg787: tensor<128x1x1xbf16>, %arg788: tensor<128x1x1xbf16>, %arg789: tensor<128x1x1xbf16>, %arg790: tensor<128x1x1xbf16>, %arg791: tensor<128x1x1xbf16>, %arg792: tensor<128x1x1xbf16>, %arg793: tensor<128x1x1xbf16>, %arg794: tensor<128x1x1xbf16>, %arg795: tensor<128x1x1xbf16>, %arg796: tensor<128x1x1xbf16>, %arg797: tensor<128x1x1xbf16>, %arg798: tensor<128x1x1xbf16>, %arg799: tensor<128x1x1xbf16>, %arg800: tensor<128x1x1xbf16>, %arg801: tensor<128x1x1xbf16>, %arg802: tensor<128x1x1xbf16>, %arg803: tensor<128x1x1xbf16>, %arg804: tensor<128x1x1xbf16>, %arg805: tensor<128x1x1xbf16>, %arg806: tensor<128x1x1xbf16>, %arg807: tensor<128x1x1xbf16>, %arg808: tensor<128x1x1xbf16>, %arg809: tensor<128x1x1xbf16>, %arg810: tensor<128x1x1xbf16>, %arg811: tensor<256x1x1xbf16>, %arg812: tensor<256x1x1xbf16>, %arg813: tensor<256x1x1xbf16>, %arg814: tensor<256x1x1xbf16>, %arg815: tensor<512x1x1xbf16>, %arg816: tensor<512x1x1xbf16>, %arg817: tensor<512x1x1xbf16>, %arg818: tensor<512x1x1xbf16>, %arg819: tensor<256x1x1xbf16>, %arg820: tensor<256x1x1xbf16>, %arg821: tensor<256x1x1xbf16>, %arg822: tensor<256x1x1xbf16>, %arg823: tensor<256x1x1xbf16>, %arg824: tensor<256x1x1xbf16>, %arg825: tensor<256x1x1xbf16>, %arg826: tensor<256x1x1xbf16>, %arg827: tensor<256x1x1xbf16>, %arg828: tensor<256x1x1xbf16>, %arg829: tensor<256x1x1xbf16>, %arg830: tensor<256x1x1xbf16>, %arg831: tensor<256x1x1xbf16>, %arg832: tensor<256x1x1xbf16>, %arg833: tensor<256x1x1xbf16>, %arg834: tensor<256x1x1xbf16>, %arg835: tensor<256x1x1xbf16>, %arg836: tensor<256x1x1xbf16>, %arg837: tensor<256x1x1xbf16>, %arg838: tensor<256x1x1xbf16>, %arg839: tensor<256x1x1xbf16>, %arg840: tensor<256x1x1xbf16>, %arg841: tensor<256x1x1xbf16>, %arg842: tensor<256x1x1xbf16>, %arg843: tensor<256x1x1xbf16>, %arg844: tensor<256x1x1xbf16>, %arg845: tensor<256x1x1xbf16>, %arg846: tensor<256x1x1xbf16>, %arg847: tensor<256x1x1xbf16>, %arg848: tensor<256x1x1xbf16>, %arg849: tensor<256x1x1xbf16>, %arg850: tensor<256x1x1xbf16>, %arg851: tensor<256x1x1xbf16>, %arg852: tensor<256x1x1xbf16>, %arg853: tensor<256x1x1xbf16>, %arg854: tensor<256x1x1xbf16>, %arg855: tensor<256x1x1xbf16>, %arg856: tensor<256x1x1xbf16>, %arg857: tensor<256x1x1xbf16>, %arg858: tensor<256x1x1xbf16>, %arg859: tensor<256x1x1xbf16>, %arg860: tensor<256x1x1xbf16>, %arg861: tensor<256x1x1xbf16>, %arg862: tensor<256x1x1xbf16>, %arg863: tensor<256x1x1xbf16>, %arg864: tensor<256x1x1xbf16>, %arg865: tensor<256x1x1xbf16>, %arg866: tensor<256x1x1xbf16>, %arg867: tensor<256x1x1xbf16>, %arg868: tensor<256x1x1xbf16>, %arg869: tensor<256x1x1xbf16>, %arg870: tensor<256x1x1xbf16>, %arg871: tensor<256x1x1xbf16>, %arg872: tensor<256x1x1xbf16>, %arg873: tensor<256x1x1xbf16>, %arg874: tensor<256x1x1xbf16>, %arg875: tensor<256x1x1xbf16>, %arg876: tensor<256x1x1xbf16>, %arg877: tensor<256x1x1xbf16>, %arg878: tensor<256x1x1xbf16>, %arg879: tensor<256x1x1xbf16>, %arg880: tensor<256x1x1xbf16>, %arg881: tensor<256x1x1xbf16>, %arg882: tensor<256x1x1xbf16>, %arg883: tensor<256x1x1xbf16>, %arg884: tensor<256x1x1xbf16>, %arg885: tensor<256x1x1xbf16>, %arg886: tensor<256x1x1xbf16>, %arg887: tensor<256x1x1xbf16>, %arg888: tensor<256x1x1xbf16>, %arg889: tensor<256x1x1xbf16>, %arg890: tensor<256x1x1xbf16>, %arg891: tensor<256x1x1xbf16>, %arg892: tensor<256x1x1xbf16>, %arg893: tensor<256x1x1xbf16>, %arg894: tensor<256x1x1xbf16>, %arg895: tensor<512x1x1xbf16>, %arg896: tensor<512x1x1xbf16>, %arg897: tensor<512x1x1xbf16>, %arg898: tensor<512x1x1xbf16>, %arg899: tensor<1024x1x1xbf16>, %arg900: tensor<1024x1x1xbf16>, %arg901: tensor<1024x1x1xbf16>, %arg902: tensor<1024x1x1xbf16>, %arg903: tensor<512x1x1xbf16>, %arg904: tensor<512x1x1xbf16>, %arg905: tensor<512x1x1xbf16>, %arg906: tensor<512x1x1xbf16>, %arg907: tensor<512x1x1xbf16>, %arg908: tensor<512x1x1xbf16>, %arg909: tensor<512x1x1xbf16>, %arg910: tensor<512x1x1xbf16>, %arg911: tensor<512x1x1xbf16>, %arg912: tensor<512x1x1xbf16>, %arg913: tensor<512x1x1xbf16>, %arg914: tensor<512x1x1xbf16>, %arg915: tensor<512x1x1xbf16>, %arg916: tensor<512x1x1xbf16>, %arg917: tensor<512x1x1xbf16>, %arg918: tensor<512x1x1xbf16>, %arg919: tensor<512x1x1xbf16>, %arg920: tensor<512x1x1xbf16>, %arg921: tensor<512x1x1xbf16>, %arg922: tensor<512x1x1xbf16>, %arg923: tensor<512x1x1xbf16>, %arg924: tensor<512x1x1xbf16>, %arg925: tensor<512x1x1xbf16>, %arg926: tensor<512x1x1xbf16>, %arg927: tensor<512x1x1xbf16>, %arg928: tensor<512x1x1xbf16>, %arg929: tensor<512x1x1xbf16>, %arg930: tensor<512x1x1xbf16>, %arg931: tensor<512x1x1xbf16>, %arg932: tensor<512x1x1xbf16>, %arg933: tensor<512x1x1xbf16>, %arg934: tensor<512x1x1xbf16>, %arg935: tensor<512x1x1xbf16>, %arg936: tensor<512x1x1xbf16>, %arg937: tensor<512x1x1xbf16>, %arg938: tensor<512x1x1xbf16>, %arg939: tensor<512x1x1xbf16>, %arg940: tensor<512x1x1xbf16>, %arg941: tensor<512x1x1xbf16>, %arg942: tensor<512x1x1xbf16>, %arg943: tensor<512x1x1xbf16>, %arg944: tensor<512x1x1xbf16>, %arg945: tensor<512x1x1xbf16>, %arg946: tensor<512x1x1xbf16>, %arg947: tensor<1024x1x1xbf16>, %arg948: tensor<1024x1x1xbf16>, %arg949: tensor<1024x1x1xbf16>, %arg950: tensor<1024x1x1xbf16>, %arg951: tensor<512x1x1xbf16>, %arg952: tensor<512x1x1xbf16>, %arg953: tensor<512x1x1xbf16>, %arg954: tensor<512x1x1xbf16>, %arg955: tensor<1024x1x1xbf16>, %arg956: tensor<1024x1x1xbf16>, %arg957: tensor<1024x1x1xbf16>, %arg958: tensor<1024x1x1xbf16>, %arg959: tensor<512x1x1xbf16>, %arg960: tensor<512x1x1xbf16>, %arg961: tensor<512x1x1xbf16>, %arg962: tensor<512x1x1xbf16>, %arg963: tensor<512x1x1xbf16>, %arg964: tensor<512x1x1xbf16>, %arg965: tensor<512x1x1xbf16>, %arg966: tensor<512x1x1xbf16>, %arg967: tensor<1024x1x1xbf16>, %arg968: tensor<1024x1x1xbf16>, %arg969: tensor<1024x1x1xbf16>, %arg970: tensor<1024x1x1xbf16>, %arg971: tensor<512x1x1xbf16>, %arg972: tensor<512x1x1xbf16>, %arg973: tensor<512x1x1xbf16>, %arg974: tensor<512x1x1xbf16>, %arg975: tensor<256x1x1xbf16>, %arg976: tensor<256x1x1xbf16>, %arg977: tensor<256x1x1xbf16>, %arg978: tensor<256x1x1xbf16>, %arg979: tensor<30xf32>, %arg980: tensor<40xf32>, %arg981: tensor<1024x15x30xbf16>, %arg982: tensor<1024x20x40xbf16>, %arg983: tensor<256x1x1xbf16>, %arg984: tensor<256x1x1xbf16>, %arg985: tensor<256x1x1xbf16>, %arg986: tensor<256x1x1xbf16>, %arg987: tensor<256x1x1xbf16>, %arg988: tensor<256x1x1xbf16>, %arg989: tensor<256x1x1xbf16>, %arg990: tensor<256x1x1xbf16>, %arg991: tensor<512x1x1xbf16>, %arg992: tensor<512x1x1xbf16>, %arg993: tensor<512x1x1xbf16>, %arg994: tensor<512x1x1xbf16>, %arg995: tensor<256x1x1xbf16>, %arg996: tensor<256x1x1xbf16>, %arg997: tensor<256x1x1xbf16>, %arg998: tensor<256x1x1xbf16>, %arg999: tensor<512x1x1xbf16>, %arg1000: tensor<512x1x1xbf16>, %arg1001: tensor<512x1x1xbf16>, %arg1002: tensor<512x1x1xbf16>, %arg1003: tensor<256x1x1xbf16>, %arg1004: tensor<256x1x1xbf16>, %arg1005: tensor<256x1x1xbf16>, %arg1006: tensor<256x1x1xbf16>, %arg1007: tensor<128x1x1xbf16>, %arg1008: tensor<128x1x1xbf16>, %arg1009: tensor<128x1x1xbf16>, %arg1010: tensor<128x1x1xbf16>, %arg1011: tensor<60xf32>, %arg1012: tensor<80xf32>, %arg1013: tensor<512x30x60xbf16>, %arg1014: tensor<512x40x80xbf16>, %arg1015: tensor<128x1x1xbf16>, %arg1016: tensor<128x1x1xbf16>, %arg1017: tensor<128x1x1xbf16>, %arg1018: tensor<128x1x1xbf16>, %arg1019: tensor<128x1x1xbf16>, %arg1020: tensor<128x1x1xbf16>, %arg1021: tensor<128x1x1xbf16>, %arg1022: tensor<128x1x1xbf16>, %arg1023: tensor<256x1x1xbf16>, %arg1024: tensor<256x1x1xbf16>, %arg1025: tensor<256x1x1xbf16>, %arg1026: tensor<256x1x1xbf16>, %arg1027: tensor<128x1x1xbf16>, %arg1028: tensor<128x1x1xbf16>, %arg1029: tensor<128x1x1xbf16>, %arg1030: tensor<128x1x1xbf16>, %arg1031: tensor<256x1x1xbf16>, %arg1032: tensor<256x1x1xbf16>, %arg1033: tensor<256x1x1xbf16>, %arg1034: tensor<256x1x1xbf16>, %arg1035: tensor<128x1x1xbf16>, %arg1036: tensor<128x1x1xbf16>, %arg1037: tensor<128x1x1xbf16>, %arg1038: tensor<128x1x1xbf16>, %arg1039: tensor<256x1x1xbf16>, %arg1040: tensor<256x1x1xbf16>, %arg1041: tensor<256x1x1xbf16>, %arg1042: tensor<256x1x1xbf16>, %arg1043: tensor<256x1x1xbf16>, %arg1044: tensor<256x1x1xbf16>, %arg1045: tensor<256x1x1xbf16>, %arg1046: tensor<256x1x1xbf16>, %arg1047: tensor<256x1x1xbf16>, %arg1048: tensor<256x1x1xbf16>, %arg1049: tensor<256x1x1xbf16>, %arg1050: tensor<256x1x1xbf16>, %arg1051: tensor<512x1x1xbf16>, %arg1052: tensor<512x1x1xbf16>, %arg1053: tensor<512x1x1xbf16>, %arg1054: tensor<512x1x1xbf16>, %arg1055: tensor<256x1x1xbf16>, %arg1056: tensor<256x1x1xbf16>, %arg1057: tensor<256x1x1xbf16>, %arg1058: tensor<256x1x1xbf16>, %arg1059: tensor<512x1x1xbf16>, %arg1060: tensor<512x1x1xbf16>, %arg1061: tensor<512x1x1xbf16>, %arg1062: tensor<512x1x1xbf16>, %arg1063: tensor<256x1x1xbf16>, %arg1064: tensor<256x1x1xbf16>, %arg1065: tensor<256x1x1xbf16>, %arg1066: tensor<256x1x1xbf16>, %arg1067: tensor<512x1x1xbf16>, %arg1068: tensor<512x1x1xbf16>, %arg1069: tensor<512x1x1xbf16>, %arg1070: tensor<512x1x1xbf16>, %arg1071: tensor<512x1x1xbf16>, %arg1072: tensor<512x1x1xbf16>, %arg1073: tensor<512x1x1xbf16>, %arg1074: tensor<512x1x1xbf16>, %arg1075: tensor<512x1x1xbf16>, %arg1076: tensor<512x1x1xbf16>, %arg1077: tensor<512x1x1xbf16>, %arg1078: tensor<512x1x1xbf16>, %arg1079: tensor<1024x1x1xbf16>, %arg1080: tensor<1024x1x1xbf16>, %arg1081: tensor<1024x1x1xbf16>, %arg1082: tensor<1024x1x1xbf16>, %arg1083: tensor<512x1x1xbf16>, %arg1084: tensor<512x1x1xbf16>, %arg1085: tensor<512x1x1xbf16>, %arg1086: tensor<512x1x1xbf16>, %arg1087: tensor<1024x1x1xbf16>, %arg1088: tensor<1024x1x1xbf16>, %arg1089: tensor<1024x1x1xbf16>, %arg1090: tensor<1024x1x1xbf16>, %arg1091: tensor<512x1x1xbf16>, %arg1092: tensor<512x1x1xbf16>, %arg1093: tensor<512x1x1xbf16>, %arg1094: tensor<512x1x1xbf16>, %arg1095: tensor<1024x1x1xbf16>, %arg1096: tensor<1024x1x1xbf16>, %arg1097: tensor<1024x1x1xbf16>, %arg1098: tensor<1024x1x1xbf16>, %arg1099: tensor<4x3x480x640xbf16>) {
      %c = stablehlo.constant dense<20> : tensor<i64>
      %c_0 = stablehlo.constant dense<0> : tensor<i64>
      %cst = stablehlo.constant dense<0xFF80> : tensor<bf16>
      %cst_1 = arith.constant dense<1.000000e-01> : tensor<1xf64>
      %1 = stablehlo.convolution(%arg1099, %arg550) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x3x480x640xbf16>, tensor<32x3x3x3xbf16>) -> tensor<4x32x480x640xbf16>
      %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x32x480x640xbf16>) -> tensor<4x32x480x640xbf16>
      %3 = stablehlo.broadcast_in_dim %arg663, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x1xbf16>) -> tensor<4x32x480x640xbf16>
      %4 = stablehlo.subtract %2, %3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x32x480x640xbf16>
      %5 = stablehlo.broadcast_in_dim %4, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x32x480x640xbf16>) -> tensor<4x32x480x640xbf16>
      %6 = stablehlo.broadcast_in_dim %arg664, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x1xbf16>) -> tensor<4x32x480x640xbf16>
      %7 = stablehlo.multiply %5, %6 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x32x480x640xbf16>
      %8 = stablehlo.broadcast_in_dim %7, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x32x480x640xbf16>) -> tensor<4x32x480x640xbf16>
      %9 = stablehlo.broadcast_in_dim %arg665, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x1xbf16>) -> tensor<4x32x480x640xbf16>
      %10 = stablehlo.multiply %8, %9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x32x480x640xbf16>
      %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x32x480x640xbf16>) -> tensor<4x32x480x640xbf16>
      %12 = stablehlo.broadcast_in_dim %arg666, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x1xbf16>) -> tensor<4x32x480x640xbf16>
      %13 = stablehlo.add %11, %12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x32x480x640xbf16>
      %14 = stablehlo.exponential %13 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x32x480x640xbf16>
      %15 = stablehlo.log_plus_one %14 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x32x480x640xbf16>
      %16 = stablehlo.convert %c : (tensor<i64>) -> tensor<bf16>
      %17 = stablehlo.broadcast_in_dim %13, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x32x480x640xbf16>) -> tensor<4x32x480x640xbf16>
      %18 = stablehlo.broadcast_in_dim %16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<4x32x480x640xbf16>
      %19 = stablehlo.compare  GT, %17, %18,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x32x480x640xbf16>, tensor<4x32x480x640xbf16>) -> tensor<4x32x480x640xi1>
      %20 = stablehlo.broadcast_in_dim %19, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x32x480x640xi1>) -> tensor<4x32x480x640xi1>
      %21 = stablehlo.broadcast_in_dim %15, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x32x480x640xbf16>) -> tensor<4x32x480x640xbf16>
      %22 = stablehlo.select %20, %17, %21 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x32x480x640xi1>, tensor<4x32x480x640xbf16>
      %23 = stablehlo.tanh %22 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x32x480x640xbf16>
      %24 = stablehlo.multiply %13, %23 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x32x480x640xbf16>
      %25 = stablehlo.convolution(%24, %arg551) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [2, 2], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x32x480x640xbf16>, tensor<64x32x3x3xbf16>) -> tensor<4x64x240x320xbf16>
      %26 = stablehlo.broadcast_in_dim %25, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %27 = stablehlo.broadcast_in_dim %arg667, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x240x320xbf16>
      %28 = stablehlo.subtract %26, %27 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %29 = stablehlo.broadcast_in_dim %28, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %30 = stablehlo.broadcast_in_dim %arg668, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x240x320xbf16>
      %31 = stablehlo.multiply %29, %30 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %32 = stablehlo.broadcast_in_dim %31, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %33 = stablehlo.broadcast_in_dim %arg669, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x240x320xbf16>
      %34 = stablehlo.multiply %32, %33 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %35 = stablehlo.broadcast_in_dim %34, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %36 = stablehlo.broadcast_in_dim %arg670, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x240x320xbf16>
      %37 = stablehlo.add %35, %36 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %38 = stablehlo.exponential %37 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %39 = stablehlo.log_plus_one %38 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %40 = stablehlo.broadcast_in_dim %37, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %41 = stablehlo.broadcast_in_dim %16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<4x64x240x320xbf16>
      %42 = stablehlo.compare  GT, %40, %41,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>, tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xi1>
      %43 = stablehlo.broadcast_in_dim %42, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xi1>) -> tensor<4x64x240x320xi1>
      %44 = stablehlo.broadcast_in_dim %39, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %45 = stablehlo.select %43, %40, %44 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xi1>, tensor<4x64x240x320xbf16>
      %46 = stablehlo.tanh %45 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %47 = stablehlo.multiply %37, %46 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %48 = stablehlo.convolution(%47, %arg552) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>, tensor<64x64x1x1xbf16>) -> tensor<4x64x240x320xbf16>
      %49 = stablehlo.broadcast_in_dim %48, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %50 = stablehlo.broadcast_in_dim %arg671, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x240x320xbf16>
      %51 = stablehlo.subtract %49, %50 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %52 = stablehlo.broadcast_in_dim %51, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %53 = stablehlo.broadcast_in_dim %arg672, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x240x320xbf16>
      %54 = stablehlo.multiply %52, %53 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %55 = stablehlo.broadcast_in_dim %54, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %56 = stablehlo.broadcast_in_dim %arg673, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x240x320xbf16>
      %57 = stablehlo.multiply %55, %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %58 = stablehlo.broadcast_in_dim %57, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %59 = stablehlo.broadcast_in_dim %arg674, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x240x320xbf16>
      %60 = stablehlo.add %58, %59 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %61 = stablehlo.exponential %60 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %62 = stablehlo.log_plus_one %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %63 = stablehlo.broadcast_in_dim %60, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %64 = stablehlo.compare  GT, %63, %41,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>, tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xi1>
      %65 = stablehlo.broadcast_in_dim %64, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xi1>) -> tensor<4x64x240x320xi1>
      %66 = stablehlo.broadcast_in_dim %62, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %67 = stablehlo.select %65, %63, %66 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xi1>, tensor<4x64x240x320xbf16>
      %68 = stablehlo.tanh %67 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %69 = stablehlo.multiply %60, %68 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %70 = stablehlo.convolution(%47, %arg553) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>, tensor<64x64x1x1xbf16>) -> tensor<4x64x240x320xbf16>
      %71 = stablehlo.broadcast_in_dim %70, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %72 = stablehlo.broadcast_in_dim %arg675, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x240x320xbf16>
      %73 = stablehlo.subtract %71, %72 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %74 = stablehlo.broadcast_in_dim %73, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %75 = stablehlo.broadcast_in_dim %arg676, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x240x320xbf16>
      %76 = stablehlo.multiply %74, %75 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %77 = stablehlo.broadcast_in_dim %76, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %78 = stablehlo.broadcast_in_dim %arg677, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x240x320xbf16>
      %79 = stablehlo.multiply %77, %78 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %80 = stablehlo.broadcast_in_dim %79, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %81 = stablehlo.broadcast_in_dim %arg678, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x240x320xbf16>
      %82 = stablehlo.add %80, %81 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %83 = stablehlo.exponential %82 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %84 = stablehlo.log_plus_one %83 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %85 = stablehlo.broadcast_in_dim %82, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %86 = stablehlo.compare  GT, %85, %41,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>, tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xi1>
      %87 = stablehlo.broadcast_in_dim %86, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xi1>) -> tensor<4x64x240x320xi1>
      %88 = stablehlo.broadcast_in_dim %84, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %89 = stablehlo.select %87, %85, %88 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xi1>, tensor<4x64x240x320xbf16>
      %90 = stablehlo.tanh %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %91 = stablehlo.multiply %82, %90 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %92 = stablehlo.convolution(%91, %arg554) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>, tensor<32x64x1x1xbf16>) -> tensor<4x32x240x320xbf16>
      %93 = stablehlo.broadcast_in_dim %92, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x32x240x320xbf16>) -> tensor<4x32x240x320xbf16>
      %94 = stablehlo.broadcast_in_dim %arg679, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x1xbf16>) -> tensor<4x32x240x320xbf16>
      %95 = stablehlo.subtract %93, %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x32x240x320xbf16>
      %96 = stablehlo.broadcast_in_dim %95, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x32x240x320xbf16>) -> tensor<4x32x240x320xbf16>
      %97 = stablehlo.broadcast_in_dim %arg680, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x1xbf16>) -> tensor<4x32x240x320xbf16>
      %98 = stablehlo.multiply %96, %97 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x32x240x320xbf16>
      %99 = stablehlo.broadcast_in_dim %98, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x32x240x320xbf16>) -> tensor<4x32x240x320xbf16>
      %100 = stablehlo.broadcast_in_dim %arg681, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x1xbf16>) -> tensor<4x32x240x320xbf16>
      %101 = stablehlo.multiply %99, %100 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x32x240x320xbf16>
      %102 = stablehlo.broadcast_in_dim %101, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x32x240x320xbf16>) -> tensor<4x32x240x320xbf16>
      %103 = stablehlo.broadcast_in_dim %arg682, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x1xbf16>) -> tensor<4x32x240x320xbf16>
      %104 = stablehlo.add %102, %103 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x32x240x320xbf16>
      %105 = stablehlo.exponential %104 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x32x240x320xbf16>
      %106 = stablehlo.log_plus_one %105 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x32x240x320xbf16>
      %107 = stablehlo.broadcast_in_dim %104, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x32x240x320xbf16>) -> tensor<4x32x240x320xbf16>
      %108 = stablehlo.broadcast_in_dim %16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<4x32x240x320xbf16>
      %109 = stablehlo.compare  GT, %107, %108,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x32x240x320xbf16>, tensor<4x32x240x320xbf16>) -> tensor<4x32x240x320xi1>
      %110 = stablehlo.broadcast_in_dim %109, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x32x240x320xi1>) -> tensor<4x32x240x320xi1>
      %111 = stablehlo.broadcast_in_dim %106, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x32x240x320xbf16>) -> tensor<4x32x240x320xbf16>
      %112 = stablehlo.select %110, %107, %111 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x32x240x320xi1>, tensor<4x32x240x320xbf16>
      %113 = stablehlo.tanh %112 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x32x240x320xbf16>
      %114 = stablehlo.multiply %104, %113 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x32x240x320xbf16>
      %115 = stablehlo.convolution(%114, %arg555) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x32x240x320xbf16>, tensor<64x32x3x3xbf16>) -> tensor<4x64x240x320xbf16>
      %116 = stablehlo.broadcast_in_dim %115, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %117 = stablehlo.broadcast_in_dim %arg683, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x240x320xbf16>
      %118 = stablehlo.subtract %116, %117 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %119 = stablehlo.broadcast_in_dim %118, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %120 = stablehlo.broadcast_in_dim %arg684, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x240x320xbf16>
      %121 = stablehlo.multiply %119, %120 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %122 = stablehlo.broadcast_in_dim %121, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %123 = stablehlo.broadcast_in_dim %arg685, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x240x320xbf16>
      %124 = stablehlo.multiply %122, %123 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %125 = stablehlo.broadcast_in_dim %124, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %126 = stablehlo.broadcast_in_dim %arg686, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x240x320xbf16>
      %127 = stablehlo.add %125, %126 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %128 = stablehlo.exponential %127 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %129 = stablehlo.log_plus_one %128 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %130 = stablehlo.broadcast_in_dim %127, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %131 = stablehlo.compare  GT, %130, %41,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>, tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xi1>
      %132 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xi1>) -> tensor<4x64x240x320xi1>
      %133 = stablehlo.broadcast_in_dim %129, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %134 = stablehlo.select %132, %130, %133 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xi1>, tensor<4x64x240x320xbf16>
      %135 = stablehlo.tanh %134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %136 = stablehlo.multiply %127, %135 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %137 = stablehlo.add %136, %91 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %138 = stablehlo.convolution(%137, %arg556) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>, tensor<64x64x1x1xbf16>) -> tensor<4x64x240x320xbf16>
      %139 = stablehlo.broadcast_in_dim %138, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %140 = stablehlo.broadcast_in_dim %arg687, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x240x320xbf16>
      %141 = stablehlo.subtract %139, %140 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %143 = stablehlo.broadcast_in_dim %arg688, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x240x320xbf16>
      %144 = stablehlo.multiply %142, %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %145 = stablehlo.broadcast_in_dim %144, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %146 = stablehlo.broadcast_in_dim %arg689, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x240x320xbf16>
      %147 = stablehlo.multiply %145, %146 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %148 = stablehlo.broadcast_in_dim %147, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %149 = stablehlo.broadcast_in_dim %arg690, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x240x320xbf16>
      %150 = stablehlo.add %148, %149 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %151 = stablehlo.exponential %150 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %152 = stablehlo.log_plus_one %151 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %153 = stablehlo.broadcast_in_dim %150, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %154 = stablehlo.compare  GT, %153, %41,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>, tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xi1>
      %155 = stablehlo.broadcast_in_dim %154, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xi1>) -> tensor<4x64x240x320xi1>
      %156 = stablehlo.broadcast_in_dim %152, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %157 = stablehlo.select %155, %153, %156 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xi1>, tensor<4x64x240x320xbf16>
      %158 = stablehlo.tanh %157 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %159 = stablehlo.multiply %150, %158 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %160 = stablehlo.concatenate %159, %69, dim = 1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>, tensor<4x64x240x320xbf16>) -> tensor<4x128x240x320xbf16>
      %161 = stablehlo.convolution(%160, %arg557) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x240x320xbf16>, tensor<64x128x1x1xbf16>) -> tensor<4x64x240x320xbf16>
      %162 = stablehlo.broadcast_in_dim %161, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %163 = stablehlo.broadcast_in_dim %arg691, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x240x320xbf16>
      %164 = stablehlo.subtract %162, %163 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %166 = stablehlo.broadcast_in_dim %arg692, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x240x320xbf16>
      %167 = stablehlo.multiply %165, %166 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %168 = stablehlo.broadcast_in_dim %167, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %169 = stablehlo.broadcast_in_dim %arg693, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x240x320xbf16>
      %170 = stablehlo.multiply %168, %169 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %171 = stablehlo.broadcast_in_dim %170, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %172 = stablehlo.broadcast_in_dim %arg694, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x240x320xbf16>
      %173 = stablehlo.add %171, %172 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %174 = stablehlo.exponential %173 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %175 = stablehlo.log_plus_one %174 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %176 = stablehlo.broadcast_in_dim %173, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %177 = stablehlo.compare  GT, %176, %41,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>, tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xi1>
      %178 = stablehlo.broadcast_in_dim %177, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xi1>) -> tensor<4x64x240x320xi1>
      %179 = stablehlo.broadcast_in_dim %175, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>) -> tensor<4x64x240x320xbf16>
      %180 = stablehlo.select %178, %176, %179 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xi1>, tensor<4x64x240x320xbf16>
      %181 = stablehlo.tanh %180 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %182 = stablehlo.multiply %173, %181 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x240x320xbf16>
      %183 = stablehlo.convolution(%182, %arg558) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [2, 2], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x240x320xbf16>, tensor<128x64x3x3xbf16>) -> tensor<4x128x120x160xbf16>
      %184 = stablehlo.broadcast_in_dim %183, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x120x160xbf16>) -> tensor<4x128x120x160xbf16>
      %185 = stablehlo.broadcast_in_dim %arg695, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x120x160xbf16>
      %186 = stablehlo.subtract %184, %185 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x120x160xbf16>
      %187 = stablehlo.broadcast_in_dim %186, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x120x160xbf16>) -> tensor<4x128x120x160xbf16>
      %188 = stablehlo.broadcast_in_dim %arg696, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x120x160xbf16>
      %189 = stablehlo.multiply %187, %188 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x120x160xbf16>
      %190 = stablehlo.broadcast_in_dim %189, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x120x160xbf16>) -> tensor<4x128x120x160xbf16>
      %191 = stablehlo.broadcast_in_dim %arg697, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x120x160xbf16>
      %192 = stablehlo.multiply %190, %191 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x120x160xbf16>
      %193 = stablehlo.broadcast_in_dim %192, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x120x160xbf16>) -> tensor<4x128x120x160xbf16>
      %194 = stablehlo.broadcast_in_dim %arg698, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x120x160xbf16>
      %195 = stablehlo.add %193, %194 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x120x160xbf16>
      %196 = stablehlo.exponential %195 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x120x160xbf16>
      %197 = stablehlo.log_plus_one %196 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x120x160xbf16>
      %198 = stablehlo.broadcast_in_dim %195, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x120x160xbf16>) -> tensor<4x128x120x160xbf16>
      %199 = stablehlo.broadcast_in_dim %16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<4x128x120x160xbf16>
      %200 = stablehlo.compare  GT, %198, %199,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x120x160xbf16>, tensor<4x128x120x160xbf16>) -> tensor<4x128x120x160xi1>
      %201 = stablehlo.broadcast_in_dim %200, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x120x160xi1>) -> tensor<4x128x120x160xi1>
      %202 = stablehlo.broadcast_in_dim %197, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x120x160xbf16>) -> tensor<4x128x120x160xbf16>
      %203 = stablehlo.select %201, %198, %202 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x120x160xi1>, tensor<4x128x120x160xbf16>
      %204 = stablehlo.tanh %203 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x120x160xbf16>
      %205 = stablehlo.multiply %195, %204 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x120x160xbf16>
      %206 = stablehlo.convolution(%205, %arg559) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x120x160xbf16>, tensor<64x128x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %207 = stablehlo.broadcast_in_dim %206, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %208 = stablehlo.broadcast_in_dim %arg699, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %209 = stablehlo.subtract %207, %208 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %210 = stablehlo.broadcast_in_dim %209, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %211 = stablehlo.broadcast_in_dim %arg700, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %212 = stablehlo.multiply %210, %211 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %213 = stablehlo.broadcast_in_dim %212, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %214 = stablehlo.broadcast_in_dim %arg701, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %215 = stablehlo.multiply %213, %214 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %216 = stablehlo.broadcast_in_dim %215, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %217 = stablehlo.broadcast_in_dim %arg702, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %218 = stablehlo.add %216, %217 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %219 = stablehlo.exponential %218 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %220 = stablehlo.log_plus_one %219 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %221 = stablehlo.broadcast_in_dim %218, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %222 = stablehlo.broadcast_in_dim %16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<4x64x120x160xbf16>
      %223 = stablehlo.compare  GT, %221, %222,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>, tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xi1>
      %224 = stablehlo.broadcast_in_dim %223, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xi1>) -> tensor<4x64x120x160xi1>
      %225 = stablehlo.broadcast_in_dim %220, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %226 = stablehlo.select %224, %221, %225 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xi1>, tensor<4x64x120x160xbf16>
      %227 = stablehlo.tanh %226 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %228 = stablehlo.multiply %218, %227 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %229 = stablehlo.convolution(%205, %arg560) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x120x160xbf16>, tensor<64x128x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %230 = stablehlo.broadcast_in_dim %229, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %231 = stablehlo.broadcast_in_dim %arg703, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %232 = stablehlo.subtract %230, %231 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %233 = stablehlo.broadcast_in_dim %232, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %234 = stablehlo.broadcast_in_dim %arg704, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %235 = stablehlo.multiply %233, %234 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %236 = stablehlo.broadcast_in_dim %235, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %237 = stablehlo.broadcast_in_dim %arg705, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %238 = stablehlo.multiply %236, %237 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %239 = stablehlo.broadcast_in_dim %238, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %240 = stablehlo.broadcast_in_dim %arg706, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %241 = stablehlo.add %239, %240 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %242 = stablehlo.exponential %241 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %243 = stablehlo.log_plus_one %242 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %244 = stablehlo.broadcast_in_dim %241, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %245 = stablehlo.compare  GT, %244, %222,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>, tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xi1>
      %246 = stablehlo.broadcast_in_dim %245, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xi1>) -> tensor<4x64x120x160xi1>
      %247 = stablehlo.broadcast_in_dim %243, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %248 = stablehlo.select %246, %244, %247 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xi1>, tensor<4x64x120x160xbf16>
      %249 = stablehlo.tanh %248 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %250 = stablehlo.multiply %241, %249 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %251 = stablehlo.convolution(%250, %arg561) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>, tensor<64x64x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %252 = stablehlo.broadcast_in_dim %251, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %253 = stablehlo.broadcast_in_dim %arg707, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %254 = stablehlo.subtract %252, %253 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %255 = stablehlo.broadcast_in_dim %254, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %256 = stablehlo.broadcast_in_dim %arg708, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %257 = stablehlo.multiply %255, %256 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %258 = stablehlo.broadcast_in_dim %257, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %259 = stablehlo.broadcast_in_dim %arg709, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %260 = stablehlo.multiply %258, %259 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %261 = stablehlo.broadcast_in_dim %260, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %262 = stablehlo.broadcast_in_dim %arg710, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %263 = stablehlo.add %261, %262 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %264 = stablehlo.exponential %263 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %265 = stablehlo.log_plus_one %264 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %266 = stablehlo.broadcast_in_dim %263, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %267 = stablehlo.compare  GT, %266, %222,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>, tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xi1>
      %268 = stablehlo.broadcast_in_dim %267, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xi1>) -> tensor<4x64x120x160xi1>
      %269 = stablehlo.broadcast_in_dim %265, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %270 = stablehlo.select %268, %266, %269 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xi1>, tensor<4x64x120x160xbf16>
      %271 = stablehlo.tanh %270 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %272 = stablehlo.multiply %263, %271 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %273 = stablehlo.convolution(%272, %arg562) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>, tensor<64x64x3x3xbf16>) -> tensor<4x64x120x160xbf16>
      %274 = stablehlo.broadcast_in_dim %273, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %275 = stablehlo.broadcast_in_dim %arg711, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %276 = stablehlo.subtract %274, %275 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %277 = stablehlo.broadcast_in_dim %276, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %278 = stablehlo.broadcast_in_dim %arg712, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %279 = stablehlo.multiply %277, %278 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %280 = stablehlo.broadcast_in_dim %279, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %281 = stablehlo.broadcast_in_dim %arg713, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %282 = stablehlo.multiply %280, %281 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %283 = stablehlo.broadcast_in_dim %282, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %284 = stablehlo.broadcast_in_dim %arg714, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %285 = stablehlo.add %283, %284 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %286 = stablehlo.exponential %285 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %287 = stablehlo.log_plus_one %286 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %288 = stablehlo.broadcast_in_dim %285, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %289 = stablehlo.compare  GT, %288, %222,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>, tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xi1>
      %290 = stablehlo.broadcast_in_dim %289, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xi1>) -> tensor<4x64x120x160xi1>
      %291 = stablehlo.broadcast_in_dim %287, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %292 = stablehlo.select %290, %288, %291 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xi1>, tensor<4x64x120x160xbf16>
      %293 = stablehlo.tanh %292 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %294 = stablehlo.multiply %285, %293 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %295 = stablehlo.add %250, %294 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %296 = stablehlo.convolution(%295, %arg563) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>, tensor<64x64x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %297 = stablehlo.broadcast_in_dim %296, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %298 = stablehlo.broadcast_in_dim %arg715, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %299 = stablehlo.subtract %297, %298 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %300 = stablehlo.broadcast_in_dim %299, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %301 = stablehlo.broadcast_in_dim %arg716, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %302 = stablehlo.multiply %300, %301 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %303 = stablehlo.broadcast_in_dim %302, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %304 = stablehlo.broadcast_in_dim %arg717, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %305 = stablehlo.multiply %303, %304 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %306 = stablehlo.broadcast_in_dim %305, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %307 = stablehlo.broadcast_in_dim %arg718, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %308 = stablehlo.add %306, %307 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %309 = stablehlo.exponential %308 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %310 = stablehlo.log_plus_one %309 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %311 = stablehlo.broadcast_in_dim %308, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %312 = stablehlo.compare  GT, %311, %222,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>, tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xi1>
      %313 = stablehlo.broadcast_in_dim %312, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xi1>) -> tensor<4x64x120x160xi1>
      %314 = stablehlo.broadcast_in_dim %310, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %315 = stablehlo.select %313, %311, %314 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xi1>, tensor<4x64x120x160xbf16>
      %316 = stablehlo.tanh %315 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %317 = stablehlo.multiply %308, %316 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %318 = stablehlo.convolution(%317, %arg564) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>, tensor<64x64x3x3xbf16>) -> tensor<4x64x120x160xbf16>
      %319 = stablehlo.broadcast_in_dim %318, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %320 = stablehlo.broadcast_in_dim %arg719, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %321 = stablehlo.subtract %319, %320 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %322 = stablehlo.broadcast_in_dim %321, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %323 = stablehlo.broadcast_in_dim %arg720, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %324 = stablehlo.multiply %322, %323 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %325 = stablehlo.broadcast_in_dim %324, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %326 = stablehlo.broadcast_in_dim %arg721, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %327 = stablehlo.multiply %325, %326 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %328 = stablehlo.broadcast_in_dim %327, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %329 = stablehlo.broadcast_in_dim %arg722, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %330 = stablehlo.add %328, %329 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %331 = stablehlo.exponential %330 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %332 = stablehlo.log_plus_one %331 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %333 = stablehlo.broadcast_in_dim %330, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %334 = stablehlo.compare  GT, %333, %222,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>, tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xi1>
      %335 = stablehlo.broadcast_in_dim %334, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xi1>) -> tensor<4x64x120x160xi1>
      %336 = stablehlo.broadcast_in_dim %332, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %337 = stablehlo.select %335, %333, %336 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xi1>, tensor<4x64x120x160xbf16>
      %338 = stablehlo.tanh %337 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %339 = stablehlo.multiply %330, %338 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %340 = stablehlo.add %295, %339 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %341 = stablehlo.convolution(%340, %arg565) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>, tensor<64x64x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %342 = stablehlo.broadcast_in_dim %341, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %343 = stablehlo.broadcast_in_dim %arg723, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %344 = stablehlo.subtract %342, %343 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %345 = stablehlo.broadcast_in_dim %344, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %346 = stablehlo.broadcast_in_dim %arg724, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %347 = stablehlo.multiply %345, %346 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %348 = stablehlo.broadcast_in_dim %347, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %349 = stablehlo.broadcast_in_dim %arg725, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %350 = stablehlo.multiply %348, %349 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %351 = stablehlo.broadcast_in_dim %350, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %352 = stablehlo.broadcast_in_dim %arg726, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<64x1x1xbf16>) -> tensor<4x64x120x160xbf16>
      %353 = stablehlo.add %351, %352 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %354 = stablehlo.exponential %353 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %355 = stablehlo.log_plus_one %354 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %356 = stablehlo.broadcast_in_dim %353, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %357 = stablehlo.compare  GT, %356, %222,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>, tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xi1>
      %358 = stablehlo.broadcast_in_dim %357, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xi1>) -> tensor<4x64x120x160xi1>
      %359 = stablehlo.broadcast_in_dim %355, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>) -> tensor<4x64x120x160xbf16>
      %360 = stablehlo.select %358, %356, %359 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xi1>, tensor<4x64x120x160xbf16>
      %361 = stablehlo.tanh %360 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %362 = stablehlo.multiply %353, %361 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x64x120x160xbf16>
      %363 = stablehlo.concatenate %362, %228, dim = 1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x64x120x160xbf16>, tensor<4x64x120x160xbf16>) -> tensor<4x128x120x160xbf16>
      %364 = stablehlo.convolution(%363, %arg566) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x120x160xbf16>, tensor<128x128x1x1xbf16>) -> tensor<4x128x120x160xbf16>
      %365 = stablehlo.broadcast_in_dim %364, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x120x160xbf16>) -> tensor<4x128x120x160xbf16>
      %366 = stablehlo.broadcast_in_dim %arg727, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x120x160xbf16>
      %367 = stablehlo.subtract %365, %366 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x120x160xbf16>
      %368 = stablehlo.broadcast_in_dim %367, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x120x160xbf16>) -> tensor<4x128x120x160xbf16>
      %369 = stablehlo.broadcast_in_dim %arg728, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x120x160xbf16>
      %370 = stablehlo.multiply %368, %369 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x120x160xbf16>
      %371 = stablehlo.broadcast_in_dim %370, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x120x160xbf16>) -> tensor<4x128x120x160xbf16>
      %372 = stablehlo.broadcast_in_dim %arg729, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x120x160xbf16>
      %373 = stablehlo.multiply %371, %372 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x120x160xbf16>
      %374 = stablehlo.broadcast_in_dim %373, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x120x160xbf16>) -> tensor<4x128x120x160xbf16>
      %375 = stablehlo.broadcast_in_dim %arg730, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x120x160xbf16>
      %376 = stablehlo.add %374, %375 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x120x160xbf16>
      %377 = stablehlo.exponential %376 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x120x160xbf16>
      %378 = stablehlo.log_plus_one %377 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x120x160xbf16>
      %379 = stablehlo.broadcast_in_dim %376, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x120x160xbf16>) -> tensor<4x128x120x160xbf16>
      %380 = stablehlo.compare  GT, %379, %199,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x120x160xbf16>, tensor<4x128x120x160xbf16>) -> tensor<4x128x120x160xi1>
      %381 = stablehlo.broadcast_in_dim %380, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x120x160xi1>) -> tensor<4x128x120x160xi1>
      %382 = stablehlo.broadcast_in_dim %378, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x120x160xbf16>) -> tensor<4x128x120x160xbf16>
      %383 = stablehlo.select %381, %379, %382 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x120x160xi1>, tensor<4x128x120x160xbf16>
      %384 = stablehlo.tanh %383 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x120x160xbf16>
      %385 = stablehlo.multiply %376, %384 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x120x160xbf16>
      %386 = stablehlo.convolution(%385, %arg567) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [2, 2], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x120x160xbf16>, tensor<256x128x3x3xbf16>) -> tensor<4x256x60x80xbf16>
      %387 = stablehlo.broadcast_in_dim %386, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %388 = stablehlo.broadcast_in_dim %arg731, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x60x80xbf16>
      %389 = stablehlo.subtract %387, %388 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %390 = stablehlo.broadcast_in_dim %389, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %391 = stablehlo.broadcast_in_dim %arg732, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x60x80xbf16>
      %392 = stablehlo.multiply %390, %391 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %393 = stablehlo.broadcast_in_dim %392, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %394 = stablehlo.broadcast_in_dim %arg733, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x60x80xbf16>
      %395 = stablehlo.multiply %393, %394 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %396 = stablehlo.broadcast_in_dim %395, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %397 = stablehlo.broadcast_in_dim %arg734, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x60x80xbf16>
      %398 = stablehlo.add %396, %397 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %399 = stablehlo.exponential %398 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %400 = stablehlo.log_plus_one %399 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %401 = stablehlo.broadcast_in_dim %398, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %402 = stablehlo.broadcast_in_dim %16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<4x256x60x80xbf16>
      %403 = stablehlo.compare  GT, %401, %402,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>, tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xi1>
      %404 = stablehlo.broadcast_in_dim %403, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xi1>) -> tensor<4x256x60x80xi1>
      %405 = stablehlo.broadcast_in_dim %400, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %406 = stablehlo.select %404, %401, %405 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xi1>, tensor<4x256x60x80xbf16>
      %407 = stablehlo.tanh %406 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %408 = stablehlo.multiply %398, %407 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %409 = stablehlo.convolution(%408, %arg568) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>, tensor<128x256x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %410 = stablehlo.broadcast_in_dim %409, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %411 = stablehlo.broadcast_in_dim %arg735, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %412 = stablehlo.subtract %410, %411 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %413 = stablehlo.broadcast_in_dim %412, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %414 = stablehlo.broadcast_in_dim %arg736, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %415 = stablehlo.multiply %413, %414 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %416 = stablehlo.broadcast_in_dim %415, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %417 = stablehlo.broadcast_in_dim %arg737, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %418 = stablehlo.multiply %416, %417 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %419 = stablehlo.broadcast_in_dim %418, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %420 = stablehlo.broadcast_in_dim %arg738, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %421 = stablehlo.add %419, %420 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %422 = stablehlo.exponential %421 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %423 = stablehlo.log_plus_one %422 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %424 = stablehlo.broadcast_in_dim %421, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %425 = stablehlo.broadcast_in_dim %16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<4x128x60x80xbf16>
      %426 = stablehlo.compare  GT, %424, %425,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xi1>
      %427 = stablehlo.broadcast_in_dim %426, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xi1>) -> tensor<4x128x60x80xi1>
      %428 = stablehlo.broadcast_in_dim %423, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %429 = stablehlo.select %427, %424, %428 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xi1>, tensor<4x128x60x80xbf16>
      %430 = stablehlo.tanh %429 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %431 = stablehlo.multiply %421, %430 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %432 = stablehlo.convolution(%408, %arg569) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>, tensor<128x256x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %433 = stablehlo.broadcast_in_dim %432, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %434 = stablehlo.broadcast_in_dim %arg739, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %435 = stablehlo.subtract %433, %434 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %436 = stablehlo.broadcast_in_dim %435, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %437 = stablehlo.broadcast_in_dim %arg740, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %438 = stablehlo.multiply %436, %437 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %439 = stablehlo.broadcast_in_dim %438, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %440 = stablehlo.broadcast_in_dim %arg741, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %441 = stablehlo.multiply %439, %440 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %442 = stablehlo.broadcast_in_dim %441, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %443 = stablehlo.broadcast_in_dim %arg742, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %444 = stablehlo.add %442, %443 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %445 = stablehlo.exponential %444 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %446 = stablehlo.log_plus_one %445 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %447 = stablehlo.broadcast_in_dim %444, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %448 = stablehlo.compare  GT, %447, %425,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xi1>
      %449 = stablehlo.broadcast_in_dim %448, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xi1>) -> tensor<4x128x60x80xi1>
      %450 = stablehlo.broadcast_in_dim %446, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %451 = stablehlo.select %449, %447, %450 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xi1>, tensor<4x128x60x80xbf16>
      %452 = stablehlo.tanh %451 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %453 = stablehlo.multiply %444, %452 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %454 = stablehlo.convolution(%453, %arg570) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<128x128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %455 = stablehlo.broadcast_in_dim %454, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %456 = stablehlo.broadcast_in_dim %arg743, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %457 = stablehlo.subtract %455, %456 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %458 = stablehlo.broadcast_in_dim %457, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %459 = stablehlo.broadcast_in_dim %arg744, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %460 = stablehlo.multiply %458, %459 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %461 = stablehlo.broadcast_in_dim %460, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %462 = stablehlo.broadcast_in_dim %arg745, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %463 = stablehlo.multiply %461, %462 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %464 = stablehlo.broadcast_in_dim %463, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %465 = stablehlo.broadcast_in_dim %arg746, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %466 = stablehlo.add %464, %465 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %467 = stablehlo.exponential %466 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %468 = stablehlo.log_plus_one %467 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %469 = stablehlo.broadcast_in_dim %466, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %470 = stablehlo.compare  GT, %469, %425,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xi1>
      %471 = stablehlo.broadcast_in_dim %470, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xi1>) -> tensor<4x128x60x80xi1>
      %472 = stablehlo.broadcast_in_dim %468, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %473 = stablehlo.select %471, %469, %472 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xi1>, tensor<4x128x60x80xbf16>
      %474 = stablehlo.tanh %473 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %475 = stablehlo.multiply %466, %474 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %476 = stablehlo.convolution(%475, %arg571) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<128x128x3x3xbf16>) -> tensor<4x128x60x80xbf16>
      %477 = stablehlo.broadcast_in_dim %476, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %478 = stablehlo.broadcast_in_dim %arg747, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %479 = stablehlo.subtract %477, %478 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %480 = stablehlo.broadcast_in_dim %479, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %481 = stablehlo.broadcast_in_dim %arg748, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %482 = stablehlo.multiply %480, %481 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %483 = stablehlo.broadcast_in_dim %482, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %484 = stablehlo.broadcast_in_dim %arg749, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %485 = stablehlo.multiply %483, %484 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %486 = stablehlo.broadcast_in_dim %485, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %487 = stablehlo.broadcast_in_dim %arg750, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %488 = stablehlo.add %486, %487 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %489 = stablehlo.exponential %488 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %490 = stablehlo.log_plus_one %489 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %491 = stablehlo.broadcast_in_dim %488, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %492 = stablehlo.compare  GT, %491, %425,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xi1>
      %493 = stablehlo.broadcast_in_dim %492, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xi1>) -> tensor<4x128x60x80xi1>
      %494 = stablehlo.broadcast_in_dim %490, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %495 = stablehlo.select %493, %491, %494 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xi1>, tensor<4x128x60x80xbf16>
      %496 = stablehlo.tanh %495 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %497 = stablehlo.multiply %488, %496 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %498 = stablehlo.add %453, %497 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %499 = stablehlo.convolution(%498, %arg572) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<128x128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %500 = stablehlo.broadcast_in_dim %499, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %501 = stablehlo.broadcast_in_dim %arg751, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %502 = stablehlo.subtract %500, %501 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %503 = stablehlo.broadcast_in_dim %502, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %504 = stablehlo.broadcast_in_dim %arg752, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %505 = stablehlo.multiply %503, %504 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %506 = stablehlo.broadcast_in_dim %505, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %507 = stablehlo.broadcast_in_dim %arg753, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %508 = stablehlo.multiply %506, %507 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %509 = stablehlo.broadcast_in_dim %508, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %510 = stablehlo.broadcast_in_dim %arg754, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %511 = stablehlo.add %509, %510 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %512 = stablehlo.exponential %511 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %513 = stablehlo.log_plus_one %512 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %514 = stablehlo.broadcast_in_dim %511, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %515 = stablehlo.compare  GT, %514, %425,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xi1>
      %516 = stablehlo.broadcast_in_dim %515, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xi1>) -> tensor<4x128x60x80xi1>
      %517 = stablehlo.broadcast_in_dim %513, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %518 = stablehlo.select %516, %514, %517 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xi1>, tensor<4x128x60x80xbf16>
      %519 = stablehlo.tanh %518 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %520 = stablehlo.multiply %511, %519 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %521 = stablehlo.convolution(%520, %arg573) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<128x128x3x3xbf16>) -> tensor<4x128x60x80xbf16>
      %522 = stablehlo.broadcast_in_dim %521, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %523 = stablehlo.broadcast_in_dim %arg755, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %524 = stablehlo.subtract %522, %523 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %525 = stablehlo.broadcast_in_dim %524, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %526 = stablehlo.broadcast_in_dim %arg756, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %527 = stablehlo.multiply %525, %526 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %528 = stablehlo.broadcast_in_dim %527, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %529 = stablehlo.broadcast_in_dim %arg757, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %530 = stablehlo.multiply %528, %529 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %531 = stablehlo.broadcast_in_dim %530, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %532 = stablehlo.broadcast_in_dim %arg758, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %533 = stablehlo.add %531, %532 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %534 = stablehlo.exponential %533 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %535 = stablehlo.log_plus_one %534 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %536 = stablehlo.broadcast_in_dim %533, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %537 = stablehlo.compare  GT, %536, %425,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xi1>
      %538 = stablehlo.broadcast_in_dim %537, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xi1>) -> tensor<4x128x60x80xi1>
      %539 = stablehlo.broadcast_in_dim %535, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %540 = stablehlo.select %538, %536, %539 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xi1>, tensor<4x128x60x80xbf16>
      %541 = stablehlo.tanh %540 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %542 = stablehlo.multiply %533, %541 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %543 = stablehlo.add %498, %542 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %544 = stablehlo.convolution(%543, %arg574) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<128x128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %545 = stablehlo.broadcast_in_dim %544, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %546 = stablehlo.broadcast_in_dim %arg759, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %547 = stablehlo.subtract %545, %546 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %548 = stablehlo.broadcast_in_dim %547, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %549 = stablehlo.broadcast_in_dim %arg760, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %550 = stablehlo.multiply %548, %549 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %551 = stablehlo.broadcast_in_dim %550, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %552 = stablehlo.broadcast_in_dim %arg761, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %553 = stablehlo.multiply %551, %552 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %554 = stablehlo.broadcast_in_dim %553, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %555 = stablehlo.broadcast_in_dim %arg762, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %556 = stablehlo.add %554, %555 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %557 = stablehlo.exponential %556 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %558 = stablehlo.log_plus_one %557 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %559 = stablehlo.broadcast_in_dim %556, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %560 = stablehlo.compare  GT, %559, %425,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xi1>
      %561 = stablehlo.broadcast_in_dim %560, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xi1>) -> tensor<4x128x60x80xi1>
      %562 = stablehlo.broadcast_in_dim %558, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %563 = stablehlo.select %561, %559, %562 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xi1>, tensor<4x128x60x80xbf16>
      %564 = stablehlo.tanh %563 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %565 = stablehlo.multiply %556, %564 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %566 = stablehlo.convolution(%565, %arg575) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<128x128x3x3xbf16>) -> tensor<4x128x60x80xbf16>
      %567 = stablehlo.broadcast_in_dim %566, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %568 = stablehlo.broadcast_in_dim %arg763, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %569 = stablehlo.subtract %567, %568 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %570 = stablehlo.broadcast_in_dim %569, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %571 = stablehlo.broadcast_in_dim %arg764, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %572 = stablehlo.multiply %570, %571 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %573 = stablehlo.broadcast_in_dim %572, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %574 = stablehlo.broadcast_in_dim %arg765, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %575 = stablehlo.multiply %573, %574 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %576 = stablehlo.broadcast_in_dim %575, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %577 = stablehlo.broadcast_in_dim %arg766, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %578 = stablehlo.add %576, %577 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %579 = stablehlo.exponential %578 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %580 = stablehlo.log_plus_one %579 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %581 = stablehlo.broadcast_in_dim %578, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %582 = stablehlo.compare  GT, %581, %425,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xi1>
      %583 = stablehlo.broadcast_in_dim %582, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xi1>) -> tensor<4x128x60x80xi1>
      %584 = stablehlo.broadcast_in_dim %580, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %585 = stablehlo.select %583, %581, %584 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xi1>, tensor<4x128x60x80xbf16>
      %586 = stablehlo.tanh %585 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %587 = stablehlo.multiply %578, %586 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %588 = stablehlo.add %543, %587 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %589 = stablehlo.convolution(%588, %arg576) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<128x128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %590 = stablehlo.broadcast_in_dim %589, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %591 = stablehlo.broadcast_in_dim %arg767, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %592 = stablehlo.subtract %590, %591 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %593 = stablehlo.broadcast_in_dim %592, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %594 = stablehlo.broadcast_in_dim %arg768, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %595 = stablehlo.multiply %593, %594 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %596 = stablehlo.broadcast_in_dim %595, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %597 = stablehlo.broadcast_in_dim %arg769, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %598 = stablehlo.multiply %596, %597 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %599 = stablehlo.broadcast_in_dim %598, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %600 = stablehlo.broadcast_in_dim %arg770, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %601 = stablehlo.add %599, %600 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %602 = stablehlo.exponential %601 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %603 = stablehlo.log_plus_one %602 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %604 = stablehlo.broadcast_in_dim %601, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %605 = stablehlo.compare  GT, %604, %425,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xi1>
      %606 = stablehlo.broadcast_in_dim %605, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xi1>) -> tensor<4x128x60x80xi1>
      %607 = stablehlo.broadcast_in_dim %603, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %608 = stablehlo.select %606, %604, %607 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xi1>, tensor<4x128x60x80xbf16>
      %609 = stablehlo.tanh %608 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %610 = stablehlo.multiply %601, %609 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %611 = stablehlo.convolution(%610, %arg577) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<128x128x3x3xbf16>) -> tensor<4x128x60x80xbf16>
      %612 = stablehlo.broadcast_in_dim %611, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %613 = stablehlo.broadcast_in_dim %arg771, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %614 = stablehlo.subtract %612, %613 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %615 = stablehlo.broadcast_in_dim %614, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %616 = stablehlo.broadcast_in_dim %arg772, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %617 = stablehlo.multiply %615, %616 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %618 = stablehlo.broadcast_in_dim %617, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %619 = stablehlo.broadcast_in_dim %arg773, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %620 = stablehlo.multiply %618, %619 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %621 = stablehlo.broadcast_in_dim %620, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %622 = stablehlo.broadcast_in_dim %arg774, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %623 = stablehlo.add %621, %622 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %624 = stablehlo.exponential %623 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %625 = stablehlo.log_plus_one %624 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %626 = stablehlo.broadcast_in_dim %623, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %627 = stablehlo.compare  GT, %626, %425,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xi1>
      %628 = stablehlo.broadcast_in_dim %627, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xi1>) -> tensor<4x128x60x80xi1>
      %629 = stablehlo.broadcast_in_dim %625, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %630 = stablehlo.select %628, %626, %629 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xi1>, tensor<4x128x60x80xbf16>
      %631 = stablehlo.tanh %630 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %632 = stablehlo.multiply %623, %631 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %633 = stablehlo.add %588, %632 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %634 = stablehlo.convolution(%633, %arg578) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<128x128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %635 = stablehlo.broadcast_in_dim %634, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %636 = stablehlo.broadcast_in_dim %arg775, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %637 = stablehlo.subtract %635, %636 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %638 = stablehlo.broadcast_in_dim %637, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %639 = stablehlo.broadcast_in_dim %arg776, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %640 = stablehlo.multiply %638, %639 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %641 = stablehlo.broadcast_in_dim %640, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %642 = stablehlo.broadcast_in_dim %arg777, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %643 = stablehlo.multiply %641, %642 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %644 = stablehlo.broadcast_in_dim %643, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %645 = stablehlo.broadcast_in_dim %arg778, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %646 = stablehlo.add %644, %645 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %647 = stablehlo.exponential %646 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %648 = stablehlo.log_plus_one %647 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %649 = stablehlo.broadcast_in_dim %646, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %650 = stablehlo.compare  GT, %649, %425,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xi1>
      %651 = stablehlo.broadcast_in_dim %650, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xi1>) -> tensor<4x128x60x80xi1>
      %652 = stablehlo.broadcast_in_dim %648, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %653 = stablehlo.select %651, %649, %652 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xi1>, tensor<4x128x60x80xbf16>
      %654 = stablehlo.tanh %653 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %655 = stablehlo.multiply %646, %654 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %656 = stablehlo.convolution(%655, %arg579) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<128x128x3x3xbf16>) -> tensor<4x128x60x80xbf16>
      %657 = stablehlo.broadcast_in_dim %656, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %658 = stablehlo.broadcast_in_dim %arg779, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %659 = stablehlo.subtract %657, %658 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %660 = stablehlo.broadcast_in_dim %659, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %661 = stablehlo.broadcast_in_dim %arg780, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %662 = stablehlo.multiply %660, %661 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %663 = stablehlo.broadcast_in_dim %662, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %664 = stablehlo.broadcast_in_dim %arg781, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %665 = stablehlo.multiply %663, %664 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %666 = stablehlo.broadcast_in_dim %665, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %667 = stablehlo.broadcast_in_dim %arg782, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %668 = stablehlo.add %666, %667 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %669 = stablehlo.exponential %668 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %670 = stablehlo.log_plus_one %669 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %671 = stablehlo.broadcast_in_dim %668, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %672 = stablehlo.compare  GT, %671, %425,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xi1>
      %673 = stablehlo.broadcast_in_dim %672, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xi1>) -> tensor<4x128x60x80xi1>
      %674 = stablehlo.broadcast_in_dim %670, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %675 = stablehlo.select %673, %671, %674 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xi1>, tensor<4x128x60x80xbf16>
      %676 = stablehlo.tanh %675 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %677 = stablehlo.multiply %668, %676 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %678 = stablehlo.add %633, %677 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %679 = stablehlo.convolution(%678, %arg580) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<128x128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %680 = stablehlo.broadcast_in_dim %679, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %681 = stablehlo.broadcast_in_dim %arg783, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %682 = stablehlo.subtract %680, %681 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %683 = stablehlo.broadcast_in_dim %682, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %684 = stablehlo.broadcast_in_dim %arg784, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %685 = stablehlo.multiply %683, %684 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %686 = stablehlo.broadcast_in_dim %685, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %687 = stablehlo.broadcast_in_dim %arg785, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %688 = stablehlo.multiply %686, %687 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %689 = stablehlo.broadcast_in_dim %688, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %690 = stablehlo.broadcast_in_dim %arg786, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %691 = stablehlo.add %689, %690 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %692 = stablehlo.exponential %691 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %693 = stablehlo.log_plus_one %692 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %694 = stablehlo.broadcast_in_dim %691, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %695 = stablehlo.compare  GT, %694, %425,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xi1>
      %696 = stablehlo.broadcast_in_dim %695, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xi1>) -> tensor<4x128x60x80xi1>
      %697 = stablehlo.broadcast_in_dim %693, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %698 = stablehlo.select %696, %694, %697 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xi1>, tensor<4x128x60x80xbf16>
      %699 = stablehlo.tanh %698 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %700 = stablehlo.multiply %691, %699 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %701 = stablehlo.convolution(%700, %arg581) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<128x128x3x3xbf16>) -> tensor<4x128x60x80xbf16>
      %702 = stablehlo.broadcast_in_dim %701, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %703 = stablehlo.broadcast_in_dim %arg787, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %704 = stablehlo.subtract %702, %703 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %705 = stablehlo.broadcast_in_dim %704, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %706 = stablehlo.broadcast_in_dim %arg788, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %707 = stablehlo.multiply %705, %706 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %708 = stablehlo.broadcast_in_dim %707, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %709 = stablehlo.broadcast_in_dim %arg789, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %710 = stablehlo.multiply %708, %709 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %711 = stablehlo.broadcast_in_dim %710, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %712 = stablehlo.broadcast_in_dim %arg790, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %713 = stablehlo.add %711, %712 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %714 = stablehlo.exponential %713 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %715 = stablehlo.log_plus_one %714 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %716 = stablehlo.broadcast_in_dim %713, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %717 = stablehlo.compare  GT, %716, %425,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xi1>
      %718 = stablehlo.broadcast_in_dim %717, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xi1>) -> tensor<4x128x60x80xi1>
      %719 = stablehlo.broadcast_in_dim %715, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %720 = stablehlo.select %718, %716, %719 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xi1>, tensor<4x128x60x80xbf16>
      %721 = stablehlo.tanh %720 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %722 = stablehlo.multiply %713, %721 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %723 = stablehlo.add %678, %722 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %724 = stablehlo.convolution(%723, %arg582) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<128x128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %725 = stablehlo.broadcast_in_dim %724, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %726 = stablehlo.broadcast_in_dim %arg791, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %727 = stablehlo.subtract %725, %726 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %728 = stablehlo.broadcast_in_dim %727, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %729 = stablehlo.broadcast_in_dim %arg792, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %730 = stablehlo.multiply %728, %729 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %731 = stablehlo.broadcast_in_dim %730, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %732 = stablehlo.broadcast_in_dim %arg793, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %733 = stablehlo.multiply %731, %732 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %734 = stablehlo.broadcast_in_dim %733, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %735 = stablehlo.broadcast_in_dim %arg794, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %736 = stablehlo.add %734, %735 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %737 = stablehlo.exponential %736 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %738 = stablehlo.log_plus_one %737 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %739 = stablehlo.broadcast_in_dim %736, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %740 = stablehlo.compare  GT, %739, %425,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xi1>
      %741 = stablehlo.broadcast_in_dim %740, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xi1>) -> tensor<4x128x60x80xi1>
      %742 = stablehlo.broadcast_in_dim %738, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %743 = stablehlo.select %741, %739, %742 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xi1>, tensor<4x128x60x80xbf16>
      %744 = stablehlo.tanh %743 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %745 = stablehlo.multiply %736, %744 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %746 = stablehlo.convolution(%745, %arg583) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<128x128x3x3xbf16>) -> tensor<4x128x60x80xbf16>
      %747 = stablehlo.broadcast_in_dim %746, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %748 = stablehlo.broadcast_in_dim %arg795, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %749 = stablehlo.subtract %747, %748 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %750 = stablehlo.broadcast_in_dim %749, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %751 = stablehlo.broadcast_in_dim %arg796, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %752 = stablehlo.multiply %750, %751 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %753 = stablehlo.broadcast_in_dim %752, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %754 = stablehlo.broadcast_in_dim %arg797, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %755 = stablehlo.multiply %753, %754 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %756 = stablehlo.broadcast_in_dim %755, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %757 = stablehlo.broadcast_in_dim %arg798, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %758 = stablehlo.add %756, %757 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %759 = stablehlo.exponential %758 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %760 = stablehlo.log_plus_one %759 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %761 = stablehlo.broadcast_in_dim %758, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %762 = stablehlo.compare  GT, %761, %425,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xi1>
      %763 = stablehlo.broadcast_in_dim %762, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xi1>) -> tensor<4x128x60x80xi1>
      %764 = stablehlo.broadcast_in_dim %760, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %765 = stablehlo.select %763, %761, %764 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xi1>, tensor<4x128x60x80xbf16>
      %766 = stablehlo.tanh %765 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %767 = stablehlo.multiply %758, %766 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %768 = stablehlo.add %723, %767 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %769 = stablehlo.convolution(%768, %arg584) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<128x128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %770 = stablehlo.broadcast_in_dim %769, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %771 = stablehlo.broadcast_in_dim %arg799, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %772 = stablehlo.subtract %770, %771 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %773 = stablehlo.broadcast_in_dim %772, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %774 = stablehlo.broadcast_in_dim %arg800, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %775 = stablehlo.multiply %773, %774 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %776 = stablehlo.broadcast_in_dim %775, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %777 = stablehlo.broadcast_in_dim %arg801, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %778 = stablehlo.multiply %776, %777 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %779 = stablehlo.broadcast_in_dim %778, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %780 = stablehlo.broadcast_in_dim %arg802, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %781 = stablehlo.add %779, %780 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %782 = stablehlo.exponential %781 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %783 = stablehlo.log_plus_one %782 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %784 = stablehlo.broadcast_in_dim %781, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %785 = stablehlo.compare  GT, %784, %425,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xi1>
      %786 = stablehlo.broadcast_in_dim %785, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xi1>) -> tensor<4x128x60x80xi1>
      %787 = stablehlo.broadcast_in_dim %783, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %788 = stablehlo.select %786, %784, %787 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xi1>, tensor<4x128x60x80xbf16>
      %789 = stablehlo.tanh %788 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %790 = stablehlo.multiply %781, %789 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %791 = stablehlo.convolution(%790, %arg585) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<128x128x3x3xbf16>) -> tensor<4x128x60x80xbf16>
      %792 = stablehlo.broadcast_in_dim %791, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %793 = stablehlo.broadcast_in_dim %arg803, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %794 = stablehlo.subtract %792, %793 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %795 = stablehlo.broadcast_in_dim %794, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %796 = stablehlo.broadcast_in_dim %arg804, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %797 = stablehlo.multiply %795, %796 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %798 = stablehlo.broadcast_in_dim %797, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %799 = stablehlo.broadcast_in_dim %arg805, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %800 = stablehlo.multiply %798, %799 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %801 = stablehlo.broadcast_in_dim %800, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %802 = stablehlo.broadcast_in_dim %arg806, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %803 = stablehlo.add %801, %802 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %804 = stablehlo.exponential %803 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %805 = stablehlo.log_plus_one %804 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %806 = stablehlo.broadcast_in_dim %803, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %807 = stablehlo.compare  GT, %806, %425,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xi1>
      %808 = stablehlo.broadcast_in_dim %807, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xi1>) -> tensor<4x128x60x80xi1>
      %809 = stablehlo.broadcast_in_dim %805, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %810 = stablehlo.select %808, %806, %809 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xi1>, tensor<4x128x60x80xbf16>
      %811 = stablehlo.tanh %810 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %812 = stablehlo.multiply %803, %811 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %813 = stablehlo.add %768, %812 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %814 = stablehlo.convolution(%813, %arg586) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<128x128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %815 = stablehlo.broadcast_in_dim %814, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %816 = stablehlo.broadcast_in_dim %arg807, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %817 = stablehlo.subtract %815, %816 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %818 = stablehlo.broadcast_in_dim %817, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %819 = stablehlo.broadcast_in_dim %arg808, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %820 = stablehlo.multiply %818, %819 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %821 = stablehlo.broadcast_in_dim %820, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %822 = stablehlo.broadcast_in_dim %arg809, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %823 = stablehlo.multiply %821, %822 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %824 = stablehlo.broadcast_in_dim %823, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %825 = stablehlo.broadcast_in_dim %arg810, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %826 = stablehlo.add %824, %825 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %827 = stablehlo.exponential %826 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %828 = stablehlo.log_plus_one %827 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %829 = stablehlo.broadcast_in_dim %826, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %830 = stablehlo.compare  GT, %829, %425,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xi1>
      %831 = stablehlo.broadcast_in_dim %830, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xi1>) -> tensor<4x128x60x80xi1>
      %832 = stablehlo.broadcast_in_dim %828, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %833 = stablehlo.select %831, %829, %832 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xi1>, tensor<4x128x60x80xbf16>
      %834 = stablehlo.tanh %833 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %835 = stablehlo.multiply %826, %834 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %836 = stablehlo.concatenate %835, %431, dim = 1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<4x128x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %837 = stablehlo.convolution(%836, %arg587) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>, tensor<256x256x1x1xbf16>) -> tensor<4x256x60x80xbf16>
      %838 = stablehlo.broadcast_in_dim %837, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %839 = stablehlo.broadcast_in_dim %arg811, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x60x80xbf16>
      %840 = stablehlo.subtract %838, %839 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %841 = stablehlo.broadcast_in_dim %840, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %842 = stablehlo.broadcast_in_dim %arg812, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x60x80xbf16>
      %843 = stablehlo.multiply %841, %842 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %844 = stablehlo.broadcast_in_dim %843, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %845 = stablehlo.broadcast_in_dim %arg813, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x60x80xbf16>
      %846 = stablehlo.multiply %844, %845 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %847 = stablehlo.broadcast_in_dim %846, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %848 = stablehlo.broadcast_in_dim %arg814, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x60x80xbf16>
      %849 = stablehlo.add %847, %848 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %850 = stablehlo.exponential %849 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %851 = stablehlo.log_plus_one %850 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %852 = stablehlo.broadcast_in_dim %849, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %853 = stablehlo.compare  GT, %852, %402,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>, tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xi1>
      %854 = stablehlo.broadcast_in_dim %853, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xi1>) -> tensor<4x256x60x80xi1>
      %855 = stablehlo.broadcast_in_dim %851, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %856 = stablehlo.select %854, %852, %855 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xi1>, tensor<4x256x60x80xbf16>
      %857 = stablehlo.tanh %856 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %858 = stablehlo.multiply %849, %857 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %859 = stablehlo.convolution(%858, %arg588) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [2, 2], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>, tensor<512x256x3x3xbf16>) -> tensor<4x512x30x40xbf16>
      %860 = stablehlo.broadcast_in_dim %859, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %861 = stablehlo.broadcast_in_dim %arg815, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x30x40xbf16>
      %862 = stablehlo.subtract %860, %861 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %863 = stablehlo.broadcast_in_dim %862, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %864 = stablehlo.broadcast_in_dim %arg816, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x30x40xbf16>
      %865 = stablehlo.multiply %863, %864 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %866 = stablehlo.broadcast_in_dim %865, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %867 = stablehlo.broadcast_in_dim %arg817, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x30x40xbf16>
      %868 = stablehlo.multiply %866, %867 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %869 = stablehlo.broadcast_in_dim %868, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %870 = stablehlo.broadcast_in_dim %arg818, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x30x40xbf16>
      %871 = stablehlo.add %869, %870 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %872 = stablehlo.exponential %871 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %873 = stablehlo.log_plus_one %872 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %874 = stablehlo.broadcast_in_dim %871, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %875 = stablehlo.broadcast_in_dim %16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<4x512x30x40xbf16>
      %876 = stablehlo.compare  GT, %874, %875,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>, tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xi1>
      %877 = stablehlo.broadcast_in_dim %876, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xi1>) -> tensor<4x512x30x40xi1>
      %878 = stablehlo.broadcast_in_dim %873, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %879 = stablehlo.select %877, %874, %878 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xi1>, tensor<4x512x30x40xbf16>
      %880 = stablehlo.tanh %879 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %881 = stablehlo.multiply %871, %880 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %882 = stablehlo.convolution(%881, %arg589) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>, tensor<256x512x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %883 = stablehlo.broadcast_in_dim %882, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %884 = stablehlo.broadcast_in_dim %arg819, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %885 = stablehlo.subtract %883, %884 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %886 = stablehlo.broadcast_in_dim %885, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %887 = stablehlo.broadcast_in_dim %arg820, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %888 = stablehlo.multiply %886, %887 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %889 = stablehlo.broadcast_in_dim %888, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %890 = stablehlo.broadcast_in_dim %arg821, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %891 = stablehlo.multiply %889, %890 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %892 = stablehlo.broadcast_in_dim %891, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %893 = stablehlo.broadcast_in_dim %arg822, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %894 = stablehlo.add %892, %893 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %895 = stablehlo.exponential %894 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %896 = stablehlo.log_plus_one %895 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %897 = stablehlo.broadcast_in_dim %894, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %898 = stablehlo.broadcast_in_dim %16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<4x256x30x40xbf16>
      %899 = stablehlo.compare  GT, %897, %898,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xi1>
      %900 = stablehlo.broadcast_in_dim %899, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xi1>) -> tensor<4x256x30x40xi1>
      %901 = stablehlo.broadcast_in_dim %896, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %902 = stablehlo.select %900, %897, %901 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xi1>, tensor<4x256x30x40xbf16>
      %903 = stablehlo.tanh %902 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %904 = stablehlo.multiply %894, %903 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %905 = stablehlo.convolution(%881, %arg590) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>, tensor<256x512x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %906 = stablehlo.broadcast_in_dim %905, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %907 = stablehlo.broadcast_in_dim %arg823, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %908 = stablehlo.subtract %906, %907 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %909 = stablehlo.broadcast_in_dim %908, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %910 = stablehlo.broadcast_in_dim %arg824, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %911 = stablehlo.multiply %909, %910 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %912 = stablehlo.broadcast_in_dim %911, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %913 = stablehlo.broadcast_in_dim %arg825, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %914 = stablehlo.multiply %912, %913 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %915 = stablehlo.broadcast_in_dim %914, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %916 = stablehlo.broadcast_in_dim %arg826, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %917 = stablehlo.add %915, %916 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %918 = stablehlo.exponential %917 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %919 = stablehlo.log_plus_one %918 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %920 = stablehlo.broadcast_in_dim %917, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %921 = stablehlo.compare  GT, %920, %898,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xi1>
      %922 = stablehlo.broadcast_in_dim %921, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xi1>) -> tensor<4x256x30x40xi1>
      %923 = stablehlo.broadcast_in_dim %919, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %924 = stablehlo.select %922, %920, %923 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xi1>, tensor<4x256x30x40xbf16>
      %925 = stablehlo.tanh %924 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %926 = stablehlo.multiply %917, %925 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %927 = stablehlo.convolution(%926, %arg591) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<256x256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %928 = stablehlo.broadcast_in_dim %927, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %929 = stablehlo.broadcast_in_dim %arg827, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %930 = stablehlo.subtract %928, %929 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %931 = stablehlo.broadcast_in_dim %930, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %932 = stablehlo.broadcast_in_dim %arg828, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %933 = stablehlo.multiply %931, %932 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %934 = stablehlo.broadcast_in_dim %933, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %935 = stablehlo.broadcast_in_dim %arg829, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %936 = stablehlo.multiply %934, %935 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %937 = stablehlo.broadcast_in_dim %936, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %938 = stablehlo.broadcast_in_dim %arg830, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %939 = stablehlo.add %937, %938 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %940 = stablehlo.exponential %939 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %941 = stablehlo.log_plus_one %940 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %942 = stablehlo.broadcast_in_dim %939, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %943 = stablehlo.compare  GT, %942, %898,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xi1>
      %944 = stablehlo.broadcast_in_dim %943, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xi1>) -> tensor<4x256x30x40xi1>
      %945 = stablehlo.broadcast_in_dim %941, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %946 = stablehlo.select %944, %942, %945 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xi1>, tensor<4x256x30x40xbf16>
      %947 = stablehlo.tanh %946 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %948 = stablehlo.multiply %939, %947 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %949 = stablehlo.convolution(%948, %arg592) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<256x256x3x3xbf16>) -> tensor<4x256x30x40xbf16>
      %950 = stablehlo.broadcast_in_dim %949, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %951 = stablehlo.broadcast_in_dim %arg831, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %952 = stablehlo.subtract %950, %951 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %953 = stablehlo.broadcast_in_dim %952, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %954 = stablehlo.broadcast_in_dim %arg832, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %955 = stablehlo.multiply %953, %954 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %956 = stablehlo.broadcast_in_dim %955, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %957 = stablehlo.broadcast_in_dim %arg833, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %958 = stablehlo.multiply %956, %957 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %959 = stablehlo.broadcast_in_dim %958, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %960 = stablehlo.broadcast_in_dim %arg834, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %961 = stablehlo.add %959, %960 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %962 = stablehlo.exponential %961 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %963 = stablehlo.log_plus_one %962 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %964 = stablehlo.broadcast_in_dim %961, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %965 = stablehlo.compare  GT, %964, %898,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xi1>
      %966 = stablehlo.broadcast_in_dim %965, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xi1>) -> tensor<4x256x30x40xi1>
      %967 = stablehlo.broadcast_in_dim %963, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %968 = stablehlo.select %966, %964, %967 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xi1>, tensor<4x256x30x40xbf16>
      %969 = stablehlo.tanh %968 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %970 = stablehlo.multiply %961, %969 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %971 = stablehlo.add %926, %970 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %972 = stablehlo.convolution(%971, %arg593) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<256x256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %973 = stablehlo.broadcast_in_dim %972, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %974 = stablehlo.broadcast_in_dim %arg835, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %975 = stablehlo.subtract %973, %974 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %976 = stablehlo.broadcast_in_dim %975, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %977 = stablehlo.broadcast_in_dim %arg836, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %978 = stablehlo.multiply %976, %977 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %979 = stablehlo.broadcast_in_dim %978, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %980 = stablehlo.broadcast_in_dim %arg837, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %981 = stablehlo.multiply %979, %980 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %982 = stablehlo.broadcast_in_dim %981, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %983 = stablehlo.broadcast_in_dim %arg838, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %984 = stablehlo.add %982, %983 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %985 = stablehlo.exponential %984 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %986 = stablehlo.log_plus_one %985 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %987 = stablehlo.broadcast_in_dim %984, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %988 = stablehlo.compare  GT, %987, %898,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xi1>
      %989 = stablehlo.broadcast_in_dim %988, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xi1>) -> tensor<4x256x30x40xi1>
      %990 = stablehlo.broadcast_in_dim %986, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %991 = stablehlo.select %989, %987, %990 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xi1>, tensor<4x256x30x40xbf16>
      %992 = stablehlo.tanh %991 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %993 = stablehlo.multiply %984, %992 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %994 = stablehlo.convolution(%993, %arg594) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<256x256x3x3xbf16>) -> tensor<4x256x30x40xbf16>
      %995 = stablehlo.broadcast_in_dim %994, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %996 = stablehlo.broadcast_in_dim %arg839, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %997 = stablehlo.subtract %995, %996 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %998 = stablehlo.broadcast_in_dim %997, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %999 = stablehlo.broadcast_in_dim %arg840, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1000 = stablehlo.multiply %998, %999 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1001 = stablehlo.broadcast_in_dim %1000, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1002 = stablehlo.broadcast_in_dim %arg841, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1003 = stablehlo.multiply %1001, %1002 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1004 = stablehlo.broadcast_in_dim %1003, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1005 = stablehlo.broadcast_in_dim %arg842, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1006 = stablehlo.add %1004, %1005 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1007 = stablehlo.exponential %1006 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1008 = stablehlo.log_plus_one %1007 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1009 = stablehlo.broadcast_in_dim %1006, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1010 = stablehlo.compare  GT, %1009, %898,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xi1>
      %1011 = stablehlo.broadcast_in_dim %1010, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xi1>) -> tensor<4x256x30x40xi1>
      %1012 = stablehlo.broadcast_in_dim %1008, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1013 = stablehlo.select %1011, %1009, %1012 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xi1>, tensor<4x256x30x40xbf16>
      %1014 = stablehlo.tanh %1013 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1015 = stablehlo.multiply %1006, %1014 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1016 = stablehlo.add %971, %1015 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1017 = stablehlo.convolution(%1016, %arg595) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<256x256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1018 = stablehlo.broadcast_in_dim %1017, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1019 = stablehlo.broadcast_in_dim %arg843, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1020 = stablehlo.subtract %1018, %1019 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1021 = stablehlo.broadcast_in_dim %1020, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1022 = stablehlo.broadcast_in_dim %arg844, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1023 = stablehlo.multiply %1021, %1022 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1024 = stablehlo.broadcast_in_dim %1023, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1025 = stablehlo.broadcast_in_dim %arg845, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1026 = stablehlo.multiply %1024, %1025 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1027 = stablehlo.broadcast_in_dim %1026, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1028 = stablehlo.broadcast_in_dim %arg846, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1029 = stablehlo.add %1027, %1028 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1030 = stablehlo.exponential %1029 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1031 = stablehlo.log_plus_one %1030 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1032 = stablehlo.broadcast_in_dim %1029, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1033 = stablehlo.compare  GT, %1032, %898,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xi1>
      %1034 = stablehlo.broadcast_in_dim %1033, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xi1>) -> tensor<4x256x30x40xi1>
      %1035 = stablehlo.broadcast_in_dim %1031, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1036 = stablehlo.select %1034, %1032, %1035 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xi1>, tensor<4x256x30x40xbf16>
      %1037 = stablehlo.tanh %1036 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1038 = stablehlo.multiply %1029, %1037 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1039 = stablehlo.convolution(%1038, %arg596) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<256x256x3x3xbf16>) -> tensor<4x256x30x40xbf16>
      %1040 = stablehlo.broadcast_in_dim %1039, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1041 = stablehlo.broadcast_in_dim %arg847, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1042 = stablehlo.subtract %1040, %1041 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1043 = stablehlo.broadcast_in_dim %1042, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1044 = stablehlo.broadcast_in_dim %arg848, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1045 = stablehlo.multiply %1043, %1044 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1046 = stablehlo.broadcast_in_dim %1045, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1047 = stablehlo.broadcast_in_dim %arg849, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1048 = stablehlo.multiply %1046, %1047 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1049 = stablehlo.broadcast_in_dim %1048, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1050 = stablehlo.broadcast_in_dim %arg850, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1051 = stablehlo.add %1049, %1050 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1052 = stablehlo.exponential %1051 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1053 = stablehlo.log_plus_one %1052 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1054 = stablehlo.broadcast_in_dim %1051, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1055 = stablehlo.compare  GT, %1054, %898,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xi1>
      %1056 = stablehlo.broadcast_in_dim %1055, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xi1>) -> tensor<4x256x30x40xi1>
      %1057 = stablehlo.broadcast_in_dim %1053, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1058 = stablehlo.select %1056, %1054, %1057 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xi1>, tensor<4x256x30x40xbf16>
      %1059 = stablehlo.tanh %1058 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1060 = stablehlo.multiply %1051, %1059 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1061 = stablehlo.add %1016, %1060 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1062 = stablehlo.convolution(%1061, %arg597) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<256x256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1063 = stablehlo.broadcast_in_dim %1062, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1064 = stablehlo.broadcast_in_dim %arg851, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1065 = stablehlo.subtract %1063, %1064 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1066 = stablehlo.broadcast_in_dim %1065, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1067 = stablehlo.broadcast_in_dim %arg852, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1068 = stablehlo.multiply %1066, %1067 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1069 = stablehlo.broadcast_in_dim %1068, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1070 = stablehlo.broadcast_in_dim %arg853, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1071 = stablehlo.multiply %1069, %1070 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1072 = stablehlo.broadcast_in_dim %1071, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1073 = stablehlo.broadcast_in_dim %arg854, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1074 = stablehlo.add %1072, %1073 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1075 = stablehlo.exponential %1074 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1076 = stablehlo.log_plus_one %1075 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1077 = stablehlo.broadcast_in_dim %1074, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1078 = stablehlo.compare  GT, %1077, %898,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xi1>
      %1079 = stablehlo.broadcast_in_dim %1078, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xi1>) -> tensor<4x256x30x40xi1>
      %1080 = stablehlo.broadcast_in_dim %1076, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1081 = stablehlo.select %1079, %1077, %1080 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xi1>, tensor<4x256x30x40xbf16>
      %1082 = stablehlo.tanh %1081 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1083 = stablehlo.multiply %1074, %1082 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1084 = stablehlo.convolution(%1083, %arg598) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<256x256x3x3xbf16>) -> tensor<4x256x30x40xbf16>
      %1085 = stablehlo.broadcast_in_dim %1084, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1086 = stablehlo.broadcast_in_dim %arg855, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1087 = stablehlo.subtract %1085, %1086 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1088 = stablehlo.broadcast_in_dim %1087, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1089 = stablehlo.broadcast_in_dim %arg856, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1090 = stablehlo.multiply %1088, %1089 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1091 = stablehlo.broadcast_in_dim %1090, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1092 = stablehlo.broadcast_in_dim %arg857, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1093 = stablehlo.multiply %1091, %1092 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1094 = stablehlo.broadcast_in_dim %1093, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1095 = stablehlo.broadcast_in_dim %arg858, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1096 = stablehlo.add %1094, %1095 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1097 = stablehlo.exponential %1096 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1098 = stablehlo.log_plus_one %1097 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1099 = stablehlo.broadcast_in_dim %1096, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1100 = stablehlo.compare  GT, %1099, %898,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xi1>
      %1101 = stablehlo.broadcast_in_dim %1100, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xi1>) -> tensor<4x256x30x40xi1>
      %1102 = stablehlo.broadcast_in_dim %1098, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1103 = stablehlo.select %1101, %1099, %1102 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xi1>, tensor<4x256x30x40xbf16>
      %1104 = stablehlo.tanh %1103 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1105 = stablehlo.multiply %1096, %1104 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1106 = stablehlo.add %1061, %1105 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1107 = stablehlo.convolution(%1106, %arg599) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<256x256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1108 = stablehlo.broadcast_in_dim %1107, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1109 = stablehlo.broadcast_in_dim %arg859, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1110 = stablehlo.subtract %1108, %1109 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1111 = stablehlo.broadcast_in_dim %1110, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1112 = stablehlo.broadcast_in_dim %arg860, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1113 = stablehlo.multiply %1111, %1112 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1114 = stablehlo.broadcast_in_dim %1113, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1115 = stablehlo.broadcast_in_dim %arg861, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1116 = stablehlo.multiply %1114, %1115 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1117 = stablehlo.broadcast_in_dim %1116, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1118 = stablehlo.broadcast_in_dim %arg862, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1119 = stablehlo.add %1117, %1118 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1120 = stablehlo.exponential %1119 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1121 = stablehlo.log_plus_one %1120 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1122 = stablehlo.broadcast_in_dim %1119, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1123 = stablehlo.compare  GT, %1122, %898,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xi1>
      %1124 = stablehlo.broadcast_in_dim %1123, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xi1>) -> tensor<4x256x30x40xi1>
      %1125 = stablehlo.broadcast_in_dim %1121, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1126 = stablehlo.select %1124, %1122, %1125 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xi1>, tensor<4x256x30x40xbf16>
      %1127 = stablehlo.tanh %1126 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1128 = stablehlo.multiply %1119, %1127 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1129 = stablehlo.convolution(%1128, %arg600) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<256x256x3x3xbf16>) -> tensor<4x256x30x40xbf16>
      %1130 = stablehlo.broadcast_in_dim %1129, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1131 = stablehlo.broadcast_in_dim %arg863, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1132 = stablehlo.subtract %1130, %1131 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1133 = stablehlo.broadcast_in_dim %1132, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1134 = stablehlo.broadcast_in_dim %arg864, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1135 = stablehlo.multiply %1133, %1134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1136 = stablehlo.broadcast_in_dim %1135, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1137 = stablehlo.broadcast_in_dim %arg865, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1138 = stablehlo.multiply %1136, %1137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1139 = stablehlo.broadcast_in_dim %1138, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1140 = stablehlo.broadcast_in_dim %arg866, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1141 = stablehlo.add %1139, %1140 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1142 = stablehlo.exponential %1141 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1143 = stablehlo.log_plus_one %1142 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1144 = stablehlo.broadcast_in_dim %1141, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1145 = stablehlo.compare  GT, %1144, %898,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xi1>
      %1146 = stablehlo.broadcast_in_dim %1145, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xi1>) -> tensor<4x256x30x40xi1>
      %1147 = stablehlo.broadcast_in_dim %1143, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1148 = stablehlo.select %1146, %1144, %1147 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xi1>, tensor<4x256x30x40xbf16>
      %1149 = stablehlo.tanh %1148 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1150 = stablehlo.multiply %1141, %1149 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1151 = stablehlo.add %1106, %1150 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1152 = stablehlo.convolution(%1151, %arg601) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<256x256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1153 = stablehlo.broadcast_in_dim %1152, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1154 = stablehlo.broadcast_in_dim %arg867, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1155 = stablehlo.subtract %1153, %1154 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1156 = stablehlo.broadcast_in_dim %1155, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1157 = stablehlo.broadcast_in_dim %arg868, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1158 = stablehlo.multiply %1156, %1157 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1159 = stablehlo.broadcast_in_dim %1158, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1160 = stablehlo.broadcast_in_dim %arg869, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1161 = stablehlo.multiply %1159, %1160 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1162 = stablehlo.broadcast_in_dim %1161, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1163 = stablehlo.broadcast_in_dim %arg870, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1164 = stablehlo.add %1162, %1163 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1165 = stablehlo.exponential %1164 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1166 = stablehlo.log_plus_one %1165 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1167 = stablehlo.broadcast_in_dim %1164, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1168 = stablehlo.compare  GT, %1167, %898,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xi1>
      %1169 = stablehlo.broadcast_in_dim %1168, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xi1>) -> tensor<4x256x30x40xi1>
      %1170 = stablehlo.broadcast_in_dim %1166, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1171 = stablehlo.select %1169, %1167, %1170 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xi1>, tensor<4x256x30x40xbf16>
      %1172 = stablehlo.tanh %1171 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1173 = stablehlo.multiply %1164, %1172 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1174 = stablehlo.convolution(%1173, %arg602) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<256x256x3x3xbf16>) -> tensor<4x256x30x40xbf16>
      %1175 = stablehlo.broadcast_in_dim %1174, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1176 = stablehlo.broadcast_in_dim %arg871, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1177 = stablehlo.subtract %1175, %1176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1178 = stablehlo.broadcast_in_dim %1177, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1179 = stablehlo.broadcast_in_dim %arg872, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1180 = stablehlo.multiply %1178, %1179 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1181 = stablehlo.broadcast_in_dim %1180, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1182 = stablehlo.broadcast_in_dim %arg873, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1183 = stablehlo.multiply %1181, %1182 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1184 = stablehlo.broadcast_in_dim %1183, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1185 = stablehlo.broadcast_in_dim %arg874, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1186 = stablehlo.add %1184, %1185 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1187 = stablehlo.exponential %1186 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1188 = stablehlo.log_plus_one %1187 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1189 = stablehlo.broadcast_in_dim %1186, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1190 = stablehlo.compare  GT, %1189, %898,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xi1>
      %1191 = stablehlo.broadcast_in_dim %1190, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xi1>) -> tensor<4x256x30x40xi1>
      %1192 = stablehlo.broadcast_in_dim %1188, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1193 = stablehlo.select %1191, %1189, %1192 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xi1>, tensor<4x256x30x40xbf16>
      %1194 = stablehlo.tanh %1193 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1195 = stablehlo.multiply %1186, %1194 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1196 = stablehlo.add %1151, %1195 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1197 = stablehlo.convolution(%1196, %arg603) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<256x256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1198 = stablehlo.broadcast_in_dim %1197, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1199 = stablehlo.broadcast_in_dim %arg875, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1200 = stablehlo.subtract %1198, %1199 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1201 = stablehlo.broadcast_in_dim %1200, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1202 = stablehlo.broadcast_in_dim %arg876, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1203 = stablehlo.multiply %1201, %1202 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1204 = stablehlo.broadcast_in_dim %1203, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1205 = stablehlo.broadcast_in_dim %arg877, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1206 = stablehlo.multiply %1204, %1205 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1207 = stablehlo.broadcast_in_dim %1206, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1208 = stablehlo.broadcast_in_dim %arg878, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1209 = stablehlo.add %1207, %1208 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1210 = stablehlo.exponential %1209 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1211 = stablehlo.log_plus_one %1210 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1212 = stablehlo.broadcast_in_dim %1209, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1213 = stablehlo.compare  GT, %1212, %898,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xi1>
      %1214 = stablehlo.broadcast_in_dim %1213, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xi1>) -> tensor<4x256x30x40xi1>
      %1215 = stablehlo.broadcast_in_dim %1211, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1216 = stablehlo.select %1214, %1212, %1215 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xi1>, tensor<4x256x30x40xbf16>
      %1217 = stablehlo.tanh %1216 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1218 = stablehlo.multiply %1209, %1217 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1219 = stablehlo.convolution(%1218, %arg604) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<256x256x3x3xbf16>) -> tensor<4x256x30x40xbf16>
      %1220 = stablehlo.broadcast_in_dim %1219, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1221 = stablehlo.broadcast_in_dim %arg879, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1222 = stablehlo.subtract %1220, %1221 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1223 = stablehlo.broadcast_in_dim %1222, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1224 = stablehlo.broadcast_in_dim %arg880, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1225 = stablehlo.multiply %1223, %1224 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1226 = stablehlo.broadcast_in_dim %1225, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1227 = stablehlo.broadcast_in_dim %arg881, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1228 = stablehlo.multiply %1226, %1227 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1229 = stablehlo.broadcast_in_dim %1228, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1230 = stablehlo.broadcast_in_dim %arg882, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1231 = stablehlo.add %1229, %1230 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1232 = stablehlo.exponential %1231 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1233 = stablehlo.log_plus_one %1232 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1234 = stablehlo.broadcast_in_dim %1231, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1235 = stablehlo.compare  GT, %1234, %898,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xi1>
      %1236 = stablehlo.broadcast_in_dim %1235, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xi1>) -> tensor<4x256x30x40xi1>
      %1237 = stablehlo.broadcast_in_dim %1233, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1238 = stablehlo.select %1236, %1234, %1237 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xi1>, tensor<4x256x30x40xbf16>
      %1239 = stablehlo.tanh %1238 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1240 = stablehlo.multiply %1231, %1239 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1241 = stablehlo.add %1196, %1240 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1242 = stablehlo.convolution(%1241, %arg605) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<256x256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1243 = stablehlo.broadcast_in_dim %1242, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1244 = stablehlo.broadcast_in_dim %arg883, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1245 = stablehlo.subtract %1243, %1244 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1246 = stablehlo.broadcast_in_dim %1245, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1247 = stablehlo.broadcast_in_dim %arg884, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1248 = stablehlo.multiply %1246, %1247 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1249 = stablehlo.broadcast_in_dim %1248, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1250 = stablehlo.broadcast_in_dim %arg885, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1251 = stablehlo.multiply %1249, %1250 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1252 = stablehlo.broadcast_in_dim %1251, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1253 = stablehlo.broadcast_in_dim %arg886, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1254 = stablehlo.add %1252, %1253 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1255 = stablehlo.exponential %1254 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1256 = stablehlo.log_plus_one %1255 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1257 = stablehlo.broadcast_in_dim %1254, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1258 = stablehlo.compare  GT, %1257, %898,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xi1>
      %1259 = stablehlo.broadcast_in_dim %1258, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xi1>) -> tensor<4x256x30x40xi1>
      %1260 = stablehlo.broadcast_in_dim %1256, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1261 = stablehlo.select %1259, %1257, %1260 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xi1>, tensor<4x256x30x40xbf16>
      %1262 = stablehlo.tanh %1261 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1263 = stablehlo.multiply %1254, %1262 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1264 = stablehlo.convolution(%1263, %arg606) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<256x256x3x3xbf16>) -> tensor<4x256x30x40xbf16>
      %1265 = stablehlo.broadcast_in_dim %1264, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1266 = stablehlo.broadcast_in_dim %arg887, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1267 = stablehlo.subtract %1265, %1266 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1268 = stablehlo.broadcast_in_dim %1267, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1269 = stablehlo.broadcast_in_dim %arg888, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1270 = stablehlo.multiply %1268, %1269 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1271 = stablehlo.broadcast_in_dim %1270, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1272 = stablehlo.broadcast_in_dim %arg889, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1273 = stablehlo.multiply %1271, %1272 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1274 = stablehlo.broadcast_in_dim %1273, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1275 = stablehlo.broadcast_in_dim %arg890, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1276 = stablehlo.add %1274, %1275 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1277 = stablehlo.exponential %1276 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1278 = stablehlo.log_plus_one %1277 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1279 = stablehlo.broadcast_in_dim %1276, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1280 = stablehlo.compare  GT, %1279, %898,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xi1>
      %1281 = stablehlo.broadcast_in_dim %1280, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xi1>) -> tensor<4x256x30x40xi1>
      %1282 = stablehlo.broadcast_in_dim %1278, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1283 = stablehlo.select %1281, %1279, %1282 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xi1>, tensor<4x256x30x40xbf16>
      %1284 = stablehlo.tanh %1283 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1285 = stablehlo.multiply %1276, %1284 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1286 = stablehlo.add %1241, %1285 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1287 = stablehlo.convolution(%1286, %arg607) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<256x256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1288 = stablehlo.broadcast_in_dim %1287, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1289 = stablehlo.broadcast_in_dim %arg891, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1290 = stablehlo.subtract %1288, %1289 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1291 = stablehlo.broadcast_in_dim %1290, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1292 = stablehlo.broadcast_in_dim %arg892, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1293 = stablehlo.multiply %1291, %1292 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1294 = stablehlo.broadcast_in_dim %1293, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1295 = stablehlo.broadcast_in_dim %arg893, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1296 = stablehlo.multiply %1294, %1295 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1297 = stablehlo.broadcast_in_dim %1296, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1298 = stablehlo.broadcast_in_dim %arg894, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1299 = stablehlo.add %1297, %1298 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1300 = stablehlo.exponential %1299 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1301 = stablehlo.log_plus_one %1300 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1302 = stablehlo.broadcast_in_dim %1299, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1303 = stablehlo.compare  GT, %1302, %898,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xi1>
      %1304 = stablehlo.broadcast_in_dim %1303, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xi1>) -> tensor<4x256x30x40xi1>
      %1305 = stablehlo.broadcast_in_dim %1301, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1306 = stablehlo.select %1304, %1302, %1305 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xi1>, tensor<4x256x30x40xbf16>
      %1307 = stablehlo.tanh %1306 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1308 = stablehlo.multiply %1299, %1307 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1309 = stablehlo.concatenate %1308, %904, dim = 1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<4x256x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %1310 = stablehlo.convolution(%1309, %arg608) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>, tensor<512x512x1x1xbf16>) -> tensor<4x512x30x40xbf16>
      %1311 = stablehlo.broadcast_in_dim %1310, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %1312 = stablehlo.broadcast_in_dim %arg895, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x30x40xbf16>
      %1313 = stablehlo.subtract %1311, %1312 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %1314 = stablehlo.broadcast_in_dim %1313, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %1315 = stablehlo.broadcast_in_dim %arg896, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x30x40xbf16>
      %1316 = stablehlo.multiply %1314, %1315 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %1317 = stablehlo.broadcast_in_dim %1316, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %1318 = stablehlo.broadcast_in_dim %arg897, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x30x40xbf16>
      %1319 = stablehlo.multiply %1317, %1318 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %1320 = stablehlo.broadcast_in_dim %1319, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %1321 = stablehlo.broadcast_in_dim %arg898, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x30x40xbf16>
      %1322 = stablehlo.add %1320, %1321 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %1323 = stablehlo.exponential %1322 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %1324 = stablehlo.log_plus_one %1323 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %1325 = stablehlo.broadcast_in_dim %1322, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %1326 = stablehlo.compare  GT, %1325, %875,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>, tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xi1>
      %1327 = stablehlo.broadcast_in_dim %1326, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xi1>) -> tensor<4x512x30x40xi1>
      %1328 = stablehlo.broadcast_in_dim %1324, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %1329 = stablehlo.select %1327, %1325, %1328 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xi1>, tensor<4x512x30x40xbf16>
      %1330 = stablehlo.tanh %1329 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %1331 = stablehlo.multiply %1322, %1330 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %1332 = stablehlo.convolution(%1331, %arg609) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [2, 2], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>, tensor<1024x512x3x3xbf16>) -> tensor<4x1024x15x20xbf16>
      %1333 = stablehlo.broadcast_in_dim %1332, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %1334 = stablehlo.broadcast_in_dim %arg899, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<1024x1x1xbf16>) -> tensor<4x1024x15x20xbf16>
      %1335 = stablehlo.subtract %1333, %1334 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1336 = stablehlo.broadcast_in_dim %1335, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %1337 = stablehlo.broadcast_in_dim %arg900, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<1024x1x1xbf16>) -> tensor<4x1024x15x20xbf16>
      %1338 = stablehlo.multiply %1336, %1337 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1339 = stablehlo.broadcast_in_dim %1338, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %1340 = stablehlo.broadcast_in_dim %arg901, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<1024x1x1xbf16>) -> tensor<4x1024x15x20xbf16>
      %1341 = stablehlo.multiply %1339, %1340 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1342 = stablehlo.broadcast_in_dim %1341, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %1343 = stablehlo.broadcast_in_dim %arg902, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<1024x1x1xbf16>) -> tensor<4x1024x15x20xbf16>
      %1344 = stablehlo.add %1342, %1343 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1345 = stablehlo.exponential %1344 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1346 = stablehlo.log_plus_one %1345 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1347 = stablehlo.broadcast_in_dim %1344, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %1348 = stablehlo.broadcast_in_dim %16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<4x1024x15x20xbf16>
      %1349 = stablehlo.compare  GT, %1347, %1348,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>, tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xi1>
      %1350 = stablehlo.broadcast_in_dim %1349, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xi1>) -> tensor<4x1024x15x20xi1>
      %1351 = stablehlo.broadcast_in_dim %1346, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %1352 = stablehlo.select %1350, %1347, %1351 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xi1>, tensor<4x1024x15x20xbf16>
      %1353 = stablehlo.tanh %1352 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1354 = stablehlo.multiply %1344, %1353 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1355 = stablehlo.convolution(%1354, %arg610) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>, tensor<512x1024x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1356 = stablehlo.broadcast_in_dim %1355, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1357 = stablehlo.broadcast_in_dim %arg903, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1358 = stablehlo.subtract %1356, %1357 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1359 = stablehlo.broadcast_in_dim %1358, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1360 = stablehlo.broadcast_in_dim %arg904, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1361 = stablehlo.multiply %1359, %1360 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1362 = stablehlo.broadcast_in_dim %1361, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1363 = stablehlo.broadcast_in_dim %arg905, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1364 = stablehlo.multiply %1362, %1363 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1365 = stablehlo.broadcast_in_dim %1364, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1366 = stablehlo.broadcast_in_dim %arg906, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1367 = stablehlo.add %1365, %1366 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1368 = stablehlo.exponential %1367 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1369 = stablehlo.log_plus_one %1368 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1370 = stablehlo.broadcast_in_dim %1367, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1371 = stablehlo.broadcast_in_dim %16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<4x512x15x20xbf16>
      %1372 = stablehlo.compare  GT, %1370, %1371,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xi1>
      %1373 = stablehlo.broadcast_in_dim %1372, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xi1>) -> tensor<4x512x15x20xi1>
      %1374 = stablehlo.broadcast_in_dim %1369, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1375 = stablehlo.select %1373, %1370, %1374 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xi1>, tensor<4x512x15x20xbf16>
      %1376 = stablehlo.tanh %1375 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1377 = stablehlo.multiply %1367, %1376 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1378 = stablehlo.convolution(%1354, %arg611) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>, tensor<512x1024x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1379 = stablehlo.broadcast_in_dim %1378, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1380 = stablehlo.broadcast_in_dim %arg907, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1381 = stablehlo.subtract %1379, %1380 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1382 = stablehlo.broadcast_in_dim %1381, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1383 = stablehlo.broadcast_in_dim %arg908, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1384 = stablehlo.multiply %1382, %1383 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1385 = stablehlo.broadcast_in_dim %1384, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1386 = stablehlo.broadcast_in_dim %arg909, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1387 = stablehlo.multiply %1385, %1386 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1388 = stablehlo.broadcast_in_dim %1387, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1389 = stablehlo.broadcast_in_dim %arg910, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1390 = stablehlo.add %1388, %1389 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1391 = stablehlo.exponential %1390 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1392 = stablehlo.log_plus_one %1391 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1393 = stablehlo.broadcast_in_dim %1390, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1394 = stablehlo.compare  GT, %1393, %1371,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xi1>
      %1395 = stablehlo.broadcast_in_dim %1394, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xi1>) -> tensor<4x512x15x20xi1>
      %1396 = stablehlo.broadcast_in_dim %1392, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1397 = stablehlo.select %1395, %1393, %1396 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xi1>, tensor<4x512x15x20xbf16>
      %1398 = stablehlo.tanh %1397 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1399 = stablehlo.multiply %1390, %1398 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1400 = stablehlo.convolution(%1399, %arg612) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<512x512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1401 = stablehlo.broadcast_in_dim %1400, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1402 = stablehlo.broadcast_in_dim %arg911, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1403 = stablehlo.subtract %1401, %1402 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1404 = stablehlo.broadcast_in_dim %1403, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1405 = stablehlo.broadcast_in_dim %arg912, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1406 = stablehlo.multiply %1404, %1405 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1407 = stablehlo.broadcast_in_dim %1406, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1408 = stablehlo.broadcast_in_dim %arg913, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1409 = stablehlo.multiply %1407, %1408 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1410 = stablehlo.broadcast_in_dim %1409, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1411 = stablehlo.broadcast_in_dim %arg914, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1412 = stablehlo.add %1410, %1411 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1413 = stablehlo.exponential %1412 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1414 = stablehlo.log_plus_one %1413 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1415 = stablehlo.broadcast_in_dim %1412, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1416 = stablehlo.compare  GT, %1415, %1371,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xi1>
      %1417 = stablehlo.broadcast_in_dim %1416, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xi1>) -> tensor<4x512x15x20xi1>
      %1418 = stablehlo.broadcast_in_dim %1414, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1419 = stablehlo.select %1417, %1415, %1418 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xi1>, tensor<4x512x15x20xbf16>
      %1420 = stablehlo.tanh %1419 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1421 = stablehlo.multiply %1412, %1420 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1422 = stablehlo.convolution(%1421, %arg613) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<512x512x3x3xbf16>) -> tensor<4x512x15x20xbf16>
      %1423 = stablehlo.broadcast_in_dim %1422, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1424 = stablehlo.broadcast_in_dim %arg915, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1425 = stablehlo.subtract %1423, %1424 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1426 = stablehlo.broadcast_in_dim %1425, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1427 = stablehlo.broadcast_in_dim %arg916, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1428 = stablehlo.multiply %1426, %1427 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1429 = stablehlo.broadcast_in_dim %1428, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1430 = stablehlo.broadcast_in_dim %arg917, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1431 = stablehlo.multiply %1429, %1430 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1432 = stablehlo.broadcast_in_dim %1431, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1433 = stablehlo.broadcast_in_dim %arg918, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1434 = stablehlo.add %1432, %1433 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1435 = stablehlo.exponential %1434 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1436 = stablehlo.log_plus_one %1435 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1437 = stablehlo.broadcast_in_dim %1434, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1438 = stablehlo.compare  GT, %1437, %1371,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xi1>
      %1439 = stablehlo.broadcast_in_dim %1438, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xi1>) -> tensor<4x512x15x20xi1>
      %1440 = stablehlo.broadcast_in_dim %1436, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1441 = stablehlo.select %1439, %1437, %1440 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xi1>, tensor<4x512x15x20xbf16>
      %1442 = stablehlo.tanh %1441 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1443 = stablehlo.multiply %1434, %1442 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1444 = stablehlo.add %1399, %1443 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1445 = stablehlo.convolution(%1444, %arg614) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<512x512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1446 = stablehlo.broadcast_in_dim %1445, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1447 = stablehlo.broadcast_in_dim %arg919, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1448 = stablehlo.subtract %1446, %1447 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1449 = stablehlo.broadcast_in_dim %1448, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1450 = stablehlo.broadcast_in_dim %arg920, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1451 = stablehlo.multiply %1449, %1450 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1452 = stablehlo.broadcast_in_dim %1451, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1453 = stablehlo.broadcast_in_dim %arg921, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1454 = stablehlo.multiply %1452, %1453 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1455 = stablehlo.broadcast_in_dim %1454, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1456 = stablehlo.broadcast_in_dim %arg922, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1457 = stablehlo.add %1455, %1456 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1458 = stablehlo.exponential %1457 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1459 = stablehlo.log_plus_one %1458 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1460 = stablehlo.broadcast_in_dim %1457, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1461 = stablehlo.compare  GT, %1460, %1371,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xi1>
      %1462 = stablehlo.broadcast_in_dim %1461, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xi1>) -> tensor<4x512x15x20xi1>
      %1463 = stablehlo.broadcast_in_dim %1459, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1464 = stablehlo.select %1462, %1460, %1463 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xi1>, tensor<4x512x15x20xbf16>
      %1465 = stablehlo.tanh %1464 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1466 = stablehlo.multiply %1457, %1465 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1467 = stablehlo.convolution(%1466, %arg615) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<512x512x3x3xbf16>) -> tensor<4x512x15x20xbf16>
      %1468 = stablehlo.broadcast_in_dim %1467, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1469 = stablehlo.broadcast_in_dim %arg923, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1470 = stablehlo.subtract %1468, %1469 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1471 = stablehlo.broadcast_in_dim %1470, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1472 = stablehlo.broadcast_in_dim %arg924, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1473 = stablehlo.multiply %1471, %1472 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1474 = stablehlo.broadcast_in_dim %1473, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1475 = stablehlo.broadcast_in_dim %arg925, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1476 = stablehlo.multiply %1474, %1475 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1477 = stablehlo.broadcast_in_dim %1476, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1478 = stablehlo.broadcast_in_dim %arg926, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1479 = stablehlo.add %1477, %1478 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1480 = stablehlo.exponential %1479 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1481 = stablehlo.log_plus_one %1480 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1482 = stablehlo.broadcast_in_dim %1479, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1483 = stablehlo.compare  GT, %1482, %1371,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xi1>
      %1484 = stablehlo.broadcast_in_dim %1483, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xi1>) -> tensor<4x512x15x20xi1>
      %1485 = stablehlo.broadcast_in_dim %1481, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1486 = stablehlo.select %1484, %1482, %1485 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xi1>, tensor<4x512x15x20xbf16>
      %1487 = stablehlo.tanh %1486 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1488 = stablehlo.multiply %1479, %1487 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1489 = stablehlo.add %1444, %1488 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1490 = stablehlo.convolution(%1489, %arg616) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<512x512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1491 = stablehlo.broadcast_in_dim %1490, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1492 = stablehlo.broadcast_in_dim %arg927, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1493 = stablehlo.subtract %1491, %1492 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1494 = stablehlo.broadcast_in_dim %1493, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1495 = stablehlo.broadcast_in_dim %arg928, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1496 = stablehlo.multiply %1494, %1495 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1497 = stablehlo.broadcast_in_dim %1496, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1498 = stablehlo.broadcast_in_dim %arg929, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1499 = stablehlo.multiply %1497, %1498 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1500 = stablehlo.broadcast_in_dim %1499, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1501 = stablehlo.broadcast_in_dim %arg930, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1502 = stablehlo.add %1500, %1501 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1503 = stablehlo.exponential %1502 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1504 = stablehlo.log_plus_one %1503 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1505 = stablehlo.broadcast_in_dim %1502, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1506 = stablehlo.compare  GT, %1505, %1371,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xi1>
      %1507 = stablehlo.broadcast_in_dim %1506, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xi1>) -> tensor<4x512x15x20xi1>
      %1508 = stablehlo.broadcast_in_dim %1504, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1509 = stablehlo.select %1507, %1505, %1508 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xi1>, tensor<4x512x15x20xbf16>
      %1510 = stablehlo.tanh %1509 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1511 = stablehlo.multiply %1502, %1510 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1512 = stablehlo.convolution(%1511, %arg617) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<512x512x3x3xbf16>) -> tensor<4x512x15x20xbf16>
      %1513 = stablehlo.broadcast_in_dim %1512, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1514 = stablehlo.broadcast_in_dim %arg931, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1515 = stablehlo.subtract %1513, %1514 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1516 = stablehlo.broadcast_in_dim %1515, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1517 = stablehlo.broadcast_in_dim %arg932, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1518 = stablehlo.multiply %1516, %1517 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1519 = stablehlo.broadcast_in_dim %1518, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1520 = stablehlo.broadcast_in_dim %arg933, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1521 = stablehlo.multiply %1519, %1520 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1522 = stablehlo.broadcast_in_dim %1521, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1523 = stablehlo.broadcast_in_dim %arg934, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1524 = stablehlo.add %1522, %1523 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1525 = stablehlo.exponential %1524 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1526 = stablehlo.log_plus_one %1525 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1527 = stablehlo.broadcast_in_dim %1524, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1528 = stablehlo.compare  GT, %1527, %1371,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xi1>
      %1529 = stablehlo.broadcast_in_dim %1528, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xi1>) -> tensor<4x512x15x20xi1>
      %1530 = stablehlo.broadcast_in_dim %1526, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1531 = stablehlo.select %1529, %1527, %1530 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xi1>, tensor<4x512x15x20xbf16>
      %1532 = stablehlo.tanh %1531 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1533 = stablehlo.multiply %1524, %1532 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1534 = stablehlo.add %1489, %1533 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1535 = stablehlo.convolution(%1534, %arg618) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<512x512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1536 = stablehlo.broadcast_in_dim %1535, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1537 = stablehlo.broadcast_in_dim %arg935, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1538 = stablehlo.subtract %1536, %1537 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1539 = stablehlo.broadcast_in_dim %1538, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1540 = stablehlo.broadcast_in_dim %arg936, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1541 = stablehlo.multiply %1539, %1540 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1542 = stablehlo.broadcast_in_dim %1541, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1543 = stablehlo.broadcast_in_dim %arg937, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1544 = stablehlo.multiply %1542, %1543 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1545 = stablehlo.broadcast_in_dim %1544, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1546 = stablehlo.broadcast_in_dim %arg938, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1547 = stablehlo.add %1545, %1546 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1548 = stablehlo.exponential %1547 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1549 = stablehlo.log_plus_one %1548 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1550 = stablehlo.broadcast_in_dim %1547, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1551 = stablehlo.compare  GT, %1550, %1371,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xi1>
      %1552 = stablehlo.broadcast_in_dim %1551, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xi1>) -> tensor<4x512x15x20xi1>
      %1553 = stablehlo.broadcast_in_dim %1549, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1554 = stablehlo.select %1552, %1550, %1553 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xi1>, tensor<4x512x15x20xbf16>
      %1555 = stablehlo.tanh %1554 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1556 = stablehlo.multiply %1547, %1555 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1557 = stablehlo.convolution(%1556, %arg619) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<512x512x3x3xbf16>) -> tensor<4x512x15x20xbf16>
      %1558 = stablehlo.broadcast_in_dim %1557, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1559 = stablehlo.broadcast_in_dim %arg939, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1560 = stablehlo.subtract %1558, %1559 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1561 = stablehlo.broadcast_in_dim %1560, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1562 = stablehlo.broadcast_in_dim %arg940, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1563 = stablehlo.multiply %1561, %1562 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1564 = stablehlo.broadcast_in_dim %1563, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1565 = stablehlo.broadcast_in_dim %arg941, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1566 = stablehlo.multiply %1564, %1565 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1567 = stablehlo.broadcast_in_dim %1566, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1568 = stablehlo.broadcast_in_dim %arg942, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1569 = stablehlo.add %1567, %1568 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1570 = stablehlo.exponential %1569 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1571 = stablehlo.log_plus_one %1570 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1572 = stablehlo.broadcast_in_dim %1569, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1573 = stablehlo.compare  GT, %1572, %1371,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xi1>
      %1574 = stablehlo.broadcast_in_dim %1573, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xi1>) -> tensor<4x512x15x20xi1>
      %1575 = stablehlo.broadcast_in_dim %1571, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1576 = stablehlo.select %1574, %1572, %1575 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xi1>, tensor<4x512x15x20xbf16>
      %1577 = stablehlo.tanh %1576 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1578 = stablehlo.multiply %1569, %1577 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1579 = stablehlo.add %1534, %1578 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1580 = stablehlo.convolution(%1579, %arg620) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<512x512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1581 = stablehlo.broadcast_in_dim %1580, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1582 = stablehlo.broadcast_in_dim %arg943, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1583 = stablehlo.subtract %1581, %1582 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1584 = stablehlo.broadcast_in_dim %1583, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1585 = stablehlo.broadcast_in_dim %arg944, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1586 = stablehlo.multiply %1584, %1585 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1587 = stablehlo.broadcast_in_dim %1586, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1588 = stablehlo.broadcast_in_dim %arg945, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1589 = stablehlo.multiply %1587, %1588 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1590 = stablehlo.broadcast_in_dim %1589, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1591 = stablehlo.broadcast_in_dim %arg946, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1592 = stablehlo.add %1590, %1591 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1593 = stablehlo.exponential %1592 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1594 = stablehlo.log_plus_one %1593 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1595 = stablehlo.broadcast_in_dim %1592, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1596 = stablehlo.compare  GT, %1595, %1371,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xi1>
      %1597 = stablehlo.broadcast_in_dim %1596, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xi1>) -> tensor<4x512x15x20xi1>
      %1598 = stablehlo.broadcast_in_dim %1594, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1599 = stablehlo.select %1597, %1595, %1598 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xi1>, tensor<4x512x15x20xbf16>
      %1600 = stablehlo.tanh %1599 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1601 = stablehlo.multiply %1592, %1600 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1602 = stablehlo.concatenate %1601, %1377, dim = 1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<4x512x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %1603 = stablehlo.convolution(%1602, %arg621) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>, tensor<1024x1024x1x1xbf16>) -> tensor<4x1024x15x20xbf16>
      %1604 = stablehlo.broadcast_in_dim %1603, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %1605 = stablehlo.broadcast_in_dim %arg947, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<1024x1x1xbf16>) -> tensor<4x1024x15x20xbf16>
      %1606 = stablehlo.subtract %1604, %1605 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1607 = stablehlo.broadcast_in_dim %1606, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %1608 = stablehlo.broadcast_in_dim %arg948, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<1024x1x1xbf16>) -> tensor<4x1024x15x20xbf16>
      %1609 = stablehlo.multiply %1607, %1608 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1610 = stablehlo.broadcast_in_dim %1609, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %1611 = stablehlo.broadcast_in_dim %arg949, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<1024x1x1xbf16>) -> tensor<4x1024x15x20xbf16>
      %1612 = stablehlo.multiply %1610, %1611 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1613 = stablehlo.broadcast_in_dim %1612, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %1614 = stablehlo.broadcast_in_dim %arg950, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<1024x1x1xbf16>) -> tensor<4x1024x15x20xbf16>
      %1615 = stablehlo.add %1613, %1614 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1616 = stablehlo.exponential %1615 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1617 = stablehlo.log_plus_one %1616 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1618 = stablehlo.broadcast_in_dim %1615, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %1619 = stablehlo.compare  GT, %1618, %1348,  FLOAT {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>, tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xi1>
      %1620 = stablehlo.broadcast_in_dim %1619, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xi1>) -> tensor<4x1024x15x20xi1>
      %1621 = stablehlo.broadcast_in_dim %1617, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %1622 = stablehlo.select %1620, %1618, %1621 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xi1>, tensor<4x1024x15x20xbf16>
      %1623 = stablehlo.tanh %1622 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1624 = stablehlo.multiply %1615, %1623 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1625 = stablehlo.convolution(%1624, %arg622) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>, tensor<512x1024x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1626 = stablehlo.broadcast_in_dim %1625, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1627 = stablehlo.broadcast_in_dim %arg951, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1628 = stablehlo.subtract %1626, %1627 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1629 = stablehlo.broadcast_in_dim %1628, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1630 = stablehlo.broadcast_in_dim %arg952, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1631 = stablehlo.multiply %1629, %1630 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1632 = stablehlo.broadcast_in_dim %1631, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1633 = stablehlo.broadcast_in_dim %arg953, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1634 = stablehlo.multiply %1632, %1633 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1635 = stablehlo.broadcast_in_dim %1634, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1636 = stablehlo.broadcast_in_dim %arg954, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1637 = stablehlo.add %1635, %1636 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1638 = stablehlo.convert %c_0 : (tensor<i64>) -> tensor<bf16>
      %1639 = stablehlo.broadcast_in_dim %1638, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<4x512x15x20xbf16>
      %1640 = stablehlo.broadcast_in_dim %1637, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1641 = stablehlo.maximum %1639, %1640 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1642 = stablehlo.minimum %1639, %1640 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1643 = stablehlo.convert %cst_1 : (tensor<1xf64>) -> tensor<1xbf16>
      %1644 = stablehlo.reshape %1643 : (tensor<1xbf16>) -> tensor<bf16>
      %1645 = stablehlo.broadcast_in_dim %1642, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1646 = stablehlo.broadcast_in_dim %1644, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<4x512x15x20xbf16>
      %1647 = stablehlo.multiply %1645, %1646 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1648 = stablehlo.add %1641, %1647 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1649 = stablehlo.convolution(%1648, %arg623) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<1024x512x3x3xbf16>) -> tensor<4x1024x15x20xbf16>
      %1650 = stablehlo.broadcast_in_dim %1649, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %1651 = stablehlo.broadcast_in_dim %arg955, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<1024x1x1xbf16>) -> tensor<4x1024x15x20xbf16>
      %1652 = stablehlo.subtract %1650, %1651 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1653 = stablehlo.broadcast_in_dim %1652, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %1654 = stablehlo.broadcast_in_dim %arg956, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<1024x1x1xbf16>) -> tensor<4x1024x15x20xbf16>
      %1655 = stablehlo.multiply %1653, %1654 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1656 = stablehlo.broadcast_in_dim %1655, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %1657 = stablehlo.broadcast_in_dim %arg957, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<1024x1x1xbf16>) -> tensor<4x1024x15x20xbf16>
      %1658 = stablehlo.multiply %1656, %1657 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1659 = stablehlo.broadcast_in_dim %1658, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %1660 = stablehlo.broadcast_in_dim %arg958, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<1024x1x1xbf16>) -> tensor<4x1024x15x20xbf16>
      %1661 = stablehlo.add %1659, %1660 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1662 = stablehlo.broadcast_in_dim %1638, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<4x1024x15x20xbf16>
      %1663 = stablehlo.broadcast_in_dim %1661, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %1664 = stablehlo.maximum %1662, %1663 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1665 = stablehlo.minimum %1662, %1663 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1666 = stablehlo.broadcast_in_dim %1665, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %1667 = stablehlo.broadcast_in_dim %1644, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<4x1024x15x20xbf16>
      %1668 = stablehlo.multiply %1666, %1667 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1669 = stablehlo.add %1664, %1668 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1670 = stablehlo.convolution(%1669, %arg624) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>, tensor<512x1024x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1671 = stablehlo.broadcast_in_dim %1670, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1672 = stablehlo.broadcast_in_dim %arg959, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1673 = stablehlo.subtract %1671, %1672 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1674 = stablehlo.broadcast_in_dim %1673, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1675 = stablehlo.broadcast_in_dim %arg960, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1676 = stablehlo.multiply %1674, %1675 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1677 = stablehlo.broadcast_in_dim %1676, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1678 = stablehlo.broadcast_in_dim %arg961, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1679 = stablehlo.multiply %1677, %1678 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1680 = stablehlo.broadcast_in_dim %1679, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1681 = stablehlo.broadcast_in_dim %arg962, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1682 = stablehlo.add %1680, %1681 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1683 = stablehlo.broadcast_in_dim %1682, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1684 = stablehlo.maximum %1639, %1683 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1685 = stablehlo.minimum %1639, %1683 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1686 = stablehlo.broadcast_in_dim %1685, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1687 = stablehlo.multiply %1686, %1646 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1688 = stablehlo.add %1684, %1687 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1689 = "stablehlo.reduce_window"(%1688, %cst) <{padding = dense<[[0, 0], [0, 0], [2, 2], [2, 2]]> : tensor<4x2xi64>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 5, 5>, window_strides = array<i64: 1, 1, 1, 1>}> ({
      ^bb0(%arg1100: tensor<bf16>, %arg1101: tensor<bf16>):
        %2352 = stablehlo.maximum %arg1100, %arg1101 : tensor<bf16>
        stablehlo.return %2352 : tensor<bf16>
      }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<bf16>) -> tensor<4x512x15x20xbf16>
      %1690 = "stablehlo.reduce_window"(%1688, %cst) <{padding = dense<[[0, 0], [0, 0], [4, 4], [4, 4]]> : tensor<4x2xi64>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 9, 9>, window_strides = array<i64: 1, 1, 1, 1>}> ({
      ^bb0(%arg1100: tensor<bf16>, %arg1101: tensor<bf16>):
        %2352 = stablehlo.maximum %arg1100, %arg1101 : tensor<bf16>
        stablehlo.return %2352 : tensor<bf16>
      }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<bf16>) -> tensor<4x512x15x20xbf16>
      %1691 = "stablehlo.reduce_window"(%1688, %cst) <{padding = dense<[[0, 0], [0, 0], [6, 6], [6, 6]]> : tensor<4x2xi64>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 13, 13>, window_strides = array<i64: 1, 1, 1, 1>}> ({
      ^bb0(%arg1100: tensor<bf16>, %arg1101: tensor<bf16>):
        %2352 = stablehlo.maximum %arg1100, %arg1101 : tensor<bf16>
        stablehlo.return %2352 : tensor<bf16>
      }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<bf16>) -> tensor<4x512x15x20xbf16>
      %1692 = stablehlo.concatenate %1691, %1690, %1689, %1688, dim = 1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<4x512x15x20xbf16>, tensor<4x512x15x20xbf16>, tensor<4x512x15x20xbf16>) -> tensor<4x2048x15x20xbf16>
      %1693 = stablehlo.convolution(%1692, %arg625) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x2048x15x20xbf16>, tensor<512x2048x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1694 = stablehlo.broadcast_in_dim %1693, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1695 = stablehlo.broadcast_in_dim %arg963, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1696 = stablehlo.subtract %1694, %1695 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1697 = stablehlo.broadcast_in_dim %1696, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1698 = stablehlo.broadcast_in_dim %arg964, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1699 = stablehlo.multiply %1697, %1698 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1700 = stablehlo.broadcast_in_dim %1699, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1701 = stablehlo.broadcast_in_dim %arg965, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1702 = stablehlo.multiply %1700, %1701 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1703 = stablehlo.broadcast_in_dim %1702, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1704 = stablehlo.broadcast_in_dim %arg966, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1705 = stablehlo.add %1703, %1704 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1706 = stablehlo.broadcast_in_dim %1705, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1707 = stablehlo.maximum %1639, %1706 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1708 = stablehlo.minimum %1639, %1706 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1709 = stablehlo.broadcast_in_dim %1708, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1710 = stablehlo.multiply %1709, %1646 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1711 = stablehlo.add %1707, %1710 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1712 = stablehlo.convolution(%1711, %arg626) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<1024x512x3x3xbf16>) -> tensor<4x1024x15x20xbf16>
      %1713 = stablehlo.broadcast_in_dim %1712, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %1714 = stablehlo.broadcast_in_dim %arg967, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<1024x1x1xbf16>) -> tensor<4x1024x15x20xbf16>
      %1715 = stablehlo.subtract %1713, %1714 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1716 = stablehlo.broadcast_in_dim %1715, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %1717 = stablehlo.broadcast_in_dim %arg968, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<1024x1x1xbf16>) -> tensor<4x1024x15x20xbf16>
      %1718 = stablehlo.multiply %1716, %1717 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1719 = stablehlo.broadcast_in_dim %1718, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %1720 = stablehlo.broadcast_in_dim %arg969, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<1024x1x1xbf16>) -> tensor<4x1024x15x20xbf16>
      %1721 = stablehlo.multiply %1719, %1720 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1722 = stablehlo.broadcast_in_dim %1721, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %1723 = stablehlo.broadcast_in_dim %arg970, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<1024x1x1xbf16>) -> tensor<4x1024x15x20xbf16>
      %1724 = stablehlo.add %1722, %1723 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1725 = stablehlo.broadcast_in_dim %1724, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %1726 = stablehlo.maximum %1662, %1725 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1727 = stablehlo.minimum %1662, %1725 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1728 = stablehlo.broadcast_in_dim %1727, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %1729 = stablehlo.multiply %1728, %1667 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1730 = stablehlo.add %1726, %1729 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %1731 = stablehlo.convolution(%1730, %arg627) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>, tensor<512x1024x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1732 = stablehlo.broadcast_in_dim %1731, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1733 = stablehlo.broadcast_in_dim %arg971, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1734 = stablehlo.subtract %1732, %1733 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1735 = stablehlo.broadcast_in_dim %1734, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1736 = stablehlo.broadcast_in_dim %arg972, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1737 = stablehlo.multiply %1735, %1736 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1738 = stablehlo.broadcast_in_dim %1737, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1739 = stablehlo.broadcast_in_dim %arg973, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1740 = stablehlo.multiply %1738, %1739 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1741 = stablehlo.broadcast_in_dim %1740, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1742 = stablehlo.broadcast_in_dim %arg974, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %1743 = stablehlo.add %1741, %1742 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1744 = stablehlo.broadcast_in_dim %1743, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1745 = stablehlo.maximum %1639, %1744 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1746 = stablehlo.minimum %1639, %1744 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1747 = stablehlo.broadcast_in_dim %1746, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %1748 = stablehlo.multiply %1747, %1646 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1749 = stablehlo.add %1745, %1748 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %1750 = stablehlo.convolution(%1749, %arg628) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<256x512x1x1xbf16>) -> tensor<4x256x15x20xbf16>
      %1751 = stablehlo.broadcast_in_dim %1750, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x15x20xbf16>) -> tensor<4x256x15x20xbf16>
      %1752 = stablehlo.broadcast_in_dim %arg975, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x15x20xbf16>
      %1753 = stablehlo.subtract %1751, %1752 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x15x20xbf16>
      %1754 = stablehlo.broadcast_in_dim %1753, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x15x20xbf16>) -> tensor<4x256x15x20xbf16>
      %1755 = stablehlo.broadcast_in_dim %arg976, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x15x20xbf16>
      %1756 = stablehlo.multiply %1754, %1755 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x15x20xbf16>
      %1757 = stablehlo.broadcast_in_dim %1756, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x15x20xbf16>) -> tensor<4x256x15x20xbf16>
      %1758 = stablehlo.broadcast_in_dim %arg977, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x15x20xbf16>
      %1759 = stablehlo.multiply %1757, %1758 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x15x20xbf16>
      %1760 = stablehlo.broadcast_in_dim %1759, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x15x20xbf16>) -> tensor<4x256x15x20xbf16>
      %1761 = stablehlo.broadcast_in_dim %arg978, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x15x20xbf16>
      %1762 = stablehlo.add %1760, %1761 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x15x20xbf16>
      %1763 = stablehlo.broadcast_in_dim %1638, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<4x256x15x20xbf16>
      %1764 = stablehlo.broadcast_in_dim %1762, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x15x20xbf16>) -> tensor<4x256x15x20xbf16>
      %1765 = stablehlo.maximum %1763, %1764 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x15x20xbf16>
      %1766 = stablehlo.minimum %1763, %1764 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x15x20xbf16>
      %1767 = stablehlo.broadcast_in_dim %1766, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x15x20xbf16>) -> tensor<4x256x15x20xbf16>
      %1768 = stablehlo.broadcast_in_dim %1644, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<4x256x15x20xbf16>
      %1769 = stablehlo.multiply %1767, %1768 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x15x20xbf16>
      %1770 = stablehlo.add %1765, %1769 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x15x20xbf16>
      %1771 = stablehlo.transpose %1770, dims = [0, 1, 3, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x15x20xbf16>) -> tensor<4x256x20x15xbf16>
      %1772 = stablehlo.reshape %1771 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}]>]>} : (tensor<4x256x20x15xbf16>) -> tensor<1024x20x15xbf16>
      %1773 = stablehlo.broadcast_in_dim %arg981, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}]>]>} : (tensor<1024x15x30xbf16>) -> tensor<1024x15x30xbf16>
      %1774 = stablehlo.dot_general %1772, %1773, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}]>]>} : (tensor<1024x20x15xbf16>, tensor<1024x15x30xbf16>) -> tensor<1024x20x30xbf16>
      %1775 = stablehlo.reshape %1774 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<1024x20x30xbf16>) -> tensor<4x256x20x30xbf16>
      %1776 = stablehlo.transpose %1775, dims = [0, 1, 3, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x20x30xbf16>) -> tensor<4x256x30x20xbf16>
      %1777 = stablehlo.reshape %1776 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}]>]>} : (tensor<4x256x30x20xbf16>) -> tensor<1024x30x20xbf16>
      %1778 = stablehlo.broadcast_in_dim %arg982, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}]>]>} : (tensor<1024x20x40xbf16>) -> tensor<1024x20x40xbf16>
      %1779 = stablehlo.dot_general %1777, %1778, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}]>]>} : (tensor<1024x30x20xbf16>, tensor<1024x20x40xbf16>) -> tensor<1024x30x40xbf16>
      %1780 = stablehlo.reshape %1779 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<1024x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1781 = stablehlo.convolution(%1331, %arg629) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>, tensor<256x512x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1782 = stablehlo.broadcast_in_dim %1781, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1783 = stablehlo.broadcast_in_dim %arg983, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1784 = stablehlo.subtract %1782, %1783 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1785 = stablehlo.broadcast_in_dim %1784, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1786 = stablehlo.broadcast_in_dim %arg984, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1787 = stablehlo.multiply %1785, %1786 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1788 = stablehlo.broadcast_in_dim %1787, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1789 = stablehlo.broadcast_in_dim %arg985, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1790 = stablehlo.multiply %1788, %1789 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1791 = stablehlo.broadcast_in_dim %1790, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1792 = stablehlo.broadcast_in_dim %arg986, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1793 = stablehlo.add %1791, %1792 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1794 = stablehlo.broadcast_in_dim %1638, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<4x256x30x40xbf16>
      %1795 = stablehlo.broadcast_in_dim %1793, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1796 = stablehlo.maximum %1794, %1795 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1797 = stablehlo.minimum %1794, %1795 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1798 = stablehlo.broadcast_in_dim %1797, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1799 = stablehlo.broadcast_in_dim %1644, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<4x256x30x40xbf16>
      %1800 = stablehlo.multiply %1798, %1799 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1801 = stablehlo.add %1796, %1800 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1802 = stablehlo.concatenate %1801, %1780, dim = 1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<4x256x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %1803 = stablehlo.convolution(%1802, %arg630) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>, tensor<256x512x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1804 = stablehlo.broadcast_in_dim %1803, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1805 = stablehlo.broadcast_in_dim %arg987, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1806 = stablehlo.subtract %1804, %1805 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1807 = stablehlo.broadcast_in_dim %1806, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1808 = stablehlo.broadcast_in_dim %arg988, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1809 = stablehlo.multiply %1807, %1808 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1810 = stablehlo.broadcast_in_dim %1809, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1811 = stablehlo.broadcast_in_dim %arg989, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1812 = stablehlo.multiply %1810, %1811 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1813 = stablehlo.broadcast_in_dim %1812, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1814 = stablehlo.broadcast_in_dim %arg990, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1815 = stablehlo.add %1813, %1814 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1816 = stablehlo.broadcast_in_dim %1815, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1817 = stablehlo.maximum %1794, %1816 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1818 = stablehlo.minimum %1794, %1816 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1819 = stablehlo.broadcast_in_dim %1818, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1820 = stablehlo.multiply %1819, %1799 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1821 = stablehlo.add %1817, %1820 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1822 = stablehlo.convolution(%1821, %arg631) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<512x256x3x3xbf16>) -> tensor<4x512x30x40xbf16>
      %1823 = stablehlo.broadcast_in_dim %1822, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %1824 = stablehlo.broadcast_in_dim %arg991, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x30x40xbf16>
      %1825 = stablehlo.subtract %1823, %1824 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %1826 = stablehlo.broadcast_in_dim %1825, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %1827 = stablehlo.broadcast_in_dim %arg992, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x30x40xbf16>
      %1828 = stablehlo.multiply %1826, %1827 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %1829 = stablehlo.broadcast_in_dim %1828, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %1830 = stablehlo.broadcast_in_dim %arg993, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x30x40xbf16>
      %1831 = stablehlo.multiply %1829, %1830 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %1832 = stablehlo.broadcast_in_dim %1831, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %1833 = stablehlo.broadcast_in_dim %arg994, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x30x40xbf16>
      %1834 = stablehlo.add %1832, %1833 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %1835 = stablehlo.broadcast_in_dim %1638, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<4x512x30x40xbf16>
      %1836 = stablehlo.broadcast_in_dim %1834, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %1837 = stablehlo.maximum %1835, %1836 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %1838 = stablehlo.minimum %1835, %1836 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %1839 = stablehlo.broadcast_in_dim %1838, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %1840 = stablehlo.broadcast_in_dim %1644, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<4x512x30x40xbf16>
      %1841 = stablehlo.multiply %1839, %1840 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %1842 = stablehlo.add %1837, %1841 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %1843 = stablehlo.convolution(%1842, %arg632) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>, tensor<256x512x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1844 = stablehlo.broadcast_in_dim %1843, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1845 = stablehlo.broadcast_in_dim %arg995, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1846 = stablehlo.subtract %1844, %1845 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1847 = stablehlo.broadcast_in_dim %1846, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1848 = stablehlo.broadcast_in_dim %arg996, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1849 = stablehlo.multiply %1847, %1848 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1850 = stablehlo.broadcast_in_dim %1849, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1851 = stablehlo.broadcast_in_dim %arg997, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1852 = stablehlo.multiply %1850, %1851 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1853 = stablehlo.broadcast_in_dim %1852, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1854 = stablehlo.broadcast_in_dim %arg998, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1855 = stablehlo.add %1853, %1854 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1856 = stablehlo.broadcast_in_dim %1855, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1857 = stablehlo.maximum %1794, %1856 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1858 = stablehlo.minimum %1794, %1856 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1859 = stablehlo.broadcast_in_dim %1858, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1860 = stablehlo.multiply %1859, %1799 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1861 = stablehlo.add %1857, %1860 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1862 = stablehlo.convolution(%1861, %arg633) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<512x256x3x3xbf16>) -> tensor<4x512x30x40xbf16>
      %1863 = stablehlo.broadcast_in_dim %1862, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %1864 = stablehlo.broadcast_in_dim %arg999, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x30x40xbf16>
      %1865 = stablehlo.subtract %1863, %1864 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %1866 = stablehlo.broadcast_in_dim %1865, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %1867 = stablehlo.broadcast_in_dim %arg1000, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x30x40xbf16>
      %1868 = stablehlo.multiply %1866, %1867 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %1869 = stablehlo.broadcast_in_dim %1868, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %1870 = stablehlo.broadcast_in_dim %arg1001, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x30x40xbf16>
      %1871 = stablehlo.multiply %1869, %1870 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %1872 = stablehlo.broadcast_in_dim %1871, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %1873 = stablehlo.broadcast_in_dim %arg1002, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x30x40xbf16>
      %1874 = stablehlo.add %1872, %1873 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %1875 = stablehlo.broadcast_in_dim %1874, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %1876 = stablehlo.maximum %1835, %1875 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %1877 = stablehlo.minimum %1835, %1875 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %1878 = stablehlo.broadcast_in_dim %1877, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %1879 = stablehlo.multiply %1878, %1840 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %1880 = stablehlo.add %1876, %1879 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %1881 = stablehlo.convolution(%1880, %arg634) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>, tensor<256x512x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1882 = stablehlo.broadcast_in_dim %1881, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1883 = stablehlo.broadcast_in_dim %arg1003, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1884 = stablehlo.subtract %1882, %1883 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1885 = stablehlo.broadcast_in_dim %1884, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1886 = stablehlo.broadcast_in_dim %arg1004, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1887 = stablehlo.multiply %1885, %1886 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1888 = stablehlo.broadcast_in_dim %1887, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1889 = stablehlo.broadcast_in_dim %arg1005, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1890 = stablehlo.multiply %1888, %1889 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1891 = stablehlo.broadcast_in_dim %1890, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1892 = stablehlo.broadcast_in_dim %arg1006, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %1893 = stablehlo.add %1891, %1892 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1894 = stablehlo.broadcast_in_dim %1893, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1895 = stablehlo.maximum %1794, %1894 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1896 = stablehlo.minimum %1794, %1894 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1897 = stablehlo.broadcast_in_dim %1896, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %1898 = stablehlo.multiply %1897, %1799 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1899 = stablehlo.add %1895, %1898 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %1900 = stablehlo.convolution(%1899, %arg635) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<128x256x1x1xbf16>) -> tensor<4x128x30x40xbf16>
      %1901 = stablehlo.broadcast_in_dim %1900, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x30x40xbf16>) -> tensor<4x128x30x40xbf16>
      %1902 = stablehlo.broadcast_in_dim %arg1007, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x30x40xbf16>
      %1903 = stablehlo.subtract %1901, %1902 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x30x40xbf16>
      %1904 = stablehlo.broadcast_in_dim %1903, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x30x40xbf16>) -> tensor<4x128x30x40xbf16>
      %1905 = stablehlo.broadcast_in_dim %arg1008, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x30x40xbf16>
      %1906 = stablehlo.multiply %1904, %1905 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x30x40xbf16>
      %1907 = stablehlo.broadcast_in_dim %1906, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x30x40xbf16>) -> tensor<4x128x30x40xbf16>
      %1908 = stablehlo.broadcast_in_dim %arg1009, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x30x40xbf16>
      %1909 = stablehlo.multiply %1907, %1908 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x30x40xbf16>
      %1910 = stablehlo.broadcast_in_dim %1909, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x30x40xbf16>) -> tensor<4x128x30x40xbf16>
      %1911 = stablehlo.broadcast_in_dim %arg1010, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x30x40xbf16>
      %1912 = stablehlo.add %1910, %1911 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x30x40xbf16>
      %1913 = stablehlo.broadcast_in_dim %1638, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<4x128x30x40xbf16>
      %1914 = stablehlo.broadcast_in_dim %1912, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x30x40xbf16>) -> tensor<4x128x30x40xbf16>
      %1915 = stablehlo.maximum %1913, %1914 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x30x40xbf16>
      %1916 = stablehlo.minimum %1913, %1914 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x30x40xbf16>
      %1917 = stablehlo.broadcast_in_dim %1916, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x30x40xbf16>) -> tensor<4x128x30x40xbf16>
      %1918 = stablehlo.broadcast_in_dim %1644, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<4x128x30x40xbf16>
      %1919 = stablehlo.multiply %1917, %1918 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x30x40xbf16>
      %1920 = stablehlo.add %1915, %1919 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x30x40xbf16>
      %1921 = stablehlo.transpose %1920, dims = [0, 1, 3, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x30x40xbf16>) -> tensor<4x128x40x30xbf16>
      %1922 = stablehlo.reshape %1921 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}]>]>} : (tensor<4x128x40x30xbf16>) -> tensor<512x40x30xbf16>
      %1923 = stablehlo.broadcast_in_dim %arg1013, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}]>]>} : (tensor<512x30x60xbf16>) -> tensor<512x30x60xbf16>
      %1924 = stablehlo.dot_general %1922, %1923, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}]>]>} : (tensor<512x40x30xbf16>, tensor<512x30x60xbf16>) -> tensor<512x40x60xbf16>
      %1925 = stablehlo.reshape %1924 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x40x60xbf16>) -> tensor<4x128x40x60xbf16>
      %1926 = stablehlo.transpose %1925, dims = [0, 1, 3, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x40x60xbf16>) -> tensor<4x128x60x40xbf16>
      %1927 = stablehlo.reshape %1926 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}]>]>} : (tensor<4x128x60x40xbf16>) -> tensor<512x60x40xbf16>
      %1928 = stablehlo.broadcast_in_dim %arg1014, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}]>]>} : (tensor<512x40x80xbf16>) -> tensor<512x40x80xbf16>
      %1929 = stablehlo.dot_general %1927, %1928, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}]>]>} : (tensor<512x60x40xbf16>, tensor<512x40x80xbf16>) -> tensor<512x60x80xbf16>
      %1930 = stablehlo.reshape %1929 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %1931 = stablehlo.convolution(%858, %arg636) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>, tensor<128x256x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %1932 = stablehlo.broadcast_in_dim %1931, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %1933 = stablehlo.broadcast_in_dim %arg1015, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %1934 = stablehlo.subtract %1932, %1933 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %1935 = stablehlo.broadcast_in_dim %1934, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %1936 = stablehlo.broadcast_in_dim %arg1016, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %1937 = stablehlo.multiply %1935, %1936 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %1938 = stablehlo.broadcast_in_dim %1937, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %1939 = stablehlo.broadcast_in_dim %arg1017, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %1940 = stablehlo.multiply %1938, %1939 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %1941 = stablehlo.broadcast_in_dim %1940, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %1942 = stablehlo.broadcast_in_dim %arg1018, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %1943 = stablehlo.add %1941, %1942 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %1944 = stablehlo.broadcast_in_dim %1638, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<4x128x60x80xbf16>
      %1945 = stablehlo.broadcast_in_dim %1943, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %1946 = stablehlo.maximum %1944, %1945 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %1947 = stablehlo.minimum %1944, %1945 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %1948 = stablehlo.broadcast_in_dim %1947, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %1949 = stablehlo.broadcast_in_dim %1644, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<4x128x60x80xbf16>
      %1950 = stablehlo.multiply %1948, %1949 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %1951 = stablehlo.add %1946, %1950 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %1952 = stablehlo.concatenate %1951, %1930, dim = 1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<4x128x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %1953 = stablehlo.convolution(%1952, %arg637) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>, tensor<128x256x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %1954 = stablehlo.broadcast_in_dim %1953, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %1955 = stablehlo.broadcast_in_dim %arg1019, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %1956 = stablehlo.subtract %1954, %1955 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %1957 = stablehlo.broadcast_in_dim %1956, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %1958 = stablehlo.broadcast_in_dim %arg1020, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %1959 = stablehlo.multiply %1957, %1958 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %1960 = stablehlo.broadcast_in_dim %1959, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %1961 = stablehlo.broadcast_in_dim %arg1021, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %1962 = stablehlo.multiply %1960, %1961 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %1963 = stablehlo.broadcast_in_dim %1962, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %1964 = stablehlo.broadcast_in_dim %arg1022, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %1965 = stablehlo.add %1963, %1964 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %1966 = stablehlo.broadcast_in_dim %1965, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %1967 = stablehlo.maximum %1944, %1966 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %1968 = stablehlo.minimum %1944, %1966 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %1969 = stablehlo.broadcast_in_dim %1968, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %1970 = stablehlo.multiply %1969, %1949 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %1971 = stablehlo.add %1967, %1970 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %1972 = stablehlo.convolution(%1971, %arg638) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<256x128x3x3xbf16>) -> tensor<4x256x60x80xbf16>
      %1973 = stablehlo.broadcast_in_dim %1972, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %1974 = stablehlo.broadcast_in_dim %arg1023, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x60x80xbf16>
      %1975 = stablehlo.subtract %1973, %1974 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %1976 = stablehlo.broadcast_in_dim %1975, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %1977 = stablehlo.broadcast_in_dim %arg1024, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x60x80xbf16>
      %1978 = stablehlo.multiply %1976, %1977 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %1979 = stablehlo.broadcast_in_dim %1978, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %1980 = stablehlo.broadcast_in_dim %arg1025, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x60x80xbf16>
      %1981 = stablehlo.multiply %1979, %1980 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %1982 = stablehlo.broadcast_in_dim %1981, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %1983 = stablehlo.broadcast_in_dim %arg1026, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x60x80xbf16>
      %1984 = stablehlo.add %1982, %1983 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %1985 = stablehlo.broadcast_in_dim %1638, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<4x256x60x80xbf16>
      %1986 = stablehlo.broadcast_in_dim %1984, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %1987 = stablehlo.maximum %1985, %1986 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %1988 = stablehlo.minimum %1985, %1986 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %1989 = stablehlo.broadcast_in_dim %1988, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %1990 = stablehlo.broadcast_in_dim %1644, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<4x256x60x80xbf16>
      %1991 = stablehlo.multiply %1989, %1990 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %1992 = stablehlo.add %1987, %1991 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %1993 = stablehlo.convolution(%1992, %arg639) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>, tensor<128x256x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %1994 = stablehlo.broadcast_in_dim %1993, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %1995 = stablehlo.broadcast_in_dim %arg1027, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %1996 = stablehlo.subtract %1994, %1995 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %1997 = stablehlo.broadcast_in_dim %1996, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %1998 = stablehlo.broadcast_in_dim %arg1028, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %1999 = stablehlo.multiply %1997, %1998 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %2000 = stablehlo.broadcast_in_dim %1999, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %2001 = stablehlo.broadcast_in_dim %arg1029, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %2002 = stablehlo.multiply %2000, %2001 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %2003 = stablehlo.broadcast_in_dim %2002, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %2004 = stablehlo.broadcast_in_dim %arg1030, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %2005 = stablehlo.add %2003, %2004 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %2006 = stablehlo.broadcast_in_dim %2005, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %2007 = stablehlo.maximum %1944, %2006 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %2008 = stablehlo.minimum %1944, %2006 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %2009 = stablehlo.broadcast_in_dim %2008, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %2010 = stablehlo.multiply %2009, %1949 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %2011 = stablehlo.add %2007, %2010 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %2012 = stablehlo.convolution(%2011, %arg640) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<256x128x3x3xbf16>) -> tensor<4x256x60x80xbf16>
      %2013 = stablehlo.broadcast_in_dim %2012, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %2014 = stablehlo.broadcast_in_dim %arg1031, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x60x80xbf16>
      %2015 = stablehlo.subtract %2013, %2014 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %2016 = stablehlo.broadcast_in_dim %2015, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %2017 = stablehlo.broadcast_in_dim %arg1032, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x60x80xbf16>
      %2018 = stablehlo.multiply %2016, %2017 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %2019 = stablehlo.broadcast_in_dim %2018, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %2020 = stablehlo.broadcast_in_dim %arg1033, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x60x80xbf16>
      %2021 = stablehlo.multiply %2019, %2020 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %2022 = stablehlo.broadcast_in_dim %2021, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %2023 = stablehlo.broadcast_in_dim %arg1034, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x60x80xbf16>
      %2024 = stablehlo.add %2022, %2023 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %2025 = stablehlo.broadcast_in_dim %2024, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %2026 = stablehlo.maximum %1985, %2025 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %2027 = stablehlo.minimum %1985, %2025 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %2028 = stablehlo.broadcast_in_dim %2027, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %2029 = stablehlo.multiply %2028, %1990 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %2030 = stablehlo.add %2026, %2029 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %2031 = stablehlo.convolution(%2030, %arg641) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>, tensor<128x256x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %2032 = stablehlo.broadcast_in_dim %2031, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %2033 = stablehlo.broadcast_in_dim %arg1035, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %2034 = stablehlo.subtract %2032, %2033 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %2035 = stablehlo.broadcast_in_dim %2034, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %2036 = stablehlo.broadcast_in_dim %arg1036, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %2037 = stablehlo.multiply %2035, %2036 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %2038 = stablehlo.broadcast_in_dim %2037, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %2039 = stablehlo.broadcast_in_dim %arg1037, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %2040 = stablehlo.multiply %2038, %2039 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %2041 = stablehlo.broadcast_in_dim %2040, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %2042 = stablehlo.broadcast_in_dim %arg1038, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<128x1x1xbf16>) -> tensor<4x128x60x80xbf16>
      %2043 = stablehlo.add %2041, %2042 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %2044 = stablehlo.broadcast_in_dim %2043, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %2045 = stablehlo.maximum %1944, %2044 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %2046 = stablehlo.minimum %1944, %2044 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %2047 = stablehlo.broadcast_in_dim %2046, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>) -> tensor<4x128x60x80xbf16>
      %2048 = stablehlo.multiply %2047, %1949 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %2049 = stablehlo.add %2045, %2048 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x128x60x80xbf16>
      %2050 = stablehlo.convolution(%2049, %arg642) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<256x128x3x3xbf16>) -> tensor<4x256x60x80xbf16>
      %2051 = stablehlo.broadcast_in_dim %2050, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %2052 = stablehlo.broadcast_in_dim %arg1039, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x60x80xbf16>
      %2053 = stablehlo.subtract %2051, %2052 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %2054 = stablehlo.broadcast_in_dim %2053, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %2055 = stablehlo.broadcast_in_dim %arg1040, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x60x80xbf16>
      %2056 = stablehlo.multiply %2054, %2055 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %2057 = stablehlo.broadcast_in_dim %2056, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %2058 = stablehlo.broadcast_in_dim %arg1041, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x60x80xbf16>
      %2059 = stablehlo.multiply %2057, %2058 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %2060 = stablehlo.broadcast_in_dim %2059, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %2061 = stablehlo.broadcast_in_dim %arg1042, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x60x80xbf16>
      %2062 = stablehlo.add %2060, %2061 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %2063 = stablehlo.broadcast_in_dim %2062, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %2064 = stablehlo.maximum %1985, %2063 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %2065 = stablehlo.minimum %1985, %2063 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %2066 = stablehlo.broadcast_in_dim %2065, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>) -> tensor<4x256x60x80xbf16>
      %2067 = stablehlo.multiply %2066, %1990 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %2068 = stablehlo.add %2064, %2067 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x60x80xbf16>
      %2069 = stablehlo.convolution(%2068, %arg643) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x60x80xbf16>, tensor<255x256x1x1xbf16>) -> tensor<4x255x60x80xbf16>
      %2070 = stablehlo.reshape %arg644 : (tensor<255xbf16>) -> tensor<255x1x1xbf16>
      %2071 = stablehlo.broadcast_in_dim %2069, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x255x60x80xbf16>) -> tensor<4x255x60x80xbf16>
      %2072 = stablehlo.broadcast_in_dim %2070, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<255x1x1xbf16>) -> tensor<4x255x60x80xbf16>
      %2073 = stablehlo.add %2071, %2072 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x255x60x80xbf16>
      %2074 = stablehlo.convolution(%2049, %arg645) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [2, 2], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x128x60x80xbf16>, tensor<256x128x3x3xbf16>) -> tensor<4x256x30x40xbf16>
      %2075 = stablehlo.broadcast_in_dim %2074, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %2076 = stablehlo.broadcast_in_dim %arg1043, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %2077 = stablehlo.subtract %2075, %2076 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2078 = stablehlo.broadcast_in_dim %2077, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %2079 = stablehlo.broadcast_in_dim %arg1044, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %2080 = stablehlo.multiply %2078, %2079 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2081 = stablehlo.broadcast_in_dim %2080, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %2082 = stablehlo.broadcast_in_dim %arg1045, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %2083 = stablehlo.multiply %2081, %2082 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2084 = stablehlo.broadcast_in_dim %2083, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %2085 = stablehlo.broadcast_in_dim %arg1046, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %2086 = stablehlo.add %2084, %2085 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2087 = stablehlo.broadcast_in_dim %2086, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %2088 = stablehlo.maximum %1794, %2087 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2089 = stablehlo.minimum %1794, %2087 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2090 = stablehlo.broadcast_in_dim %2089, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %2091 = stablehlo.multiply %2090, %1799 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2092 = stablehlo.add %2088, %2091 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2093 = stablehlo.concatenate %2092, %1899, dim = 1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<4x256x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %2094 = stablehlo.convolution(%2093, %arg646) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>, tensor<256x512x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %2095 = stablehlo.broadcast_in_dim %2094, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %2096 = stablehlo.broadcast_in_dim %arg1047, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %2097 = stablehlo.subtract %2095, %2096 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2098 = stablehlo.broadcast_in_dim %2097, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %2099 = stablehlo.broadcast_in_dim %arg1048, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %2100 = stablehlo.multiply %2098, %2099 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2101 = stablehlo.broadcast_in_dim %2100, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %2102 = stablehlo.broadcast_in_dim %arg1049, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %2103 = stablehlo.multiply %2101, %2102 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2104 = stablehlo.broadcast_in_dim %2103, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %2105 = stablehlo.broadcast_in_dim %arg1050, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %2106 = stablehlo.add %2104, %2105 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2107 = stablehlo.broadcast_in_dim %2106, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %2108 = stablehlo.maximum %1794, %2107 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2109 = stablehlo.minimum %1794, %2107 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2110 = stablehlo.broadcast_in_dim %2109, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %2111 = stablehlo.multiply %2110, %1799 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2112 = stablehlo.add %2108, %2111 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2113 = stablehlo.convolution(%2112, %arg647) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<512x256x3x3xbf16>) -> tensor<4x512x30x40xbf16>
      %2114 = stablehlo.broadcast_in_dim %2113, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %2115 = stablehlo.broadcast_in_dim %arg1051, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x30x40xbf16>
      %2116 = stablehlo.subtract %2114, %2115 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %2117 = stablehlo.broadcast_in_dim %2116, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %2118 = stablehlo.broadcast_in_dim %arg1052, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x30x40xbf16>
      %2119 = stablehlo.multiply %2117, %2118 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %2120 = stablehlo.broadcast_in_dim %2119, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %2121 = stablehlo.broadcast_in_dim %arg1053, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x30x40xbf16>
      %2122 = stablehlo.multiply %2120, %2121 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %2123 = stablehlo.broadcast_in_dim %2122, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %2124 = stablehlo.broadcast_in_dim %arg1054, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x30x40xbf16>
      %2125 = stablehlo.add %2123, %2124 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %2126 = stablehlo.broadcast_in_dim %2125, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %2127 = stablehlo.maximum %1835, %2126 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %2128 = stablehlo.minimum %1835, %2126 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %2129 = stablehlo.broadcast_in_dim %2128, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %2130 = stablehlo.multiply %2129, %1840 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %2131 = stablehlo.add %2127, %2130 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %2132 = stablehlo.convolution(%2131, %arg648) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>, tensor<256x512x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %2133 = stablehlo.broadcast_in_dim %2132, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %2134 = stablehlo.broadcast_in_dim %arg1055, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %2135 = stablehlo.subtract %2133, %2134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2136 = stablehlo.broadcast_in_dim %2135, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %2137 = stablehlo.broadcast_in_dim %arg1056, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %2138 = stablehlo.multiply %2136, %2137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2139 = stablehlo.broadcast_in_dim %2138, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %2140 = stablehlo.broadcast_in_dim %arg1057, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %2141 = stablehlo.multiply %2139, %2140 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2142 = stablehlo.broadcast_in_dim %2141, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %2143 = stablehlo.broadcast_in_dim %arg1058, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %2144 = stablehlo.add %2142, %2143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2145 = stablehlo.broadcast_in_dim %2144, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %2146 = stablehlo.maximum %1794, %2145 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2147 = stablehlo.minimum %1794, %2145 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2148 = stablehlo.broadcast_in_dim %2147, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %2149 = stablehlo.multiply %2148, %1799 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2150 = stablehlo.add %2146, %2149 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2151 = stablehlo.convolution(%2150, %arg649) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<512x256x3x3xbf16>) -> tensor<4x512x30x40xbf16>
      %2152 = stablehlo.broadcast_in_dim %2151, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %2153 = stablehlo.broadcast_in_dim %arg1059, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x30x40xbf16>
      %2154 = stablehlo.subtract %2152, %2153 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %2155 = stablehlo.broadcast_in_dim %2154, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %2156 = stablehlo.broadcast_in_dim %arg1060, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x30x40xbf16>
      %2157 = stablehlo.multiply %2155, %2156 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %2158 = stablehlo.broadcast_in_dim %2157, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %2159 = stablehlo.broadcast_in_dim %arg1061, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x30x40xbf16>
      %2160 = stablehlo.multiply %2158, %2159 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %2161 = stablehlo.broadcast_in_dim %2160, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %2162 = stablehlo.broadcast_in_dim %arg1062, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x30x40xbf16>
      %2163 = stablehlo.add %2161, %2162 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %2164 = stablehlo.broadcast_in_dim %2163, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %2165 = stablehlo.maximum %1835, %2164 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %2166 = stablehlo.minimum %1835, %2164 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %2167 = stablehlo.broadcast_in_dim %2166, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %2168 = stablehlo.multiply %2167, %1840 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %2169 = stablehlo.add %2165, %2168 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %2170 = stablehlo.convolution(%2169, %arg650) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>, tensor<256x512x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %2171 = stablehlo.broadcast_in_dim %2170, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %2172 = stablehlo.broadcast_in_dim %arg1063, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %2173 = stablehlo.subtract %2171, %2172 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2174 = stablehlo.broadcast_in_dim %2173, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %2175 = stablehlo.broadcast_in_dim %arg1064, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %2176 = stablehlo.multiply %2174, %2175 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2177 = stablehlo.broadcast_in_dim %2176, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %2178 = stablehlo.broadcast_in_dim %arg1065, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %2179 = stablehlo.multiply %2177, %2178 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2180 = stablehlo.broadcast_in_dim %2179, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %2181 = stablehlo.broadcast_in_dim %arg1066, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<256x1x1xbf16>) -> tensor<4x256x30x40xbf16>
      %2182 = stablehlo.add %2180, %2181 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2183 = stablehlo.broadcast_in_dim %2182, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %2184 = stablehlo.maximum %1794, %2183 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2185 = stablehlo.minimum %1794, %2183 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2186 = stablehlo.broadcast_in_dim %2185, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>) -> tensor<4x256x30x40xbf16>
      %2187 = stablehlo.multiply %2186, %1799 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2188 = stablehlo.add %2184, %2187 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x256x30x40xbf16>
      %2189 = stablehlo.convolution(%2188, %arg651) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<512x256x3x3xbf16>) -> tensor<4x512x30x40xbf16>
      %2190 = stablehlo.broadcast_in_dim %2189, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %2191 = stablehlo.broadcast_in_dim %arg1067, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x30x40xbf16>
      %2192 = stablehlo.subtract %2190, %2191 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %2193 = stablehlo.broadcast_in_dim %2192, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %2194 = stablehlo.broadcast_in_dim %arg1068, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x30x40xbf16>
      %2195 = stablehlo.multiply %2193, %2194 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %2196 = stablehlo.broadcast_in_dim %2195, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %2197 = stablehlo.broadcast_in_dim %arg1069, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x30x40xbf16>
      %2198 = stablehlo.multiply %2196, %2197 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %2199 = stablehlo.broadcast_in_dim %2198, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %2200 = stablehlo.broadcast_in_dim %arg1070, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x30x40xbf16>
      %2201 = stablehlo.add %2199, %2200 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %2202 = stablehlo.broadcast_in_dim %2201, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %2203 = stablehlo.maximum %1835, %2202 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %2204 = stablehlo.minimum %1835, %2202 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %2205 = stablehlo.broadcast_in_dim %2204, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>) -> tensor<4x512x30x40xbf16>
      %2206 = stablehlo.multiply %2205, %1840 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %2207 = stablehlo.add %2203, %2206 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x30x40xbf16>
      %2208 = stablehlo.convolution(%2207, %arg652) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x30x40xbf16>, tensor<255x512x1x1xbf16>) -> tensor<4x255x30x40xbf16>
      %2209 = stablehlo.reshape %arg653 : (tensor<255xbf16>) -> tensor<255x1x1xbf16>
      %2210 = stablehlo.broadcast_in_dim %2208, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x255x30x40xbf16>) -> tensor<4x255x30x40xbf16>
      %2211 = stablehlo.broadcast_in_dim %2209, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<255x1x1xbf16>) -> tensor<4x255x30x40xbf16>
      %2212 = stablehlo.add %2210, %2211 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x255x30x40xbf16>
      %2213 = stablehlo.convolution(%2188, %arg654) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [2, 2], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x256x30x40xbf16>, tensor<512x256x3x3xbf16>) -> tensor<4x512x15x20xbf16>
      %2214 = stablehlo.broadcast_in_dim %2213, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %2215 = stablehlo.broadcast_in_dim %arg1071, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %2216 = stablehlo.subtract %2214, %2215 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2217 = stablehlo.broadcast_in_dim %2216, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %2218 = stablehlo.broadcast_in_dim %arg1072, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %2219 = stablehlo.multiply %2217, %2218 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2220 = stablehlo.broadcast_in_dim %2219, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %2221 = stablehlo.broadcast_in_dim %arg1073, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %2222 = stablehlo.multiply %2220, %2221 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2223 = stablehlo.broadcast_in_dim %2222, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %2224 = stablehlo.broadcast_in_dim %arg1074, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %2225 = stablehlo.add %2223, %2224 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2226 = stablehlo.broadcast_in_dim %2225, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %2227 = stablehlo.maximum %1639, %2226 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2228 = stablehlo.minimum %1639, %2226 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2229 = stablehlo.broadcast_in_dim %2228, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %2230 = stablehlo.multiply %2229, %1646 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2231 = stablehlo.add %2227, %2230 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2232 = stablehlo.concatenate %2231, %1749, dim = 1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<4x512x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %2233 = stablehlo.convolution(%2232, %arg655) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>, tensor<512x1024x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %2234 = stablehlo.broadcast_in_dim %2233, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %2235 = stablehlo.broadcast_in_dim %arg1075, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %2236 = stablehlo.subtract %2234, %2235 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2237 = stablehlo.broadcast_in_dim %2236, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %2238 = stablehlo.broadcast_in_dim %arg1076, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %2239 = stablehlo.multiply %2237, %2238 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2240 = stablehlo.broadcast_in_dim %2239, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %2241 = stablehlo.broadcast_in_dim %arg1077, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %2242 = stablehlo.multiply %2240, %2241 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2243 = stablehlo.broadcast_in_dim %2242, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %2244 = stablehlo.broadcast_in_dim %arg1078, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %2245 = stablehlo.add %2243, %2244 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2246 = stablehlo.broadcast_in_dim %2245, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %2247 = stablehlo.maximum %1639, %2246 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2248 = stablehlo.minimum %1639, %2246 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2249 = stablehlo.broadcast_in_dim %2248, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %2250 = stablehlo.multiply %2249, %1646 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2251 = stablehlo.add %2247, %2250 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2252 = stablehlo.convolution(%2251, %arg656) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<1024x512x3x3xbf16>) -> tensor<4x1024x15x20xbf16>
      %2253 = stablehlo.broadcast_in_dim %2252, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %2254 = stablehlo.broadcast_in_dim %arg1079, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<1024x1x1xbf16>) -> tensor<4x1024x15x20xbf16>
      %2255 = stablehlo.subtract %2253, %2254 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %2256 = stablehlo.broadcast_in_dim %2255, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %2257 = stablehlo.broadcast_in_dim %arg1080, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<1024x1x1xbf16>) -> tensor<4x1024x15x20xbf16>
      %2258 = stablehlo.multiply %2256, %2257 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %2259 = stablehlo.broadcast_in_dim %2258, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %2260 = stablehlo.broadcast_in_dim %arg1081, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<1024x1x1xbf16>) -> tensor<4x1024x15x20xbf16>
      %2261 = stablehlo.multiply %2259, %2260 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %2262 = stablehlo.broadcast_in_dim %2261, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %2263 = stablehlo.broadcast_in_dim %arg1082, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<1024x1x1xbf16>) -> tensor<4x1024x15x20xbf16>
      %2264 = stablehlo.add %2262, %2263 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %2265 = stablehlo.broadcast_in_dim %2264, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %2266 = stablehlo.maximum %1662, %2265 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %2267 = stablehlo.minimum %1662, %2265 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %2268 = stablehlo.broadcast_in_dim %2267, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %2269 = stablehlo.multiply %2268, %1667 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %2270 = stablehlo.add %2266, %2269 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %2271 = stablehlo.convolution(%2270, %arg657) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>, tensor<512x1024x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %2272 = stablehlo.broadcast_in_dim %2271, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %2273 = stablehlo.broadcast_in_dim %arg1083, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %2274 = stablehlo.subtract %2272, %2273 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2275 = stablehlo.broadcast_in_dim %2274, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %2276 = stablehlo.broadcast_in_dim %arg1084, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %2277 = stablehlo.multiply %2275, %2276 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2278 = stablehlo.broadcast_in_dim %2277, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %2279 = stablehlo.broadcast_in_dim %arg1085, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %2280 = stablehlo.multiply %2278, %2279 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2281 = stablehlo.broadcast_in_dim %2280, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %2282 = stablehlo.broadcast_in_dim %arg1086, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %2283 = stablehlo.add %2281, %2282 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2284 = stablehlo.broadcast_in_dim %2283, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %2285 = stablehlo.maximum %1639, %2284 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2286 = stablehlo.minimum %1639, %2284 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2287 = stablehlo.broadcast_in_dim %2286, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %2288 = stablehlo.multiply %2287, %1646 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2289 = stablehlo.add %2285, %2288 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2290 = stablehlo.convolution(%2289, %arg658) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<1024x512x3x3xbf16>) -> tensor<4x1024x15x20xbf16>
      %2291 = stablehlo.broadcast_in_dim %2290, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %2292 = stablehlo.broadcast_in_dim %arg1087, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<1024x1x1xbf16>) -> tensor<4x1024x15x20xbf16>
      %2293 = stablehlo.subtract %2291, %2292 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %2294 = stablehlo.broadcast_in_dim %2293, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %2295 = stablehlo.broadcast_in_dim %arg1088, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<1024x1x1xbf16>) -> tensor<4x1024x15x20xbf16>
      %2296 = stablehlo.multiply %2294, %2295 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %2297 = stablehlo.broadcast_in_dim %2296, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %2298 = stablehlo.broadcast_in_dim %arg1089, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<1024x1x1xbf16>) -> tensor<4x1024x15x20xbf16>
      %2299 = stablehlo.multiply %2297, %2298 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %2300 = stablehlo.broadcast_in_dim %2299, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %2301 = stablehlo.broadcast_in_dim %arg1090, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<1024x1x1xbf16>) -> tensor<4x1024x15x20xbf16>
      %2302 = stablehlo.add %2300, %2301 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %2303 = stablehlo.broadcast_in_dim %2302, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %2304 = stablehlo.maximum %1662, %2303 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %2305 = stablehlo.minimum %1662, %2303 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %2306 = stablehlo.broadcast_in_dim %2305, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %2307 = stablehlo.multiply %2306, %1667 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %2308 = stablehlo.add %2304, %2307 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %2309 = stablehlo.convolution(%2308, %arg659) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>, tensor<512x1024x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %2310 = stablehlo.broadcast_in_dim %2309, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %2311 = stablehlo.broadcast_in_dim %arg1091, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %2312 = stablehlo.subtract %2310, %2311 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2313 = stablehlo.broadcast_in_dim %2312, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %2314 = stablehlo.broadcast_in_dim %arg1092, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %2315 = stablehlo.multiply %2313, %2314 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2316 = stablehlo.broadcast_in_dim %2315, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %2317 = stablehlo.broadcast_in_dim %arg1093, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %2318 = stablehlo.multiply %2316, %2317 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2319 = stablehlo.broadcast_in_dim %2318, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %2320 = stablehlo.broadcast_in_dim %arg1094, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<512x1x1xbf16>) -> tensor<4x512x15x20xbf16>
      %2321 = stablehlo.add %2319, %2320 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2322 = stablehlo.broadcast_in_dim %2321, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %2323 = stablehlo.maximum %1639, %2322 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2324 = stablehlo.minimum %1639, %2322 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2325 = stablehlo.broadcast_in_dim %2324, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>) -> tensor<4x512x15x20xbf16>
      %2326 = stablehlo.multiply %2325, %1646 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2327 = stablehlo.add %2323, %2326 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x512x15x20xbf16>
      %2328 = stablehlo.convolution(%2327, %arg660) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x512x15x20xbf16>, tensor<1024x512x3x3xbf16>) -> tensor<4x1024x15x20xbf16>
      %2329 = stablehlo.broadcast_in_dim %2328, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %2330 = stablehlo.broadcast_in_dim %arg1095, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<1024x1x1xbf16>) -> tensor<4x1024x15x20xbf16>
      %2331 = stablehlo.subtract %2329, %2330 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %2332 = stablehlo.broadcast_in_dim %2331, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %2333 = stablehlo.broadcast_in_dim %arg1096, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<1024x1x1xbf16>) -> tensor<4x1024x15x20xbf16>
      %2334 = stablehlo.multiply %2332, %2333 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %2335 = stablehlo.broadcast_in_dim %2334, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %2336 = stablehlo.broadcast_in_dim %arg1097, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<1024x1x1xbf16>) -> tensor<4x1024x15x20xbf16>
      %2337 = stablehlo.multiply %2335, %2336 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %2338 = stablehlo.broadcast_in_dim %2337, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %2339 = stablehlo.broadcast_in_dim %arg1098, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<1024x1x1xbf16>) -> tensor<4x1024x15x20xbf16>
      %2340 = stablehlo.add %2338, %2339 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %2341 = stablehlo.broadcast_in_dim %2340, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %2342 = stablehlo.maximum %1662, %2341 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %2343 = stablehlo.minimum %1662, %2341 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %2344 = stablehlo.broadcast_in_dim %2343, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>) -> tensor<4x1024x15x20xbf16>
      %2345 = stablehlo.multiply %2344, %1667 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %2346 = stablehlo.add %2342, %2345 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x1024x15x20xbf16>
      %2347 = stablehlo.convolution(%2346, %arg661) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x1024x15x20xbf16>, tensor<255x1024x1x1xbf16>) -> tensor<4x255x15x20xbf16>
      %2348 = stablehlo.reshape %arg662 : (tensor<255xbf16>) -> tensor<255x1x1xbf16>
      %2349 = stablehlo.broadcast_in_dim %2347, dims = [0, 1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<4x255x15x20xbf16>) -> tensor<4x255x15x20xbf16>
      %2350 = stablehlo.broadcast_in_dim %2348, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : (tensor<255x1x1xbf16>) -> tensor<4x255x15x20xbf16>
      %2351 = stablehlo.add %2349, %2350 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>]>} : tensor<4x255x15x20xbf16>
      sdy.return %2073, %2212, %2351 : tensor<4x255x60x80xbf16>, tensor<4x255x30x40xbf16>, tensor<4x255x15x20xbf16>
    } : (tensor<32x3x3x3xbf16>, tensor<64x32x3x3xbf16>, tensor<64x64x1x1xbf16>, tensor<64x64x1x1xbf16>, tensor<32x64x1x1xbf16>, tensor<64x32x3x3xbf16>, tensor<64x64x1x1xbf16>, tensor<64x128x1x1xbf16>, tensor<128x64x3x3xbf16>, tensor<64x128x1x1xbf16>, tensor<64x128x1x1xbf16>, tensor<64x64x1x1xbf16>, tensor<64x64x3x3xbf16>, tensor<64x64x1x1xbf16>, tensor<64x64x3x3xbf16>, tensor<64x64x1x1xbf16>, tensor<128x128x1x1xbf16>, tensor<256x128x3x3xbf16>, tensor<128x256x1x1xbf16>, tensor<128x256x1x1xbf16>, tensor<128x128x1x1xbf16>, tensor<128x128x3x3xbf16>, tensor<128x128x1x1xbf16>, tensor<128x128x3x3xbf16>, tensor<128x128x1x1xbf16>, tensor<128x128x3x3xbf16>, tensor<128x128x1x1xbf16>, tensor<128x128x3x3xbf16>, tensor<128x128x1x1xbf16>, tensor<128x128x3x3xbf16>, tensor<128x128x1x1xbf16>, tensor<128x128x3x3xbf16>, tensor<128x128x1x1xbf16>, tensor<128x128x3x3xbf16>, tensor<128x128x1x1xbf16>, tensor<128x128x3x3xbf16>, tensor<128x128x1x1xbf16>, tensor<256x256x1x1xbf16>, tensor<512x256x3x3xbf16>, tensor<256x512x1x1xbf16>, tensor<256x512x1x1xbf16>, tensor<256x256x1x1xbf16>, tensor<256x256x3x3xbf16>, tensor<256x256x1x1xbf16>, tensor<256x256x3x3xbf16>, tensor<256x256x1x1xbf16>, tensor<256x256x3x3xbf16>, tensor<256x256x1x1xbf16>, tensor<256x256x3x3xbf16>, tensor<256x256x1x1xbf16>, tensor<256x256x3x3xbf16>, tensor<256x256x1x1xbf16>, tensor<256x256x3x3xbf16>, tensor<256x256x1x1xbf16>, tensor<256x256x3x3xbf16>, tensor<256x256x1x1xbf16>, tensor<256x256x3x3xbf16>, tensor<256x256x1x1xbf16>, tensor<512x512x1x1xbf16>, tensor<1024x512x3x3xbf16>, tensor<512x1024x1x1xbf16>, tensor<512x1024x1x1xbf16>, tensor<512x512x1x1xbf16>, tensor<512x512x3x3xbf16>, tensor<512x512x1x1xbf16>, tensor<512x512x3x3xbf16>, tensor<512x512x1x1xbf16>, tensor<512x512x3x3xbf16>, tensor<512x512x1x1xbf16>, tensor<512x512x3x3xbf16>, tensor<512x512x1x1xbf16>, tensor<1024x1024x1x1xbf16>, tensor<512x1024x1x1xbf16>, tensor<1024x512x3x3xbf16>, tensor<512x1024x1x1xbf16>, tensor<512x2048x1x1xbf16>, tensor<1024x512x3x3xbf16>, tensor<512x1024x1x1xbf16>, tensor<256x512x1x1xbf16>, tensor<256x512x1x1xbf16>, tensor<256x512x1x1xbf16>, tensor<512x256x3x3xbf16>, tensor<256x512x1x1xbf16>, tensor<512x256x3x3xbf16>, tensor<256x512x1x1xbf16>, tensor<128x256x1x1xbf16>, tensor<128x256x1x1xbf16>, tensor<128x256x1x1xbf16>, tensor<256x128x3x3xbf16>, tensor<128x256x1x1xbf16>, tensor<256x128x3x3xbf16>, tensor<128x256x1x1xbf16>, tensor<256x128x3x3xbf16>, tensor<255x256x1x1xbf16>, tensor<255xbf16>, tensor<256x128x3x3xbf16>, tensor<256x512x1x1xbf16>, tensor<512x256x3x3xbf16>, tensor<256x512x1x1xbf16>, tensor<512x256x3x3xbf16>, tensor<256x512x1x1xbf16>, tensor<512x256x3x3xbf16>, tensor<255x512x1x1xbf16>, tensor<255xbf16>, tensor<512x256x3x3xbf16>, tensor<512x1024x1x1xbf16>, tensor<1024x512x3x3xbf16>, tensor<512x1024x1x1xbf16>, tensor<1024x512x3x3xbf16>, tensor<512x1024x1x1xbf16>, tensor<1024x512x3x3xbf16>, tensor<255x1024x1x1xbf16>, tensor<255xbf16>, tensor<32x1x1xbf16>, tensor<32x1x1xbf16>, tensor<32x1x1xbf16>, tensor<32x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<32x1x1xbf16>, tensor<32x1x1xbf16>, tensor<32x1x1xbf16>, tensor<32x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<30xf32>, tensor<40xf32>, tensor<1024x15x30xbf16>, tensor<1024x20x40xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<60xf32>, tensor<80xf32>, tensor<512x30x60xbf16>, tensor<512x40x80xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<4x3x480x640xbf16>) -> (tensor<4x255x60x80xbf16>, tensor<4x255x30x40xbf16>, tensor<4x255x15x20xbf16>)
    return %0#0, %0#1, %0#2 : tensor<4x255x60x80xbf16>, tensor<4x255x30x40xbf16>, tensor<4x255x15x20xbf16>
  }
}


// -----// IR Dump After UpdateGlobalToLocalShapesPass (update-global-to-local-shapes) ('builtin.module' operation) //----- //
module {
  sdy.mesh @mesh = <["default"=1, "batch"=2]>
  func.func @main(%arg0: tensor<32x3x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<64x32x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg2: tensor<64x64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg3: tensor<64x64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg4: tensor<32x64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg5: tensor<64x32x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg6: tensor<64x64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg7: tensor<64x128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg8: tensor<128x64x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg9: tensor<64x128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg10: tensor<64x128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg11: tensor<64x64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg12: tensor<64x64x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg13: tensor<64x64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg14: tensor<64x64x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg15: tensor<64x64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg16: tensor<128x128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg17: tensor<256x128x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg18: tensor<128x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg19: tensor<128x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg20: tensor<128x128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg21: tensor<128x128x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg22: tensor<128x128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg23: tensor<128x128x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg24: tensor<128x128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg25: tensor<128x128x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg26: tensor<128x128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg27: tensor<128x128x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg28: tensor<128x128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg29: tensor<128x128x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg30: tensor<128x128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg31: tensor<128x128x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg32: tensor<128x128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg33: tensor<128x128x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg34: tensor<128x128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg35: tensor<128x128x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg36: tensor<128x128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg37: tensor<256x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg38: tensor<512x256x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg39: tensor<256x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg40: tensor<256x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg41: tensor<256x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg42: tensor<256x256x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg43: tensor<256x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg44: tensor<256x256x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg45: tensor<256x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg46: tensor<256x256x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg47: tensor<256x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg48: tensor<256x256x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg49: tensor<256x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg50: tensor<256x256x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg51: tensor<256x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg52: tensor<256x256x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg53: tensor<256x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg54: tensor<256x256x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg55: tensor<256x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg56: tensor<256x256x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg57: tensor<256x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg58: tensor<512x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg59: tensor<1024x512x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg60: tensor<512x1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg61: tensor<512x1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg62: tensor<512x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg63: tensor<512x512x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg64: tensor<512x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg65: tensor<512x512x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg66: tensor<512x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg67: tensor<512x512x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg68: tensor<512x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg69: tensor<512x512x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg70: tensor<512x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg71: tensor<1024x1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg72: tensor<512x1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg73: tensor<1024x512x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg74: tensor<512x1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg75: tensor<512x2048x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg76: tensor<1024x512x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg77: tensor<512x1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg78: tensor<256x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg79: tensor<256x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg80: tensor<256x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg81: tensor<512x256x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg82: tensor<256x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg83: tensor<512x256x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg84: tensor<256x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg85: tensor<128x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg86: tensor<128x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg87: tensor<128x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg88: tensor<256x128x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg89: tensor<128x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg90: tensor<256x128x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg91: tensor<128x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg92: tensor<256x128x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg93: tensor<255x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg94: tensor<255xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg95: tensor<256x128x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg96: tensor<256x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg97: tensor<512x256x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg98: tensor<256x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg99: tensor<512x256x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg100: tensor<256x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg101: tensor<512x256x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg102: tensor<255x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg103: tensor<255xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg104: tensor<512x256x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg105: tensor<512x1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg106: tensor<1024x512x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg107: tensor<512x1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg108: tensor<1024x512x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg109: tensor<512x1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg110: tensor<1024x512x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg111: tensor<255x1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg112: tensor<255xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg113: tensor<32x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg114: tensor<32x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg115: tensor<32x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg116: tensor<32x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg117: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg118: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg119: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg120: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg121: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg122: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg123: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg124: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg125: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg126: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg127: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg128: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg129: tensor<32x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg130: tensor<32x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg131: tensor<32x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg132: tensor<32x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg133: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg134: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg135: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg136: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg137: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg138: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg139: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg140: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg141: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg142: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg143: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg144: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg145: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg146: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg147: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg148: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg149: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg150: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg151: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg152: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg153: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg154: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg155: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg156: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg157: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg158: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg159: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg160: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg161: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg162: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg163: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg164: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg165: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg166: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg167: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg168: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg169: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg170: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg171: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg172: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg173: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg174: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg175: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg176: tensor<64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg177: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg178: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg179: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg180: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg181: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg182: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg183: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg184: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg185: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg186: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg187: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg188: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg189: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg190: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg191: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg192: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg193: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg194: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg195: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg196: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg197: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg198: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg199: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg200: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg201: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg202: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg203: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg204: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg205: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg206: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg207: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg208: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg209: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg210: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg211: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg212: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg213: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg214: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg215: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg216: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg217: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg218: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg219: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg220: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg221: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg222: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg223: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg224: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg225: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg226: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg227: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg228: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg229: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg230: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg231: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg232: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg233: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg234: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg235: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg236: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg237: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg238: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg239: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg240: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg241: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg242: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg243: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg244: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg245: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg246: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg247: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg248: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg249: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg250: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg251: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg252: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg253: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg254: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg255: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg256: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg257: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg258: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg259: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg260: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg261: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg262: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg263: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg264: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg265: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg266: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg267: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg268: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg269: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg270: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg271: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg272: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg273: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg274: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg275: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg276: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg277: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg278: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg279: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg280: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg281: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg282: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg283: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg284: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg285: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg286: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg287: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg288: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg289: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg290: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg291: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg292: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg293: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg294: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg295: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg296: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg297: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg298: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg299: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg300: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg301: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg302: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg303: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg304: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg305: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg306: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg307: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg308: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg309: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg310: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg311: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg312: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg313: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg314: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg315: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg316: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg317: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg318: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg319: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg320: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg321: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg322: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg323: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg324: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg325: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg326: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg327: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg328: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg329: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg330: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg331: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg332: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg333: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg334: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg335: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg336: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg337: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg338: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg339: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg340: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg341: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg342: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg343: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg344: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg345: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg346: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg347: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg348: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg349: tensor<1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg350: tensor<1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg351: tensor<1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg352: tensor<1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg353: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg354: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg355: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg356: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg357: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg358: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg359: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg360: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg361: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg362: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg363: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg364: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg365: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg366: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg367: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg368: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg369: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg370: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg371: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg372: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg373: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg374: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg375: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg376: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg377: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg378: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg379: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg380: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg381: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg382: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg383: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg384: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg385: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg386: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg387: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg388: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg389: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg390: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg391: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg392: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg393: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg394: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg395: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg396: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg397: tensor<1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg398: tensor<1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg399: tensor<1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg400: tensor<1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg401: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg402: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg403: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg404: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg405: tensor<1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg406: tensor<1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg407: tensor<1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg408: tensor<1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg409: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg410: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg411: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg412: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg413: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg414: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg415: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg416: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg417: tensor<1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg418: tensor<1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg419: tensor<1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg420: tensor<1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg421: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg422: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg423: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg424: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg425: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg426: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg427: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg428: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg429: tensor<30xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg430: tensor<40xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg431: tensor<1024x15x30xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg432: tensor<1024x20x40xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg433: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg434: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg435: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg436: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg437: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg438: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg439: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg440: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg441: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg442: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg443: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg444: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg445: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg446: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg447: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg448: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg449: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg450: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg451: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg452: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg453: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg454: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg455: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg456: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg457: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg458: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg459: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg460: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg461: tensor<60xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg462: tensor<80xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg463: tensor<512x30x60xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg464: tensor<512x40x80xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg465: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg466: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg467: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg468: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg469: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg470: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg471: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg472: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg473: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg474: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg475: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg476: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg477: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg478: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg479: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg480: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg481: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg482: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg483: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg484: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg485: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg486: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg487: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg488: tensor<128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg489: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg490: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg491: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg492: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg493: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg494: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg495: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg496: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg497: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg498: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg499: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg500: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg501: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg502: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg503: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg504: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg505: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg506: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg507: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg508: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg509: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg510: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg511: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg512: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg513: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg514: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg515: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg516: tensor<256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg517: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg518: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg519: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg520: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg521: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg522: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg523: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg524: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg525: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg526: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg527: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg528: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg529: tensor<1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg530: tensor<1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg531: tensor<1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg532: tensor<1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg533: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg534: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg535: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg536: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg537: tensor<1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg538: tensor<1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg539: tensor<1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg540: tensor<1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg541: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg542: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg543: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg544: tensor<512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg545: tensor<1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg546: tensor<1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg547: tensor<1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg548: tensor<1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg549: tensor<4x3x480x640xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) -> (tensor<4x255x60x80xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<4x255x30x40xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<4x255x15x20xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:3 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %arg16, %arg17, %arg18, %arg19, %arg20, %arg21, %arg22, %arg23, %arg24, %arg25, %arg26, %arg27, %arg28, %arg29, %arg30, %arg31, %arg32, %arg33, %arg34, %arg35, %arg36, %arg37, %arg38, %arg39, %arg40, %arg41, %arg42, %arg43, %arg44, %arg45, %arg46, %arg47, %arg48, %arg49, %arg50, %arg51, %arg52, %arg53, %arg54, %arg55, %arg56, %arg57, %arg58, %arg59, %arg60, %arg61, %arg62, %arg63, %arg64, %arg65, %arg66, %arg67, %arg68, %arg69, %arg70, %arg71, %arg72, %arg73, %arg74, %arg75, %arg76, %arg77, %arg78, %arg79, %arg80, %arg81, %arg82, %arg83, %arg84, %arg85, %arg86, %arg87, %arg88, %arg89, %arg90, %arg91, %arg92, %arg93, %arg94, %arg95, %arg96, %arg97, %arg98, %arg99, %arg100, %arg101, %arg102, %arg103, %arg104, %arg105, %arg106, %arg107, %arg108, %arg109, %arg110, %arg111, %arg112, %arg113, %arg114, %arg115, %arg116, %arg117, %arg118, %arg119, %arg120, %arg121, %arg122, %arg123, %arg124, %arg125, %arg126, %arg127, %arg128, %arg129, %arg130, %arg131, %arg132, %arg133, %arg134, %arg135, %arg136, %arg137, %arg138, %arg139, %arg140, %arg141, %arg142, %arg143, %arg144, %arg145, %arg146, %arg147, %arg148, %arg149, %arg150, %arg151, %arg152, %arg153, %arg154, %arg155, %arg156, %arg157, %arg158, %arg159, %arg160, %arg161, %arg162, %arg163, %arg164, %arg165, %arg166, %arg167, %arg168, %arg169, %arg170, %arg171, %arg172, %arg173, %arg174, %arg175, %arg176, %arg177, %arg178, %arg179, %arg180, %arg181, %arg182, %arg183, %arg184, %arg185, %arg186, %arg187, %arg188, %arg189, %arg190, %arg191, %arg192, %arg193, %arg194, %arg195, %arg196, %arg197, %arg198, %arg199, %arg200, %arg201, %arg202, %arg203, %arg204, %arg205, %arg206, %arg207, %arg208, %arg209, %arg210, %arg211, %arg212, %arg213, %arg214, %arg215, %arg216, %arg217, %arg218, %arg219, %arg220, %arg221, %arg222, %arg223, %arg224, %arg225, %arg226, %arg227, %arg228, %arg229, %arg230, %arg231, %arg232, %arg233, %arg234, %arg235, %arg236, %arg237, %arg238, %arg239, %arg240, %arg241, %arg242, %arg243, %arg244, %arg245, %arg246, %arg247, %arg248, %arg249, %arg250, %arg251, %arg252, %arg253, %arg254, %arg255, %arg256, %arg257, %arg258, %arg259, %arg260, %arg261, %arg262, %arg263, %arg264, %arg265, %arg266, %arg267, %arg268, %arg269, %arg270, %arg271, %arg272, %arg273, %arg274, %arg275, %arg276, %arg277, %arg278, %arg279, %arg280, %arg281, %arg282, %arg283, %arg284, %arg285, %arg286, %arg287, %arg288, %arg289, %arg290, %arg291, %arg292, %arg293, %arg294, %arg295, %arg296, %arg297, %arg298, %arg299, %arg300, %arg301, %arg302, %arg303, %arg304, %arg305, %arg306, %arg307, %arg308, %arg309, %arg310, %arg311, %arg312, %arg313, %arg314, %arg315, %arg316, %arg317, %arg318, %arg319, %arg320, %arg321, %arg322, %arg323, %arg324, %arg325, %arg326, %arg327, %arg328, %arg329, %arg330, %arg331, %arg332, %arg333, %arg334, %arg335, %arg336, %arg337, %arg338, %arg339, %arg340, %arg341, %arg342, %arg343, %arg344, %arg345, %arg346, %arg347, %arg348, %arg349, %arg350, %arg351, %arg352, %arg353, %arg354, %arg355, %arg356, %arg357, %arg358, %arg359, %arg360, %arg361, %arg362, %arg363, %arg364, %arg365, %arg366, %arg367, %arg368, %arg369, %arg370, %arg371, %arg372, %arg373, %arg374, %arg375, %arg376, %arg377, %arg378, %arg379, %arg380, %arg381, %arg382, %arg383, %arg384, %arg385, %arg386, %arg387, %arg388, %arg389, %arg390, %arg391, %arg392, %arg393, %arg394, %arg395, %arg396, %arg397, %arg398, %arg399, %arg400, %arg401, %arg402, %arg403, %arg404, %arg405, %arg406, %arg407, %arg408, %arg409, %arg410, %arg411, %arg412, %arg413, %arg414, %arg415, %arg416, %arg417, %arg418, %arg419, %arg420, %arg421, %arg422, %arg423, %arg424, %arg425, %arg426, %arg427, %arg428, %arg429, %arg430, %arg431, %arg432, %arg433, %arg434, %arg435, %arg436, %arg437, %arg438, %arg439, %arg440, %arg441, %arg442, %arg443, %arg444, %arg445, %arg446, %arg447, %arg448, %arg449, %arg450, %arg451, %arg452, %arg453, %arg454, %arg455, %arg456, %arg457, %arg458, %arg459, %arg460, %arg461, %arg462, %arg463, %arg464, %arg465, %arg466, %arg467, %arg468, %arg469, %arg470, %arg471, %arg472, %arg473, %arg474, %arg475, %arg476, %arg477, %arg478, %arg479, %arg480, %arg481, %arg482, %arg483, %arg484, %arg485, %arg486, %arg487, %arg488, %arg489, %arg490, %arg491, %arg492, %arg493, %arg494, %arg495, %arg496, %arg497, %arg498, %arg499, %arg500, %arg501, %arg502, %arg503, %arg504, %arg505, %arg506, %arg507, %arg508, %arg509, %arg510, %arg511, %arg512, %arg513, %arg514, %arg515, %arg516, %arg517, %arg518, %arg519, %arg520, %arg521, %arg522, %arg523, %arg524, %arg525, %arg526, %arg527, %arg528, %arg529, %arg530, %arg531, %arg532, %arg533, %arg534, %arg535, %arg536, %arg537, %arg538, %arg539, %arg540, %arg541, %arg542, %arg543, %arg544, %arg545, %arg546, %arg547, %arg548, %arg549) in_shardings=[<@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}, {?}]>, <@mesh, [{?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}]>, <@mesh, [{?}]>, <@mesh, [{"batch", ?}, {?}, {?}]>, <@mesh, [{"batch", ?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}]>, <@mesh, [{?}]>, <@mesh, [{"batch", ?}, {?}, {?}]>, <@mesh, [{"batch", ?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{"batch", ?}, {?}, {?}, {?}]>] out_shardings=[<@mesh, [{"batch", ?}, {?}, {?}, {?}]>, <@mesh, [{"batch", ?}, {?}, {?}, {?}]>, <@mesh, [{"batch", ?}, {?}, {?}, {?}]>] manual_axes={"default", "batch"} (%arg550: tensor<32x3x3x3xbf16>, %arg551: tensor<64x32x3x3xbf16>, %arg552: tensor<64x64x1x1xbf16>, %arg553: tensor<64x64x1x1xbf16>, %arg554: tensor<32x64x1x1xbf16>, %arg555: tensor<64x32x3x3xbf16>, %arg556: tensor<64x64x1x1xbf16>, %arg557: tensor<64x128x1x1xbf16>, %arg558: tensor<128x64x3x3xbf16>, %arg559: tensor<64x128x1x1xbf16>, %arg560: tensor<64x128x1x1xbf16>, %arg561: tensor<64x64x1x1xbf16>, %arg562: tensor<64x64x3x3xbf16>, %arg563: tensor<64x64x1x1xbf16>, %arg564: tensor<64x64x3x3xbf16>, %arg565: tensor<64x64x1x1xbf16>, %arg566: tensor<128x128x1x1xbf16>, %arg567: tensor<256x128x3x3xbf16>, %arg568: tensor<128x256x1x1xbf16>, %arg569: tensor<128x256x1x1xbf16>, %arg570: tensor<128x128x1x1xbf16>, %arg571: tensor<128x128x3x3xbf16>, %arg572: tensor<128x128x1x1xbf16>, %arg573: tensor<128x128x3x3xbf16>, %arg574: tensor<128x128x1x1xbf16>, %arg575: tensor<128x128x3x3xbf16>, %arg576: tensor<128x128x1x1xbf16>, %arg577: tensor<128x128x3x3xbf16>, %arg578: tensor<128x128x1x1xbf16>, %arg579: tensor<128x128x3x3xbf16>, %arg580: tensor<128x128x1x1xbf16>, %arg581: tensor<128x128x3x3xbf16>, %arg582: tensor<128x128x1x1xbf16>, %arg583: tensor<128x128x3x3xbf16>, %arg584: tensor<128x128x1x1xbf16>, %arg585: tensor<128x128x3x3xbf16>, %arg586: tensor<128x128x1x1xbf16>, %arg587: tensor<256x256x1x1xbf16>, %arg588: tensor<512x256x3x3xbf16>, %arg589: tensor<256x512x1x1xbf16>, %arg590: tensor<256x512x1x1xbf16>, %arg591: tensor<256x256x1x1xbf16>, %arg592: tensor<256x256x3x3xbf16>, %arg593: tensor<256x256x1x1xbf16>, %arg594: tensor<256x256x3x3xbf16>, %arg595: tensor<256x256x1x1xbf16>, %arg596: tensor<256x256x3x3xbf16>, %arg597: tensor<256x256x1x1xbf16>, %arg598: tensor<256x256x3x3xbf16>, %arg599: tensor<256x256x1x1xbf16>, %arg600: tensor<256x256x3x3xbf16>, %arg601: tensor<256x256x1x1xbf16>, %arg602: tensor<256x256x3x3xbf16>, %arg603: tensor<256x256x1x1xbf16>, %arg604: tensor<256x256x3x3xbf16>, %arg605: tensor<256x256x1x1xbf16>, %arg606: tensor<256x256x3x3xbf16>, %arg607: tensor<256x256x1x1xbf16>, %arg608: tensor<512x512x1x1xbf16>, %arg609: tensor<1024x512x3x3xbf16>, %arg610: tensor<512x1024x1x1xbf16>, %arg611: tensor<512x1024x1x1xbf16>, %arg612: tensor<512x512x1x1xbf16>, %arg613: tensor<512x512x3x3xbf16>, %arg614: tensor<512x512x1x1xbf16>, %arg615: tensor<512x512x3x3xbf16>, %arg616: tensor<512x512x1x1xbf16>, %arg617: tensor<512x512x3x3xbf16>, %arg618: tensor<512x512x1x1xbf16>, %arg619: tensor<512x512x3x3xbf16>, %arg620: tensor<512x512x1x1xbf16>, %arg621: tensor<1024x1024x1x1xbf16>, %arg622: tensor<512x1024x1x1xbf16>, %arg623: tensor<1024x512x3x3xbf16>, %arg624: tensor<512x1024x1x1xbf16>, %arg625: tensor<512x2048x1x1xbf16>, %arg626: tensor<1024x512x3x3xbf16>, %arg627: tensor<512x1024x1x1xbf16>, %arg628: tensor<256x512x1x1xbf16>, %arg629: tensor<256x512x1x1xbf16>, %arg630: tensor<256x512x1x1xbf16>, %arg631: tensor<512x256x3x3xbf16>, %arg632: tensor<256x512x1x1xbf16>, %arg633: tensor<512x256x3x3xbf16>, %arg634: tensor<256x512x1x1xbf16>, %arg635: tensor<128x256x1x1xbf16>, %arg636: tensor<128x256x1x1xbf16>, %arg637: tensor<128x256x1x1xbf16>, %arg638: tensor<256x128x3x3xbf16>, %arg639: tensor<128x256x1x1xbf16>, %arg640: tensor<256x128x3x3xbf16>, %arg641: tensor<128x256x1x1xbf16>, %arg642: tensor<256x128x3x3xbf16>, %arg643: tensor<255x256x1x1xbf16>, %arg644: tensor<255xbf16>, %arg645: tensor<256x128x3x3xbf16>, %arg646: tensor<256x512x1x1xbf16>, %arg647: tensor<512x256x3x3xbf16>, %arg648: tensor<256x512x1x1xbf16>, %arg649: tensor<512x256x3x3xbf16>, %arg650: tensor<256x512x1x1xbf16>, %arg651: tensor<512x256x3x3xbf16>, %arg652: tensor<255x512x1x1xbf16>, %arg653: tensor<255xbf16>, %arg654: tensor<512x256x3x3xbf16>, %arg655: tensor<512x1024x1x1xbf16>, %arg656: tensor<1024x512x3x3xbf16>, %arg657: tensor<512x1024x1x1xbf16>, %arg658: tensor<1024x512x3x3xbf16>, %arg659: tensor<512x1024x1x1xbf16>, %arg660: tensor<1024x512x3x3xbf16>, %arg661: tensor<255x1024x1x1xbf16>, %arg662: tensor<255xbf16>, %arg663: tensor<32x1x1xbf16>, %arg664: tensor<32x1x1xbf16>, %arg665: tensor<32x1x1xbf16>, %arg666: tensor<32x1x1xbf16>, %arg667: tensor<64x1x1xbf16>, %arg668: tensor<64x1x1xbf16>, %arg669: tensor<64x1x1xbf16>, %arg670: tensor<64x1x1xbf16>, %arg671: tensor<64x1x1xbf16>, %arg672: tensor<64x1x1xbf16>, %arg673: tensor<64x1x1xbf16>, %arg674: tensor<64x1x1xbf16>, %arg675: tensor<64x1x1xbf16>, %arg676: tensor<64x1x1xbf16>, %arg677: tensor<64x1x1xbf16>, %arg678: tensor<64x1x1xbf16>, %arg679: tensor<32x1x1xbf16>, %arg680: tensor<32x1x1xbf16>, %arg681: tensor<32x1x1xbf16>, %arg682: tensor<32x1x1xbf16>, %arg683: tensor<64x1x1xbf16>, %arg684: tensor<64x1x1xbf16>, %arg685: tensor<64x1x1xbf16>, %arg686: tensor<64x1x1xbf16>, %arg687: tensor<64x1x1xbf16>, %arg688: tensor<64x1x1xbf16>, %arg689: tensor<64x1x1xbf16>, %arg690: tensor<64x1x1xbf16>, %arg691: tensor<64x1x1xbf16>, %arg692: tensor<64x1x1xbf16>, %arg693: tensor<64x1x1xbf16>, %arg694: tensor<64x1x1xbf16>, %arg695: tensor<128x1x1xbf16>, %arg696: tensor<128x1x1xbf16>, %arg697: tensor<128x1x1xbf16>, %arg698: tensor<128x1x1xbf16>, %arg699: tensor<64x1x1xbf16>, %arg700: tensor<64x1x1xbf16>, %arg701: tensor<64x1x1xbf16>, %arg702: tensor<64x1x1xbf16>, %arg703: tensor<64x1x1xbf16>, %arg704: tensor<64x1x1xbf16>, %arg705: tensor<64x1x1xbf16>, %arg706: tensor<64x1x1xbf16>, %arg707: tensor<64x1x1xbf16>, %arg708: tensor<64x1x1xbf16>, %arg709: tensor<64x1x1xbf16>, %arg710: tensor<64x1x1xbf16>, %arg711: tensor<64x1x1xbf16>, %arg712: tensor<64x1x1xbf16>, %arg713: tensor<64x1x1xbf16>, %arg714: tensor<64x1x1xbf16>, %arg715: tensor<64x1x1xbf16>, %arg716: tensor<64x1x1xbf16>, %arg717: tensor<64x1x1xbf16>, %arg718: tensor<64x1x1xbf16>, %arg719: tensor<64x1x1xbf16>, %arg720: tensor<64x1x1xbf16>, %arg721: tensor<64x1x1xbf16>, %arg722: tensor<64x1x1xbf16>, %arg723: tensor<64x1x1xbf16>, %arg724: tensor<64x1x1xbf16>, %arg725: tensor<64x1x1xbf16>, %arg726: tensor<64x1x1xbf16>, %arg727: tensor<128x1x1xbf16>, %arg728: tensor<128x1x1xbf16>, %arg729: tensor<128x1x1xbf16>, %arg730: tensor<128x1x1xbf16>, %arg731: tensor<256x1x1xbf16>, %arg732: tensor<256x1x1xbf16>, %arg733: tensor<256x1x1xbf16>, %arg734: tensor<256x1x1xbf16>, %arg735: tensor<128x1x1xbf16>, %arg736: tensor<128x1x1xbf16>, %arg737: tensor<128x1x1xbf16>, %arg738: tensor<128x1x1xbf16>, %arg739: tensor<128x1x1xbf16>, %arg740: tensor<128x1x1xbf16>, %arg741: tensor<128x1x1xbf16>, %arg742: tensor<128x1x1xbf16>, %arg743: tensor<128x1x1xbf16>, %arg744: tensor<128x1x1xbf16>, %arg745: tensor<128x1x1xbf16>, %arg746: tensor<128x1x1xbf16>, %arg747: tensor<128x1x1xbf16>, %arg748: tensor<128x1x1xbf16>, %arg749: tensor<128x1x1xbf16>, %arg750: tensor<128x1x1xbf16>, %arg751: tensor<128x1x1xbf16>, %arg752: tensor<128x1x1xbf16>, %arg753: tensor<128x1x1xbf16>, %arg754: tensor<128x1x1xbf16>, %arg755: tensor<128x1x1xbf16>, %arg756: tensor<128x1x1xbf16>, %arg757: tensor<128x1x1xbf16>, %arg758: tensor<128x1x1xbf16>, %arg759: tensor<128x1x1xbf16>, %arg760: tensor<128x1x1xbf16>, %arg761: tensor<128x1x1xbf16>, %arg762: tensor<128x1x1xbf16>, %arg763: tensor<128x1x1xbf16>, %arg764: tensor<128x1x1xbf16>, %arg765: tensor<128x1x1xbf16>, %arg766: tensor<128x1x1xbf16>, %arg767: tensor<128x1x1xbf16>, %arg768: tensor<128x1x1xbf16>, %arg769: tensor<128x1x1xbf16>, %arg770: tensor<128x1x1xbf16>, %arg771: tensor<128x1x1xbf16>, %arg772: tensor<128x1x1xbf16>, %arg773: tensor<128x1x1xbf16>, %arg774: tensor<128x1x1xbf16>, %arg775: tensor<128x1x1xbf16>, %arg776: tensor<128x1x1xbf16>, %arg777: tensor<128x1x1xbf16>, %arg778: tensor<128x1x1xbf16>, %arg779: tensor<128x1x1xbf16>, %arg780: tensor<128x1x1xbf16>, %arg781: tensor<128x1x1xbf16>, %arg782: tensor<128x1x1xbf16>, %arg783: tensor<128x1x1xbf16>, %arg784: tensor<128x1x1xbf16>, %arg785: tensor<128x1x1xbf16>, %arg786: tensor<128x1x1xbf16>, %arg787: tensor<128x1x1xbf16>, %arg788: tensor<128x1x1xbf16>, %arg789: tensor<128x1x1xbf16>, %arg790: tensor<128x1x1xbf16>, %arg791: tensor<128x1x1xbf16>, %arg792: tensor<128x1x1xbf16>, %arg793: tensor<128x1x1xbf16>, %arg794: tensor<128x1x1xbf16>, %arg795: tensor<128x1x1xbf16>, %arg796: tensor<128x1x1xbf16>, %arg797: tensor<128x1x1xbf16>, %arg798: tensor<128x1x1xbf16>, %arg799: tensor<128x1x1xbf16>, %arg800: tensor<128x1x1xbf16>, %arg801: tensor<128x1x1xbf16>, %arg802: tensor<128x1x1xbf16>, %arg803: tensor<128x1x1xbf16>, %arg804: tensor<128x1x1xbf16>, %arg805: tensor<128x1x1xbf16>, %arg806: tensor<128x1x1xbf16>, %arg807: tensor<128x1x1xbf16>, %arg808: tensor<128x1x1xbf16>, %arg809: tensor<128x1x1xbf16>, %arg810: tensor<128x1x1xbf16>, %arg811: tensor<256x1x1xbf16>, %arg812: tensor<256x1x1xbf16>, %arg813: tensor<256x1x1xbf16>, %arg814: tensor<256x1x1xbf16>, %arg815: tensor<512x1x1xbf16>, %arg816: tensor<512x1x1xbf16>, %arg817: tensor<512x1x1xbf16>, %arg818: tensor<512x1x1xbf16>, %arg819: tensor<256x1x1xbf16>, %arg820: tensor<256x1x1xbf16>, %arg821: tensor<256x1x1xbf16>, %arg822: tensor<256x1x1xbf16>, %arg823: tensor<256x1x1xbf16>, %arg824: tensor<256x1x1xbf16>, %arg825: tensor<256x1x1xbf16>, %arg826: tensor<256x1x1xbf16>, %arg827: tensor<256x1x1xbf16>, %arg828: tensor<256x1x1xbf16>, %arg829: tensor<256x1x1xbf16>, %arg830: tensor<256x1x1xbf16>, %arg831: tensor<256x1x1xbf16>, %arg832: tensor<256x1x1xbf16>, %arg833: tensor<256x1x1xbf16>, %arg834: tensor<256x1x1xbf16>, %arg835: tensor<256x1x1xbf16>, %arg836: tensor<256x1x1xbf16>, %arg837: tensor<256x1x1xbf16>, %arg838: tensor<256x1x1xbf16>, %arg839: tensor<256x1x1xbf16>, %arg840: tensor<256x1x1xbf16>, %arg841: tensor<256x1x1xbf16>, %arg842: tensor<256x1x1xbf16>, %arg843: tensor<256x1x1xbf16>, %arg844: tensor<256x1x1xbf16>, %arg845: tensor<256x1x1xbf16>, %arg846: tensor<256x1x1xbf16>, %arg847: tensor<256x1x1xbf16>, %arg848: tensor<256x1x1xbf16>, %arg849: tensor<256x1x1xbf16>, %arg850: tensor<256x1x1xbf16>, %arg851: tensor<256x1x1xbf16>, %arg852: tensor<256x1x1xbf16>, %arg853: tensor<256x1x1xbf16>, %arg854: tensor<256x1x1xbf16>, %arg855: tensor<256x1x1xbf16>, %arg856: tensor<256x1x1xbf16>, %arg857: tensor<256x1x1xbf16>, %arg858: tensor<256x1x1xbf16>, %arg859: tensor<256x1x1xbf16>, %arg860: tensor<256x1x1xbf16>, %arg861: tensor<256x1x1xbf16>, %arg862: tensor<256x1x1xbf16>, %arg863: tensor<256x1x1xbf16>, %arg864: tensor<256x1x1xbf16>, %arg865: tensor<256x1x1xbf16>, %arg866: tensor<256x1x1xbf16>, %arg867: tensor<256x1x1xbf16>, %arg868: tensor<256x1x1xbf16>, %arg869: tensor<256x1x1xbf16>, %arg870: tensor<256x1x1xbf16>, %arg871: tensor<256x1x1xbf16>, %arg872: tensor<256x1x1xbf16>, %arg873: tensor<256x1x1xbf16>, %arg874: tensor<256x1x1xbf16>, %arg875: tensor<256x1x1xbf16>, %arg876: tensor<256x1x1xbf16>, %arg877: tensor<256x1x1xbf16>, %arg878: tensor<256x1x1xbf16>, %arg879: tensor<256x1x1xbf16>, %arg880: tensor<256x1x1xbf16>, %arg881: tensor<256x1x1xbf16>, %arg882: tensor<256x1x1xbf16>, %arg883: tensor<256x1x1xbf16>, %arg884: tensor<256x1x1xbf16>, %arg885: tensor<256x1x1xbf16>, %arg886: tensor<256x1x1xbf16>, %arg887: tensor<256x1x1xbf16>, %arg888: tensor<256x1x1xbf16>, %arg889: tensor<256x1x1xbf16>, %arg890: tensor<256x1x1xbf16>, %arg891: tensor<256x1x1xbf16>, %arg892: tensor<256x1x1xbf16>, %arg893: tensor<256x1x1xbf16>, %arg894: tensor<256x1x1xbf16>, %arg895: tensor<512x1x1xbf16>, %arg896: tensor<512x1x1xbf16>, %arg897: tensor<512x1x1xbf16>, %arg898: tensor<512x1x1xbf16>, %arg899: tensor<1024x1x1xbf16>, %arg900: tensor<1024x1x1xbf16>, %arg901: tensor<1024x1x1xbf16>, %arg902: tensor<1024x1x1xbf16>, %arg903: tensor<512x1x1xbf16>, %arg904: tensor<512x1x1xbf16>, %arg905: tensor<512x1x1xbf16>, %arg906: tensor<512x1x1xbf16>, %arg907: tensor<512x1x1xbf16>, %arg908: tensor<512x1x1xbf16>, %arg909: tensor<512x1x1xbf16>, %arg910: tensor<512x1x1xbf16>, %arg911: tensor<512x1x1xbf16>, %arg912: tensor<512x1x1xbf16>, %arg913: tensor<512x1x1xbf16>, %arg914: tensor<512x1x1xbf16>, %arg915: tensor<512x1x1xbf16>, %arg916: tensor<512x1x1xbf16>, %arg917: tensor<512x1x1xbf16>, %arg918: tensor<512x1x1xbf16>, %arg919: tensor<512x1x1xbf16>, %arg920: tensor<512x1x1xbf16>, %arg921: tensor<512x1x1xbf16>, %arg922: tensor<512x1x1xbf16>, %arg923: tensor<512x1x1xbf16>, %arg924: tensor<512x1x1xbf16>, %arg925: tensor<512x1x1xbf16>, %arg926: tensor<512x1x1xbf16>, %arg927: tensor<512x1x1xbf16>, %arg928: tensor<512x1x1xbf16>, %arg929: tensor<512x1x1xbf16>, %arg930: tensor<512x1x1xbf16>, %arg931: tensor<512x1x1xbf16>, %arg932: tensor<512x1x1xbf16>, %arg933: tensor<512x1x1xbf16>, %arg934: tensor<512x1x1xbf16>, %arg935: tensor<512x1x1xbf16>, %arg936: tensor<512x1x1xbf16>, %arg937: tensor<512x1x1xbf16>, %arg938: tensor<512x1x1xbf16>, %arg939: tensor<512x1x1xbf16>, %arg940: tensor<512x1x1xbf16>, %arg941: tensor<512x1x1xbf16>, %arg942: tensor<512x1x1xbf16>, %arg943: tensor<512x1x1xbf16>, %arg944: tensor<512x1x1xbf16>, %arg945: tensor<512x1x1xbf16>, %arg946: tensor<512x1x1xbf16>, %arg947: tensor<1024x1x1xbf16>, %arg948: tensor<1024x1x1xbf16>, %arg949: tensor<1024x1x1xbf16>, %arg950: tensor<1024x1x1xbf16>, %arg951: tensor<512x1x1xbf16>, %arg952: tensor<512x1x1xbf16>, %arg953: tensor<512x1x1xbf16>, %arg954: tensor<512x1x1xbf16>, %arg955: tensor<1024x1x1xbf16>, %arg956: tensor<1024x1x1xbf16>, %arg957: tensor<1024x1x1xbf16>, %arg958: tensor<1024x1x1xbf16>, %arg959: tensor<512x1x1xbf16>, %arg960: tensor<512x1x1xbf16>, %arg961: tensor<512x1x1xbf16>, %arg962: tensor<512x1x1xbf16>, %arg963: tensor<512x1x1xbf16>, %arg964: tensor<512x1x1xbf16>, %arg965: tensor<512x1x1xbf16>, %arg966: tensor<512x1x1xbf16>, %arg967: tensor<1024x1x1xbf16>, %arg968: tensor<1024x1x1xbf16>, %arg969: tensor<1024x1x1xbf16>, %arg970: tensor<1024x1x1xbf16>, %arg971: tensor<512x1x1xbf16>, %arg972: tensor<512x1x1xbf16>, %arg973: tensor<512x1x1xbf16>, %arg974: tensor<512x1x1xbf16>, %arg975: tensor<256x1x1xbf16>, %arg976: tensor<256x1x1xbf16>, %arg977: tensor<256x1x1xbf16>, %arg978: tensor<256x1x1xbf16>, %arg979: tensor<30xf32>, %arg980: tensor<40xf32>, %arg981: tensor<512x15x30xbf16>, %arg982: tensor<512x20x40xbf16>, %arg983: tensor<256x1x1xbf16>, %arg984: tensor<256x1x1xbf16>, %arg985: tensor<256x1x1xbf16>, %arg986: tensor<256x1x1xbf16>, %arg987: tensor<256x1x1xbf16>, %arg988: tensor<256x1x1xbf16>, %arg989: tensor<256x1x1xbf16>, %arg990: tensor<256x1x1xbf16>, %arg991: tensor<512x1x1xbf16>, %arg992: tensor<512x1x1xbf16>, %arg993: tensor<512x1x1xbf16>, %arg994: tensor<512x1x1xbf16>, %arg995: tensor<256x1x1xbf16>, %arg996: tensor<256x1x1xbf16>, %arg997: tensor<256x1x1xbf16>, %arg998: tensor<256x1x1xbf16>, %arg999: tensor<512x1x1xbf16>, %arg1000: tensor<512x1x1xbf16>, %arg1001: tensor<512x1x1xbf16>, %arg1002: tensor<512x1x1xbf16>, %arg1003: tensor<256x1x1xbf16>, %arg1004: tensor<256x1x1xbf16>, %arg1005: tensor<256x1x1xbf16>, %arg1006: tensor<256x1x1xbf16>, %arg1007: tensor<128x1x1xbf16>, %arg1008: tensor<128x1x1xbf16>, %arg1009: tensor<128x1x1xbf16>, %arg1010: tensor<128x1x1xbf16>, %arg1011: tensor<60xf32>, %arg1012: tensor<80xf32>, %arg1013: tensor<256x30x60xbf16>, %arg1014: tensor<256x40x80xbf16>, %arg1015: tensor<128x1x1xbf16>, %arg1016: tensor<128x1x1xbf16>, %arg1017: tensor<128x1x1xbf16>, %arg1018: tensor<128x1x1xbf16>, %arg1019: tensor<128x1x1xbf16>, %arg1020: tensor<128x1x1xbf16>, %arg1021: tensor<128x1x1xbf16>, %arg1022: tensor<128x1x1xbf16>, %arg1023: tensor<256x1x1xbf16>, %arg1024: tensor<256x1x1xbf16>, %arg1025: tensor<256x1x1xbf16>, %arg1026: tensor<256x1x1xbf16>, %arg1027: tensor<128x1x1xbf16>, %arg1028: tensor<128x1x1xbf16>, %arg1029: tensor<128x1x1xbf16>, %arg1030: tensor<128x1x1xbf16>, %arg1031: tensor<256x1x1xbf16>, %arg1032: tensor<256x1x1xbf16>, %arg1033: tensor<256x1x1xbf16>, %arg1034: tensor<256x1x1xbf16>, %arg1035: tensor<128x1x1xbf16>, %arg1036: tensor<128x1x1xbf16>, %arg1037: tensor<128x1x1xbf16>, %arg1038: tensor<128x1x1xbf16>, %arg1039: tensor<256x1x1xbf16>, %arg1040: tensor<256x1x1xbf16>, %arg1041: tensor<256x1x1xbf16>, %arg1042: tensor<256x1x1xbf16>, %arg1043: tensor<256x1x1xbf16>, %arg1044: tensor<256x1x1xbf16>, %arg1045: tensor<256x1x1xbf16>, %arg1046: tensor<256x1x1xbf16>, %arg1047: tensor<256x1x1xbf16>, %arg1048: tensor<256x1x1xbf16>, %arg1049: tensor<256x1x1xbf16>, %arg1050: tensor<256x1x1xbf16>, %arg1051: tensor<512x1x1xbf16>, %arg1052: tensor<512x1x1xbf16>, %arg1053: tensor<512x1x1xbf16>, %arg1054: tensor<512x1x1xbf16>, %arg1055: tensor<256x1x1xbf16>, %arg1056: tensor<256x1x1xbf16>, %arg1057: tensor<256x1x1xbf16>, %arg1058: tensor<256x1x1xbf16>, %arg1059: tensor<512x1x1xbf16>, %arg1060: tensor<512x1x1xbf16>, %arg1061: tensor<512x1x1xbf16>, %arg1062: tensor<512x1x1xbf16>, %arg1063: tensor<256x1x1xbf16>, %arg1064: tensor<256x1x1xbf16>, %arg1065: tensor<256x1x1xbf16>, %arg1066: tensor<256x1x1xbf16>, %arg1067: tensor<512x1x1xbf16>, %arg1068: tensor<512x1x1xbf16>, %arg1069: tensor<512x1x1xbf16>, %arg1070: tensor<512x1x1xbf16>, %arg1071: tensor<512x1x1xbf16>, %arg1072: tensor<512x1x1xbf16>, %arg1073: tensor<512x1x1xbf16>, %arg1074: tensor<512x1x1xbf16>, %arg1075: tensor<512x1x1xbf16>, %arg1076: tensor<512x1x1xbf16>, %arg1077: tensor<512x1x1xbf16>, %arg1078: tensor<512x1x1xbf16>, %arg1079: tensor<1024x1x1xbf16>, %arg1080: tensor<1024x1x1xbf16>, %arg1081: tensor<1024x1x1xbf16>, %arg1082: tensor<1024x1x1xbf16>, %arg1083: tensor<512x1x1xbf16>, %arg1084: tensor<512x1x1xbf16>, %arg1085: tensor<512x1x1xbf16>, %arg1086: tensor<512x1x1xbf16>, %arg1087: tensor<1024x1x1xbf16>, %arg1088: tensor<1024x1x1xbf16>, %arg1089: tensor<1024x1x1xbf16>, %arg1090: tensor<1024x1x1xbf16>, %arg1091: tensor<512x1x1xbf16>, %arg1092: tensor<512x1x1xbf16>, %arg1093: tensor<512x1x1xbf16>, %arg1094: tensor<512x1x1xbf16>, %arg1095: tensor<1024x1x1xbf16>, %arg1096: tensor<1024x1x1xbf16>, %arg1097: tensor<1024x1x1xbf16>, %arg1098: tensor<1024x1x1xbf16>, %arg1099: tensor<2x3x480x640xbf16>) {
      %c = stablehlo.constant dense<20> : tensor<i64>
      %c_0 = stablehlo.constant dense<0> : tensor<i64>
      %cst = stablehlo.constant dense<0xFF80> : tensor<bf16>
      %cst_1 = arith.constant dense<1.000000e-01> : tensor<1xf64>
      %1 = stablehlo.convolution(%arg1099, %arg550) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x3x480x640xbf16>, tensor<32x3x3x3xbf16>) -> tensor<2x32x480x640xbf16>
      %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1, 2, 3] : (tensor<2x32x480x640xbf16>) -> tensor<2x32x480x640xbf16>
      %3 = stablehlo.broadcast_in_dim %arg663, dims = [1, 2, 3] : (tensor<32x1x1xbf16>) -> tensor<2x32x480x640xbf16>
      %4 = stablehlo.subtract %2, %3 : tensor<2x32x480x640xbf16>
      %5 = stablehlo.broadcast_in_dim %4, dims = [0, 1, 2, 3] : (tensor<2x32x480x640xbf16>) -> tensor<2x32x480x640xbf16>
      %6 = stablehlo.broadcast_in_dim %arg664, dims = [1, 2, 3] : (tensor<32x1x1xbf16>) -> tensor<2x32x480x640xbf16>
      %7 = stablehlo.multiply %5, %6 : tensor<2x32x480x640xbf16>
      %8 = stablehlo.broadcast_in_dim %7, dims = [0, 1, 2, 3] : (tensor<2x32x480x640xbf16>) -> tensor<2x32x480x640xbf16>
      %9 = stablehlo.broadcast_in_dim %arg665, dims = [1, 2, 3] : (tensor<32x1x1xbf16>) -> tensor<2x32x480x640xbf16>
      %10 = stablehlo.multiply %8, %9 : tensor<2x32x480x640xbf16>
      %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1, 2, 3] : (tensor<2x32x480x640xbf16>) -> tensor<2x32x480x640xbf16>
      %12 = stablehlo.broadcast_in_dim %arg666, dims = [1, 2, 3] : (tensor<32x1x1xbf16>) -> tensor<2x32x480x640xbf16>
      %13 = stablehlo.add %11, %12 : tensor<2x32x480x640xbf16>
      %14 = stablehlo.exponential %13 : tensor<2x32x480x640xbf16>
      %15 = stablehlo.log_plus_one %14 : tensor<2x32x480x640xbf16>
      %16 = stablehlo.convert %c : (tensor<i64>) -> tensor<bf16>
      %17 = stablehlo.broadcast_in_dim %13, dims = [0, 1, 2, 3] : (tensor<2x32x480x640xbf16>) -> tensor<2x32x480x640xbf16>
      %18 = stablehlo.broadcast_in_dim %16, dims = [] : (tensor<bf16>) -> tensor<2x32x480x640xbf16>
      %19 = stablehlo.compare  GT, %17, %18,  FLOAT : (tensor<2x32x480x640xbf16>, tensor<2x32x480x640xbf16>) -> tensor<2x32x480x640xi1>
      %20 = stablehlo.broadcast_in_dim %19, dims = [0, 1, 2, 3] : (tensor<2x32x480x640xi1>) -> tensor<2x32x480x640xi1>
      %21 = stablehlo.broadcast_in_dim %15, dims = [0, 1, 2, 3] : (tensor<2x32x480x640xbf16>) -> tensor<2x32x480x640xbf16>
      %22 = stablehlo.select %20, %17, %21 : tensor<2x32x480x640xi1>, tensor<2x32x480x640xbf16>
      %23 = stablehlo.tanh %22 : tensor<2x32x480x640xbf16>
      %24 = stablehlo.multiply %13, %23 : tensor<2x32x480x640xbf16>
      %25 = stablehlo.convolution(%24, %arg551) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [2, 2], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x32x480x640xbf16>, tensor<64x32x3x3xbf16>) -> tensor<2x64x240x320xbf16>
      %26 = stablehlo.broadcast_in_dim %25, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %27 = stablehlo.broadcast_in_dim %arg667, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x240x320xbf16>
      %28 = stablehlo.subtract %26, %27 : tensor<2x64x240x320xbf16>
      %29 = stablehlo.broadcast_in_dim %28, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %30 = stablehlo.broadcast_in_dim %arg668, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x240x320xbf16>
      %31 = stablehlo.multiply %29, %30 : tensor<2x64x240x320xbf16>
      %32 = stablehlo.broadcast_in_dim %31, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %33 = stablehlo.broadcast_in_dim %arg669, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x240x320xbf16>
      %34 = stablehlo.multiply %32, %33 : tensor<2x64x240x320xbf16>
      %35 = stablehlo.broadcast_in_dim %34, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %36 = stablehlo.broadcast_in_dim %arg670, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x240x320xbf16>
      %37 = stablehlo.add %35, %36 : tensor<2x64x240x320xbf16>
      %38 = stablehlo.exponential %37 : tensor<2x64x240x320xbf16>
      %39 = stablehlo.log_plus_one %38 : tensor<2x64x240x320xbf16>
      %40 = stablehlo.broadcast_in_dim %37, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %41 = stablehlo.broadcast_in_dim %16, dims = [] : (tensor<bf16>) -> tensor<2x64x240x320xbf16>
      %42 = stablehlo.compare  GT, %40, %41,  FLOAT : (tensor<2x64x240x320xbf16>, tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xi1>
      %43 = stablehlo.broadcast_in_dim %42, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xi1>) -> tensor<2x64x240x320xi1>
      %44 = stablehlo.broadcast_in_dim %39, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %45 = stablehlo.select %43, %40, %44 : tensor<2x64x240x320xi1>, tensor<2x64x240x320xbf16>
      %46 = stablehlo.tanh %45 : tensor<2x64x240x320xbf16>
      %47 = stablehlo.multiply %37, %46 : tensor<2x64x240x320xbf16>
      %48 = stablehlo.convolution(%47, %arg552) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x64x240x320xbf16>, tensor<64x64x1x1xbf16>) -> tensor<2x64x240x320xbf16>
      %49 = stablehlo.broadcast_in_dim %48, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %50 = stablehlo.broadcast_in_dim %arg671, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x240x320xbf16>
      %51 = stablehlo.subtract %49, %50 : tensor<2x64x240x320xbf16>
      %52 = stablehlo.broadcast_in_dim %51, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %53 = stablehlo.broadcast_in_dim %arg672, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x240x320xbf16>
      %54 = stablehlo.multiply %52, %53 : tensor<2x64x240x320xbf16>
      %55 = stablehlo.broadcast_in_dim %54, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %56 = stablehlo.broadcast_in_dim %arg673, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x240x320xbf16>
      %57 = stablehlo.multiply %55, %56 : tensor<2x64x240x320xbf16>
      %58 = stablehlo.broadcast_in_dim %57, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %59 = stablehlo.broadcast_in_dim %arg674, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x240x320xbf16>
      %60 = stablehlo.add %58, %59 : tensor<2x64x240x320xbf16>
      %61 = stablehlo.exponential %60 : tensor<2x64x240x320xbf16>
      %62 = stablehlo.log_plus_one %61 : tensor<2x64x240x320xbf16>
      %63 = stablehlo.broadcast_in_dim %60, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %64 = stablehlo.compare  GT, %63, %41,  FLOAT : (tensor<2x64x240x320xbf16>, tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xi1>
      %65 = stablehlo.broadcast_in_dim %64, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xi1>) -> tensor<2x64x240x320xi1>
      %66 = stablehlo.broadcast_in_dim %62, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %67 = stablehlo.select %65, %63, %66 : tensor<2x64x240x320xi1>, tensor<2x64x240x320xbf16>
      %68 = stablehlo.tanh %67 : tensor<2x64x240x320xbf16>
      %69 = stablehlo.multiply %60, %68 : tensor<2x64x240x320xbf16>
      %70 = stablehlo.convolution(%47, %arg553) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x64x240x320xbf16>, tensor<64x64x1x1xbf16>) -> tensor<2x64x240x320xbf16>
      %71 = stablehlo.broadcast_in_dim %70, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %72 = stablehlo.broadcast_in_dim %arg675, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x240x320xbf16>
      %73 = stablehlo.subtract %71, %72 : tensor<2x64x240x320xbf16>
      %74 = stablehlo.broadcast_in_dim %73, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %75 = stablehlo.broadcast_in_dim %arg676, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x240x320xbf16>
      %76 = stablehlo.multiply %74, %75 : tensor<2x64x240x320xbf16>
      %77 = stablehlo.broadcast_in_dim %76, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %78 = stablehlo.broadcast_in_dim %arg677, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x240x320xbf16>
      %79 = stablehlo.multiply %77, %78 : tensor<2x64x240x320xbf16>
      %80 = stablehlo.broadcast_in_dim %79, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %81 = stablehlo.broadcast_in_dim %arg678, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x240x320xbf16>
      %82 = stablehlo.add %80, %81 : tensor<2x64x240x320xbf16>
      %83 = stablehlo.exponential %82 : tensor<2x64x240x320xbf16>
      %84 = stablehlo.log_plus_one %83 : tensor<2x64x240x320xbf16>
      %85 = stablehlo.broadcast_in_dim %82, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %86 = stablehlo.compare  GT, %85, %41,  FLOAT : (tensor<2x64x240x320xbf16>, tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xi1>
      %87 = stablehlo.broadcast_in_dim %86, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xi1>) -> tensor<2x64x240x320xi1>
      %88 = stablehlo.broadcast_in_dim %84, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %89 = stablehlo.select %87, %85, %88 : tensor<2x64x240x320xi1>, tensor<2x64x240x320xbf16>
      %90 = stablehlo.tanh %89 : tensor<2x64x240x320xbf16>
      %91 = stablehlo.multiply %82, %90 : tensor<2x64x240x320xbf16>
      %92 = stablehlo.convolution(%91, %arg554) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x64x240x320xbf16>, tensor<32x64x1x1xbf16>) -> tensor<2x32x240x320xbf16>
      %93 = stablehlo.broadcast_in_dim %92, dims = [0, 1, 2, 3] : (tensor<2x32x240x320xbf16>) -> tensor<2x32x240x320xbf16>
      %94 = stablehlo.broadcast_in_dim %arg679, dims = [1, 2, 3] : (tensor<32x1x1xbf16>) -> tensor<2x32x240x320xbf16>
      %95 = stablehlo.subtract %93, %94 : tensor<2x32x240x320xbf16>
      %96 = stablehlo.broadcast_in_dim %95, dims = [0, 1, 2, 3] : (tensor<2x32x240x320xbf16>) -> tensor<2x32x240x320xbf16>
      %97 = stablehlo.broadcast_in_dim %arg680, dims = [1, 2, 3] : (tensor<32x1x1xbf16>) -> tensor<2x32x240x320xbf16>
      %98 = stablehlo.multiply %96, %97 : tensor<2x32x240x320xbf16>
      %99 = stablehlo.broadcast_in_dim %98, dims = [0, 1, 2, 3] : (tensor<2x32x240x320xbf16>) -> tensor<2x32x240x320xbf16>
      %100 = stablehlo.broadcast_in_dim %arg681, dims = [1, 2, 3] : (tensor<32x1x1xbf16>) -> tensor<2x32x240x320xbf16>
      %101 = stablehlo.multiply %99, %100 : tensor<2x32x240x320xbf16>
      %102 = stablehlo.broadcast_in_dim %101, dims = [0, 1, 2, 3] : (tensor<2x32x240x320xbf16>) -> tensor<2x32x240x320xbf16>
      %103 = stablehlo.broadcast_in_dim %arg682, dims = [1, 2, 3] : (tensor<32x1x1xbf16>) -> tensor<2x32x240x320xbf16>
      %104 = stablehlo.add %102, %103 : tensor<2x32x240x320xbf16>
      %105 = stablehlo.exponential %104 : tensor<2x32x240x320xbf16>
      %106 = stablehlo.log_plus_one %105 : tensor<2x32x240x320xbf16>
      %107 = stablehlo.broadcast_in_dim %104, dims = [0, 1, 2, 3] : (tensor<2x32x240x320xbf16>) -> tensor<2x32x240x320xbf16>
      %108 = stablehlo.broadcast_in_dim %16, dims = [] : (tensor<bf16>) -> tensor<2x32x240x320xbf16>
      %109 = stablehlo.compare  GT, %107, %108,  FLOAT : (tensor<2x32x240x320xbf16>, tensor<2x32x240x320xbf16>) -> tensor<2x32x240x320xi1>
      %110 = stablehlo.broadcast_in_dim %109, dims = [0, 1, 2, 3] : (tensor<2x32x240x320xi1>) -> tensor<2x32x240x320xi1>
      %111 = stablehlo.broadcast_in_dim %106, dims = [0, 1, 2, 3] : (tensor<2x32x240x320xbf16>) -> tensor<2x32x240x320xbf16>
      %112 = stablehlo.select %110, %107, %111 : tensor<2x32x240x320xi1>, tensor<2x32x240x320xbf16>
      %113 = stablehlo.tanh %112 : tensor<2x32x240x320xbf16>
      %114 = stablehlo.multiply %104, %113 : tensor<2x32x240x320xbf16>
      %115 = stablehlo.convolution(%114, %arg555) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x32x240x320xbf16>, tensor<64x32x3x3xbf16>) -> tensor<2x64x240x320xbf16>
      %116 = stablehlo.broadcast_in_dim %115, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %117 = stablehlo.broadcast_in_dim %arg683, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x240x320xbf16>
      %118 = stablehlo.subtract %116, %117 : tensor<2x64x240x320xbf16>
      %119 = stablehlo.broadcast_in_dim %118, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %120 = stablehlo.broadcast_in_dim %arg684, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x240x320xbf16>
      %121 = stablehlo.multiply %119, %120 : tensor<2x64x240x320xbf16>
      %122 = stablehlo.broadcast_in_dim %121, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %123 = stablehlo.broadcast_in_dim %arg685, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x240x320xbf16>
      %124 = stablehlo.multiply %122, %123 : tensor<2x64x240x320xbf16>
      %125 = stablehlo.broadcast_in_dim %124, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %126 = stablehlo.broadcast_in_dim %arg686, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x240x320xbf16>
      %127 = stablehlo.add %125, %126 : tensor<2x64x240x320xbf16>
      %128 = stablehlo.exponential %127 : tensor<2x64x240x320xbf16>
      %129 = stablehlo.log_plus_one %128 : tensor<2x64x240x320xbf16>
      %130 = stablehlo.broadcast_in_dim %127, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %131 = stablehlo.compare  GT, %130, %41,  FLOAT : (tensor<2x64x240x320xbf16>, tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xi1>
      %132 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xi1>) -> tensor<2x64x240x320xi1>
      %133 = stablehlo.broadcast_in_dim %129, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %134 = stablehlo.select %132, %130, %133 : tensor<2x64x240x320xi1>, tensor<2x64x240x320xbf16>
      %135 = stablehlo.tanh %134 : tensor<2x64x240x320xbf16>
      %136 = stablehlo.multiply %127, %135 : tensor<2x64x240x320xbf16>
      %137 = stablehlo.add %136, %91 : tensor<2x64x240x320xbf16>
      %138 = stablehlo.convolution(%137, %arg556) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x64x240x320xbf16>, tensor<64x64x1x1xbf16>) -> tensor<2x64x240x320xbf16>
      %139 = stablehlo.broadcast_in_dim %138, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %140 = stablehlo.broadcast_in_dim %arg687, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x240x320xbf16>
      %141 = stablehlo.subtract %139, %140 : tensor<2x64x240x320xbf16>
      %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %143 = stablehlo.broadcast_in_dim %arg688, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x240x320xbf16>
      %144 = stablehlo.multiply %142, %143 : tensor<2x64x240x320xbf16>
      %145 = stablehlo.broadcast_in_dim %144, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %146 = stablehlo.broadcast_in_dim %arg689, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x240x320xbf16>
      %147 = stablehlo.multiply %145, %146 : tensor<2x64x240x320xbf16>
      %148 = stablehlo.broadcast_in_dim %147, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %149 = stablehlo.broadcast_in_dim %arg690, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x240x320xbf16>
      %150 = stablehlo.add %148, %149 : tensor<2x64x240x320xbf16>
      %151 = stablehlo.exponential %150 : tensor<2x64x240x320xbf16>
      %152 = stablehlo.log_plus_one %151 : tensor<2x64x240x320xbf16>
      %153 = stablehlo.broadcast_in_dim %150, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %154 = stablehlo.compare  GT, %153, %41,  FLOAT : (tensor<2x64x240x320xbf16>, tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xi1>
      %155 = stablehlo.broadcast_in_dim %154, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xi1>) -> tensor<2x64x240x320xi1>
      %156 = stablehlo.broadcast_in_dim %152, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %157 = stablehlo.select %155, %153, %156 : tensor<2x64x240x320xi1>, tensor<2x64x240x320xbf16>
      %158 = stablehlo.tanh %157 : tensor<2x64x240x320xbf16>
      %159 = stablehlo.multiply %150, %158 : tensor<2x64x240x320xbf16>
      %160 = stablehlo.concatenate %159, %69, dim = 1 : (tensor<2x64x240x320xbf16>, tensor<2x64x240x320xbf16>) -> tensor<2x128x240x320xbf16>
      %161 = stablehlo.convolution(%160, %arg557) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x128x240x320xbf16>, tensor<64x128x1x1xbf16>) -> tensor<2x64x240x320xbf16>
      %162 = stablehlo.broadcast_in_dim %161, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %163 = stablehlo.broadcast_in_dim %arg691, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x240x320xbf16>
      %164 = stablehlo.subtract %162, %163 : tensor<2x64x240x320xbf16>
      %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %166 = stablehlo.broadcast_in_dim %arg692, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x240x320xbf16>
      %167 = stablehlo.multiply %165, %166 : tensor<2x64x240x320xbf16>
      %168 = stablehlo.broadcast_in_dim %167, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %169 = stablehlo.broadcast_in_dim %arg693, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x240x320xbf16>
      %170 = stablehlo.multiply %168, %169 : tensor<2x64x240x320xbf16>
      %171 = stablehlo.broadcast_in_dim %170, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %172 = stablehlo.broadcast_in_dim %arg694, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x240x320xbf16>
      %173 = stablehlo.add %171, %172 : tensor<2x64x240x320xbf16>
      %174 = stablehlo.exponential %173 : tensor<2x64x240x320xbf16>
      %175 = stablehlo.log_plus_one %174 : tensor<2x64x240x320xbf16>
      %176 = stablehlo.broadcast_in_dim %173, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %177 = stablehlo.compare  GT, %176, %41,  FLOAT : (tensor<2x64x240x320xbf16>, tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xi1>
      %178 = stablehlo.broadcast_in_dim %177, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xi1>) -> tensor<2x64x240x320xi1>
      %179 = stablehlo.broadcast_in_dim %175, dims = [0, 1, 2, 3] : (tensor<2x64x240x320xbf16>) -> tensor<2x64x240x320xbf16>
      %180 = stablehlo.select %178, %176, %179 : tensor<2x64x240x320xi1>, tensor<2x64x240x320xbf16>
      %181 = stablehlo.tanh %180 : tensor<2x64x240x320xbf16>
      %182 = stablehlo.multiply %173, %181 : tensor<2x64x240x320xbf16>
      %183 = stablehlo.convolution(%182, %arg558) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [2, 2], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x64x240x320xbf16>, tensor<128x64x3x3xbf16>) -> tensor<2x128x120x160xbf16>
      %184 = stablehlo.broadcast_in_dim %183, dims = [0, 1, 2, 3] : (tensor<2x128x120x160xbf16>) -> tensor<2x128x120x160xbf16>
      %185 = stablehlo.broadcast_in_dim %arg695, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x120x160xbf16>
      %186 = stablehlo.subtract %184, %185 : tensor<2x128x120x160xbf16>
      %187 = stablehlo.broadcast_in_dim %186, dims = [0, 1, 2, 3] : (tensor<2x128x120x160xbf16>) -> tensor<2x128x120x160xbf16>
      %188 = stablehlo.broadcast_in_dim %arg696, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x120x160xbf16>
      %189 = stablehlo.multiply %187, %188 : tensor<2x128x120x160xbf16>
      %190 = stablehlo.broadcast_in_dim %189, dims = [0, 1, 2, 3] : (tensor<2x128x120x160xbf16>) -> tensor<2x128x120x160xbf16>
      %191 = stablehlo.broadcast_in_dim %arg697, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x120x160xbf16>
      %192 = stablehlo.multiply %190, %191 : tensor<2x128x120x160xbf16>
      %193 = stablehlo.broadcast_in_dim %192, dims = [0, 1, 2, 3] : (tensor<2x128x120x160xbf16>) -> tensor<2x128x120x160xbf16>
      %194 = stablehlo.broadcast_in_dim %arg698, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x120x160xbf16>
      %195 = stablehlo.add %193, %194 : tensor<2x128x120x160xbf16>
      %196 = stablehlo.exponential %195 : tensor<2x128x120x160xbf16>
      %197 = stablehlo.log_plus_one %196 : tensor<2x128x120x160xbf16>
      %198 = stablehlo.broadcast_in_dim %195, dims = [0, 1, 2, 3] : (tensor<2x128x120x160xbf16>) -> tensor<2x128x120x160xbf16>
      %199 = stablehlo.broadcast_in_dim %16, dims = [] : (tensor<bf16>) -> tensor<2x128x120x160xbf16>
      %200 = stablehlo.compare  GT, %198, %199,  FLOAT : (tensor<2x128x120x160xbf16>, tensor<2x128x120x160xbf16>) -> tensor<2x128x120x160xi1>
      %201 = stablehlo.broadcast_in_dim %200, dims = [0, 1, 2, 3] : (tensor<2x128x120x160xi1>) -> tensor<2x128x120x160xi1>
      %202 = stablehlo.broadcast_in_dim %197, dims = [0, 1, 2, 3] : (tensor<2x128x120x160xbf16>) -> tensor<2x128x120x160xbf16>
      %203 = stablehlo.select %201, %198, %202 : tensor<2x128x120x160xi1>, tensor<2x128x120x160xbf16>
      %204 = stablehlo.tanh %203 : tensor<2x128x120x160xbf16>
      %205 = stablehlo.multiply %195, %204 : tensor<2x128x120x160xbf16>
      %206 = stablehlo.convolution(%205, %arg559) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x128x120x160xbf16>, tensor<64x128x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %207 = stablehlo.broadcast_in_dim %206, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %208 = stablehlo.broadcast_in_dim %arg699, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %209 = stablehlo.subtract %207, %208 : tensor<2x64x120x160xbf16>
      %210 = stablehlo.broadcast_in_dim %209, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %211 = stablehlo.broadcast_in_dim %arg700, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %212 = stablehlo.multiply %210, %211 : tensor<2x64x120x160xbf16>
      %213 = stablehlo.broadcast_in_dim %212, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %214 = stablehlo.broadcast_in_dim %arg701, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %215 = stablehlo.multiply %213, %214 : tensor<2x64x120x160xbf16>
      %216 = stablehlo.broadcast_in_dim %215, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %217 = stablehlo.broadcast_in_dim %arg702, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %218 = stablehlo.add %216, %217 : tensor<2x64x120x160xbf16>
      %219 = stablehlo.exponential %218 : tensor<2x64x120x160xbf16>
      %220 = stablehlo.log_plus_one %219 : tensor<2x64x120x160xbf16>
      %221 = stablehlo.broadcast_in_dim %218, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %222 = stablehlo.broadcast_in_dim %16, dims = [] : (tensor<bf16>) -> tensor<2x64x120x160xbf16>
      %223 = stablehlo.compare  GT, %221, %222,  FLOAT : (tensor<2x64x120x160xbf16>, tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xi1>
      %224 = stablehlo.broadcast_in_dim %223, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xi1>) -> tensor<2x64x120x160xi1>
      %225 = stablehlo.broadcast_in_dim %220, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %226 = stablehlo.select %224, %221, %225 : tensor<2x64x120x160xi1>, tensor<2x64x120x160xbf16>
      %227 = stablehlo.tanh %226 : tensor<2x64x120x160xbf16>
      %228 = stablehlo.multiply %218, %227 : tensor<2x64x120x160xbf16>
      %229 = stablehlo.convolution(%205, %arg560) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x128x120x160xbf16>, tensor<64x128x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %230 = stablehlo.broadcast_in_dim %229, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %231 = stablehlo.broadcast_in_dim %arg703, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %232 = stablehlo.subtract %230, %231 : tensor<2x64x120x160xbf16>
      %233 = stablehlo.broadcast_in_dim %232, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %234 = stablehlo.broadcast_in_dim %arg704, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %235 = stablehlo.multiply %233, %234 : tensor<2x64x120x160xbf16>
      %236 = stablehlo.broadcast_in_dim %235, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %237 = stablehlo.broadcast_in_dim %arg705, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %238 = stablehlo.multiply %236, %237 : tensor<2x64x120x160xbf16>
      %239 = stablehlo.broadcast_in_dim %238, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %240 = stablehlo.broadcast_in_dim %arg706, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %241 = stablehlo.add %239, %240 : tensor<2x64x120x160xbf16>
      %242 = stablehlo.exponential %241 : tensor<2x64x120x160xbf16>
      %243 = stablehlo.log_plus_one %242 : tensor<2x64x120x160xbf16>
      %244 = stablehlo.broadcast_in_dim %241, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %245 = stablehlo.compare  GT, %244, %222,  FLOAT : (tensor<2x64x120x160xbf16>, tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xi1>
      %246 = stablehlo.broadcast_in_dim %245, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xi1>) -> tensor<2x64x120x160xi1>
      %247 = stablehlo.broadcast_in_dim %243, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %248 = stablehlo.select %246, %244, %247 : tensor<2x64x120x160xi1>, tensor<2x64x120x160xbf16>
      %249 = stablehlo.tanh %248 : tensor<2x64x120x160xbf16>
      %250 = stablehlo.multiply %241, %249 : tensor<2x64x120x160xbf16>
      %251 = stablehlo.convolution(%250, %arg561) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x64x120x160xbf16>, tensor<64x64x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %252 = stablehlo.broadcast_in_dim %251, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %253 = stablehlo.broadcast_in_dim %arg707, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %254 = stablehlo.subtract %252, %253 : tensor<2x64x120x160xbf16>
      %255 = stablehlo.broadcast_in_dim %254, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %256 = stablehlo.broadcast_in_dim %arg708, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %257 = stablehlo.multiply %255, %256 : tensor<2x64x120x160xbf16>
      %258 = stablehlo.broadcast_in_dim %257, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %259 = stablehlo.broadcast_in_dim %arg709, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %260 = stablehlo.multiply %258, %259 : tensor<2x64x120x160xbf16>
      %261 = stablehlo.broadcast_in_dim %260, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %262 = stablehlo.broadcast_in_dim %arg710, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %263 = stablehlo.add %261, %262 : tensor<2x64x120x160xbf16>
      %264 = stablehlo.exponential %263 : tensor<2x64x120x160xbf16>
      %265 = stablehlo.log_plus_one %264 : tensor<2x64x120x160xbf16>
      %266 = stablehlo.broadcast_in_dim %263, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %267 = stablehlo.compare  GT, %266, %222,  FLOAT : (tensor<2x64x120x160xbf16>, tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xi1>
      %268 = stablehlo.broadcast_in_dim %267, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xi1>) -> tensor<2x64x120x160xi1>
      %269 = stablehlo.broadcast_in_dim %265, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %270 = stablehlo.select %268, %266, %269 : tensor<2x64x120x160xi1>, tensor<2x64x120x160xbf16>
      %271 = stablehlo.tanh %270 : tensor<2x64x120x160xbf16>
      %272 = stablehlo.multiply %263, %271 : tensor<2x64x120x160xbf16>
      %273 = stablehlo.convolution(%272, %arg562) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x64x120x160xbf16>, tensor<64x64x3x3xbf16>) -> tensor<2x64x120x160xbf16>
      %274 = stablehlo.broadcast_in_dim %273, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %275 = stablehlo.broadcast_in_dim %arg711, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %276 = stablehlo.subtract %274, %275 : tensor<2x64x120x160xbf16>
      %277 = stablehlo.broadcast_in_dim %276, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %278 = stablehlo.broadcast_in_dim %arg712, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %279 = stablehlo.multiply %277, %278 : tensor<2x64x120x160xbf16>
      %280 = stablehlo.broadcast_in_dim %279, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %281 = stablehlo.broadcast_in_dim %arg713, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %282 = stablehlo.multiply %280, %281 : tensor<2x64x120x160xbf16>
      %283 = stablehlo.broadcast_in_dim %282, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %284 = stablehlo.broadcast_in_dim %arg714, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %285 = stablehlo.add %283, %284 : tensor<2x64x120x160xbf16>
      %286 = stablehlo.exponential %285 : tensor<2x64x120x160xbf16>
      %287 = stablehlo.log_plus_one %286 : tensor<2x64x120x160xbf16>
      %288 = stablehlo.broadcast_in_dim %285, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %289 = stablehlo.compare  GT, %288, %222,  FLOAT : (tensor<2x64x120x160xbf16>, tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xi1>
      %290 = stablehlo.broadcast_in_dim %289, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xi1>) -> tensor<2x64x120x160xi1>
      %291 = stablehlo.broadcast_in_dim %287, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %292 = stablehlo.select %290, %288, %291 : tensor<2x64x120x160xi1>, tensor<2x64x120x160xbf16>
      %293 = stablehlo.tanh %292 : tensor<2x64x120x160xbf16>
      %294 = stablehlo.multiply %285, %293 : tensor<2x64x120x160xbf16>
      %295 = stablehlo.add %250, %294 : tensor<2x64x120x160xbf16>
      %296 = stablehlo.convolution(%295, %arg563) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x64x120x160xbf16>, tensor<64x64x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %297 = stablehlo.broadcast_in_dim %296, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %298 = stablehlo.broadcast_in_dim %arg715, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %299 = stablehlo.subtract %297, %298 : tensor<2x64x120x160xbf16>
      %300 = stablehlo.broadcast_in_dim %299, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %301 = stablehlo.broadcast_in_dim %arg716, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %302 = stablehlo.multiply %300, %301 : tensor<2x64x120x160xbf16>
      %303 = stablehlo.broadcast_in_dim %302, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %304 = stablehlo.broadcast_in_dim %arg717, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %305 = stablehlo.multiply %303, %304 : tensor<2x64x120x160xbf16>
      %306 = stablehlo.broadcast_in_dim %305, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %307 = stablehlo.broadcast_in_dim %arg718, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %308 = stablehlo.add %306, %307 : tensor<2x64x120x160xbf16>
      %309 = stablehlo.exponential %308 : tensor<2x64x120x160xbf16>
      %310 = stablehlo.log_plus_one %309 : tensor<2x64x120x160xbf16>
      %311 = stablehlo.broadcast_in_dim %308, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %312 = stablehlo.compare  GT, %311, %222,  FLOAT : (tensor<2x64x120x160xbf16>, tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xi1>
      %313 = stablehlo.broadcast_in_dim %312, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xi1>) -> tensor<2x64x120x160xi1>
      %314 = stablehlo.broadcast_in_dim %310, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %315 = stablehlo.select %313, %311, %314 : tensor<2x64x120x160xi1>, tensor<2x64x120x160xbf16>
      %316 = stablehlo.tanh %315 : tensor<2x64x120x160xbf16>
      %317 = stablehlo.multiply %308, %316 : tensor<2x64x120x160xbf16>
      %318 = stablehlo.convolution(%317, %arg564) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x64x120x160xbf16>, tensor<64x64x3x3xbf16>) -> tensor<2x64x120x160xbf16>
      %319 = stablehlo.broadcast_in_dim %318, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %320 = stablehlo.broadcast_in_dim %arg719, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %321 = stablehlo.subtract %319, %320 : tensor<2x64x120x160xbf16>
      %322 = stablehlo.broadcast_in_dim %321, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %323 = stablehlo.broadcast_in_dim %arg720, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %324 = stablehlo.multiply %322, %323 : tensor<2x64x120x160xbf16>
      %325 = stablehlo.broadcast_in_dim %324, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %326 = stablehlo.broadcast_in_dim %arg721, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %327 = stablehlo.multiply %325, %326 : tensor<2x64x120x160xbf16>
      %328 = stablehlo.broadcast_in_dim %327, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %329 = stablehlo.broadcast_in_dim %arg722, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %330 = stablehlo.add %328, %329 : tensor<2x64x120x160xbf16>
      %331 = stablehlo.exponential %330 : tensor<2x64x120x160xbf16>
      %332 = stablehlo.log_plus_one %331 : tensor<2x64x120x160xbf16>
      %333 = stablehlo.broadcast_in_dim %330, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %334 = stablehlo.compare  GT, %333, %222,  FLOAT : (tensor<2x64x120x160xbf16>, tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xi1>
      %335 = stablehlo.broadcast_in_dim %334, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xi1>) -> tensor<2x64x120x160xi1>
      %336 = stablehlo.broadcast_in_dim %332, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %337 = stablehlo.select %335, %333, %336 : tensor<2x64x120x160xi1>, tensor<2x64x120x160xbf16>
      %338 = stablehlo.tanh %337 : tensor<2x64x120x160xbf16>
      %339 = stablehlo.multiply %330, %338 : tensor<2x64x120x160xbf16>
      %340 = stablehlo.add %295, %339 : tensor<2x64x120x160xbf16>
      %341 = stablehlo.convolution(%340, %arg565) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x64x120x160xbf16>, tensor<64x64x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %342 = stablehlo.broadcast_in_dim %341, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %343 = stablehlo.broadcast_in_dim %arg723, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %344 = stablehlo.subtract %342, %343 : tensor<2x64x120x160xbf16>
      %345 = stablehlo.broadcast_in_dim %344, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %346 = stablehlo.broadcast_in_dim %arg724, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %347 = stablehlo.multiply %345, %346 : tensor<2x64x120x160xbf16>
      %348 = stablehlo.broadcast_in_dim %347, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %349 = stablehlo.broadcast_in_dim %arg725, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %350 = stablehlo.multiply %348, %349 : tensor<2x64x120x160xbf16>
      %351 = stablehlo.broadcast_in_dim %350, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %352 = stablehlo.broadcast_in_dim %arg726, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<2x64x120x160xbf16>
      %353 = stablehlo.add %351, %352 : tensor<2x64x120x160xbf16>
      %354 = stablehlo.exponential %353 : tensor<2x64x120x160xbf16>
      %355 = stablehlo.log_plus_one %354 : tensor<2x64x120x160xbf16>
      %356 = stablehlo.broadcast_in_dim %353, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %357 = stablehlo.compare  GT, %356, %222,  FLOAT : (tensor<2x64x120x160xbf16>, tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xi1>
      %358 = stablehlo.broadcast_in_dim %357, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xi1>) -> tensor<2x64x120x160xi1>
      %359 = stablehlo.broadcast_in_dim %355, dims = [0, 1, 2, 3] : (tensor<2x64x120x160xbf16>) -> tensor<2x64x120x160xbf16>
      %360 = stablehlo.select %358, %356, %359 : tensor<2x64x120x160xi1>, tensor<2x64x120x160xbf16>
      %361 = stablehlo.tanh %360 : tensor<2x64x120x160xbf16>
      %362 = stablehlo.multiply %353, %361 : tensor<2x64x120x160xbf16>
      %363 = stablehlo.concatenate %362, %228, dim = 1 : (tensor<2x64x120x160xbf16>, tensor<2x64x120x160xbf16>) -> tensor<2x128x120x160xbf16>
      %364 = stablehlo.convolution(%363, %arg566) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x128x120x160xbf16>, tensor<128x128x1x1xbf16>) -> tensor<2x128x120x160xbf16>
      %365 = stablehlo.broadcast_in_dim %364, dims = [0, 1, 2, 3] : (tensor<2x128x120x160xbf16>) -> tensor<2x128x120x160xbf16>
      %366 = stablehlo.broadcast_in_dim %arg727, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x120x160xbf16>
      %367 = stablehlo.subtract %365, %366 : tensor<2x128x120x160xbf16>
      %368 = stablehlo.broadcast_in_dim %367, dims = [0, 1, 2, 3] : (tensor<2x128x120x160xbf16>) -> tensor<2x128x120x160xbf16>
      %369 = stablehlo.broadcast_in_dim %arg728, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x120x160xbf16>
      %370 = stablehlo.multiply %368, %369 : tensor<2x128x120x160xbf16>
      %371 = stablehlo.broadcast_in_dim %370, dims = [0, 1, 2, 3] : (tensor<2x128x120x160xbf16>) -> tensor<2x128x120x160xbf16>
      %372 = stablehlo.broadcast_in_dim %arg729, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x120x160xbf16>
      %373 = stablehlo.multiply %371, %372 : tensor<2x128x120x160xbf16>
      %374 = stablehlo.broadcast_in_dim %373, dims = [0, 1, 2, 3] : (tensor<2x128x120x160xbf16>) -> tensor<2x128x120x160xbf16>
      %375 = stablehlo.broadcast_in_dim %arg730, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x120x160xbf16>
      %376 = stablehlo.add %374, %375 : tensor<2x128x120x160xbf16>
      %377 = stablehlo.exponential %376 : tensor<2x128x120x160xbf16>
      %378 = stablehlo.log_plus_one %377 : tensor<2x128x120x160xbf16>
      %379 = stablehlo.broadcast_in_dim %376, dims = [0, 1, 2, 3] : (tensor<2x128x120x160xbf16>) -> tensor<2x128x120x160xbf16>
      %380 = stablehlo.compare  GT, %379, %199,  FLOAT : (tensor<2x128x120x160xbf16>, tensor<2x128x120x160xbf16>) -> tensor<2x128x120x160xi1>
      %381 = stablehlo.broadcast_in_dim %380, dims = [0, 1, 2, 3] : (tensor<2x128x120x160xi1>) -> tensor<2x128x120x160xi1>
      %382 = stablehlo.broadcast_in_dim %378, dims = [0, 1, 2, 3] : (tensor<2x128x120x160xbf16>) -> tensor<2x128x120x160xbf16>
      %383 = stablehlo.select %381, %379, %382 : tensor<2x128x120x160xi1>, tensor<2x128x120x160xbf16>
      %384 = stablehlo.tanh %383 : tensor<2x128x120x160xbf16>
      %385 = stablehlo.multiply %376, %384 : tensor<2x128x120x160xbf16>
      %386 = stablehlo.convolution(%385, %arg567) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [2, 2], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x128x120x160xbf16>, tensor<256x128x3x3xbf16>) -> tensor<2x256x60x80xbf16>
      %387 = stablehlo.broadcast_in_dim %386, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %388 = stablehlo.broadcast_in_dim %arg731, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x60x80xbf16>
      %389 = stablehlo.subtract %387, %388 : tensor<2x256x60x80xbf16>
      %390 = stablehlo.broadcast_in_dim %389, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %391 = stablehlo.broadcast_in_dim %arg732, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x60x80xbf16>
      %392 = stablehlo.multiply %390, %391 : tensor<2x256x60x80xbf16>
      %393 = stablehlo.broadcast_in_dim %392, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %394 = stablehlo.broadcast_in_dim %arg733, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x60x80xbf16>
      %395 = stablehlo.multiply %393, %394 : tensor<2x256x60x80xbf16>
      %396 = stablehlo.broadcast_in_dim %395, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %397 = stablehlo.broadcast_in_dim %arg734, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x60x80xbf16>
      %398 = stablehlo.add %396, %397 : tensor<2x256x60x80xbf16>
      %399 = stablehlo.exponential %398 : tensor<2x256x60x80xbf16>
      %400 = stablehlo.log_plus_one %399 : tensor<2x256x60x80xbf16>
      %401 = stablehlo.broadcast_in_dim %398, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %402 = stablehlo.broadcast_in_dim %16, dims = [] : (tensor<bf16>) -> tensor<2x256x60x80xbf16>
      %403 = stablehlo.compare  GT, %401, %402,  FLOAT : (tensor<2x256x60x80xbf16>, tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xi1>
      %404 = stablehlo.broadcast_in_dim %403, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xi1>) -> tensor<2x256x60x80xi1>
      %405 = stablehlo.broadcast_in_dim %400, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %406 = stablehlo.select %404, %401, %405 : tensor<2x256x60x80xi1>, tensor<2x256x60x80xbf16>
      %407 = stablehlo.tanh %406 : tensor<2x256x60x80xbf16>
      %408 = stablehlo.multiply %398, %407 : tensor<2x256x60x80xbf16>
      %409 = stablehlo.convolution(%408, %arg568) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x60x80xbf16>, tensor<128x256x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %410 = stablehlo.broadcast_in_dim %409, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %411 = stablehlo.broadcast_in_dim %arg735, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %412 = stablehlo.subtract %410, %411 : tensor<2x128x60x80xbf16>
      %413 = stablehlo.broadcast_in_dim %412, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %414 = stablehlo.broadcast_in_dim %arg736, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %415 = stablehlo.multiply %413, %414 : tensor<2x128x60x80xbf16>
      %416 = stablehlo.broadcast_in_dim %415, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %417 = stablehlo.broadcast_in_dim %arg737, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %418 = stablehlo.multiply %416, %417 : tensor<2x128x60x80xbf16>
      %419 = stablehlo.broadcast_in_dim %418, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %420 = stablehlo.broadcast_in_dim %arg738, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %421 = stablehlo.add %419, %420 : tensor<2x128x60x80xbf16>
      %422 = stablehlo.exponential %421 : tensor<2x128x60x80xbf16>
      %423 = stablehlo.log_plus_one %422 : tensor<2x128x60x80xbf16>
      %424 = stablehlo.broadcast_in_dim %421, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %425 = stablehlo.broadcast_in_dim %16, dims = [] : (tensor<bf16>) -> tensor<2x128x60x80xbf16>
      %426 = stablehlo.compare  GT, %424, %425,  FLOAT : (tensor<2x128x60x80xbf16>, tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xi1>
      %427 = stablehlo.broadcast_in_dim %426, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xi1>) -> tensor<2x128x60x80xi1>
      %428 = stablehlo.broadcast_in_dim %423, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %429 = stablehlo.select %427, %424, %428 : tensor<2x128x60x80xi1>, tensor<2x128x60x80xbf16>
      %430 = stablehlo.tanh %429 : tensor<2x128x60x80xbf16>
      %431 = stablehlo.multiply %421, %430 : tensor<2x128x60x80xbf16>
      %432 = stablehlo.convolution(%408, %arg569) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x60x80xbf16>, tensor<128x256x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %433 = stablehlo.broadcast_in_dim %432, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %434 = stablehlo.broadcast_in_dim %arg739, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %435 = stablehlo.subtract %433, %434 : tensor<2x128x60x80xbf16>
      %436 = stablehlo.broadcast_in_dim %435, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %437 = stablehlo.broadcast_in_dim %arg740, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %438 = stablehlo.multiply %436, %437 : tensor<2x128x60x80xbf16>
      %439 = stablehlo.broadcast_in_dim %438, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %440 = stablehlo.broadcast_in_dim %arg741, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %441 = stablehlo.multiply %439, %440 : tensor<2x128x60x80xbf16>
      %442 = stablehlo.broadcast_in_dim %441, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %443 = stablehlo.broadcast_in_dim %arg742, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %444 = stablehlo.add %442, %443 : tensor<2x128x60x80xbf16>
      %445 = stablehlo.exponential %444 : tensor<2x128x60x80xbf16>
      %446 = stablehlo.log_plus_one %445 : tensor<2x128x60x80xbf16>
      %447 = stablehlo.broadcast_in_dim %444, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %448 = stablehlo.compare  GT, %447, %425,  FLOAT : (tensor<2x128x60x80xbf16>, tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xi1>
      %449 = stablehlo.broadcast_in_dim %448, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xi1>) -> tensor<2x128x60x80xi1>
      %450 = stablehlo.broadcast_in_dim %446, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %451 = stablehlo.select %449, %447, %450 : tensor<2x128x60x80xi1>, tensor<2x128x60x80xbf16>
      %452 = stablehlo.tanh %451 : tensor<2x128x60x80xbf16>
      %453 = stablehlo.multiply %444, %452 : tensor<2x128x60x80xbf16>
      %454 = stablehlo.convolution(%453, %arg570) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x128x60x80xbf16>, tensor<128x128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %455 = stablehlo.broadcast_in_dim %454, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %456 = stablehlo.broadcast_in_dim %arg743, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %457 = stablehlo.subtract %455, %456 : tensor<2x128x60x80xbf16>
      %458 = stablehlo.broadcast_in_dim %457, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %459 = stablehlo.broadcast_in_dim %arg744, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %460 = stablehlo.multiply %458, %459 : tensor<2x128x60x80xbf16>
      %461 = stablehlo.broadcast_in_dim %460, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %462 = stablehlo.broadcast_in_dim %arg745, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %463 = stablehlo.multiply %461, %462 : tensor<2x128x60x80xbf16>
      %464 = stablehlo.broadcast_in_dim %463, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %465 = stablehlo.broadcast_in_dim %arg746, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %466 = stablehlo.add %464, %465 : tensor<2x128x60x80xbf16>
      %467 = stablehlo.exponential %466 : tensor<2x128x60x80xbf16>
      %468 = stablehlo.log_plus_one %467 : tensor<2x128x60x80xbf16>
      %469 = stablehlo.broadcast_in_dim %466, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %470 = stablehlo.compare  GT, %469, %425,  FLOAT : (tensor<2x128x60x80xbf16>, tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xi1>
      %471 = stablehlo.broadcast_in_dim %470, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xi1>) -> tensor<2x128x60x80xi1>
      %472 = stablehlo.broadcast_in_dim %468, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %473 = stablehlo.select %471, %469, %472 : tensor<2x128x60x80xi1>, tensor<2x128x60x80xbf16>
      %474 = stablehlo.tanh %473 : tensor<2x128x60x80xbf16>
      %475 = stablehlo.multiply %466, %474 : tensor<2x128x60x80xbf16>
      %476 = stablehlo.convolution(%475, %arg571) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x128x60x80xbf16>, tensor<128x128x3x3xbf16>) -> tensor<2x128x60x80xbf16>
      %477 = stablehlo.broadcast_in_dim %476, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %478 = stablehlo.broadcast_in_dim %arg747, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %479 = stablehlo.subtract %477, %478 : tensor<2x128x60x80xbf16>
      %480 = stablehlo.broadcast_in_dim %479, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %481 = stablehlo.broadcast_in_dim %arg748, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %482 = stablehlo.multiply %480, %481 : tensor<2x128x60x80xbf16>
      %483 = stablehlo.broadcast_in_dim %482, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %484 = stablehlo.broadcast_in_dim %arg749, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %485 = stablehlo.multiply %483, %484 : tensor<2x128x60x80xbf16>
      %486 = stablehlo.broadcast_in_dim %485, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %487 = stablehlo.broadcast_in_dim %arg750, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %488 = stablehlo.add %486, %487 : tensor<2x128x60x80xbf16>
      %489 = stablehlo.exponential %488 : tensor<2x128x60x80xbf16>
      %490 = stablehlo.log_plus_one %489 : tensor<2x128x60x80xbf16>
      %491 = stablehlo.broadcast_in_dim %488, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %492 = stablehlo.compare  GT, %491, %425,  FLOAT : (tensor<2x128x60x80xbf16>, tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xi1>
      %493 = stablehlo.broadcast_in_dim %492, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xi1>) -> tensor<2x128x60x80xi1>
      %494 = stablehlo.broadcast_in_dim %490, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %495 = stablehlo.select %493, %491, %494 : tensor<2x128x60x80xi1>, tensor<2x128x60x80xbf16>
      %496 = stablehlo.tanh %495 : tensor<2x128x60x80xbf16>
      %497 = stablehlo.multiply %488, %496 : tensor<2x128x60x80xbf16>
      %498 = stablehlo.add %453, %497 : tensor<2x128x60x80xbf16>
      %499 = stablehlo.convolution(%498, %arg572) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x128x60x80xbf16>, tensor<128x128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %500 = stablehlo.broadcast_in_dim %499, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %501 = stablehlo.broadcast_in_dim %arg751, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %502 = stablehlo.subtract %500, %501 : tensor<2x128x60x80xbf16>
      %503 = stablehlo.broadcast_in_dim %502, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %504 = stablehlo.broadcast_in_dim %arg752, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %505 = stablehlo.multiply %503, %504 : tensor<2x128x60x80xbf16>
      %506 = stablehlo.broadcast_in_dim %505, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %507 = stablehlo.broadcast_in_dim %arg753, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %508 = stablehlo.multiply %506, %507 : tensor<2x128x60x80xbf16>
      %509 = stablehlo.broadcast_in_dim %508, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %510 = stablehlo.broadcast_in_dim %arg754, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %511 = stablehlo.add %509, %510 : tensor<2x128x60x80xbf16>
      %512 = stablehlo.exponential %511 : tensor<2x128x60x80xbf16>
      %513 = stablehlo.log_plus_one %512 : tensor<2x128x60x80xbf16>
      %514 = stablehlo.broadcast_in_dim %511, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %515 = stablehlo.compare  GT, %514, %425,  FLOAT : (tensor<2x128x60x80xbf16>, tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xi1>
      %516 = stablehlo.broadcast_in_dim %515, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xi1>) -> tensor<2x128x60x80xi1>
      %517 = stablehlo.broadcast_in_dim %513, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %518 = stablehlo.select %516, %514, %517 : tensor<2x128x60x80xi1>, tensor<2x128x60x80xbf16>
      %519 = stablehlo.tanh %518 : tensor<2x128x60x80xbf16>
      %520 = stablehlo.multiply %511, %519 : tensor<2x128x60x80xbf16>
      %521 = stablehlo.convolution(%520, %arg573) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x128x60x80xbf16>, tensor<128x128x3x3xbf16>) -> tensor<2x128x60x80xbf16>
      %522 = stablehlo.broadcast_in_dim %521, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %523 = stablehlo.broadcast_in_dim %arg755, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %524 = stablehlo.subtract %522, %523 : tensor<2x128x60x80xbf16>
      %525 = stablehlo.broadcast_in_dim %524, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %526 = stablehlo.broadcast_in_dim %arg756, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %527 = stablehlo.multiply %525, %526 : tensor<2x128x60x80xbf16>
      %528 = stablehlo.broadcast_in_dim %527, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %529 = stablehlo.broadcast_in_dim %arg757, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %530 = stablehlo.multiply %528, %529 : tensor<2x128x60x80xbf16>
      %531 = stablehlo.broadcast_in_dim %530, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %532 = stablehlo.broadcast_in_dim %arg758, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %533 = stablehlo.add %531, %532 : tensor<2x128x60x80xbf16>
      %534 = stablehlo.exponential %533 : tensor<2x128x60x80xbf16>
      %535 = stablehlo.log_plus_one %534 : tensor<2x128x60x80xbf16>
      %536 = stablehlo.broadcast_in_dim %533, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %537 = stablehlo.compare  GT, %536, %425,  FLOAT : (tensor<2x128x60x80xbf16>, tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xi1>
      %538 = stablehlo.broadcast_in_dim %537, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xi1>) -> tensor<2x128x60x80xi1>
      %539 = stablehlo.broadcast_in_dim %535, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %540 = stablehlo.select %538, %536, %539 : tensor<2x128x60x80xi1>, tensor<2x128x60x80xbf16>
      %541 = stablehlo.tanh %540 : tensor<2x128x60x80xbf16>
      %542 = stablehlo.multiply %533, %541 : tensor<2x128x60x80xbf16>
      %543 = stablehlo.add %498, %542 : tensor<2x128x60x80xbf16>
      %544 = stablehlo.convolution(%543, %arg574) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x128x60x80xbf16>, tensor<128x128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %545 = stablehlo.broadcast_in_dim %544, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %546 = stablehlo.broadcast_in_dim %arg759, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %547 = stablehlo.subtract %545, %546 : tensor<2x128x60x80xbf16>
      %548 = stablehlo.broadcast_in_dim %547, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %549 = stablehlo.broadcast_in_dim %arg760, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %550 = stablehlo.multiply %548, %549 : tensor<2x128x60x80xbf16>
      %551 = stablehlo.broadcast_in_dim %550, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %552 = stablehlo.broadcast_in_dim %arg761, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %553 = stablehlo.multiply %551, %552 : tensor<2x128x60x80xbf16>
      %554 = stablehlo.broadcast_in_dim %553, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %555 = stablehlo.broadcast_in_dim %arg762, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %556 = stablehlo.add %554, %555 : tensor<2x128x60x80xbf16>
      %557 = stablehlo.exponential %556 : tensor<2x128x60x80xbf16>
      %558 = stablehlo.log_plus_one %557 : tensor<2x128x60x80xbf16>
      %559 = stablehlo.broadcast_in_dim %556, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %560 = stablehlo.compare  GT, %559, %425,  FLOAT : (tensor<2x128x60x80xbf16>, tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xi1>
      %561 = stablehlo.broadcast_in_dim %560, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xi1>) -> tensor<2x128x60x80xi1>
      %562 = stablehlo.broadcast_in_dim %558, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %563 = stablehlo.select %561, %559, %562 : tensor<2x128x60x80xi1>, tensor<2x128x60x80xbf16>
      %564 = stablehlo.tanh %563 : tensor<2x128x60x80xbf16>
      %565 = stablehlo.multiply %556, %564 : tensor<2x128x60x80xbf16>
      %566 = stablehlo.convolution(%565, %arg575) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x128x60x80xbf16>, tensor<128x128x3x3xbf16>) -> tensor<2x128x60x80xbf16>
      %567 = stablehlo.broadcast_in_dim %566, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %568 = stablehlo.broadcast_in_dim %arg763, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %569 = stablehlo.subtract %567, %568 : tensor<2x128x60x80xbf16>
      %570 = stablehlo.broadcast_in_dim %569, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %571 = stablehlo.broadcast_in_dim %arg764, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %572 = stablehlo.multiply %570, %571 : tensor<2x128x60x80xbf16>
      %573 = stablehlo.broadcast_in_dim %572, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %574 = stablehlo.broadcast_in_dim %arg765, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %575 = stablehlo.multiply %573, %574 : tensor<2x128x60x80xbf16>
      %576 = stablehlo.broadcast_in_dim %575, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %577 = stablehlo.broadcast_in_dim %arg766, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %578 = stablehlo.add %576, %577 : tensor<2x128x60x80xbf16>
      %579 = stablehlo.exponential %578 : tensor<2x128x60x80xbf16>
      %580 = stablehlo.log_plus_one %579 : tensor<2x128x60x80xbf16>
      %581 = stablehlo.broadcast_in_dim %578, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %582 = stablehlo.compare  GT, %581, %425,  FLOAT : (tensor<2x128x60x80xbf16>, tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xi1>
      %583 = stablehlo.broadcast_in_dim %582, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xi1>) -> tensor<2x128x60x80xi1>
      %584 = stablehlo.broadcast_in_dim %580, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %585 = stablehlo.select %583, %581, %584 : tensor<2x128x60x80xi1>, tensor<2x128x60x80xbf16>
      %586 = stablehlo.tanh %585 : tensor<2x128x60x80xbf16>
      %587 = stablehlo.multiply %578, %586 : tensor<2x128x60x80xbf16>
      %588 = stablehlo.add %543, %587 : tensor<2x128x60x80xbf16>
      %589 = stablehlo.convolution(%588, %arg576) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x128x60x80xbf16>, tensor<128x128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %590 = stablehlo.broadcast_in_dim %589, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %591 = stablehlo.broadcast_in_dim %arg767, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %592 = stablehlo.subtract %590, %591 : tensor<2x128x60x80xbf16>
      %593 = stablehlo.broadcast_in_dim %592, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %594 = stablehlo.broadcast_in_dim %arg768, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %595 = stablehlo.multiply %593, %594 : tensor<2x128x60x80xbf16>
      %596 = stablehlo.broadcast_in_dim %595, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %597 = stablehlo.broadcast_in_dim %arg769, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %598 = stablehlo.multiply %596, %597 : tensor<2x128x60x80xbf16>
      %599 = stablehlo.broadcast_in_dim %598, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %600 = stablehlo.broadcast_in_dim %arg770, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %601 = stablehlo.add %599, %600 : tensor<2x128x60x80xbf16>
      %602 = stablehlo.exponential %601 : tensor<2x128x60x80xbf16>
      %603 = stablehlo.log_plus_one %602 : tensor<2x128x60x80xbf16>
      %604 = stablehlo.broadcast_in_dim %601, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %605 = stablehlo.compare  GT, %604, %425,  FLOAT : (tensor<2x128x60x80xbf16>, tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xi1>
      %606 = stablehlo.broadcast_in_dim %605, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xi1>) -> tensor<2x128x60x80xi1>
      %607 = stablehlo.broadcast_in_dim %603, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %608 = stablehlo.select %606, %604, %607 : tensor<2x128x60x80xi1>, tensor<2x128x60x80xbf16>
      %609 = stablehlo.tanh %608 : tensor<2x128x60x80xbf16>
      %610 = stablehlo.multiply %601, %609 : tensor<2x128x60x80xbf16>
      %611 = stablehlo.convolution(%610, %arg577) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x128x60x80xbf16>, tensor<128x128x3x3xbf16>) -> tensor<2x128x60x80xbf16>
      %612 = stablehlo.broadcast_in_dim %611, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %613 = stablehlo.broadcast_in_dim %arg771, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %614 = stablehlo.subtract %612, %613 : tensor<2x128x60x80xbf16>
      %615 = stablehlo.broadcast_in_dim %614, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %616 = stablehlo.broadcast_in_dim %arg772, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %617 = stablehlo.multiply %615, %616 : tensor<2x128x60x80xbf16>
      %618 = stablehlo.broadcast_in_dim %617, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %619 = stablehlo.broadcast_in_dim %arg773, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %620 = stablehlo.multiply %618, %619 : tensor<2x128x60x80xbf16>
      %621 = stablehlo.broadcast_in_dim %620, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %622 = stablehlo.broadcast_in_dim %arg774, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %623 = stablehlo.add %621, %622 : tensor<2x128x60x80xbf16>
      %624 = stablehlo.exponential %623 : tensor<2x128x60x80xbf16>
      %625 = stablehlo.log_plus_one %624 : tensor<2x128x60x80xbf16>
      %626 = stablehlo.broadcast_in_dim %623, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %627 = stablehlo.compare  GT, %626, %425,  FLOAT : (tensor<2x128x60x80xbf16>, tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xi1>
      %628 = stablehlo.broadcast_in_dim %627, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xi1>) -> tensor<2x128x60x80xi1>
      %629 = stablehlo.broadcast_in_dim %625, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %630 = stablehlo.select %628, %626, %629 : tensor<2x128x60x80xi1>, tensor<2x128x60x80xbf16>
      %631 = stablehlo.tanh %630 : tensor<2x128x60x80xbf16>
      %632 = stablehlo.multiply %623, %631 : tensor<2x128x60x80xbf16>
      %633 = stablehlo.add %588, %632 : tensor<2x128x60x80xbf16>
      %634 = stablehlo.convolution(%633, %arg578) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x128x60x80xbf16>, tensor<128x128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %635 = stablehlo.broadcast_in_dim %634, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %636 = stablehlo.broadcast_in_dim %arg775, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %637 = stablehlo.subtract %635, %636 : tensor<2x128x60x80xbf16>
      %638 = stablehlo.broadcast_in_dim %637, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %639 = stablehlo.broadcast_in_dim %arg776, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %640 = stablehlo.multiply %638, %639 : tensor<2x128x60x80xbf16>
      %641 = stablehlo.broadcast_in_dim %640, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %642 = stablehlo.broadcast_in_dim %arg777, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %643 = stablehlo.multiply %641, %642 : tensor<2x128x60x80xbf16>
      %644 = stablehlo.broadcast_in_dim %643, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %645 = stablehlo.broadcast_in_dim %arg778, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %646 = stablehlo.add %644, %645 : tensor<2x128x60x80xbf16>
      %647 = stablehlo.exponential %646 : tensor<2x128x60x80xbf16>
      %648 = stablehlo.log_plus_one %647 : tensor<2x128x60x80xbf16>
      %649 = stablehlo.broadcast_in_dim %646, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %650 = stablehlo.compare  GT, %649, %425,  FLOAT : (tensor<2x128x60x80xbf16>, tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xi1>
      %651 = stablehlo.broadcast_in_dim %650, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xi1>) -> tensor<2x128x60x80xi1>
      %652 = stablehlo.broadcast_in_dim %648, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %653 = stablehlo.select %651, %649, %652 : tensor<2x128x60x80xi1>, tensor<2x128x60x80xbf16>
      %654 = stablehlo.tanh %653 : tensor<2x128x60x80xbf16>
      %655 = stablehlo.multiply %646, %654 : tensor<2x128x60x80xbf16>
      %656 = stablehlo.convolution(%655, %arg579) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x128x60x80xbf16>, tensor<128x128x3x3xbf16>) -> tensor<2x128x60x80xbf16>
      %657 = stablehlo.broadcast_in_dim %656, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %658 = stablehlo.broadcast_in_dim %arg779, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %659 = stablehlo.subtract %657, %658 : tensor<2x128x60x80xbf16>
      %660 = stablehlo.broadcast_in_dim %659, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %661 = stablehlo.broadcast_in_dim %arg780, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %662 = stablehlo.multiply %660, %661 : tensor<2x128x60x80xbf16>
      %663 = stablehlo.broadcast_in_dim %662, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %664 = stablehlo.broadcast_in_dim %arg781, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %665 = stablehlo.multiply %663, %664 : tensor<2x128x60x80xbf16>
      %666 = stablehlo.broadcast_in_dim %665, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %667 = stablehlo.broadcast_in_dim %arg782, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %668 = stablehlo.add %666, %667 : tensor<2x128x60x80xbf16>
      %669 = stablehlo.exponential %668 : tensor<2x128x60x80xbf16>
      %670 = stablehlo.log_plus_one %669 : tensor<2x128x60x80xbf16>
      %671 = stablehlo.broadcast_in_dim %668, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %672 = stablehlo.compare  GT, %671, %425,  FLOAT : (tensor<2x128x60x80xbf16>, tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xi1>
      %673 = stablehlo.broadcast_in_dim %672, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xi1>) -> tensor<2x128x60x80xi1>
      %674 = stablehlo.broadcast_in_dim %670, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %675 = stablehlo.select %673, %671, %674 : tensor<2x128x60x80xi1>, tensor<2x128x60x80xbf16>
      %676 = stablehlo.tanh %675 : tensor<2x128x60x80xbf16>
      %677 = stablehlo.multiply %668, %676 : tensor<2x128x60x80xbf16>
      %678 = stablehlo.add %633, %677 : tensor<2x128x60x80xbf16>
      %679 = stablehlo.convolution(%678, %arg580) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x128x60x80xbf16>, tensor<128x128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %680 = stablehlo.broadcast_in_dim %679, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %681 = stablehlo.broadcast_in_dim %arg783, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %682 = stablehlo.subtract %680, %681 : tensor<2x128x60x80xbf16>
      %683 = stablehlo.broadcast_in_dim %682, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %684 = stablehlo.broadcast_in_dim %arg784, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %685 = stablehlo.multiply %683, %684 : tensor<2x128x60x80xbf16>
      %686 = stablehlo.broadcast_in_dim %685, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %687 = stablehlo.broadcast_in_dim %arg785, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %688 = stablehlo.multiply %686, %687 : tensor<2x128x60x80xbf16>
      %689 = stablehlo.broadcast_in_dim %688, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %690 = stablehlo.broadcast_in_dim %arg786, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %691 = stablehlo.add %689, %690 : tensor<2x128x60x80xbf16>
      %692 = stablehlo.exponential %691 : tensor<2x128x60x80xbf16>
      %693 = stablehlo.log_plus_one %692 : tensor<2x128x60x80xbf16>
      %694 = stablehlo.broadcast_in_dim %691, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %695 = stablehlo.compare  GT, %694, %425,  FLOAT : (tensor<2x128x60x80xbf16>, tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xi1>
      %696 = stablehlo.broadcast_in_dim %695, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xi1>) -> tensor<2x128x60x80xi1>
      %697 = stablehlo.broadcast_in_dim %693, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %698 = stablehlo.select %696, %694, %697 : tensor<2x128x60x80xi1>, tensor<2x128x60x80xbf16>
      %699 = stablehlo.tanh %698 : tensor<2x128x60x80xbf16>
      %700 = stablehlo.multiply %691, %699 : tensor<2x128x60x80xbf16>
      %701 = stablehlo.convolution(%700, %arg581) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x128x60x80xbf16>, tensor<128x128x3x3xbf16>) -> tensor<2x128x60x80xbf16>
      %702 = stablehlo.broadcast_in_dim %701, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %703 = stablehlo.broadcast_in_dim %arg787, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %704 = stablehlo.subtract %702, %703 : tensor<2x128x60x80xbf16>
      %705 = stablehlo.broadcast_in_dim %704, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %706 = stablehlo.broadcast_in_dim %arg788, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %707 = stablehlo.multiply %705, %706 : tensor<2x128x60x80xbf16>
      %708 = stablehlo.broadcast_in_dim %707, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %709 = stablehlo.broadcast_in_dim %arg789, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %710 = stablehlo.multiply %708, %709 : tensor<2x128x60x80xbf16>
      %711 = stablehlo.broadcast_in_dim %710, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %712 = stablehlo.broadcast_in_dim %arg790, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %713 = stablehlo.add %711, %712 : tensor<2x128x60x80xbf16>
      %714 = stablehlo.exponential %713 : tensor<2x128x60x80xbf16>
      %715 = stablehlo.log_plus_one %714 : tensor<2x128x60x80xbf16>
      %716 = stablehlo.broadcast_in_dim %713, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %717 = stablehlo.compare  GT, %716, %425,  FLOAT : (tensor<2x128x60x80xbf16>, tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xi1>
      %718 = stablehlo.broadcast_in_dim %717, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xi1>) -> tensor<2x128x60x80xi1>
      %719 = stablehlo.broadcast_in_dim %715, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %720 = stablehlo.select %718, %716, %719 : tensor<2x128x60x80xi1>, tensor<2x128x60x80xbf16>
      %721 = stablehlo.tanh %720 : tensor<2x128x60x80xbf16>
      %722 = stablehlo.multiply %713, %721 : tensor<2x128x60x80xbf16>
      %723 = stablehlo.add %678, %722 : tensor<2x128x60x80xbf16>
      %724 = stablehlo.convolution(%723, %arg582) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x128x60x80xbf16>, tensor<128x128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %725 = stablehlo.broadcast_in_dim %724, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %726 = stablehlo.broadcast_in_dim %arg791, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %727 = stablehlo.subtract %725, %726 : tensor<2x128x60x80xbf16>
      %728 = stablehlo.broadcast_in_dim %727, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %729 = stablehlo.broadcast_in_dim %arg792, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %730 = stablehlo.multiply %728, %729 : tensor<2x128x60x80xbf16>
      %731 = stablehlo.broadcast_in_dim %730, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %732 = stablehlo.broadcast_in_dim %arg793, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %733 = stablehlo.multiply %731, %732 : tensor<2x128x60x80xbf16>
      %734 = stablehlo.broadcast_in_dim %733, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %735 = stablehlo.broadcast_in_dim %arg794, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %736 = stablehlo.add %734, %735 : tensor<2x128x60x80xbf16>
      %737 = stablehlo.exponential %736 : tensor<2x128x60x80xbf16>
      %738 = stablehlo.log_plus_one %737 : tensor<2x128x60x80xbf16>
      %739 = stablehlo.broadcast_in_dim %736, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %740 = stablehlo.compare  GT, %739, %425,  FLOAT : (tensor<2x128x60x80xbf16>, tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xi1>
      %741 = stablehlo.broadcast_in_dim %740, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xi1>) -> tensor<2x128x60x80xi1>
      %742 = stablehlo.broadcast_in_dim %738, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %743 = stablehlo.select %741, %739, %742 : tensor<2x128x60x80xi1>, tensor<2x128x60x80xbf16>
      %744 = stablehlo.tanh %743 : tensor<2x128x60x80xbf16>
      %745 = stablehlo.multiply %736, %744 : tensor<2x128x60x80xbf16>
      %746 = stablehlo.convolution(%745, %arg583) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x128x60x80xbf16>, tensor<128x128x3x3xbf16>) -> tensor<2x128x60x80xbf16>
      %747 = stablehlo.broadcast_in_dim %746, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %748 = stablehlo.broadcast_in_dim %arg795, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %749 = stablehlo.subtract %747, %748 : tensor<2x128x60x80xbf16>
      %750 = stablehlo.broadcast_in_dim %749, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %751 = stablehlo.broadcast_in_dim %arg796, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %752 = stablehlo.multiply %750, %751 : tensor<2x128x60x80xbf16>
      %753 = stablehlo.broadcast_in_dim %752, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %754 = stablehlo.broadcast_in_dim %arg797, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %755 = stablehlo.multiply %753, %754 : tensor<2x128x60x80xbf16>
      %756 = stablehlo.broadcast_in_dim %755, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %757 = stablehlo.broadcast_in_dim %arg798, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %758 = stablehlo.add %756, %757 : tensor<2x128x60x80xbf16>
      %759 = stablehlo.exponential %758 : tensor<2x128x60x80xbf16>
      %760 = stablehlo.log_plus_one %759 : tensor<2x128x60x80xbf16>
      %761 = stablehlo.broadcast_in_dim %758, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %762 = stablehlo.compare  GT, %761, %425,  FLOAT : (tensor<2x128x60x80xbf16>, tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xi1>
      %763 = stablehlo.broadcast_in_dim %762, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xi1>) -> tensor<2x128x60x80xi1>
      %764 = stablehlo.broadcast_in_dim %760, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %765 = stablehlo.select %763, %761, %764 : tensor<2x128x60x80xi1>, tensor<2x128x60x80xbf16>
      %766 = stablehlo.tanh %765 : tensor<2x128x60x80xbf16>
      %767 = stablehlo.multiply %758, %766 : tensor<2x128x60x80xbf16>
      %768 = stablehlo.add %723, %767 : tensor<2x128x60x80xbf16>
      %769 = stablehlo.convolution(%768, %arg584) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x128x60x80xbf16>, tensor<128x128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %770 = stablehlo.broadcast_in_dim %769, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %771 = stablehlo.broadcast_in_dim %arg799, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %772 = stablehlo.subtract %770, %771 : tensor<2x128x60x80xbf16>
      %773 = stablehlo.broadcast_in_dim %772, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %774 = stablehlo.broadcast_in_dim %arg800, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %775 = stablehlo.multiply %773, %774 : tensor<2x128x60x80xbf16>
      %776 = stablehlo.broadcast_in_dim %775, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %777 = stablehlo.broadcast_in_dim %arg801, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %778 = stablehlo.multiply %776, %777 : tensor<2x128x60x80xbf16>
      %779 = stablehlo.broadcast_in_dim %778, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %780 = stablehlo.broadcast_in_dim %arg802, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %781 = stablehlo.add %779, %780 : tensor<2x128x60x80xbf16>
      %782 = stablehlo.exponential %781 : tensor<2x128x60x80xbf16>
      %783 = stablehlo.log_plus_one %782 : tensor<2x128x60x80xbf16>
      %784 = stablehlo.broadcast_in_dim %781, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %785 = stablehlo.compare  GT, %784, %425,  FLOAT : (tensor<2x128x60x80xbf16>, tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xi1>
      %786 = stablehlo.broadcast_in_dim %785, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xi1>) -> tensor<2x128x60x80xi1>
      %787 = stablehlo.broadcast_in_dim %783, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %788 = stablehlo.select %786, %784, %787 : tensor<2x128x60x80xi1>, tensor<2x128x60x80xbf16>
      %789 = stablehlo.tanh %788 : tensor<2x128x60x80xbf16>
      %790 = stablehlo.multiply %781, %789 : tensor<2x128x60x80xbf16>
      %791 = stablehlo.convolution(%790, %arg585) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x128x60x80xbf16>, tensor<128x128x3x3xbf16>) -> tensor<2x128x60x80xbf16>
      %792 = stablehlo.broadcast_in_dim %791, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %793 = stablehlo.broadcast_in_dim %arg803, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %794 = stablehlo.subtract %792, %793 : tensor<2x128x60x80xbf16>
      %795 = stablehlo.broadcast_in_dim %794, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %796 = stablehlo.broadcast_in_dim %arg804, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %797 = stablehlo.multiply %795, %796 : tensor<2x128x60x80xbf16>
      %798 = stablehlo.broadcast_in_dim %797, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %799 = stablehlo.broadcast_in_dim %arg805, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %800 = stablehlo.multiply %798, %799 : tensor<2x128x60x80xbf16>
      %801 = stablehlo.broadcast_in_dim %800, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %802 = stablehlo.broadcast_in_dim %arg806, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %803 = stablehlo.add %801, %802 : tensor<2x128x60x80xbf16>
      %804 = stablehlo.exponential %803 : tensor<2x128x60x80xbf16>
      %805 = stablehlo.log_plus_one %804 : tensor<2x128x60x80xbf16>
      %806 = stablehlo.broadcast_in_dim %803, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %807 = stablehlo.compare  GT, %806, %425,  FLOAT : (tensor<2x128x60x80xbf16>, tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xi1>
      %808 = stablehlo.broadcast_in_dim %807, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xi1>) -> tensor<2x128x60x80xi1>
      %809 = stablehlo.broadcast_in_dim %805, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %810 = stablehlo.select %808, %806, %809 : tensor<2x128x60x80xi1>, tensor<2x128x60x80xbf16>
      %811 = stablehlo.tanh %810 : tensor<2x128x60x80xbf16>
      %812 = stablehlo.multiply %803, %811 : tensor<2x128x60x80xbf16>
      %813 = stablehlo.add %768, %812 : tensor<2x128x60x80xbf16>
      %814 = stablehlo.convolution(%813, %arg586) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x128x60x80xbf16>, tensor<128x128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %815 = stablehlo.broadcast_in_dim %814, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %816 = stablehlo.broadcast_in_dim %arg807, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %817 = stablehlo.subtract %815, %816 : tensor<2x128x60x80xbf16>
      %818 = stablehlo.broadcast_in_dim %817, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %819 = stablehlo.broadcast_in_dim %arg808, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %820 = stablehlo.multiply %818, %819 : tensor<2x128x60x80xbf16>
      %821 = stablehlo.broadcast_in_dim %820, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %822 = stablehlo.broadcast_in_dim %arg809, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %823 = stablehlo.multiply %821, %822 : tensor<2x128x60x80xbf16>
      %824 = stablehlo.broadcast_in_dim %823, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %825 = stablehlo.broadcast_in_dim %arg810, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %826 = stablehlo.add %824, %825 : tensor<2x128x60x80xbf16>
      %827 = stablehlo.exponential %826 : tensor<2x128x60x80xbf16>
      %828 = stablehlo.log_plus_one %827 : tensor<2x128x60x80xbf16>
      %829 = stablehlo.broadcast_in_dim %826, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %830 = stablehlo.compare  GT, %829, %425,  FLOAT : (tensor<2x128x60x80xbf16>, tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xi1>
      %831 = stablehlo.broadcast_in_dim %830, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xi1>) -> tensor<2x128x60x80xi1>
      %832 = stablehlo.broadcast_in_dim %828, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %833 = stablehlo.select %831, %829, %832 : tensor<2x128x60x80xi1>, tensor<2x128x60x80xbf16>
      %834 = stablehlo.tanh %833 : tensor<2x128x60x80xbf16>
      %835 = stablehlo.multiply %826, %834 : tensor<2x128x60x80xbf16>
      %836 = stablehlo.concatenate %835, %431, dim = 1 : (tensor<2x128x60x80xbf16>, tensor<2x128x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %837 = stablehlo.convolution(%836, %arg587) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x60x80xbf16>, tensor<256x256x1x1xbf16>) -> tensor<2x256x60x80xbf16>
      %838 = stablehlo.broadcast_in_dim %837, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %839 = stablehlo.broadcast_in_dim %arg811, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x60x80xbf16>
      %840 = stablehlo.subtract %838, %839 : tensor<2x256x60x80xbf16>
      %841 = stablehlo.broadcast_in_dim %840, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %842 = stablehlo.broadcast_in_dim %arg812, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x60x80xbf16>
      %843 = stablehlo.multiply %841, %842 : tensor<2x256x60x80xbf16>
      %844 = stablehlo.broadcast_in_dim %843, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %845 = stablehlo.broadcast_in_dim %arg813, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x60x80xbf16>
      %846 = stablehlo.multiply %844, %845 : tensor<2x256x60x80xbf16>
      %847 = stablehlo.broadcast_in_dim %846, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %848 = stablehlo.broadcast_in_dim %arg814, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x60x80xbf16>
      %849 = stablehlo.add %847, %848 : tensor<2x256x60x80xbf16>
      %850 = stablehlo.exponential %849 : tensor<2x256x60x80xbf16>
      %851 = stablehlo.log_plus_one %850 : tensor<2x256x60x80xbf16>
      %852 = stablehlo.broadcast_in_dim %849, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %853 = stablehlo.compare  GT, %852, %402,  FLOAT : (tensor<2x256x60x80xbf16>, tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xi1>
      %854 = stablehlo.broadcast_in_dim %853, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xi1>) -> tensor<2x256x60x80xi1>
      %855 = stablehlo.broadcast_in_dim %851, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %856 = stablehlo.select %854, %852, %855 : tensor<2x256x60x80xi1>, tensor<2x256x60x80xbf16>
      %857 = stablehlo.tanh %856 : tensor<2x256x60x80xbf16>
      %858 = stablehlo.multiply %849, %857 : tensor<2x256x60x80xbf16>
      %859 = stablehlo.convolution(%858, %arg588) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [2, 2], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x60x80xbf16>, tensor<512x256x3x3xbf16>) -> tensor<2x512x30x40xbf16>
      %860 = stablehlo.broadcast_in_dim %859, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %861 = stablehlo.broadcast_in_dim %arg815, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x30x40xbf16>
      %862 = stablehlo.subtract %860, %861 : tensor<2x512x30x40xbf16>
      %863 = stablehlo.broadcast_in_dim %862, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %864 = stablehlo.broadcast_in_dim %arg816, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x30x40xbf16>
      %865 = stablehlo.multiply %863, %864 : tensor<2x512x30x40xbf16>
      %866 = stablehlo.broadcast_in_dim %865, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %867 = stablehlo.broadcast_in_dim %arg817, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x30x40xbf16>
      %868 = stablehlo.multiply %866, %867 : tensor<2x512x30x40xbf16>
      %869 = stablehlo.broadcast_in_dim %868, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %870 = stablehlo.broadcast_in_dim %arg818, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x30x40xbf16>
      %871 = stablehlo.add %869, %870 : tensor<2x512x30x40xbf16>
      %872 = stablehlo.exponential %871 : tensor<2x512x30x40xbf16>
      %873 = stablehlo.log_plus_one %872 : tensor<2x512x30x40xbf16>
      %874 = stablehlo.broadcast_in_dim %871, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %875 = stablehlo.broadcast_in_dim %16, dims = [] : (tensor<bf16>) -> tensor<2x512x30x40xbf16>
      %876 = stablehlo.compare  GT, %874, %875,  FLOAT : (tensor<2x512x30x40xbf16>, tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xi1>
      %877 = stablehlo.broadcast_in_dim %876, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xi1>) -> tensor<2x512x30x40xi1>
      %878 = stablehlo.broadcast_in_dim %873, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %879 = stablehlo.select %877, %874, %878 : tensor<2x512x30x40xi1>, tensor<2x512x30x40xbf16>
      %880 = stablehlo.tanh %879 : tensor<2x512x30x40xbf16>
      %881 = stablehlo.multiply %871, %880 : tensor<2x512x30x40xbf16>
      %882 = stablehlo.convolution(%881, %arg589) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x512x30x40xbf16>, tensor<256x512x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %883 = stablehlo.broadcast_in_dim %882, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %884 = stablehlo.broadcast_in_dim %arg819, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %885 = stablehlo.subtract %883, %884 : tensor<2x256x30x40xbf16>
      %886 = stablehlo.broadcast_in_dim %885, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %887 = stablehlo.broadcast_in_dim %arg820, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %888 = stablehlo.multiply %886, %887 : tensor<2x256x30x40xbf16>
      %889 = stablehlo.broadcast_in_dim %888, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %890 = stablehlo.broadcast_in_dim %arg821, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %891 = stablehlo.multiply %889, %890 : tensor<2x256x30x40xbf16>
      %892 = stablehlo.broadcast_in_dim %891, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %893 = stablehlo.broadcast_in_dim %arg822, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %894 = stablehlo.add %892, %893 : tensor<2x256x30x40xbf16>
      %895 = stablehlo.exponential %894 : tensor<2x256x30x40xbf16>
      %896 = stablehlo.log_plus_one %895 : tensor<2x256x30x40xbf16>
      %897 = stablehlo.broadcast_in_dim %894, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %898 = stablehlo.broadcast_in_dim %16, dims = [] : (tensor<bf16>) -> tensor<2x256x30x40xbf16>
      %899 = stablehlo.compare  GT, %897, %898,  FLOAT : (tensor<2x256x30x40xbf16>, tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xi1>
      %900 = stablehlo.broadcast_in_dim %899, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xi1>) -> tensor<2x256x30x40xi1>
      %901 = stablehlo.broadcast_in_dim %896, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %902 = stablehlo.select %900, %897, %901 : tensor<2x256x30x40xi1>, tensor<2x256x30x40xbf16>
      %903 = stablehlo.tanh %902 : tensor<2x256x30x40xbf16>
      %904 = stablehlo.multiply %894, %903 : tensor<2x256x30x40xbf16>
      %905 = stablehlo.convolution(%881, %arg590) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x512x30x40xbf16>, tensor<256x512x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %906 = stablehlo.broadcast_in_dim %905, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %907 = stablehlo.broadcast_in_dim %arg823, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %908 = stablehlo.subtract %906, %907 : tensor<2x256x30x40xbf16>
      %909 = stablehlo.broadcast_in_dim %908, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %910 = stablehlo.broadcast_in_dim %arg824, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %911 = stablehlo.multiply %909, %910 : tensor<2x256x30x40xbf16>
      %912 = stablehlo.broadcast_in_dim %911, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %913 = stablehlo.broadcast_in_dim %arg825, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %914 = stablehlo.multiply %912, %913 : tensor<2x256x30x40xbf16>
      %915 = stablehlo.broadcast_in_dim %914, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %916 = stablehlo.broadcast_in_dim %arg826, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %917 = stablehlo.add %915, %916 : tensor<2x256x30x40xbf16>
      %918 = stablehlo.exponential %917 : tensor<2x256x30x40xbf16>
      %919 = stablehlo.log_plus_one %918 : tensor<2x256x30x40xbf16>
      %920 = stablehlo.broadcast_in_dim %917, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %921 = stablehlo.compare  GT, %920, %898,  FLOAT : (tensor<2x256x30x40xbf16>, tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xi1>
      %922 = stablehlo.broadcast_in_dim %921, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xi1>) -> tensor<2x256x30x40xi1>
      %923 = stablehlo.broadcast_in_dim %919, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %924 = stablehlo.select %922, %920, %923 : tensor<2x256x30x40xi1>, tensor<2x256x30x40xbf16>
      %925 = stablehlo.tanh %924 : tensor<2x256x30x40xbf16>
      %926 = stablehlo.multiply %917, %925 : tensor<2x256x30x40xbf16>
      %927 = stablehlo.convolution(%926, %arg591) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x30x40xbf16>, tensor<256x256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %928 = stablehlo.broadcast_in_dim %927, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %929 = stablehlo.broadcast_in_dim %arg827, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %930 = stablehlo.subtract %928, %929 : tensor<2x256x30x40xbf16>
      %931 = stablehlo.broadcast_in_dim %930, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %932 = stablehlo.broadcast_in_dim %arg828, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %933 = stablehlo.multiply %931, %932 : tensor<2x256x30x40xbf16>
      %934 = stablehlo.broadcast_in_dim %933, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %935 = stablehlo.broadcast_in_dim %arg829, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %936 = stablehlo.multiply %934, %935 : tensor<2x256x30x40xbf16>
      %937 = stablehlo.broadcast_in_dim %936, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %938 = stablehlo.broadcast_in_dim %arg830, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %939 = stablehlo.add %937, %938 : tensor<2x256x30x40xbf16>
      %940 = stablehlo.exponential %939 : tensor<2x256x30x40xbf16>
      %941 = stablehlo.log_plus_one %940 : tensor<2x256x30x40xbf16>
      %942 = stablehlo.broadcast_in_dim %939, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %943 = stablehlo.compare  GT, %942, %898,  FLOAT : (tensor<2x256x30x40xbf16>, tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xi1>
      %944 = stablehlo.broadcast_in_dim %943, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xi1>) -> tensor<2x256x30x40xi1>
      %945 = stablehlo.broadcast_in_dim %941, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %946 = stablehlo.select %944, %942, %945 : tensor<2x256x30x40xi1>, tensor<2x256x30x40xbf16>
      %947 = stablehlo.tanh %946 : tensor<2x256x30x40xbf16>
      %948 = stablehlo.multiply %939, %947 : tensor<2x256x30x40xbf16>
      %949 = stablehlo.convolution(%948, %arg592) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x30x40xbf16>, tensor<256x256x3x3xbf16>) -> tensor<2x256x30x40xbf16>
      %950 = stablehlo.broadcast_in_dim %949, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %951 = stablehlo.broadcast_in_dim %arg831, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %952 = stablehlo.subtract %950, %951 : tensor<2x256x30x40xbf16>
      %953 = stablehlo.broadcast_in_dim %952, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %954 = stablehlo.broadcast_in_dim %arg832, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %955 = stablehlo.multiply %953, %954 : tensor<2x256x30x40xbf16>
      %956 = stablehlo.broadcast_in_dim %955, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %957 = stablehlo.broadcast_in_dim %arg833, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %958 = stablehlo.multiply %956, %957 : tensor<2x256x30x40xbf16>
      %959 = stablehlo.broadcast_in_dim %958, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %960 = stablehlo.broadcast_in_dim %arg834, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %961 = stablehlo.add %959, %960 : tensor<2x256x30x40xbf16>
      %962 = stablehlo.exponential %961 : tensor<2x256x30x40xbf16>
      %963 = stablehlo.log_plus_one %962 : tensor<2x256x30x40xbf16>
      %964 = stablehlo.broadcast_in_dim %961, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %965 = stablehlo.compare  GT, %964, %898,  FLOAT : (tensor<2x256x30x40xbf16>, tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xi1>
      %966 = stablehlo.broadcast_in_dim %965, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xi1>) -> tensor<2x256x30x40xi1>
      %967 = stablehlo.broadcast_in_dim %963, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %968 = stablehlo.select %966, %964, %967 : tensor<2x256x30x40xi1>, tensor<2x256x30x40xbf16>
      %969 = stablehlo.tanh %968 : tensor<2x256x30x40xbf16>
      %970 = stablehlo.multiply %961, %969 : tensor<2x256x30x40xbf16>
      %971 = stablehlo.add %926, %970 : tensor<2x256x30x40xbf16>
      %972 = stablehlo.convolution(%971, %arg593) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x30x40xbf16>, tensor<256x256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %973 = stablehlo.broadcast_in_dim %972, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %974 = stablehlo.broadcast_in_dim %arg835, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %975 = stablehlo.subtract %973, %974 : tensor<2x256x30x40xbf16>
      %976 = stablehlo.broadcast_in_dim %975, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %977 = stablehlo.broadcast_in_dim %arg836, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %978 = stablehlo.multiply %976, %977 : tensor<2x256x30x40xbf16>
      %979 = stablehlo.broadcast_in_dim %978, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %980 = stablehlo.broadcast_in_dim %arg837, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %981 = stablehlo.multiply %979, %980 : tensor<2x256x30x40xbf16>
      %982 = stablehlo.broadcast_in_dim %981, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %983 = stablehlo.broadcast_in_dim %arg838, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %984 = stablehlo.add %982, %983 : tensor<2x256x30x40xbf16>
      %985 = stablehlo.exponential %984 : tensor<2x256x30x40xbf16>
      %986 = stablehlo.log_plus_one %985 : tensor<2x256x30x40xbf16>
      %987 = stablehlo.broadcast_in_dim %984, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %988 = stablehlo.compare  GT, %987, %898,  FLOAT : (tensor<2x256x30x40xbf16>, tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xi1>
      %989 = stablehlo.broadcast_in_dim %988, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xi1>) -> tensor<2x256x30x40xi1>
      %990 = stablehlo.broadcast_in_dim %986, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %991 = stablehlo.select %989, %987, %990 : tensor<2x256x30x40xi1>, tensor<2x256x30x40xbf16>
      %992 = stablehlo.tanh %991 : tensor<2x256x30x40xbf16>
      %993 = stablehlo.multiply %984, %992 : tensor<2x256x30x40xbf16>
      %994 = stablehlo.convolution(%993, %arg594) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x30x40xbf16>, tensor<256x256x3x3xbf16>) -> tensor<2x256x30x40xbf16>
      %995 = stablehlo.broadcast_in_dim %994, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %996 = stablehlo.broadcast_in_dim %arg839, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %997 = stablehlo.subtract %995, %996 : tensor<2x256x30x40xbf16>
      %998 = stablehlo.broadcast_in_dim %997, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %999 = stablehlo.broadcast_in_dim %arg840, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1000 = stablehlo.multiply %998, %999 : tensor<2x256x30x40xbf16>
      %1001 = stablehlo.broadcast_in_dim %1000, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1002 = stablehlo.broadcast_in_dim %arg841, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1003 = stablehlo.multiply %1001, %1002 : tensor<2x256x30x40xbf16>
      %1004 = stablehlo.broadcast_in_dim %1003, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1005 = stablehlo.broadcast_in_dim %arg842, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1006 = stablehlo.add %1004, %1005 : tensor<2x256x30x40xbf16>
      %1007 = stablehlo.exponential %1006 : tensor<2x256x30x40xbf16>
      %1008 = stablehlo.log_plus_one %1007 : tensor<2x256x30x40xbf16>
      %1009 = stablehlo.broadcast_in_dim %1006, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1010 = stablehlo.compare  GT, %1009, %898,  FLOAT : (tensor<2x256x30x40xbf16>, tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xi1>
      %1011 = stablehlo.broadcast_in_dim %1010, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xi1>) -> tensor<2x256x30x40xi1>
      %1012 = stablehlo.broadcast_in_dim %1008, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1013 = stablehlo.select %1011, %1009, %1012 : tensor<2x256x30x40xi1>, tensor<2x256x30x40xbf16>
      %1014 = stablehlo.tanh %1013 : tensor<2x256x30x40xbf16>
      %1015 = stablehlo.multiply %1006, %1014 : tensor<2x256x30x40xbf16>
      %1016 = stablehlo.add %971, %1015 : tensor<2x256x30x40xbf16>
      %1017 = stablehlo.convolution(%1016, %arg595) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x30x40xbf16>, tensor<256x256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1018 = stablehlo.broadcast_in_dim %1017, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1019 = stablehlo.broadcast_in_dim %arg843, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1020 = stablehlo.subtract %1018, %1019 : tensor<2x256x30x40xbf16>
      %1021 = stablehlo.broadcast_in_dim %1020, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1022 = stablehlo.broadcast_in_dim %arg844, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1023 = stablehlo.multiply %1021, %1022 : tensor<2x256x30x40xbf16>
      %1024 = stablehlo.broadcast_in_dim %1023, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1025 = stablehlo.broadcast_in_dim %arg845, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1026 = stablehlo.multiply %1024, %1025 : tensor<2x256x30x40xbf16>
      %1027 = stablehlo.broadcast_in_dim %1026, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1028 = stablehlo.broadcast_in_dim %arg846, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1029 = stablehlo.add %1027, %1028 : tensor<2x256x30x40xbf16>
      %1030 = stablehlo.exponential %1029 : tensor<2x256x30x40xbf16>
      %1031 = stablehlo.log_plus_one %1030 : tensor<2x256x30x40xbf16>
      %1032 = stablehlo.broadcast_in_dim %1029, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1033 = stablehlo.compare  GT, %1032, %898,  FLOAT : (tensor<2x256x30x40xbf16>, tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xi1>
      %1034 = stablehlo.broadcast_in_dim %1033, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xi1>) -> tensor<2x256x30x40xi1>
      %1035 = stablehlo.broadcast_in_dim %1031, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1036 = stablehlo.select %1034, %1032, %1035 : tensor<2x256x30x40xi1>, tensor<2x256x30x40xbf16>
      %1037 = stablehlo.tanh %1036 : tensor<2x256x30x40xbf16>
      %1038 = stablehlo.multiply %1029, %1037 : tensor<2x256x30x40xbf16>
      %1039 = stablehlo.convolution(%1038, %arg596) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x30x40xbf16>, tensor<256x256x3x3xbf16>) -> tensor<2x256x30x40xbf16>
      %1040 = stablehlo.broadcast_in_dim %1039, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1041 = stablehlo.broadcast_in_dim %arg847, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1042 = stablehlo.subtract %1040, %1041 : tensor<2x256x30x40xbf16>
      %1043 = stablehlo.broadcast_in_dim %1042, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1044 = stablehlo.broadcast_in_dim %arg848, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1045 = stablehlo.multiply %1043, %1044 : tensor<2x256x30x40xbf16>
      %1046 = stablehlo.broadcast_in_dim %1045, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1047 = stablehlo.broadcast_in_dim %arg849, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1048 = stablehlo.multiply %1046, %1047 : tensor<2x256x30x40xbf16>
      %1049 = stablehlo.broadcast_in_dim %1048, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1050 = stablehlo.broadcast_in_dim %arg850, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1051 = stablehlo.add %1049, %1050 : tensor<2x256x30x40xbf16>
      %1052 = stablehlo.exponential %1051 : tensor<2x256x30x40xbf16>
      %1053 = stablehlo.log_plus_one %1052 : tensor<2x256x30x40xbf16>
      %1054 = stablehlo.broadcast_in_dim %1051, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1055 = stablehlo.compare  GT, %1054, %898,  FLOAT : (tensor<2x256x30x40xbf16>, tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xi1>
      %1056 = stablehlo.broadcast_in_dim %1055, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xi1>) -> tensor<2x256x30x40xi1>
      %1057 = stablehlo.broadcast_in_dim %1053, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1058 = stablehlo.select %1056, %1054, %1057 : tensor<2x256x30x40xi1>, tensor<2x256x30x40xbf16>
      %1059 = stablehlo.tanh %1058 : tensor<2x256x30x40xbf16>
      %1060 = stablehlo.multiply %1051, %1059 : tensor<2x256x30x40xbf16>
      %1061 = stablehlo.add %1016, %1060 : tensor<2x256x30x40xbf16>
      %1062 = stablehlo.convolution(%1061, %arg597) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x30x40xbf16>, tensor<256x256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1063 = stablehlo.broadcast_in_dim %1062, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1064 = stablehlo.broadcast_in_dim %arg851, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1065 = stablehlo.subtract %1063, %1064 : tensor<2x256x30x40xbf16>
      %1066 = stablehlo.broadcast_in_dim %1065, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1067 = stablehlo.broadcast_in_dim %arg852, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1068 = stablehlo.multiply %1066, %1067 : tensor<2x256x30x40xbf16>
      %1069 = stablehlo.broadcast_in_dim %1068, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1070 = stablehlo.broadcast_in_dim %arg853, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1071 = stablehlo.multiply %1069, %1070 : tensor<2x256x30x40xbf16>
      %1072 = stablehlo.broadcast_in_dim %1071, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1073 = stablehlo.broadcast_in_dim %arg854, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1074 = stablehlo.add %1072, %1073 : tensor<2x256x30x40xbf16>
      %1075 = stablehlo.exponential %1074 : tensor<2x256x30x40xbf16>
      %1076 = stablehlo.log_plus_one %1075 : tensor<2x256x30x40xbf16>
      %1077 = stablehlo.broadcast_in_dim %1074, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1078 = stablehlo.compare  GT, %1077, %898,  FLOAT : (tensor<2x256x30x40xbf16>, tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xi1>
      %1079 = stablehlo.broadcast_in_dim %1078, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xi1>) -> tensor<2x256x30x40xi1>
      %1080 = stablehlo.broadcast_in_dim %1076, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1081 = stablehlo.select %1079, %1077, %1080 : tensor<2x256x30x40xi1>, tensor<2x256x30x40xbf16>
      %1082 = stablehlo.tanh %1081 : tensor<2x256x30x40xbf16>
      %1083 = stablehlo.multiply %1074, %1082 : tensor<2x256x30x40xbf16>
      %1084 = stablehlo.convolution(%1083, %arg598) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x30x40xbf16>, tensor<256x256x3x3xbf16>) -> tensor<2x256x30x40xbf16>
      %1085 = stablehlo.broadcast_in_dim %1084, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1086 = stablehlo.broadcast_in_dim %arg855, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1087 = stablehlo.subtract %1085, %1086 : tensor<2x256x30x40xbf16>
      %1088 = stablehlo.broadcast_in_dim %1087, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1089 = stablehlo.broadcast_in_dim %arg856, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1090 = stablehlo.multiply %1088, %1089 : tensor<2x256x30x40xbf16>
      %1091 = stablehlo.broadcast_in_dim %1090, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1092 = stablehlo.broadcast_in_dim %arg857, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1093 = stablehlo.multiply %1091, %1092 : tensor<2x256x30x40xbf16>
      %1094 = stablehlo.broadcast_in_dim %1093, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1095 = stablehlo.broadcast_in_dim %arg858, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1096 = stablehlo.add %1094, %1095 : tensor<2x256x30x40xbf16>
      %1097 = stablehlo.exponential %1096 : tensor<2x256x30x40xbf16>
      %1098 = stablehlo.log_plus_one %1097 : tensor<2x256x30x40xbf16>
      %1099 = stablehlo.broadcast_in_dim %1096, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1100 = stablehlo.compare  GT, %1099, %898,  FLOAT : (tensor<2x256x30x40xbf16>, tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xi1>
      %1101 = stablehlo.broadcast_in_dim %1100, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xi1>) -> tensor<2x256x30x40xi1>
      %1102 = stablehlo.broadcast_in_dim %1098, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1103 = stablehlo.select %1101, %1099, %1102 : tensor<2x256x30x40xi1>, tensor<2x256x30x40xbf16>
      %1104 = stablehlo.tanh %1103 : tensor<2x256x30x40xbf16>
      %1105 = stablehlo.multiply %1096, %1104 : tensor<2x256x30x40xbf16>
      %1106 = stablehlo.add %1061, %1105 : tensor<2x256x30x40xbf16>
      %1107 = stablehlo.convolution(%1106, %arg599) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x30x40xbf16>, tensor<256x256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1108 = stablehlo.broadcast_in_dim %1107, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1109 = stablehlo.broadcast_in_dim %arg859, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1110 = stablehlo.subtract %1108, %1109 : tensor<2x256x30x40xbf16>
      %1111 = stablehlo.broadcast_in_dim %1110, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1112 = stablehlo.broadcast_in_dim %arg860, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1113 = stablehlo.multiply %1111, %1112 : tensor<2x256x30x40xbf16>
      %1114 = stablehlo.broadcast_in_dim %1113, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1115 = stablehlo.broadcast_in_dim %arg861, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1116 = stablehlo.multiply %1114, %1115 : tensor<2x256x30x40xbf16>
      %1117 = stablehlo.broadcast_in_dim %1116, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1118 = stablehlo.broadcast_in_dim %arg862, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1119 = stablehlo.add %1117, %1118 : tensor<2x256x30x40xbf16>
      %1120 = stablehlo.exponential %1119 : tensor<2x256x30x40xbf16>
      %1121 = stablehlo.log_plus_one %1120 : tensor<2x256x30x40xbf16>
      %1122 = stablehlo.broadcast_in_dim %1119, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1123 = stablehlo.compare  GT, %1122, %898,  FLOAT : (tensor<2x256x30x40xbf16>, tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xi1>
      %1124 = stablehlo.broadcast_in_dim %1123, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xi1>) -> tensor<2x256x30x40xi1>
      %1125 = stablehlo.broadcast_in_dim %1121, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1126 = stablehlo.select %1124, %1122, %1125 : tensor<2x256x30x40xi1>, tensor<2x256x30x40xbf16>
      %1127 = stablehlo.tanh %1126 : tensor<2x256x30x40xbf16>
      %1128 = stablehlo.multiply %1119, %1127 : tensor<2x256x30x40xbf16>
      %1129 = stablehlo.convolution(%1128, %arg600) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x30x40xbf16>, tensor<256x256x3x3xbf16>) -> tensor<2x256x30x40xbf16>
      %1130 = stablehlo.broadcast_in_dim %1129, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1131 = stablehlo.broadcast_in_dim %arg863, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1132 = stablehlo.subtract %1130, %1131 : tensor<2x256x30x40xbf16>
      %1133 = stablehlo.broadcast_in_dim %1132, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1134 = stablehlo.broadcast_in_dim %arg864, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1135 = stablehlo.multiply %1133, %1134 : tensor<2x256x30x40xbf16>
      %1136 = stablehlo.broadcast_in_dim %1135, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1137 = stablehlo.broadcast_in_dim %arg865, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1138 = stablehlo.multiply %1136, %1137 : tensor<2x256x30x40xbf16>
      %1139 = stablehlo.broadcast_in_dim %1138, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1140 = stablehlo.broadcast_in_dim %arg866, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1141 = stablehlo.add %1139, %1140 : tensor<2x256x30x40xbf16>
      %1142 = stablehlo.exponential %1141 : tensor<2x256x30x40xbf16>
      %1143 = stablehlo.log_plus_one %1142 : tensor<2x256x30x40xbf16>
      %1144 = stablehlo.broadcast_in_dim %1141, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1145 = stablehlo.compare  GT, %1144, %898,  FLOAT : (tensor<2x256x30x40xbf16>, tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xi1>
      %1146 = stablehlo.broadcast_in_dim %1145, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xi1>) -> tensor<2x256x30x40xi1>
      %1147 = stablehlo.broadcast_in_dim %1143, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1148 = stablehlo.select %1146, %1144, %1147 : tensor<2x256x30x40xi1>, tensor<2x256x30x40xbf16>
      %1149 = stablehlo.tanh %1148 : tensor<2x256x30x40xbf16>
      %1150 = stablehlo.multiply %1141, %1149 : tensor<2x256x30x40xbf16>
      %1151 = stablehlo.add %1106, %1150 : tensor<2x256x30x40xbf16>
      %1152 = stablehlo.convolution(%1151, %arg601) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x30x40xbf16>, tensor<256x256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1153 = stablehlo.broadcast_in_dim %1152, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1154 = stablehlo.broadcast_in_dim %arg867, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1155 = stablehlo.subtract %1153, %1154 : tensor<2x256x30x40xbf16>
      %1156 = stablehlo.broadcast_in_dim %1155, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1157 = stablehlo.broadcast_in_dim %arg868, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1158 = stablehlo.multiply %1156, %1157 : tensor<2x256x30x40xbf16>
      %1159 = stablehlo.broadcast_in_dim %1158, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1160 = stablehlo.broadcast_in_dim %arg869, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1161 = stablehlo.multiply %1159, %1160 : tensor<2x256x30x40xbf16>
      %1162 = stablehlo.broadcast_in_dim %1161, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1163 = stablehlo.broadcast_in_dim %arg870, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1164 = stablehlo.add %1162, %1163 : tensor<2x256x30x40xbf16>
      %1165 = stablehlo.exponential %1164 : tensor<2x256x30x40xbf16>
      %1166 = stablehlo.log_plus_one %1165 : tensor<2x256x30x40xbf16>
      %1167 = stablehlo.broadcast_in_dim %1164, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1168 = stablehlo.compare  GT, %1167, %898,  FLOAT : (tensor<2x256x30x40xbf16>, tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xi1>
      %1169 = stablehlo.broadcast_in_dim %1168, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xi1>) -> tensor<2x256x30x40xi1>
      %1170 = stablehlo.broadcast_in_dim %1166, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1171 = stablehlo.select %1169, %1167, %1170 : tensor<2x256x30x40xi1>, tensor<2x256x30x40xbf16>
      %1172 = stablehlo.tanh %1171 : tensor<2x256x30x40xbf16>
      %1173 = stablehlo.multiply %1164, %1172 : tensor<2x256x30x40xbf16>
      %1174 = stablehlo.convolution(%1173, %arg602) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x30x40xbf16>, tensor<256x256x3x3xbf16>) -> tensor<2x256x30x40xbf16>
      %1175 = stablehlo.broadcast_in_dim %1174, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1176 = stablehlo.broadcast_in_dim %arg871, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1177 = stablehlo.subtract %1175, %1176 : tensor<2x256x30x40xbf16>
      %1178 = stablehlo.broadcast_in_dim %1177, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1179 = stablehlo.broadcast_in_dim %arg872, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1180 = stablehlo.multiply %1178, %1179 : tensor<2x256x30x40xbf16>
      %1181 = stablehlo.broadcast_in_dim %1180, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1182 = stablehlo.broadcast_in_dim %arg873, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1183 = stablehlo.multiply %1181, %1182 : tensor<2x256x30x40xbf16>
      %1184 = stablehlo.broadcast_in_dim %1183, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1185 = stablehlo.broadcast_in_dim %arg874, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1186 = stablehlo.add %1184, %1185 : tensor<2x256x30x40xbf16>
      %1187 = stablehlo.exponential %1186 : tensor<2x256x30x40xbf16>
      %1188 = stablehlo.log_plus_one %1187 : tensor<2x256x30x40xbf16>
      %1189 = stablehlo.broadcast_in_dim %1186, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1190 = stablehlo.compare  GT, %1189, %898,  FLOAT : (tensor<2x256x30x40xbf16>, tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xi1>
      %1191 = stablehlo.broadcast_in_dim %1190, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xi1>) -> tensor<2x256x30x40xi1>
      %1192 = stablehlo.broadcast_in_dim %1188, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1193 = stablehlo.select %1191, %1189, %1192 : tensor<2x256x30x40xi1>, tensor<2x256x30x40xbf16>
      %1194 = stablehlo.tanh %1193 : tensor<2x256x30x40xbf16>
      %1195 = stablehlo.multiply %1186, %1194 : tensor<2x256x30x40xbf16>
      %1196 = stablehlo.add %1151, %1195 : tensor<2x256x30x40xbf16>
      %1197 = stablehlo.convolution(%1196, %arg603) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x30x40xbf16>, tensor<256x256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1198 = stablehlo.broadcast_in_dim %1197, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1199 = stablehlo.broadcast_in_dim %arg875, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1200 = stablehlo.subtract %1198, %1199 : tensor<2x256x30x40xbf16>
      %1201 = stablehlo.broadcast_in_dim %1200, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1202 = stablehlo.broadcast_in_dim %arg876, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1203 = stablehlo.multiply %1201, %1202 : tensor<2x256x30x40xbf16>
      %1204 = stablehlo.broadcast_in_dim %1203, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1205 = stablehlo.broadcast_in_dim %arg877, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1206 = stablehlo.multiply %1204, %1205 : tensor<2x256x30x40xbf16>
      %1207 = stablehlo.broadcast_in_dim %1206, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1208 = stablehlo.broadcast_in_dim %arg878, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1209 = stablehlo.add %1207, %1208 : tensor<2x256x30x40xbf16>
      %1210 = stablehlo.exponential %1209 : tensor<2x256x30x40xbf16>
      %1211 = stablehlo.log_plus_one %1210 : tensor<2x256x30x40xbf16>
      %1212 = stablehlo.broadcast_in_dim %1209, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1213 = stablehlo.compare  GT, %1212, %898,  FLOAT : (tensor<2x256x30x40xbf16>, tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xi1>
      %1214 = stablehlo.broadcast_in_dim %1213, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xi1>) -> tensor<2x256x30x40xi1>
      %1215 = stablehlo.broadcast_in_dim %1211, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1216 = stablehlo.select %1214, %1212, %1215 : tensor<2x256x30x40xi1>, tensor<2x256x30x40xbf16>
      %1217 = stablehlo.tanh %1216 : tensor<2x256x30x40xbf16>
      %1218 = stablehlo.multiply %1209, %1217 : tensor<2x256x30x40xbf16>
      %1219 = stablehlo.convolution(%1218, %arg604) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x30x40xbf16>, tensor<256x256x3x3xbf16>) -> tensor<2x256x30x40xbf16>
      %1220 = stablehlo.broadcast_in_dim %1219, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1221 = stablehlo.broadcast_in_dim %arg879, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1222 = stablehlo.subtract %1220, %1221 : tensor<2x256x30x40xbf16>
      %1223 = stablehlo.broadcast_in_dim %1222, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1224 = stablehlo.broadcast_in_dim %arg880, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1225 = stablehlo.multiply %1223, %1224 : tensor<2x256x30x40xbf16>
      %1226 = stablehlo.broadcast_in_dim %1225, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1227 = stablehlo.broadcast_in_dim %arg881, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1228 = stablehlo.multiply %1226, %1227 : tensor<2x256x30x40xbf16>
      %1229 = stablehlo.broadcast_in_dim %1228, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1230 = stablehlo.broadcast_in_dim %arg882, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1231 = stablehlo.add %1229, %1230 : tensor<2x256x30x40xbf16>
      %1232 = stablehlo.exponential %1231 : tensor<2x256x30x40xbf16>
      %1233 = stablehlo.log_plus_one %1232 : tensor<2x256x30x40xbf16>
      %1234 = stablehlo.broadcast_in_dim %1231, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1235 = stablehlo.compare  GT, %1234, %898,  FLOAT : (tensor<2x256x30x40xbf16>, tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xi1>
      %1236 = stablehlo.broadcast_in_dim %1235, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xi1>) -> tensor<2x256x30x40xi1>
      %1237 = stablehlo.broadcast_in_dim %1233, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1238 = stablehlo.select %1236, %1234, %1237 : tensor<2x256x30x40xi1>, tensor<2x256x30x40xbf16>
      %1239 = stablehlo.tanh %1238 : tensor<2x256x30x40xbf16>
      %1240 = stablehlo.multiply %1231, %1239 : tensor<2x256x30x40xbf16>
      %1241 = stablehlo.add %1196, %1240 : tensor<2x256x30x40xbf16>
      %1242 = stablehlo.convolution(%1241, %arg605) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x30x40xbf16>, tensor<256x256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1243 = stablehlo.broadcast_in_dim %1242, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1244 = stablehlo.broadcast_in_dim %arg883, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1245 = stablehlo.subtract %1243, %1244 : tensor<2x256x30x40xbf16>
      %1246 = stablehlo.broadcast_in_dim %1245, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1247 = stablehlo.broadcast_in_dim %arg884, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1248 = stablehlo.multiply %1246, %1247 : tensor<2x256x30x40xbf16>
      %1249 = stablehlo.broadcast_in_dim %1248, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1250 = stablehlo.broadcast_in_dim %arg885, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1251 = stablehlo.multiply %1249, %1250 : tensor<2x256x30x40xbf16>
      %1252 = stablehlo.broadcast_in_dim %1251, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1253 = stablehlo.broadcast_in_dim %arg886, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1254 = stablehlo.add %1252, %1253 : tensor<2x256x30x40xbf16>
      %1255 = stablehlo.exponential %1254 : tensor<2x256x30x40xbf16>
      %1256 = stablehlo.log_plus_one %1255 : tensor<2x256x30x40xbf16>
      %1257 = stablehlo.broadcast_in_dim %1254, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1258 = stablehlo.compare  GT, %1257, %898,  FLOAT : (tensor<2x256x30x40xbf16>, tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xi1>
      %1259 = stablehlo.broadcast_in_dim %1258, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xi1>) -> tensor<2x256x30x40xi1>
      %1260 = stablehlo.broadcast_in_dim %1256, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1261 = stablehlo.select %1259, %1257, %1260 : tensor<2x256x30x40xi1>, tensor<2x256x30x40xbf16>
      %1262 = stablehlo.tanh %1261 : tensor<2x256x30x40xbf16>
      %1263 = stablehlo.multiply %1254, %1262 : tensor<2x256x30x40xbf16>
      %1264 = stablehlo.convolution(%1263, %arg606) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x30x40xbf16>, tensor<256x256x3x3xbf16>) -> tensor<2x256x30x40xbf16>
      %1265 = stablehlo.broadcast_in_dim %1264, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1266 = stablehlo.broadcast_in_dim %arg887, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1267 = stablehlo.subtract %1265, %1266 : tensor<2x256x30x40xbf16>
      %1268 = stablehlo.broadcast_in_dim %1267, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1269 = stablehlo.broadcast_in_dim %arg888, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1270 = stablehlo.multiply %1268, %1269 : tensor<2x256x30x40xbf16>
      %1271 = stablehlo.broadcast_in_dim %1270, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1272 = stablehlo.broadcast_in_dim %arg889, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1273 = stablehlo.multiply %1271, %1272 : tensor<2x256x30x40xbf16>
      %1274 = stablehlo.broadcast_in_dim %1273, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1275 = stablehlo.broadcast_in_dim %arg890, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1276 = stablehlo.add %1274, %1275 : tensor<2x256x30x40xbf16>
      %1277 = stablehlo.exponential %1276 : tensor<2x256x30x40xbf16>
      %1278 = stablehlo.log_plus_one %1277 : tensor<2x256x30x40xbf16>
      %1279 = stablehlo.broadcast_in_dim %1276, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1280 = stablehlo.compare  GT, %1279, %898,  FLOAT : (tensor<2x256x30x40xbf16>, tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xi1>
      %1281 = stablehlo.broadcast_in_dim %1280, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xi1>) -> tensor<2x256x30x40xi1>
      %1282 = stablehlo.broadcast_in_dim %1278, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1283 = stablehlo.select %1281, %1279, %1282 : tensor<2x256x30x40xi1>, tensor<2x256x30x40xbf16>
      %1284 = stablehlo.tanh %1283 : tensor<2x256x30x40xbf16>
      %1285 = stablehlo.multiply %1276, %1284 : tensor<2x256x30x40xbf16>
      %1286 = stablehlo.add %1241, %1285 : tensor<2x256x30x40xbf16>
      %1287 = stablehlo.convolution(%1286, %arg607) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x30x40xbf16>, tensor<256x256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1288 = stablehlo.broadcast_in_dim %1287, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1289 = stablehlo.broadcast_in_dim %arg891, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1290 = stablehlo.subtract %1288, %1289 : tensor<2x256x30x40xbf16>
      %1291 = stablehlo.broadcast_in_dim %1290, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1292 = stablehlo.broadcast_in_dim %arg892, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1293 = stablehlo.multiply %1291, %1292 : tensor<2x256x30x40xbf16>
      %1294 = stablehlo.broadcast_in_dim %1293, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1295 = stablehlo.broadcast_in_dim %arg893, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1296 = stablehlo.multiply %1294, %1295 : tensor<2x256x30x40xbf16>
      %1297 = stablehlo.broadcast_in_dim %1296, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1298 = stablehlo.broadcast_in_dim %arg894, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1299 = stablehlo.add %1297, %1298 : tensor<2x256x30x40xbf16>
      %1300 = stablehlo.exponential %1299 : tensor<2x256x30x40xbf16>
      %1301 = stablehlo.log_plus_one %1300 : tensor<2x256x30x40xbf16>
      %1302 = stablehlo.broadcast_in_dim %1299, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1303 = stablehlo.compare  GT, %1302, %898,  FLOAT : (tensor<2x256x30x40xbf16>, tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xi1>
      %1304 = stablehlo.broadcast_in_dim %1303, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xi1>) -> tensor<2x256x30x40xi1>
      %1305 = stablehlo.broadcast_in_dim %1301, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1306 = stablehlo.select %1304, %1302, %1305 : tensor<2x256x30x40xi1>, tensor<2x256x30x40xbf16>
      %1307 = stablehlo.tanh %1306 : tensor<2x256x30x40xbf16>
      %1308 = stablehlo.multiply %1299, %1307 : tensor<2x256x30x40xbf16>
      %1309 = stablehlo.concatenate %1308, %904, dim = 1 : (tensor<2x256x30x40xbf16>, tensor<2x256x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %1310 = stablehlo.convolution(%1309, %arg608) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x512x30x40xbf16>, tensor<512x512x1x1xbf16>) -> tensor<2x512x30x40xbf16>
      %1311 = stablehlo.broadcast_in_dim %1310, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %1312 = stablehlo.broadcast_in_dim %arg895, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x30x40xbf16>
      %1313 = stablehlo.subtract %1311, %1312 : tensor<2x512x30x40xbf16>
      %1314 = stablehlo.broadcast_in_dim %1313, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %1315 = stablehlo.broadcast_in_dim %arg896, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x30x40xbf16>
      %1316 = stablehlo.multiply %1314, %1315 : tensor<2x512x30x40xbf16>
      %1317 = stablehlo.broadcast_in_dim %1316, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %1318 = stablehlo.broadcast_in_dim %arg897, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x30x40xbf16>
      %1319 = stablehlo.multiply %1317, %1318 : tensor<2x512x30x40xbf16>
      %1320 = stablehlo.broadcast_in_dim %1319, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %1321 = stablehlo.broadcast_in_dim %arg898, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x30x40xbf16>
      %1322 = stablehlo.add %1320, %1321 : tensor<2x512x30x40xbf16>
      %1323 = stablehlo.exponential %1322 : tensor<2x512x30x40xbf16>
      %1324 = stablehlo.log_plus_one %1323 : tensor<2x512x30x40xbf16>
      %1325 = stablehlo.broadcast_in_dim %1322, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %1326 = stablehlo.compare  GT, %1325, %875,  FLOAT : (tensor<2x512x30x40xbf16>, tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xi1>
      %1327 = stablehlo.broadcast_in_dim %1326, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xi1>) -> tensor<2x512x30x40xi1>
      %1328 = stablehlo.broadcast_in_dim %1324, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %1329 = stablehlo.select %1327, %1325, %1328 : tensor<2x512x30x40xi1>, tensor<2x512x30x40xbf16>
      %1330 = stablehlo.tanh %1329 : tensor<2x512x30x40xbf16>
      %1331 = stablehlo.multiply %1322, %1330 : tensor<2x512x30x40xbf16>
      %1332 = stablehlo.convolution(%1331, %arg609) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [2, 2], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x512x30x40xbf16>, tensor<1024x512x3x3xbf16>) -> tensor<2x1024x15x20xbf16>
      %1333 = stablehlo.broadcast_in_dim %1332, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %1334 = stablehlo.broadcast_in_dim %arg899, dims = [1, 2, 3] : (tensor<1024x1x1xbf16>) -> tensor<2x1024x15x20xbf16>
      %1335 = stablehlo.subtract %1333, %1334 : tensor<2x1024x15x20xbf16>
      %1336 = stablehlo.broadcast_in_dim %1335, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %1337 = stablehlo.broadcast_in_dim %arg900, dims = [1, 2, 3] : (tensor<1024x1x1xbf16>) -> tensor<2x1024x15x20xbf16>
      %1338 = stablehlo.multiply %1336, %1337 : tensor<2x1024x15x20xbf16>
      %1339 = stablehlo.broadcast_in_dim %1338, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %1340 = stablehlo.broadcast_in_dim %arg901, dims = [1, 2, 3] : (tensor<1024x1x1xbf16>) -> tensor<2x1024x15x20xbf16>
      %1341 = stablehlo.multiply %1339, %1340 : tensor<2x1024x15x20xbf16>
      %1342 = stablehlo.broadcast_in_dim %1341, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %1343 = stablehlo.broadcast_in_dim %arg902, dims = [1, 2, 3] : (tensor<1024x1x1xbf16>) -> tensor<2x1024x15x20xbf16>
      %1344 = stablehlo.add %1342, %1343 : tensor<2x1024x15x20xbf16>
      %1345 = stablehlo.exponential %1344 : tensor<2x1024x15x20xbf16>
      %1346 = stablehlo.log_plus_one %1345 : tensor<2x1024x15x20xbf16>
      %1347 = stablehlo.broadcast_in_dim %1344, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %1348 = stablehlo.broadcast_in_dim %16, dims = [] : (tensor<bf16>) -> tensor<2x1024x15x20xbf16>
      %1349 = stablehlo.compare  GT, %1347, %1348,  FLOAT : (tensor<2x1024x15x20xbf16>, tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xi1>
      %1350 = stablehlo.broadcast_in_dim %1349, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xi1>) -> tensor<2x1024x15x20xi1>
      %1351 = stablehlo.broadcast_in_dim %1346, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %1352 = stablehlo.select %1350, %1347, %1351 : tensor<2x1024x15x20xi1>, tensor<2x1024x15x20xbf16>
      %1353 = stablehlo.tanh %1352 : tensor<2x1024x15x20xbf16>
      %1354 = stablehlo.multiply %1344, %1353 : tensor<2x1024x15x20xbf16>
      %1355 = stablehlo.convolution(%1354, %arg610) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x1024x15x20xbf16>, tensor<512x1024x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1356 = stablehlo.broadcast_in_dim %1355, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1357 = stablehlo.broadcast_in_dim %arg903, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1358 = stablehlo.subtract %1356, %1357 : tensor<2x512x15x20xbf16>
      %1359 = stablehlo.broadcast_in_dim %1358, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1360 = stablehlo.broadcast_in_dim %arg904, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1361 = stablehlo.multiply %1359, %1360 : tensor<2x512x15x20xbf16>
      %1362 = stablehlo.broadcast_in_dim %1361, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1363 = stablehlo.broadcast_in_dim %arg905, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1364 = stablehlo.multiply %1362, %1363 : tensor<2x512x15x20xbf16>
      %1365 = stablehlo.broadcast_in_dim %1364, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1366 = stablehlo.broadcast_in_dim %arg906, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1367 = stablehlo.add %1365, %1366 : tensor<2x512x15x20xbf16>
      %1368 = stablehlo.exponential %1367 : tensor<2x512x15x20xbf16>
      %1369 = stablehlo.log_plus_one %1368 : tensor<2x512x15x20xbf16>
      %1370 = stablehlo.broadcast_in_dim %1367, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1371 = stablehlo.broadcast_in_dim %16, dims = [] : (tensor<bf16>) -> tensor<2x512x15x20xbf16>
      %1372 = stablehlo.compare  GT, %1370, %1371,  FLOAT : (tensor<2x512x15x20xbf16>, tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xi1>
      %1373 = stablehlo.broadcast_in_dim %1372, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xi1>) -> tensor<2x512x15x20xi1>
      %1374 = stablehlo.broadcast_in_dim %1369, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1375 = stablehlo.select %1373, %1370, %1374 : tensor<2x512x15x20xi1>, tensor<2x512x15x20xbf16>
      %1376 = stablehlo.tanh %1375 : tensor<2x512x15x20xbf16>
      %1377 = stablehlo.multiply %1367, %1376 : tensor<2x512x15x20xbf16>
      %1378 = stablehlo.convolution(%1354, %arg611) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x1024x15x20xbf16>, tensor<512x1024x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1379 = stablehlo.broadcast_in_dim %1378, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1380 = stablehlo.broadcast_in_dim %arg907, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1381 = stablehlo.subtract %1379, %1380 : tensor<2x512x15x20xbf16>
      %1382 = stablehlo.broadcast_in_dim %1381, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1383 = stablehlo.broadcast_in_dim %arg908, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1384 = stablehlo.multiply %1382, %1383 : tensor<2x512x15x20xbf16>
      %1385 = stablehlo.broadcast_in_dim %1384, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1386 = stablehlo.broadcast_in_dim %arg909, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1387 = stablehlo.multiply %1385, %1386 : tensor<2x512x15x20xbf16>
      %1388 = stablehlo.broadcast_in_dim %1387, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1389 = stablehlo.broadcast_in_dim %arg910, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1390 = stablehlo.add %1388, %1389 : tensor<2x512x15x20xbf16>
      %1391 = stablehlo.exponential %1390 : tensor<2x512x15x20xbf16>
      %1392 = stablehlo.log_plus_one %1391 : tensor<2x512x15x20xbf16>
      %1393 = stablehlo.broadcast_in_dim %1390, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1394 = stablehlo.compare  GT, %1393, %1371,  FLOAT : (tensor<2x512x15x20xbf16>, tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xi1>
      %1395 = stablehlo.broadcast_in_dim %1394, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xi1>) -> tensor<2x512x15x20xi1>
      %1396 = stablehlo.broadcast_in_dim %1392, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1397 = stablehlo.select %1395, %1393, %1396 : tensor<2x512x15x20xi1>, tensor<2x512x15x20xbf16>
      %1398 = stablehlo.tanh %1397 : tensor<2x512x15x20xbf16>
      %1399 = stablehlo.multiply %1390, %1398 : tensor<2x512x15x20xbf16>
      %1400 = stablehlo.convolution(%1399, %arg612) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x512x15x20xbf16>, tensor<512x512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1401 = stablehlo.broadcast_in_dim %1400, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1402 = stablehlo.broadcast_in_dim %arg911, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1403 = stablehlo.subtract %1401, %1402 : tensor<2x512x15x20xbf16>
      %1404 = stablehlo.broadcast_in_dim %1403, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1405 = stablehlo.broadcast_in_dim %arg912, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1406 = stablehlo.multiply %1404, %1405 : tensor<2x512x15x20xbf16>
      %1407 = stablehlo.broadcast_in_dim %1406, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1408 = stablehlo.broadcast_in_dim %arg913, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1409 = stablehlo.multiply %1407, %1408 : tensor<2x512x15x20xbf16>
      %1410 = stablehlo.broadcast_in_dim %1409, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1411 = stablehlo.broadcast_in_dim %arg914, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1412 = stablehlo.add %1410, %1411 : tensor<2x512x15x20xbf16>
      %1413 = stablehlo.exponential %1412 : tensor<2x512x15x20xbf16>
      %1414 = stablehlo.log_plus_one %1413 : tensor<2x512x15x20xbf16>
      %1415 = stablehlo.broadcast_in_dim %1412, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1416 = stablehlo.compare  GT, %1415, %1371,  FLOAT : (tensor<2x512x15x20xbf16>, tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xi1>
      %1417 = stablehlo.broadcast_in_dim %1416, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xi1>) -> tensor<2x512x15x20xi1>
      %1418 = stablehlo.broadcast_in_dim %1414, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1419 = stablehlo.select %1417, %1415, %1418 : tensor<2x512x15x20xi1>, tensor<2x512x15x20xbf16>
      %1420 = stablehlo.tanh %1419 : tensor<2x512x15x20xbf16>
      %1421 = stablehlo.multiply %1412, %1420 : tensor<2x512x15x20xbf16>
      %1422 = stablehlo.convolution(%1421, %arg613) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x512x15x20xbf16>, tensor<512x512x3x3xbf16>) -> tensor<2x512x15x20xbf16>
      %1423 = stablehlo.broadcast_in_dim %1422, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1424 = stablehlo.broadcast_in_dim %arg915, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1425 = stablehlo.subtract %1423, %1424 : tensor<2x512x15x20xbf16>
      %1426 = stablehlo.broadcast_in_dim %1425, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1427 = stablehlo.broadcast_in_dim %arg916, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1428 = stablehlo.multiply %1426, %1427 : tensor<2x512x15x20xbf16>
      %1429 = stablehlo.broadcast_in_dim %1428, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1430 = stablehlo.broadcast_in_dim %arg917, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1431 = stablehlo.multiply %1429, %1430 : tensor<2x512x15x20xbf16>
      %1432 = stablehlo.broadcast_in_dim %1431, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1433 = stablehlo.broadcast_in_dim %arg918, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1434 = stablehlo.add %1432, %1433 : tensor<2x512x15x20xbf16>
      %1435 = stablehlo.exponential %1434 : tensor<2x512x15x20xbf16>
      %1436 = stablehlo.log_plus_one %1435 : tensor<2x512x15x20xbf16>
      %1437 = stablehlo.broadcast_in_dim %1434, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1438 = stablehlo.compare  GT, %1437, %1371,  FLOAT : (tensor<2x512x15x20xbf16>, tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xi1>
      %1439 = stablehlo.broadcast_in_dim %1438, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xi1>) -> tensor<2x512x15x20xi1>
      %1440 = stablehlo.broadcast_in_dim %1436, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1441 = stablehlo.select %1439, %1437, %1440 : tensor<2x512x15x20xi1>, tensor<2x512x15x20xbf16>
      %1442 = stablehlo.tanh %1441 : tensor<2x512x15x20xbf16>
      %1443 = stablehlo.multiply %1434, %1442 : tensor<2x512x15x20xbf16>
      %1444 = stablehlo.add %1399, %1443 : tensor<2x512x15x20xbf16>
      %1445 = stablehlo.convolution(%1444, %arg614) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x512x15x20xbf16>, tensor<512x512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1446 = stablehlo.broadcast_in_dim %1445, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1447 = stablehlo.broadcast_in_dim %arg919, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1448 = stablehlo.subtract %1446, %1447 : tensor<2x512x15x20xbf16>
      %1449 = stablehlo.broadcast_in_dim %1448, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1450 = stablehlo.broadcast_in_dim %arg920, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1451 = stablehlo.multiply %1449, %1450 : tensor<2x512x15x20xbf16>
      %1452 = stablehlo.broadcast_in_dim %1451, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1453 = stablehlo.broadcast_in_dim %arg921, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1454 = stablehlo.multiply %1452, %1453 : tensor<2x512x15x20xbf16>
      %1455 = stablehlo.broadcast_in_dim %1454, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1456 = stablehlo.broadcast_in_dim %arg922, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1457 = stablehlo.add %1455, %1456 : tensor<2x512x15x20xbf16>
      %1458 = stablehlo.exponential %1457 : tensor<2x512x15x20xbf16>
      %1459 = stablehlo.log_plus_one %1458 : tensor<2x512x15x20xbf16>
      %1460 = stablehlo.broadcast_in_dim %1457, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1461 = stablehlo.compare  GT, %1460, %1371,  FLOAT : (tensor<2x512x15x20xbf16>, tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xi1>
      %1462 = stablehlo.broadcast_in_dim %1461, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xi1>) -> tensor<2x512x15x20xi1>
      %1463 = stablehlo.broadcast_in_dim %1459, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1464 = stablehlo.select %1462, %1460, %1463 : tensor<2x512x15x20xi1>, tensor<2x512x15x20xbf16>
      %1465 = stablehlo.tanh %1464 : tensor<2x512x15x20xbf16>
      %1466 = stablehlo.multiply %1457, %1465 : tensor<2x512x15x20xbf16>
      %1467 = stablehlo.convolution(%1466, %arg615) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x512x15x20xbf16>, tensor<512x512x3x3xbf16>) -> tensor<2x512x15x20xbf16>
      %1468 = stablehlo.broadcast_in_dim %1467, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1469 = stablehlo.broadcast_in_dim %arg923, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1470 = stablehlo.subtract %1468, %1469 : tensor<2x512x15x20xbf16>
      %1471 = stablehlo.broadcast_in_dim %1470, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1472 = stablehlo.broadcast_in_dim %arg924, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1473 = stablehlo.multiply %1471, %1472 : tensor<2x512x15x20xbf16>
      %1474 = stablehlo.broadcast_in_dim %1473, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1475 = stablehlo.broadcast_in_dim %arg925, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1476 = stablehlo.multiply %1474, %1475 : tensor<2x512x15x20xbf16>
      %1477 = stablehlo.broadcast_in_dim %1476, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1478 = stablehlo.broadcast_in_dim %arg926, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1479 = stablehlo.add %1477, %1478 : tensor<2x512x15x20xbf16>
      %1480 = stablehlo.exponential %1479 : tensor<2x512x15x20xbf16>
      %1481 = stablehlo.log_plus_one %1480 : tensor<2x512x15x20xbf16>
      %1482 = stablehlo.broadcast_in_dim %1479, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1483 = stablehlo.compare  GT, %1482, %1371,  FLOAT : (tensor<2x512x15x20xbf16>, tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xi1>
      %1484 = stablehlo.broadcast_in_dim %1483, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xi1>) -> tensor<2x512x15x20xi1>
      %1485 = stablehlo.broadcast_in_dim %1481, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1486 = stablehlo.select %1484, %1482, %1485 : tensor<2x512x15x20xi1>, tensor<2x512x15x20xbf16>
      %1487 = stablehlo.tanh %1486 : tensor<2x512x15x20xbf16>
      %1488 = stablehlo.multiply %1479, %1487 : tensor<2x512x15x20xbf16>
      %1489 = stablehlo.add %1444, %1488 : tensor<2x512x15x20xbf16>
      %1490 = stablehlo.convolution(%1489, %arg616) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x512x15x20xbf16>, tensor<512x512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1491 = stablehlo.broadcast_in_dim %1490, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1492 = stablehlo.broadcast_in_dim %arg927, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1493 = stablehlo.subtract %1491, %1492 : tensor<2x512x15x20xbf16>
      %1494 = stablehlo.broadcast_in_dim %1493, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1495 = stablehlo.broadcast_in_dim %arg928, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1496 = stablehlo.multiply %1494, %1495 : tensor<2x512x15x20xbf16>
      %1497 = stablehlo.broadcast_in_dim %1496, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1498 = stablehlo.broadcast_in_dim %arg929, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1499 = stablehlo.multiply %1497, %1498 : tensor<2x512x15x20xbf16>
      %1500 = stablehlo.broadcast_in_dim %1499, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1501 = stablehlo.broadcast_in_dim %arg930, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1502 = stablehlo.add %1500, %1501 : tensor<2x512x15x20xbf16>
      %1503 = stablehlo.exponential %1502 : tensor<2x512x15x20xbf16>
      %1504 = stablehlo.log_plus_one %1503 : tensor<2x512x15x20xbf16>
      %1505 = stablehlo.broadcast_in_dim %1502, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1506 = stablehlo.compare  GT, %1505, %1371,  FLOAT : (tensor<2x512x15x20xbf16>, tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xi1>
      %1507 = stablehlo.broadcast_in_dim %1506, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xi1>) -> tensor<2x512x15x20xi1>
      %1508 = stablehlo.broadcast_in_dim %1504, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1509 = stablehlo.select %1507, %1505, %1508 : tensor<2x512x15x20xi1>, tensor<2x512x15x20xbf16>
      %1510 = stablehlo.tanh %1509 : tensor<2x512x15x20xbf16>
      %1511 = stablehlo.multiply %1502, %1510 : tensor<2x512x15x20xbf16>
      %1512 = stablehlo.convolution(%1511, %arg617) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x512x15x20xbf16>, tensor<512x512x3x3xbf16>) -> tensor<2x512x15x20xbf16>
      %1513 = stablehlo.broadcast_in_dim %1512, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1514 = stablehlo.broadcast_in_dim %arg931, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1515 = stablehlo.subtract %1513, %1514 : tensor<2x512x15x20xbf16>
      %1516 = stablehlo.broadcast_in_dim %1515, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1517 = stablehlo.broadcast_in_dim %arg932, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1518 = stablehlo.multiply %1516, %1517 : tensor<2x512x15x20xbf16>
      %1519 = stablehlo.broadcast_in_dim %1518, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1520 = stablehlo.broadcast_in_dim %arg933, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1521 = stablehlo.multiply %1519, %1520 : tensor<2x512x15x20xbf16>
      %1522 = stablehlo.broadcast_in_dim %1521, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1523 = stablehlo.broadcast_in_dim %arg934, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1524 = stablehlo.add %1522, %1523 : tensor<2x512x15x20xbf16>
      %1525 = stablehlo.exponential %1524 : tensor<2x512x15x20xbf16>
      %1526 = stablehlo.log_plus_one %1525 : tensor<2x512x15x20xbf16>
      %1527 = stablehlo.broadcast_in_dim %1524, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1528 = stablehlo.compare  GT, %1527, %1371,  FLOAT : (tensor<2x512x15x20xbf16>, tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xi1>
      %1529 = stablehlo.broadcast_in_dim %1528, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xi1>) -> tensor<2x512x15x20xi1>
      %1530 = stablehlo.broadcast_in_dim %1526, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1531 = stablehlo.select %1529, %1527, %1530 : tensor<2x512x15x20xi1>, tensor<2x512x15x20xbf16>
      %1532 = stablehlo.tanh %1531 : tensor<2x512x15x20xbf16>
      %1533 = stablehlo.multiply %1524, %1532 : tensor<2x512x15x20xbf16>
      %1534 = stablehlo.add %1489, %1533 : tensor<2x512x15x20xbf16>
      %1535 = stablehlo.convolution(%1534, %arg618) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x512x15x20xbf16>, tensor<512x512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1536 = stablehlo.broadcast_in_dim %1535, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1537 = stablehlo.broadcast_in_dim %arg935, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1538 = stablehlo.subtract %1536, %1537 : tensor<2x512x15x20xbf16>
      %1539 = stablehlo.broadcast_in_dim %1538, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1540 = stablehlo.broadcast_in_dim %arg936, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1541 = stablehlo.multiply %1539, %1540 : tensor<2x512x15x20xbf16>
      %1542 = stablehlo.broadcast_in_dim %1541, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1543 = stablehlo.broadcast_in_dim %arg937, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1544 = stablehlo.multiply %1542, %1543 : tensor<2x512x15x20xbf16>
      %1545 = stablehlo.broadcast_in_dim %1544, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1546 = stablehlo.broadcast_in_dim %arg938, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1547 = stablehlo.add %1545, %1546 : tensor<2x512x15x20xbf16>
      %1548 = stablehlo.exponential %1547 : tensor<2x512x15x20xbf16>
      %1549 = stablehlo.log_plus_one %1548 : tensor<2x512x15x20xbf16>
      %1550 = stablehlo.broadcast_in_dim %1547, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1551 = stablehlo.compare  GT, %1550, %1371,  FLOAT : (tensor<2x512x15x20xbf16>, tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xi1>
      %1552 = stablehlo.broadcast_in_dim %1551, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xi1>) -> tensor<2x512x15x20xi1>
      %1553 = stablehlo.broadcast_in_dim %1549, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1554 = stablehlo.select %1552, %1550, %1553 : tensor<2x512x15x20xi1>, tensor<2x512x15x20xbf16>
      %1555 = stablehlo.tanh %1554 : tensor<2x512x15x20xbf16>
      %1556 = stablehlo.multiply %1547, %1555 : tensor<2x512x15x20xbf16>
      %1557 = stablehlo.convolution(%1556, %arg619) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x512x15x20xbf16>, tensor<512x512x3x3xbf16>) -> tensor<2x512x15x20xbf16>
      %1558 = stablehlo.broadcast_in_dim %1557, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1559 = stablehlo.broadcast_in_dim %arg939, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1560 = stablehlo.subtract %1558, %1559 : tensor<2x512x15x20xbf16>
      %1561 = stablehlo.broadcast_in_dim %1560, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1562 = stablehlo.broadcast_in_dim %arg940, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1563 = stablehlo.multiply %1561, %1562 : tensor<2x512x15x20xbf16>
      %1564 = stablehlo.broadcast_in_dim %1563, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1565 = stablehlo.broadcast_in_dim %arg941, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1566 = stablehlo.multiply %1564, %1565 : tensor<2x512x15x20xbf16>
      %1567 = stablehlo.broadcast_in_dim %1566, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1568 = stablehlo.broadcast_in_dim %arg942, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1569 = stablehlo.add %1567, %1568 : tensor<2x512x15x20xbf16>
      %1570 = stablehlo.exponential %1569 : tensor<2x512x15x20xbf16>
      %1571 = stablehlo.log_plus_one %1570 : tensor<2x512x15x20xbf16>
      %1572 = stablehlo.broadcast_in_dim %1569, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1573 = stablehlo.compare  GT, %1572, %1371,  FLOAT : (tensor<2x512x15x20xbf16>, tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xi1>
      %1574 = stablehlo.broadcast_in_dim %1573, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xi1>) -> tensor<2x512x15x20xi1>
      %1575 = stablehlo.broadcast_in_dim %1571, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1576 = stablehlo.select %1574, %1572, %1575 : tensor<2x512x15x20xi1>, tensor<2x512x15x20xbf16>
      %1577 = stablehlo.tanh %1576 : tensor<2x512x15x20xbf16>
      %1578 = stablehlo.multiply %1569, %1577 : tensor<2x512x15x20xbf16>
      %1579 = stablehlo.add %1534, %1578 : tensor<2x512x15x20xbf16>
      %1580 = stablehlo.convolution(%1579, %arg620) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x512x15x20xbf16>, tensor<512x512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1581 = stablehlo.broadcast_in_dim %1580, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1582 = stablehlo.broadcast_in_dim %arg943, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1583 = stablehlo.subtract %1581, %1582 : tensor<2x512x15x20xbf16>
      %1584 = stablehlo.broadcast_in_dim %1583, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1585 = stablehlo.broadcast_in_dim %arg944, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1586 = stablehlo.multiply %1584, %1585 : tensor<2x512x15x20xbf16>
      %1587 = stablehlo.broadcast_in_dim %1586, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1588 = stablehlo.broadcast_in_dim %arg945, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1589 = stablehlo.multiply %1587, %1588 : tensor<2x512x15x20xbf16>
      %1590 = stablehlo.broadcast_in_dim %1589, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1591 = stablehlo.broadcast_in_dim %arg946, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1592 = stablehlo.add %1590, %1591 : tensor<2x512x15x20xbf16>
      %1593 = stablehlo.exponential %1592 : tensor<2x512x15x20xbf16>
      %1594 = stablehlo.log_plus_one %1593 : tensor<2x512x15x20xbf16>
      %1595 = stablehlo.broadcast_in_dim %1592, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1596 = stablehlo.compare  GT, %1595, %1371,  FLOAT : (tensor<2x512x15x20xbf16>, tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xi1>
      %1597 = stablehlo.broadcast_in_dim %1596, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xi1>) -> tensor<2x512x15x20xi1>
      %1598 = stablehlo.broadcast_in_dim %1594, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1599 = stablehlo.select %1597, %1595, %1598 : tensor<2x512x15x20xi1>, tensor<2x512x15x20xbf16>
      %1600 = stablehlo.tanh %1599 : tensor<2x512x15x20xbf16>
      %1601 = stablehlo.multiply %1592, %1600 : tensor<2x512x15x20xbf16>
      %1602 = stablehlo.concatenate %1601, %1377, dim = 1 : (tensor<2x512x15x20xbf16>, tensor<2x512x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %1603 = stablehlo.convolution(%1602, %arg621) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x1024x15x20xbf16>, tensor<1024x1024x1x1xbf16>) -> tensor<2x1024x15x20xbf16>
      %1604 = stablehlo.broadcast_in_dim %1603, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %1605 = stablehlo.broadcast_in_dim %arg947, dims = [1, 2, 3] : (tensor<1024x1x1xbf16>) -> tensor<2x1024x15x20xbf16>
      %1606 = stablehlo.subtract %1604, %1605 : tensor<2x1024x15x20xbf16>
      %1607 = stablehlo.broadcast_in_dim %1606, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %1608 = stablehlo.broadcast_in_dim %arg948, dims = [1, 2, 3] : (tensor<1024x1x1xbf16>) -> tensor<2x1024x15x20xbf16>
      %1609 = stablehlo.multiply %1607, %1608 : tensor<2x1024x15x20xbf16>
      %1610 = stablehlo.broadcast_in_dim %1609, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %1611 = stablehlo.broadcast_in_dim %arg949, dims = [1, 2, 3] : (tensor<1024x1x1xbf16>) -> tensor<2x1024x15x20xbf16>
      %1612 = stablehlo.multiply %1610, %1611 : tensor<2x1024x15x20xbf16>
      %1613 = stablehlo.broadcast_in_dim %1612, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %1614 = stablehlo.broadcast_in_dim %arg950, dims = [1, 2, 3] : (tensor<1024x1x1xbf16>) -> tensor<2x1024x15x20xbf16>
      %1615 = stablehlo.add %1613, %1614 : tensor<2x1024x15x20xbf16>
      %1616 = stablehlo.exponential %1615 : tensor<2x1024x15x20xbf16>
      %1617 = stablehlo.log_plus_one %1616 : tensor<2x1024x15x20xbf16>
      %1618 = stablehlo.broadcast_in_dim %1615, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %1619 = stablehlo.compare  GT, %1618, %1348,  FLOAT : (tensor<2x1024x15x20xbf16>, tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xi1>
      %1620 = stablehlo.broadcast_in_dim %1619, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xi1>) -> tensor<2x1024x15x20xi1>
      %1621 = stablehlo.broadcast_in_dim %1617, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %1622 = stablehlo.select %1620, %1618, %1621 : tensor<2x1024x15x20xi1>, tensor<2x1024x15x20xbf16>
      %1623 = stablehlo.tanh %1622 : tensor<2x1024x15x20xbf16>
      %1624 = stablehlo.multiply %1615, %1623 : tensor<2x1024x15x20xbf16>
      %1625 = stablehlo.convolution(%1624, %arg622) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x1024x15x20xbf16>, tensor<512x1024x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1626 = stablehlo.broadcast_in_dim %1625, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1627 = stablehlo.broadcast_in_dim %arg951, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1628 = stablehlo.subtract %1626, %1627 : tensor<2x512x15x20xbf16>
      %1629 = stablehlo.broadcast_in_dim %1628, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1630 = stablehlo.broadcast_in_dim %arg952, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1631 = stablehlo.multiply %1629, %1630 : tensor<2x512x15x20xbf16>
      %1632 = stablehlo.broadcast_in_dim %1631, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1633 = stablehlo.broadcast_in_dim %arg953, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1634 = stablehlo.multiply %1632, %1633 : tensor<2x512x15x20xbf16>
      %1635 = stablehlo.broadcast_in_dim %1634, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1636 = stablehlo.broadcast_in_dim %arg954, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1637 = stablehlo.add %1635, %1636 : tensor<2x512x15x20xbf16>
      %1638 = stablehlo.convert %c_0 : (tensor<i64>) -> tensor<bf16>
      %1639 = stablehlo.broadcast_in_dim %1638, dims = [] : (tensor<bf16>) -> tensor<2x512x15x20xbf16>
      %1640 = stablehlo.broadcast_in_dim %1637, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1641 = stablehlo.maximum %1639, %1640 : tensor<2x512x15x20xbf16>
      %1642 = stablehlo.minimum %1639, %1640 : tensor<2x512x15x20xbf16>
      %1643 = stablehlo.convert %cst_1 : (tensor<1xf64>) -> tensor<1xbf16>
      %1644 = stablehlo.reshape %1643 : (tensor<1xbf16>) -> tensor<bf16>
      %1645 = stablehlo.broadcast_in_dim %1642, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1646 = stablehlo.broadcast_in_dim %1644, dims = [] : (tensor<bf16>) -> tensor<2x512x15x20xbf16>
      %1647 = stablehlo.multiply %1645, %1646 : tensor<2x512x15x20xbf16>
      %1648 = stablehlo.add %1641, %1647 : tensor<2x512x15x20xbf16>
      %1649 = stablehlo.convolution(%1648, %arg623) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x512x15x20xbf16>, tensor<1024x512x3x3xbf16>) -> tensor<2x1024x15x20xbf16>
      %1650 = stablehlo.broadcast_in_dim %1649, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %1651 = stablehlo.broadcast_in_dim %arg955, dims = [1, 2, 3] : (tensor<1024x1x1xbf16>) -> tensor<2x1024x15x20xbf16>
      %1652 = stablehlo.subtract %1650, %1651 : tensor<2x1024x15x20xbf16>
      %1653 = stablehlo.broadcast_in_dim %1652, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %1654 = stablehlo.broadcast_in_dim %arg956, dims = [1, 2, 3] : (tensor<1024x1x1xbf16>) -> tensor<2x1024x15x20xbf16>
      %1655 = stablehlo.multiply %1653, %1654 : tensor<2x1024x15x20xbf16>
      %1656 = stablehlo.broadcast_in_dim %1655, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %1657 = stablehlo.broadcast_in_dim %arg957, dims = [1, 2, 3] : (tensor<1024x1x1xbf16>) -> tensor<2x1024x15x20xbf16>
      %1658 = stablehlo.multiply %1656, %1657 : tensor<2x1024x15x20xbf16>
      %1659 = stablehlo.broadcast_in_dim %1658, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %1660 = stablehlo.broadcast_in_dim %arg958, dims = [1, 2, 3] : (tensor<1024x1x1xbf16>) -> tensor<2x1024x15x20xbf16>
      %1661 = stablehlo.add %1659, %1660 : tensor<2x1024x15x20xbf16>
      %1662 = stablehlo.broadcast_in_dim %1638, dims = [] : (tensor<bf16>) -> tensor<2x1024x15x20xbf16>
      %1663 = stablehlo.broadcast_in_dim %1661, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %1664 = stablehlo.maximum %1662, %1663 : tensor<2x1024x15x20xbf16>
      %1665 = stablehlo.minimum %1662, %1663 : tensor<2x1024x15x20xbf16>
      %1666 = stablehlo.broadcast_in_dim %1665, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %1667 = stablehlo.broadcast_in_dim %1644, dims = [] : (tensor<bf16>) -> tensor<2x1024x15x20xbf16>
      %1668 = stablehlo.multiply %1666, %1667 : tensor<2x1024x15x20xbf16>
      %1669 = stablehlo.add %1664, %1668 : tensor<2x1024x15x20xbf16>
      %1670 = stablehlo.convolution(%1669, %arg624) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x1024x15x20xbf16>, tensor<512x1024x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1671 = stablehlo.broadcast_in_dim %1670, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1672 = stablehlo.broadcast_in_dim %arg959, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1673 = stablehlo.subtract %1671, %1672 : tensor<2x512x15x20xbf16>
      %1674 = stablehlo.broadcast_in_dim %1673, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1675 = stablehlo.broadcast_in_dim %arg960, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1676 = stablehlo.multiply %1674, %1675 : tensor<2x512x15x20xbf16>
      %1677 = stablehlo.broadcast_in_dim %1676, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1678 = stablehlo.broadcast_in_dim %arg961, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1679 = stablehlo.multiply %1677, %1678 : tensor<2x512x15x20xbf16>
      %1680 = stablehlo.broadcast_in_dim %1679, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1681 = stablehlo.broadcast_in_dim %arg962, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1682 = stablehlo.add %1680, %1681 : tensor<2x512x15x20xbf16>
      %1683 = stablehlo.broadcast_in_dim %1682, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1684 = stablehlo.maximum %1639, %1683 : tensor<2x512x15x20xbf16>
      %1685 = stablehlo.minimum %1639, %1683 : tensor<2x512x15x20xbf16>
      %1686 = stablehlo.broadcast_in_dim %1685, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1687 = stablehlo.multiply %1686, %1646 : tensor<2x512x15x20xbf16>
      %1688 = stablehlo.add %1684, %1687 : tensor<2x512x15x20xbf16>
      %1689 = "stablehlo.reduce_window"(%1688, %cst) <{padding = dense<[[0, 0], [0, 0], [2, 2], [2, 2]]> : tensor<4x2xi64>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 5, 5>, window_strides = array<i64: 1, 1, 1, 1>}> ({
      ^bb0(%arg1100: tensor<bf16>, %arg1101: tensor<bf16>):
        %2352 = stablehlo.maximum %arg1100, %arg1101 : tensor<bf16>
        stablehlo.return %2352 : tensor<bf16>
      }) : (tensor<2x512x15x20xbf16>, tensor<bf16>) -> tensor<2x512x15x20xbf16>
      %1690 = "stablehlo.reduce_window"(%1688, %cst) <{padding = dense<[[0, 0], [0, 0], [4, 4], [4, 4]]> : tensor<4x2xi64>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 9, 9>, window_strides = array<i64: 1, 1, 1, 1>}> ({
      ^bb0(%arg1100: tensor<bf16>, %arg1101: tensor<bf16>):
        %2352 = stablehlo.maximum %arg1100, %arg1101 : tensor<bf16>
        stablehlo.return %2352 : tensor<bf16>
      }) : (tensor<2x512x15x20xbf16>, tensor<bf16>) -> tensor<2x512x15x20xbf16>
      %1691 = "stablehlo.reduce_window"(%1688, %cst) <{padding = dense<[[0, 0], [0, 0], [6, 6], [6, 6]]> : tensor<4x2xi64>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 13, 13>, window_strides = array<i64: 1, 1, 1, 1>}> ({
      ^bb0(%arg1100: tensor<bf16>, %arg1101: tensor<bf16>):
        %2352 = stablehlo.maximum %arg1100, %arg1101 : tensor<bf16>
        stablehlo.return %2352 : tensor<bf16>
      }) : (tensor<2x512x15x20xbf16>, tensor<bf16>) -> tensor<2x512x15x20xbf16>
      %1692 = stablehlo.concatenate %1691, %1690, %1689, %1688, dim = 1 : (tensor<2x512x15x20xbf16>, tensor<2x512x15x20xbf16>, tensor<2x512x15x20xbf16>, tensor<2x512x15x20xbf16>) -> tensor<2x2048x15x20xbf16>
      %1693 = stablehlo.convolution(%1692, %arg625) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x2048x15x20xbf16>, tensor<512x2048x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1694 = stablehlo.broadcast_in_dim %1693, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1695 = stablehlo.broadcast_in_dim %arg963, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1696 = stablehlo.subtract %1694, %1695 : tensor<2x512x15x20xbf16>
      %1697 = stablehlo.broadcast_in_dim %1696, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1698 = stablehlo.broadcast_in_dim %arg964, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1699 = stablehlo.multiply %1697, %1698 : tensor<2x512x15x20xbf16>
      %1700 = stablehlo.broadcast_in_dim %1699, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1701 = stablehlo.broadcast_in_dim %arg965, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1702 = stablehlo.multiply %1700, %1701 : tensor<2x512x15x20xbf16>
      %1703 = stablehlo.broadcast_in_dim %1702, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1704 = stablehlo.broadcast_in_dim %arg966, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1705 = stablehlo.add %1703, %1704 : tensor<2x512x15x20xbf16>
      %1706 = stablehlo.broadcast_in_dim %1705, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1707 = stablehlo.maximum %1639, %1706 : tensor<2x512x15x20xbf16>
      %1708 = stablehlo.minimum %1639, %1706 : tensor<2x512x15x20xbf16>
      %1709 = stablehlo.broadcast_in_dim %1708, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1710 = stablehlo.multiply %1709, %1646 : tensor<2x512x15x20xbf16>
      %1711 = stablehlo.add %1707, %1710 : tensor<2x512x15x20xbf16>
      %1712 = stablehlo.convolution(%1711, %arg626) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x512x15x20xbf16>, tensor<1024x512x3x3xbf16>) -> tensor<2x1024x15x20xbf16>
      %1713 = stablehlo.broadcast_in_dim %1712, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %1714 = stablehlo.broadcast_in_dim %arg967, dims = [1, 2, 3] : (tensor<1024x1x1xbf16>) -> tensor<2x1024x15x20xbf16>
      %1715 = stablehlo.subtract %1713, %1714 : tensor<2x1024x15x20xbf16>
      %1716 = stablehlo.broadcast_in_dim %1715, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %1717 = stablehlo.broadcast_in_dim %arg968, dims = [1, 2, 3] : (tensor<1024x1x1xbf16>) -> tensor<2x1024x15x20xbf16>
      %1718 = stablehlo.multiply %1716, %1717 : tensor<2x1024x15x20xbf16>
      %1719 = stablehlo.broadcast_in_dim %1718, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %1720 = stablehlo.broadcast_in_dim %arg969, dims = [1, 2, 3] : (tensor<1024x1x1xbf16>) -> tensor<2x1024x15x20xbf16>
      %1721 = stablehlo.multiply %1719, %1720 : tensor<2x1024x15x20xbf16>
      %1722 = stablehlo.broadcast_in_dim %1721, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %1723 = stablehlo.broadcast_in_dim %arg970, dims = [1, 2, 3] : (tensor<1024x1x1xbf16>) -> tensor<2x1024x15x20xbf16>
      %1724 = stablehlo.add %1722, %1723 : tensor<2x1024x15x20xbf16>
      %1725 = stablehlo.broadcast_in_dim %1724, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %1726 = stablehlo.maximum %1662, %1725 : tensor<2x1024x15x20xbf16>
      %1727 = stablehlo.minimum %1662, %1725 : tensor<2x1024x15x20xbf16>
      %1728 = stablehlo.broadcast_in_dim %1727, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %1729 = stablehlo.multiply %1728, %1667 : tensor<2x1024x15x20xbf16>
      %1730 = stablehlo.add %1726, %1729 : tensor<2x1024x15x20xbf16>
      %1731 = stablehlo.convolution(%1730, %arg627) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x1024x15x20xbf16>, tensor<512x1024x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1732 = stablehlo.broadcast_in_dim %1731, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1733 = stablehlo.broadcast_in_dim %arg971, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1734 = stablehlo.subtract %1732, %1733 : tensor<2x512x15x20xbf16>
      %1735 = stablehlo.broadcast_in_dim %1734, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1736 = stablehlo.broadcast_in_dim %arg972, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1737 = stablehlo.multiply %1735, %1736 : tensor<2x512x15x20xbf16>
      %1738 = stablehlo.broadcast_in_dim %1737, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1739 = stablehlo.broadcast_in_dim %arg973, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1740 = stablehlo.multiply %1738, %1739 : tensor<2x512x15x20xbf16>
      %1741 = stablehlo.broadcast_in_dim %1740, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1742 = stablehlo.broadcast_in_dim %arg974, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %1743 = stablehlo.add %1741, %1742 : tensor<2x512x15x20xbf16>
      %1744 = stablehlo.broadcast_in_dim %1743, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1745 = stablehlo.maximum %1639, %1744 : tensor<2x512x15x20xbf16>
      %1746 = stablehlo.minimum %1639, %1744 : tensor<2x512x15x20xbf16>
      %1747 = stablehlo.broadcast_in_dim %1746, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %1748 = stablehlo.multiply %1747, %1646 : tensor<2x512x15x20xbf16>
      %1749 = stablehlo.add %1745, %1748 : tensor<2x512x15x20xbf16>
      %1750 = stablehlo.convolution(%1749, %arg628) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x512x15x20xbf16>, tensor<256x512x1x1xbf16>) -> tensor<2x256x15x20xbf16>
      %1751 = stablehlo.broadcast_in_dim %1750, dims = [0, 1, 2, 3] : (tensor<2x256x15x20xbf16>) -> tensor<2x256x15x20xbf16>
      %1752 = stablehlo.broadcast_in_dim %arg975, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x15x20xbf16>
      %1753 = stablehlo.subtract %1751, %1752 : tensor<2x256x15x20xbf16>
      %1754 = stablehlo.broadcast_in_dim %1753, dims = [0, 1, 2, 3] : (tensor<2x256x15x20xbf16>) -> tensor<2x256x15x20xbf16>
      %1755 = stablehlo.broadcast_in_dim %arg976, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x15x20xbf16>
      %1756 = stablehlo.multiply %1754, %1755 : tensor<2x256x15x20xbf16>
      %1757 = stablehlo.broadcast_in_dim %1756, dims = [0, 1, 2, 3] : (tensor<2x256x15x20xbf16>) -> tensor<2x256x15x20xbf16>
      %1758 = stablehlo.broadcast_in_dim %arg977, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x15x20xbf16>
      %1759 = stablehlo.multiply %1757, %1758 : tensor<2x256x15x20xbf16>
      %1760 = stablehlo.broadcast_in_dim %1759, dims = [0, 1, 2, 3] : (tensor<2x256x15x20xbf16>) -> tensor<2x256x15x20xbf16>
      %1761 = stablehlo.broadcast_in_dim %arg978, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x15x20xbf16>
      %1762 = stablehlo.add %1760, %1761 : tensor<2x256x15x20xbf16>
      %1763 = stablehlo.broadcast_in_dim %1638, dims = [] : (tensor<bf16>) -> tensor<2x256x15x20xbf16>
      %1764 = stablehlo.broadcast_in_dim %1762, dims = [0, 1, 2, 3] : (tensor<2x256x15x20xbf16>) -> tensor<2x256x15x20xbf16>
      %1765 = stablehlo.maximum %1763, %1764 : tensor<2x256x15x20xbf16>
      %1766 = stablehlo.minimum %1763, %1764 : tensor<2x256x15x20xbf16>
      %1767 = stablehlo.broadcast_in_dim %1766, dims = [0, 1, 2, 3] : (tensor<2x256x15x20xbf16>) -> tensor<2x256x15x20xbf16>
      %1768 = stablehlo.broadcast_in_dim %1644, dims = [] : (tensor<bf16>) -> tensor<2x256x15x20xbf16>
      %1769 = stablehlo.multiply %1767, %1768 : tensor<2x256x15x20xbf16>
      %1770 = stablehlo.add %1765, %1769 : tensor<2x256x15x20xbf16>
      %1771 = stablehlo.transpose %1770, dims = [0, 1, 3, 2] : (tensor<2x256x15x20xbf16>) -> tensor<2x256x20x15xbf16>
      %1772 = stablehlo.reshape %1771 : (tensor<2x256x20x15xbf16>) -> tensor<512x20x15xbf16>
      %1773 = stablehlo.broadcast_in_dim %arg981, dims = [0, 1, 2] : (tensor<512x15x30xbf16>) -> tensor<512x15x30xbf16>
      %1774 = stablehlo.dot_general %1772, %1773, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<512x20x15xbf16>, tensor<512x15x30xbf16>) -> tensor<512x20x30xbf16>
      %1775 = stablehlo.reshape %1774 : (tensor<512x20x30xbf16>) -> tensor<2x256x20x30xbf16>
      %1776 = stablehlo.transpose %1775, dims = [0, 1, 3, 2] : (tensor<2x256x20x30xbf16>) -> tensor<2x256x30x20xbf16>
      %1777 = stablehlo.reshape %1776 : (tensor<2x256x30x20xbf16>) -> tensor<512x30x20xbf16>
      %1778 = stablehlo.broadcast_in_dim %arg982, dims = [0, 1, 2] : (tensor<512x20x40xbf16>) -> tensor<512x20x40xbf16>
      %1779 = stablehlo.dot_general %1777, %1778, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<512x30x20xbf16>, tensor<512x20x40xbf16>) -> tensor<512x30x40xbf16>
      %1780 = stablehlo.reshape %1779 : (tensor<512x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1781 = stablehlo.convolution(%1331, %arg629) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x512x30x40xbf16>, tensor<256x512x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1782 = stablehlo.broadcast_in_dim %1781, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1783 = stablehlo.broadcast_in_dim %arg983, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1784 = stablehlo.subtract %1782, %1783 : tensor<2x256x30x40xbf16>
      %1785 = stablehlo.broadcast_in_dim %1784, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1786 = stablehlo.broadcast_in_dim %arg984, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1787 = stablehlo.multiply %1785, %1786 : tensor<2x256x30x40xbf16>
      %1788 = stablehlo.broadcast_in_dim %1787, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1789 = stablehlo.broadcast_in_dim %arg985, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1790 = stablehlo.multiply %1788, %1789 : tensor<2x256x30x40xbf16>
      %1791 = stablehlo.broadcast_in_dim %1790, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1792 = stablehlo.broadcast_in_dim %arg986, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1793 = stablehlo.add %1791, %1792 : tensor<2x256x30x40xbf16>
      %1794 = stablehlo.broadcast_in_dim %1638, dims = [] : (tensor<bf16>) -> tensor<2x256x30x40xbf16>
      %1795 = stablehlo.broadcast_in_dim %1793, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1796 = stablehlo.maximum %1794, %1795 : tensor<2x256x30x40xbf16>
      %1797 = stablehlo.minimum %1794, %1795 : tensor<2x256x30x40xbf16>
      %1798 = stablehlo.broadcast_in_dim %1797, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1799 = stablehlo.broadcast_in_dim %1644, dims = [] : (tensor<bf16>) -> tensor<2x256x30x40xbf16>
      %1800 = stablehlo.multiply %1798, %1799 : tensor<2x256x30x40xbf16>
      %1801 = stablehlo.add %1796, %1800 : tensor<2x256x30x40xbf16>
      %1802 = stablehlo.concatenate %1801, %1780, dim = 1 : (tensor<2x256x30x40xbf16>, tensor<2x256x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %1803 = stablehlo.convolution(%1802, %arg630) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x512x30x40xbf16>, tensor<256x512x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1804 = stablehlo.broadcast_in_dim %1803, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1805 = stablehlo.broadcast_in_dim %arg987, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1806 = stablehlo.subtract %1804, %1805 : tensor<2x256x30x40xbf16>
      %1807 = stablehlo.broadcast_in_dim %1806, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1808 = stablehlo.broadcast_in_dim %arg988, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1809 = stablehlo.multiply %1807, %1808 : tensor<2x256x30x40xbf16>
      %1810 = stablehlo.broadcast_in_dim %1809, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1811 = stablehlo.broadcast_in_dim %arg989, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1812 = stablehlo.multiply %1810, %1811 : tensor<2x256x30x40xbf16>
      %1813 = stablehlo.broadcast_in_dim %1812, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1814 = stablehlo.broadcast_in_dim %arg990, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1815 = stablehlo.add %1813, %1814 : tensor<2x256x30x40xbf16>
      %1816 = stablehlo.broadcast_in_dim %1815, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1817 = stablehlo.maximum %1794, %1816 : tensor<2x256x30x40xbf16>
      %1818 = stablehlo.minimum %1794, %1816 : tensor<2x256x30x40xbf16>
      %1819 = stablehlo.broadcast_in_dim %1818, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1820 = stablehlo.multiply %1819, %1799 : tensor<2x256x30x40xbf16>
      %1821 = stablehlo.add %1817, %1820 : tensor<2x256x30x40xbf16>
      %1822 = stablehlo.convolution(%1821, %arg631) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x30x40xbf16>, tensor<512x256x3x3xbf16>) -> tensor<2x512x30x40xbf16>
      %1823 = stablehlo.broadcast_in_dim %1822, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %1824 = stablehlo.broadcast_in_dim %arg991, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x30x40xbf16>
      %1825 = stablehlo.subtract %1823, %1824 : tensor<2x512x30x40xbf16>
      %1826 = stablehlo.broadcast_in_dim %1825, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %1827 = stablehlo.broadcast_in_dim %arg992, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x30x40xbf16>
      %1828 = stablehlo.multiply %1826, %1827 : tensor<2x512x30x40xbf16>
      %1829 = stablehlo.broadcast_in_dim %1828, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %1830 = stablehlo.broadcast_in_dim %arg993, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x30x40xbf16>
      %1831 = stablehlo.multiply %1829, %1830 : tensor<2x512x30x40xbf16>
      %1832 = stablehlo.broadcast_in_dim %1831, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %1833 = stablehlo.broadcast_in_dim %arg994, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x30x40xbf16>
      %1834 = stablehlo.add %1832, %1833 : tensor<2x512x30x40xbf16>
      %1835 = stablehlo.broadcast_in_dim %1638, dims = [] : (tensor<bf16>) -> tensor<2x512x30x40xbf16>
      %1836 = stablehlo.broadcast_in_dim %1834, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %1837 = stablehlo.maximum %1835, %1836 : tensor<2x512x30x40xbf16>
      %1838 = stablehlo.minimum %1835, %1836 : tensor<2x512x30x40xbf16>
      %1839 = stablehlo.broadcast_in_dim %1838, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %1840 = stablehlo.broadcast_in_dim %1644, dims = [] : (tensor<bf16>) -> tensor<2x512x30x40xbf16>
      %1841 = stablehlo.multiply %1839, %1840 : tensor<2x512x30x40xbf16>
      %1842 = stablehlo.add %1837, %1841 : tensor<2x512x30x40xbf16>
      %1843 = stablehlo.convolution(%1842, %arg632) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x512x30x40xbf16>, tensor<256x512x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1844 = stablehlo.broadcast_in_dim %1843, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1845 = stablehlo.broadcast_in_dim %arg995, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1846 = stablehlo.subtract %1844, %1845 : tensor<2x256x30x40xbf16>
      %1847 = stablehlo.broadcast_in_dim %1846, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1848 = stablehlo.broadcast_in_dim %arg996, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1849 = stablehlo.multiply %1847, %1848 : tensor<2x256x30x40xbf16>
      %1850 = stablehlo.broadcast_in_dim %1849, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1851 = stablehlo.broadcast_in_dim %arg997, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1852 = stablehlo.multiply %1850, %1851 : tensor<2x256x30x40xbf16>
      %1853 = stablehlo.broadcast_in_dim %1852, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1854 = stablehlo.broadcast_in_dim %arg998, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1855 = stablehlo.add %1853, %1854 : tensor<2x256x30x40xbf16>
      %1856 = stablehlo.broadcast_in_dim %1855, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1857 = stablehlo.maximum %1794, %1856 : tensor<2x256x30x40xbf16>
      %1858 = stablehlo.minimum %1794, %1856 : tensor<2x256x30x40xbf16>
      %1859 = stablehlo.broadcast_in_dim %1858, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1860 = stablehlo.multiply %1859, %1799 : tensor<2x256x30x40xbf16>
      %1861 = stablehlo.add %1857, %1860 : tensor<2x256x30x40xbf16>
      %1862 = stablehlo.convolution(%1861, %arg633) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x30x40xbf16>, tensor<512x256x3x3xbf16>) -> tensor<2x512x30x40xbf16>
      %1863 = stablehlo.broadcast_in_dim %1862, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %1864 = stablehlo.broadcast_in_dim %arg999, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x30x40xbf16>
      %1865 = stablehlo.subtract %1863, %1864 : tensor<2x512x30x40xbf16>
      %1866 = stablehlo.broadcast_in_dim %1865, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %1867 = stablehlo.broadcast_in_dim %arg1000, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x30x40xbf16>
      %1868 = stablehlo.multiply %1866, %1867 : tensor<2x512x30x40xbf16>
      %1869 = stablehlo.broadcast_in_dim %1868, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %1870 = stablehlo.broadcast_in_dim %arg1001, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x30x40xbf16>
      %1871 = stablehlo.multiply %1869, %1870 : tensor<2x512x30x40xbf16>
      %1872 = stablehlo.broadcast_in_dim %1871, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %1873 = stablehlo.broadcast_in_dim %arg1002, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x30x40xbf16>
      %1874 = stablehlo.add %1872, %1873 : tensor<2x512x30x40xbf16>
      %1875 = stablehlo.broadcast_in_dim %1874, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %1876 = stablehlo.maximum %1835, %1875 : tensor<2x512x30x40xbf16>
      %1877 = stablehlo.minimum %1835, %1875 : tensor<2x512x30x40xbf16>
      %1878 = stablehlo.broadcast_in_dim %1877, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %1879 = stablehlo.multiply %1878, %1840 : tensor<2x512x30x40xbf16>
      %1880 = stablehlo.add %1876, %1879 : tensor<2x512x30x40xbf16>
      %1881 = stablehlo.convolution(%1880, %arg634) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x512x30x40xbf16>, tensor<256x512x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1882 = stablehlo.broadcast_in_dim %1881, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1883 = stablehlo.broadcast_in_dim %arg1003, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1884 = stablehlo.subtract %1882, %1883 : tensor<2x256x30x40xbf16>
      %1885 = stablehlo.broadcast_in_dim %1884, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1886 = stablehlo.broadcast_in_dim %arg1004, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1887 = stablehlo.multiply %1885, %1886 : tensor<2x256x30x40xbf16>
      %1888 = stablehlo.broadcast_in_dim %1887, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1889 = stablehlo.broadcast_in_dim %arg1005, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1890 = stablehlo.multiply %1888, %1889 : tensor<2x256x30x40xbf16>
      %1891 = stablehlo.broadcast_in_dim %1890, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1892 = stablehlo.broadcast_in_dim %arg1006, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %1893 = stablehlo.add %1891, %1892 : tensor<2x256x30x40xbf16>
      %1894 = stablehlo.broadcast_in_dim %1893, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1895 = stablehlo.maximum %1794, %1894 : tensor<2x256x30x40xbf16>
      %1896 = stablehlo.minimum %1794, %1894 : tensor<2x256x30x40xbf16>
      %1897 = stablehlo.broadcast_in_dim %1896, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %1898 = stablehlo.multiply %1897, %1799 : tensor<2x256x30x40xbf16>
      %1899 = stablehlo.add %1895, %1898 : tensor<2x256x30x40xbf16>
      %1900 = stablehlo.convolution(%1899, %arg635) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x30x40xbf16>, tensor<128x256x1x1xbf16>) -> tensor<2x128x30x40xbf16>
      %1901 = stablehlo.broadcast_in_dim %1900, dims = [0, 1, 2, 3] : (tensor<2x128x30x40xbf16>) -> tensor<2x128x30x40xbf16>
      %1902 = stablehlo.broadcast_in_dim %arg1007, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x30x40xbf16>
      %1903 = stablehlo.subtract %1901, %1902 : tensor<2x128x30x40xbf16>
      %1904 = stablehlo.broadcast_in_dim %1903, dims = [0, 1, 2, 3] : (tensor<2x128x30x40xbf16>) -> tensor<2x128x30x40xbf16>
      %1905 = stablehlo.broadcast_in_dim %arg1008, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x30x40xbf16>
      %1906 = stablehlo.multiply %1904, %1905 : tensor<2x128x30x40xbf16>
      %1907 = stablehlo.broadcast_in_dim %1906, dims = [0, 1, 2, 3] : (tensor<2x128x30x40xbf16>) -> tensor<2x128x30x40xbf16>
      %1908 = stablehlo.broadcast_in_dim %arg1009, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x30x40xbf16>
      %1909 = stablehlo.multiply %1907, %1908 : tensor<2x128x30x40xbf16>
      %1910 = stablehlo.broadcast_in_dim %1909, dims = [0, 1, 2, 3] : (tensor<2x128x30x40xbf16>) -> tensor<2x128x30x40xbf16>
      %1911 = stablehlo.broadcast_in_dim %arg1010, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x30x40xbf16>
      %1912 = stablehlo.add %1910, %1911 : tensor<2x128x30x40xbf16>
      %1913 = stablehlo.broadcast_in_dim %1638, dims = [] : (tensor<bf16>) -> tensor<2x128x30x40xbf16>
      %1914 = stablehlo.broadcast_in_dim %1912, dims = [0, 1, 2, 3] : (tensor<2x128x30x40xbf16>) -> tensor<2x128x30x40xbf16>
      %1915 = stablehlo.maximum %1913, %1914 : tensor<2x128x30x40xbf16>
      %1916 = stablehlo.minimum %1913, %1914 : tensor<2x128x30x40xbf16>
      %1917 = stablehlo.broadcast_in_dim %1916, dims = [0, 1, 2, 3] : (tensor<2x128x30x40xbf16>) -> tensor<2x128x30x40xbf16>
      %1918 = stablehlo.broadcast_in_dim %1644, dims = [] : (tensor<bf16>) -> tensor<2x128x30x40xbf16>
      %1919 = stablehlo.multiply %1917, %1918 : tensor<2x128x30x40xbf16>
      %1920 = stablehlo.add %1915, %1919 : tensor<2x128x30x40xbf16>
      %1921 = stablehlo.transpose %1920, dims = [0, 1, 3, 2] : (tensor<2x128x30x40xbf16>) -> tensor<2x128x40x30xbf16>
      %1922 = stablehlo.reshape %1921 : (tensor<2x128x40x30xbf16>) -> tensor<256x40x30xbf16>
      %1923 = stablehlo.broadcast_in_dim %arg1013, dims = [0, 1, 2] : (tensor<256x30x60xbf16>) -> tensor<256x30x60xbf16>
      %1924 = stablehlo.dot_general %1922, %1923, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<256x40x30xbf16>, tensor<256x30x60xbf16>) -> tensor<256x40x60xbf16>
      %1925 = stablehlo.reshape %1924 : (tensor<256x40x60xbf16>) -> tensor<2x128x40x60xbf16>
      %1926 = stablehlo.transpose %1925, dims = [0, 1, 3, 2] : (tensor<2x128x40x60xbf16>) -> tensor<2x128x60x40xbf16>
      %1927 = stablehlo.reshape %1926 : (tensor<2x128x60x40xbf16>) -> tensor<256x60x40xbf16>
      %1928 = stablehlo.broadcast_in_dim %arg1014, dims = [0, 1, 2] : (tensor<256x40x80xbf16>) -> tensor<256x40x80xbf16>
      %1929 = stablehlo.dot_general %1927, %1928, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<256x60x40xbf16>, tensor<256x40x80xbf16>) -> tensor<256x60x80xbf16>
      %1930 = stablehlo.reshape %1929 : (tensor<256x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %1931 = stablehlo.convolution(%858, %arg636) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x60x80xbf16>, tensor<128x256x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %1932 = stablehlo.broadcast_in_dim %1931, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %1933 = stablehlo.broadcast_in_dim %arg1015, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %1934 = stablehlo.subtract %1932, %1933 : tensor<2x128x60x80xbf16>
      %1935 = stablehlo.broadcast_in_dim %1934, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %1936 = stablehlo.broadcast_in_dim %arg1016, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %1937 = stablehlo.multiply %1935, %1936 : tensor<2x128x60x80xbf16>
      %1938 = stablehlo.broadcast_in_dim %1937, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %1939 = stablehlo.broadcast_in_dim %arg1017, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %1940 = stablehlo.multiply %1938, %1939 : tensor<2x128x60x80xbf16>
      %1941 = stablehlo.broadcast_in_dim %1940, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %1942 = stablehlo.broadcast_in_dim %arg1018, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %1943 = stablehlo.add %1941, %1942 : tensor<2x128x60x80xbf16>
      %1944 = stablehlo.broadcast_in_dim %1638, dims = [] : (tensor<bf16>) -> tensor<2x128x60x80xbf16>
      %1945 = stablehlo.broadcast_in_dim %1943, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %1946 = stablehlo.maximum %1944, %1945 : tensor<2x128x60x80xbf16>
      %1947 = stablehlo.minimum %1944, %1945 : tensor<2x128x60x80xbf16>
      %1948 = stablehlo.broadcast_in_dim %1947, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %1949 = stablehlo.broadcast_in_dim %1644, dims = [] : (tensor<bf16>) -> tensor<2x128x60x80xbf16>
      %1950 = stablehlo.multiply %1948, %1949 : tensor<2x128x60x80xbf16>
      %1951 = stablehlo.add %1946, %1950 : tensor<2x128x60x80xbf16>
      %1952 = stablehlo.concatenate %1951, %1930, dim = 1 : (tensor<2x128x60x80xbf16>, tensor<2x128x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %1953 = stablehlo.convolution(%1952, %arg637) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x60x80xbf16>, tensor<128x256x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %1954 = stablehlo.broadcast_in_dim %1953, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %1955 = stablehlo.broadcast_in_dim %arg1019, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %1956 = stablehlo.subtract %1954, %1955 : tensor<2x128x60x80xbf16>
      %1957 = stablehlo.broadcast_in_dim %1956, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %1958 = stablehlo.broadcast_in_dim %arg1020, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %1959 = stablehlo.multiply %1957, %1958 : tensor<2x128x60x80xbf16>
      %1960 = stablehlo.broadcast_in_dim %1959, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %1961 = stablehlo.broadcast_in_dim %arg1021, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %1962 = stablehlo.multiply %1960, %1961 : tensor<2x128x60x80xbf16>
      %1963 = stablehlo.broadcast_in_dim %1962, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %1964 = stablehlo.broadcast_in_dim %arg1022, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %1965 = stablehlo.add %1963, %1964 : tensor<2x128x60x80xbf16>
      %1966 = stablehlo.broadcast_in_dim %1965, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %1967 = stablehlo.maximum %1944, %1966 : tensor<2x128x60x80xbf16>
      %1968 = stablehlo.minimum %1944, %1966 : tensor<2x128x60x80xbf16>
      %1969 = stablehlo.broadcast_in_dim %1968, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %1970 = stablehlo.multiply %1969, %1949 : tensor<2x128x60x80xbf16>
      %1971 = stablehlo.add %1967, %1970 : tensor<2x128x60x80xbf16>
      %1972 = stablehlo.convolution(%1971, %arg638) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x128x60x80xbf16>, tensor<256x128x3x3xbf16>) -> tensor<2x256x60x80xbf16>
      %1973 = stablehlo.broadcast_in_dim %1972, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %1974 = stablehlo.broadcast_in_dim %arg1023, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x60x80xbf16>
      %1975 = stablehlo.subtract %1973, %1974 : tensor<2x256x60x80xbf16>
      %1976 = stablehlo.broadcast_in_dim %1975, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %1977 = stablehlo.broadcast_in_dim %arg1024, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x60x80xbf16>
      %1978 = stablehlo.multiply %1976, %1977 : tensor<2x256x60x80xbf16>
      %1979 = stablehlo.broadcast_in_dim %1978, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %1980 = stablehlo.broadcast_in_dim %arg1025, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x60x80xbf16>
      %1981 = stablehlo.multiply %1979, %1980 : tensor<2x256x60x80xbf16>
      %1982 = stablehlo.broadcast_in_dim %1981, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %1983 = stablehlo.broadcast_in_dim %arg1026, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x60x80xbf16>
      %1984 = stablehlo.add %1982, %1983 : tensor<2x256x60x80xbf16>
      %1985 = stablehlo.broadcast_in_dim %1638, dims = [] : (tensor<bf16>) -> tensor<2x256x60x80xbf16>
      %1986 = stablehlo.broadcast_in_dim %1984, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %1987 = stablehlo.maximum %1985, %1986 : tensor<2x256x60x80xbf16>
      %1988 = stablehlo.minimum %1985, %1986 : tensor<2x256x60x80xbf16>
      %1989 = stablehlo.broadcast_in_dim %1988, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %1990 = stablehlo.broadcast_in_dim %1644, dims = [] : (tensor<bf16>) -> tensor<2x256x60x80xbf16>
      %1991 = stablehlo.multiply %1989, %1990 : tensor<2x256x60x80xbf16>
      %1992 = stablehlo.add %1987, %1991 : tensor<2x256x60x80xbf16>
      %1993 = stablehlo.convolution(%1992, %arg639) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x60x80xbf16>, tensor<128x256x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %1994 = stablehlo.broadcast_in_dim %1993, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %1995 = stablehlo.broadcast_in_dim %arg1027, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %1996 = stablehlo.subtract %1994, %1995 : tensor<2x128x60x80xbf16>
      %1997 = stablehlo.broadcast_in_dim %1996, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %1998 = stablehlo.broadcast_in_dim %arg1028, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %1999 = stablehlo.multiply %1997, %1998 : tensor<2x128x60x80xbf16>
      %2000 = stablehlo.broadcast_in_dim %1999, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %2001 = stablehlo.broadcast_in_dim %arg1029, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %2002 = stablehlo.multiply %2000, %2001 : tensor<2x128x60x80xbf16>
      %2003 = stablehlo.broadcast_in_dim %2002, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %2004 = stablehlo.broadcast_in_dim %arg1030, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %2005 = stablehlo.add %2003, %2004 : tensor<2x128x60x80xbf16>
      %2006 = stablehlo.broadcast_in_dim %2005, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %2007 = stablehlo.maximum %1944, %2006 : tensor<2x128x60x80xbf16>
      %2008 = stablehlo.minimum %1944, %2006 : tensor<2x128x60x80xbf16>
      %2009 = stablehlo.broadcast_in_dim %2008, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %2010 = stablehlo.multiply %2009, %1949 : tensor<2x128x60x80xbf16>
      %2011 = stablehlo.add %2007, %2010 : tensor<2x128x60x80xbf16>
      %2012 = stablehlo.convolution(%2011, %arg640) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x128x60x80xbf16>, tensor<256x128x3x3xbf16>) -> tensor<2x256x60x80xbf16>
      %2013 = stablehlo.broadcast_in_dim %2012, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %2014 = stablehlo.broadcast_in_dim %arg1031, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x60x80xbf16>
      %2015 = stablehlo.subtract %2013, %2014 : tensor<2x256x60x80xbf16>
      %2016 = stablehlo.broadcast_in_dim %2015, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %2017 = stablehlo.broadcast_in_dim %arg1032, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x60x80xbf16>
      %2018 = stablehlo.multiply %2016, %2017 : tensor<2x256x60x80xbf16>
      %2019 = stablehlo.broadcast_in_dim %2018, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %2020 = stablehlo.broadcast_in_dim %arg1033, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x60x80xbf16>
      %2021 = stablehlo.multiply %2019, %2020 : tensor<2x256x60x80xbf16>
      %2022 = stablehlo.broadcast_in_dim %2021, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %2023 = stablehlo.broadcast_in_dim %arg1034, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x60x80xbf16>
      %2024 = stablehlo.add %2022, %2023 : tensor<2x256x60x80xbf16>
      %2025 = stablehlo.broadcast_in_dim %2024, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %2026 = stablehlo.maximum %1985, %2025 : tensor<2x256x60x80xbf16>
      %2027 = stablehlo.minimum %1985, %2025 : tensor<2x256x60x80xbf16>
      %2028 = stablehlo.broadcast_in_dim %2027, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %2029 = stablehlo.multiply %2028, %1990 : tensor<2x256x60x80xbf16>
      %2030 = stablehlo.add %2026, %2029 : tensor<2x256x60x80xbf16>
      %2031 = stablehlo.convolution(%2030, %arg641) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x60x80xbf16>, tensor<128x256x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %2032 = stablehlo.broadcast_in_dim %2031, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %2033 = stablehlo.broadcast_in_dim %arg1035, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %2034 = stablehlo.subtract %2032, %2033 : tensor<2x128x60x80xbf16>
      %2035 = stablehlo.broadcast_in_dim %2034, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %2036 = stablehlo.broadcast_in_dim %arg1036, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %2037 = stablehlo.multiply %2035, %2036 : tensor<2x128x60x80xbf16>
      %2038 = stablehlo.broadcast_in_dim %2037, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %2039 = stablehlo.broadcast_in_dim %arg1037, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %2040 = stablehlo.multiply %2038, %2039 : tensor<2x128x60x80xbf16>
      %2041 = stablehlo.broadcast_in_dim %2040, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %2042 = stablehlo.broadcast_in_dim %arg1038, dims = [1, 2, 3] : (tensor<128x1x1xbf16>) -> tensor<2x128x60x80xbf16>
      %2043 = stablehlo.add %2041, %2042 : tensor<2x128x60x80xbf16>
      %2044 = stablehlo.broadcast_in_dim %2043, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %2045 = stablehlo.maximum %1944, %2044 : tensor<2x128x60x80xbf16>
      %2046 = stablehlo.minimum %1944, %2044 : tensor<2x128x60x80xbf16>
      %2047 = stablehlo.broadcast_in_dim %2046, dims = [0, 1, 2, 3] : (tensor<2x128x60x80xbf16>) -> tensor<2x128x60x80xbf16>
      %2048 = stablehlo.multiply %2047, %1949 : tensor<2x128x60x80xbf16>
      %2049 = stablehlo.add %2045, %2048 : tensor<2x128x60x80xbf16>
      %2050 = stablehlo.convolution(%2049, %arg642) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x128x60x80xbf16>, tensor<256x128x3x3xbf16>) -> tensor<2x256x60x80xbf16>
      %2051 = stablehlo.broadcast_in_dim %2050, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %2052 = stablehlo.broadcast_in_dim %arg1039, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x60x80xbf16>
      %2053 = stablehlo.subtract %2051, %2052 : tensor<2x256x60x80xbf16>
      %2054 = stablehlo.broadcast_in_dim %2053, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %2055 = stablehlo.broadcast_in_dim %arg1040, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x60x80xbf16>
      %2056 = stablehlo.multiply %2054, %2055 : tensor<2x256x60x80xbf16>
      %2057 = stablehlo.broadcast_in_dim %2056, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %2058 = stablehlo.broadcast_in_dim %arg1041, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x60x80xbf16>
      %2059 = stablehlo.multiply %2057, %2058 : tensor<2x256x60x80xbf16>
      %2060 = stablehlo.broadcast_in_dim %2059, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %2061 = stablehlo.broadcast_in_dim %arg1042, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x60x80xbf16>
      %2062 = stablehlo.add %2060, %2061 : tensor<2x256x60x80xbf16>
      %2063 = stablehlo.broadcast_in_dim %2062, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %2064 = stablehlo.maximum %1985, %2063 : tensor<2x256x60x80xbf16>
      %2065 = stablehlo.minimum %1985, %2063 : tensor<2x256x60x80xbf16>
      %2066 = stablehlo.broadcast_in_dim %2065, dims = [0, 1, 2, 3] : (tensor<2x256x60x80xbf16>) -> tensor<2x256x60x80xbf16>
      %2067 = stablehlo.multiply %2066, %1990 : tensor<2x256x60x80xbf16>
      %2068 = stablehlo.add %2064, %2067 : tensor<2x256x60x80xbf16>
      %2069 = stablehlo.convolution(%2068, %arg643) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x60x80xbf16>, tensor<255x256x1x1xbf16>) -> tensor<2x255x60x80xbf16>
      %2070 = stablehlo.reshape %arg644 : (tensor<255xbf16>) -> tensor<255x1x1xbf16>
      %2071 = stablehlo.broadcast_in_dim %2069, dims = [0, 1, 2, 3] : (tensor<2x255x60x80xbf16>) -> tensor<2x255x60x80xbf16>
      %2072 = stablehlo.broadcast_in_dim %2070, dims = [1, 2, 3] : (tensor<255x1x1xbf16>) -> tensor<2x255x60x80xbf16>
      %2073 = stablehlo.add %2071, %2072 : tensor<2x255x60x80xbf16>
      %2074 = stablehlo.convolution(%2049, %arg645) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [2, 2], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x128x60x80xbf16>, tensor<256x128x3x3xbf16>) -> tensor<2x256x30x40xbf16>
      %2075 = stablehlo.broadcast_in_dim %2074, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %2076 = stablehlo.broadcast_in_dim %arg1043, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %2077 = stablehlo.subtract %2075, %2076 : tensor<2x256x30x40xbf16>
      %2078 = stablehlo.broadcast_in_dim %2077, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %2079 = stablehlo.broadcast_in_dim %arg1044, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %2080 = stablehlo.multiply %2078, %2079 : tensor<2x256x30x40xbf16>
      %2081 = stablehlo.broadcast_in_dim %2080, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %2082 = stablehlo.broadcast_in_dim %arg1045, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %2083 = stablehlo.multiply %2081, %2082 : tensor<2x256x30x40xbf16>
      %2084 = stablehlo.broadcast_in_dim %2083, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %2085 = stablehlo.broadcast_in_dim %arg1046, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %2086 = stablehlo.add %2084, %2085 : tensor<2x256x30x40xbf16>
      %2087 = stablehlo.broadcast_in_dim %2086, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %2088 = stablehlo.maximum %1794, %2087 : tensor<2x256x30x40xbf16>
      %2089 = stablehlo.minimum %1794, %2087 : tensor<2x256x30x40xbf16>
      %2090 = stablehlo.broadcast_in_dim %2089, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %2091 = stablehlo.multiply %2090, %1799 : tensor<2x256x30x40xbf16>
      %2092 = stablehlo.add %2088, %2091 : tensor<2x256x30x40xbf16>
      %2093 = stablehlo.concatenate %2092, %1899, dim = 1 : (tensor<2x256x30x40xbf16>, tensor<2x256x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %2094 = stablehlo.convolution(%2093, %arg646) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x512x30x40xbf16>, tensor<256x512x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %2095 = stablehlo.broadcast_in_dim %2094, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %2096 = stablehlo.broadcast_in_dim %arg1047, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %2097 = stablehlo.subtract %2095, %2096 : tensor<2x256x30x40xbf16>
      %2098 = stablehlo.broadcast_in_dim %2097, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %2099 = stablehlo.broadcast_in_dim %arg1048, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %2100 = stablehlo.multiply %2098, %2099 : tensor<2x256x30x40xbf16>
      %2101 = stablehlo.broadcast_in_dim %2100, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %2102 = stablehlo.broadcast_in_dim %arg1049, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %2103 = stablehlo.multiply %2101, %2102 : tensor<2x256x30x40xbf16>
      %2104 = stablehlo.broadcast_in_dim %2103, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %2105 = stablehlo.broadcast_in_dim %arg1050, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %2106 = stablehlo.add %2104, %2105 : tensor<2x256x30x40xbf16>
      %2107 = stablehlo.broadcast_in_dim %2106, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %2108 = stablehlo.maximum %1794, %2107 : tensor<2x256x30x40xbf16>
      %2109 = stablehlo.minimum %1794, %2107 : tensor<2x256x30x40xbf16>
      %2110 = stablehlo.broadcast_in_dim %2109, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %2111 = stablehlo.multiply %2110, %1799 : tensor<2x256x30x40xbf16>
      %2112 = stablehlo.add %2108, %2111 : tensor<2x256x30x40xbf16>
      %2113 = stablehlo.convolution(%2112, %arg647) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x30x40xbf16>, tensor<512x256x3x3xbf16>) -> tensor<2x512x30x40xbf16>
      %2114 = stablehlo.broadcast_in_dim %2113, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %2115 = stablehlo.broadcast_in_dim %arg1051, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x30x40xbf16>
      %2116 = stablehlo.subtract %2114, %2115 : tensor<2x512x30x40xbf16>
      %2117 = stablehlo.broadcast_in_dim %2116, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %2118 = stablehlo.broadcast_in_dim %arg1052, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x30x40xbf16>
      %2119 = stablehlo.multiply %2117, %2118 : tensor<2x512x30x40xbf16>
      %2120 = stablehlo.broadcast_in_dim %2119, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %2121 = stablehlo.broadcast_in_dim %arg1053, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x30x40xbf16>
      %2122 = stablehlo.multiply %2120, %2121 : tensor<2x512x30x40xbf16>
      %2123 = stablehlo.broadcast_in_dim %2122, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %2124 = stablehlo.broadcast_in_dim %arg1054, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x30x40xbf16>
      %2125 = stablehlo.add %2123, %2124 : tensor<2x512x30x40xbf16>
      %2126 = stablehlo.broadcast_in_dim %2125, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %2127 = stablehlo.maximum %1835, %2126 : tensor<2x512x30x40xbf16>
      %2128 = stablehlo.minimum %1835, %2126 : tensor<2x512x30x40xbf16>
      %2129 = stablehlo.broadcast_in_dim %2128, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %2130 = stablehlo.multiply %2129, %1840 : tensor<2x512x30x40xbf16>
      %2131 = stablehlo.add %2127, %2130 : tensor<2x512x30x40xbf16>
      %2132 = stablehlo.convolution(%2131, %arg648) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x512x30x40xbf16>, tensor<256x512x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %2133 = stablehlo.broadcast_in_dim %2132, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %2134 = stablehlo.broadcast_in_dim %arg1055, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %2135 = stablehlo.subtract %2133, %2134 : tensor<2x256x30x40xbf16>
      %2136 = stablehlo.broadcast_in_dim %2135, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %2137 = stablehlo.broadcast_in_dim %arg1056, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %2138 = stablehlo.multiply %2136, %2137 : tensor<2x256x30x40xbf16>
      %2139 = stablehlo.broadcast_in_dim %2138, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %2140 = stablehlo.broadcast_in_dim %arg1057, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %2141 = stablehlo.multiply %2139, %2140 : tensor<2x256x30x40xbf16>
      %2142 = stablehlo.broadcast_in_dim %2141, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %2143 = stablehlo.broadcast_in_dim %arg1058, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %2144 = stablehlo.add %2142, %2143 : tensor<2x256x30x40xbf16>
      %2145 = stablehlo.broadcast_in_dim %2144, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %2146 = stablehlo.maximum %1794, %2145 : tensor<2x256x30x40xbf16>
      %2147 = stablehlo.minimum %1794, %2145 : tensor<2x256x30x40xbf16>
      %2148 = stablehlo.broadcast_in_dim %2147, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %2149 = stablehlo.multiply %2148, %1799 : tensor<2x256x30x40xbf16>
      %2150 = stablehlo.add %2146, %2149 : tensor<2x256x30x40xbf16>
      %2151 = stablehlo.convolution(%2150, %arg649) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x30x40xbf16>, tensor<512x256x3x3xbf16>) -> tensor<2x512x30x40xbf16>
      %2152 = stablehlo.broadcast_in_dim %2151, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %2153 = stablehlo.broadcast_in_dim %arg1059, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x30x40xbf16>
      %2154 = stablehlo.subtract %2152, %2153 : tensor<2x512x30x40xbf16>
      %2155 = stablehlo.broadcast_in_dim %2154, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %2156 = stablehlo.broadcast_in_dim %arg1060, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x30x40xbf16>
      %2157 = stablehlo.multiply %2155, %2156 : tensor<2x512x30x40xbf16>
      %2158 = stablehlo.broadcast_in_dim %2157, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %2159 = stablehlo.broadcast_in_dim %arg1061, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x30x40xbf16>
      %2160 = stablehlo.multiply %2158, %2159 : tensor<2x512x30x40xbf16>
      %2161 = stablehlo.broadcast_in_dim %2160, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %2162 = stablehlo.broadcast_in_dim %arg1062, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x30x40xbf16>
      %2163 = stablehlo.add %2161, %2162 : tensor<2x512x30x40xbf16>
      %2164 = stablehlo.broadcast_in_dim %2163, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %2165 = stablehlo.maximum %1835, %2164 : tensor<2x512x30x40xbf16>
      %2166 = stablehlo.minimum %1835, %2164 : tensor<2x512x30x40xbf16>
      %2167 = stablehlo.broadcast_in_dim %2166, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %2168 = stablehlo.multiply %2167, %1840 : tensor<2x512x30x40xbf16>
      %2169 = stablehlo.add %2165, %2168 : tensor<2x512x30x40xbf16>
      %2170 = stablehlo.convolution(%2169, %arg650) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x512x30x40xbf16>, tensor<256x512x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %2171 = stablehlo.broadcast_in_dim %2170, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %2172 = stablehlo.broadcast_in_dim %arg1063, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %2173 = stablehlo.subtract %2171, %2172 : tensor<2x256x30x40xbf16>
      %2174 = stablehlo.broadcast_in_dim %2173, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %2175 = stablehlo.broadcast_in_dim %arg1064, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %2176 = stablehlo.multiply %2174, %2175 : tensor<2x256x30x40xbf16>
      %2177 = stablehlo.broadcast_in_dim %2176, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %2178 = stablehlo.broadcast_in_dim %arg1065, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %2179 = stablehlo.multiply %2177, %2178 : tensor<2x256x30x40xbf16>
      %2180 = stablehlo.broadcast_in_dim %2179, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %2181 = stablehlo.broadcast_in_dim %arg1066, dims = [1, 2, 3] : (tensor<256x1x1xbf16>) -> tensor<2x256x30x40xbf16>
      %2182 = stablehlo.add %2180, %2181 : tensor<2x256x30x40xbf16>
      %2183 = stablehlo.broadcast_in_dim %2182, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %2184 = stablehlo.maximum %1794, %2183 : tensor<2x256x30x40xbf16>
      %2185 = stablehlo.minimum %1794, %2183 : tensor<2x256x30x40xbf16>
      %2186 = stablehlo.broadcast_in_dim %2185, dims = [0, 1, 2, 3] : (tensor<2x256x30x40xbf16>) -> tensor<2x256x30x40xbf16>
      %2187 = stablehlo.multiply %2186, %1799 : tensor<2x256x30x40xbf16>
      %2188 = stablehlo.add %2184, %2187 : tensor<2x256x30x40xbf16>
      %2189 = stablehlo.convolution(%2188, %arg651) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x30x40xbf16>, tensor<512x256x3x3xbf16>) -> tensor<2x512x30x40xbf16>
      %2190 = stablehlo.broadcast_in_dim %2189, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %2191 = stablehlo.broadcast_in_dim %arg1067, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x30x40xbf16>
      %2192 = stablehlo.subtract %2190, %2191 : tensor<2x512x30x40xbf16>
      %2193 = stablehlo.broadcast_in_dim %2192, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %2194 = stablehlo.broadcast_in_dim %arg1068, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x30x40xbf16>
      %2195 = stablehlo.multiply %2193, %2194 : tensor<2x512x30x40xbf16>
      %2196 = stablehlo.broadcast_in_dim %2195, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %2197 = stablehlo.broadcast_in_dim %arg1069, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x30x40xbf16>
      %2198 = stablehlo.multiply %2196, %2197 : tensor<2x512x30x40xbf16>
      %2199 = stablehlo.broadcast_in_dim %2198, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %2200 = stablehlo.broadcast_in_dim %arg1070, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x30x40xbf16>
      %2201 = stablehlo.add %2199, %2200 : tensor<2x512x30x40xbf16>
      %2202 = stablehlo.broadcast_in_dim %2201, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %2203 = stablehlo.maximum %1835, %2202 : tensor<2x512x30x40xbf16>
      %2204 = stablehlo.minimum %1835, %2202 : tensor<2x512x30x40xbf16>
      %2205 = stablehlo.broadcast_in_dim %2204, dims = [0, 1, 2, 3] : (tensor<2x512x30x40xbf16>) -> tensor<2x512x30x40xbf16>
      %2206 = stablehlo.multiply %2205, %1840 : tensor<2x512x30x40xbf16>
      %2207 = stablehlo.add %2203, %2206 : tensor<2x512x30x40xbf16>
      %2208 = stablehlo.convolution(%2207, %arg652) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x512x30x40xbf16>, tensor<255x512x1x1xbf16>) -> tensor<2x255x30x40xbf16>
      %2209 = stablehlo.reshape %arg653 : (tensor<255xbf16>) -> tensor<255x1x1xbf16>
      %2210 = stablehlo.broadcast_in_dim %2208, dims = [0, 1, 2, 3] : (tensor<2x255x30x40xbf16>) -> tensor<2x255x30x40xbf16>
      %2211 = stablehlo.broadcast_in_dim %2209, dims = [1, 2, 3] : (tensor<255x1x1xbf16>) -> tensor<2x255x30x40xbf16>
      %2212 = stablehlo.add %2210, %2211 : tensor<2x255x30x40xbf16>
      %2213 = stablehlo.convolution(%2188, %arg654) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [2, 2], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x256x30x40xbf16>, tensor<512x256x3x3xbf16>) -> tensor<2x512x15x20xbf16>
      %2214 = stablehlo.broadcast_in_dim %2213, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %2215 = stablehlo.broadcast_in_dim %arg1071, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %2216 = stablehlo.subtract %2214, %2215 : tensor<2x512x15x20xbf16>
      %2217 = stablehlo.broadcast_in_dim %2216, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %2218 = stablehlo.broadcast_in_dim %arg1072, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %2219 = stablehlo.multiply %2217, %2218 : tensor<2x512x15x20xbf16>
      %2220 = stablehlo.broadcast_in_dim %2219, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %2221 = stablehlo.broadcast_in_dim %arg1073, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %2222 = stablehlo.multiply %2220, %2221 : tensor<2x512x15x20xbf16>
      %2223 = stablehlo.broadcast_in_dim %2222, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %2224 = stablehlo.broadcast_in_dim %arg1074, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %2225 = stablehlo.add %2223, %2224 : tensor<2x512x15x20xbf16>
      %2226 = stablehlo.broadcast_in_dim %2225, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %2227 = stablehlo.maximum %1639, %2226 : tensor<2x512x15x20xbf16>
      %2228 = stablehlo.minimum %1639, %2226 : tensor<2x512x15x20xbf16>
      %2229 = stablehlo.broadcast_in_dim %2228, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %2230 = stablehlo.multiply %2229, %1646 : tensor<2x512x15x20xbf16>
      %2231 = stablehlo.add %2227, %2230 : tensor<2x512x15x20xbf16>
      %2232 = stablehlo.concatenate %2231, %1749, dim = 1 : (tensor<2x512x15x20xbf16>, tensor<2x512x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %2233 = stablehlo.convolution(%2232, %arg655) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x1024x15x20xbf16>, tensor<512x1024x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %2234 = stablehlo.broadcast_in_dim %2233, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %2235 = stablehlo.broadcast_in_dim %arg1075, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %2236 = stablehlo.subtract %2234, %2235 : tensor<2x512x15x20xbf16>
      %2237 = stablehlo.broadcast_in_dim %2236, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %2238 = stablehlo.broadcast_in_dim %arg1076, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %2239 = stablehlo.multiply %2237, %2238 : tensor<2x512x15x20xbf16>
      %2240 = stablehlo.broadcast_in_dim %2239, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %2241 = stablehlo.broadcast_in_dim %arg1077, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %2242 = stablehlo.multiply %2240, %2241 : tensor<2x512x15x20xbf16>
      %2243 = stablehlo.broadcast_in_dim %2242, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %2244 = stablehlo.broadcast_in_dim %arg1078, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %2245 = stablehlo.add %2243, %2244 : tensor<2x512x15x20xbf16>
      %2246 = stablehlo.broadcast_in_dim %2245, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %2247 = stablehlo.maximum %1639, %2246 : tensor<2x512x15x20xbf16>
      %2248 = stablehlo.minimum %1639, %2246 : tensor<2x512x15x20xbf16>
      %2249 = stablehlo.broadcast_in_dim %2248, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %2250 = stablehlo.multiply %2249, %1646 : tensor<2x512x15x20xbf16>
      %2251 = stablehlo.add %2247, %2250 : tensor<2x512x15x20xbf16>
      %2252 = stablehlo.convolution(%2251, %arg656) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x512x15x20xbf16>, tensor<1024x512x3x3xbf16>) -> tensor<2x1024x15x20xbf16>
      %2253 = stablehlo.broadcast_in_dim %2252, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %2254 = stablehlo.broadcast_in_dim %arg1079, dims = [1, 2, 3] : (tensor<1024x1x1xbf16>) -> tensor<2x1024x15x20xbf16>
      %2255 = stablehlo.subtract %2253, %2254 : tensor<2x1024x15x20xbf16>
      %2256 = stablehlo.broadcast_in_dim %2255, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %2257 = stablehlo.broadcast_in_dim %arg1080, dims = [1, 2, 3] : (tensor<1024x1x1xbf16>) -> tensor<2x1024x15x20xbf16>
      %2258 = stablehlo.multiply %2256, %2257 : tensor<2x1024x15x20xbf16>
      %2259 = stablehlo.broadcast_in_dim %2258, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %2260 = stablehlo.broadcast_in_dim %arg1081, dims = [1, 2, 3] : (tensor<1024x1x1xbf16>) -> tensor<2x1024x15x20xbf16>
      %2261 = stablehlo.multiply %2259, %2260 : tensor<2x1024x15x20xbf16>
      %2262 = stablehlo.broadcast_in_dim %2261, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %2263 = stablehlo.broadcast_in_dim %arg1082, dims = [1, 2, 3] : (tensor<1024x1x1xbf16>) -> tensor<2x1024x15x20xbf16>
      %2264 = stablehlo.add %2262, %2263 : tensor<2x1024x15x20xbf16>
      %2265 = stablehlo.broadcast_in_dim %2264, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %2266 = stablehlo.maximum %1662, %2265 : tensor<2x1024x15x20xbf16>
      %2267 = stablehlo.minimum %1662, %2265 : tensor<2x1024x15x20xbf16>
      %2268 = stablehlo.broadcast_in_dim %2267, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %2269 = stablehlo.multiply %2268, %1667 : tensor<2x1024x15x20xbf16>
      %2270 = stablehlo.add %2266, %2269 : tensor<2x1024x15x20xbf16>
      %2271 = stablehlo.convolution(%2270, %arg657) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x1024x15x20xbf16>, tensor<512x1024x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %2272 = stablehlo.broadcast_in_dim %2271, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %2273 = stablehlo.broadcast_in_dim %arg1083, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %2274 = stablehlo.subtract %2272, %2273 : tensor<2x512x15x20xbf16>
      %2275 = stablehlo.broadcast_in_dim %2274, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %2276 = stablehlo.broadcast_in_dim %arg1084, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %2277 = stablehlo.multiply %2275, %2276 : tensor<2x512x15x20xbf16>
      %2278 = stablehlo.broadcast_in_dim %2277, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %2279 = stablehlo.broadcast_in_dim %arg1085, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %2280 = stablehlo.multiply %2278, %2279 : tensor<2x512x15x20xbf16>
      %2281 = stablehlo.broadcast_in_dim %2280, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %2282 = stablehlo.broadcast_in_dim %arg1086, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %2283 = stablehlo.add %2281, %2282 : tensor<2x512x15x20xbf16>
      %2284 = stablehlo.broadcast_in_dim %2283, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %2285 = stablehlo.maximum %1639, %2284 : tensor<2x512x15x20xbf16>
      %2286 = stablehlo.minimum %1639, %2284 : tensor<2x512x15x20xbf16>
      %2287 = stablehlo.broadcast_in_dim %2286, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %2288 = stablehlo.multiply %2287, %1646 : tensor<2x512x15x20xbf16>
      %2289 = stablehlo.add %2285, %2288 : tensor<2x512x15x20xbf16>
      %2290 = stablehlo.convolution(%2289, %arg658) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x512x15x20xbf16>, tensor<1024x512x3x3xbf16>) -> tensor<2x1024x15x20xbf16>
      %2291 = stablehlo.broadcast_in_dim %2290, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %2292 = stablehlo.broadcast_in_dim %arg1087, dims = [1, 2, 3] : (tensor<1024x1x1xbf16>) -> tensor<2x1024x15x20xbf16>
      %2293 = stablehlo.subtract %2291, %2292 : tensor<2x1024x15x20xbf16>
      %2294 = stablehlo.broadcast_in_dim %2293, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %2295 = stablehlo.broadcast_in_dim %arg1088, dims = [1, 2, 3] : (tensor<1024x1x1xbf16>) -> tensor<2x1024x15x20xbf16>
      %2296 = stablehlo.multiply %2294, %2295 : tensor<2x1024x15x20xbf16>
      %2297 = stablehlo.broadcast_in_dim %2296, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %2298 = stablehlo.broadcast_in_dim %arg1089, dims = [1, 2, 3] : (tensor<1024x1x1xbf16>) -> tensor<2x1024x15x20xbf16>
      %2299 = stablehlo.multiply %2297, %2298 : tensor<2x1024x15x20xbf16>
      %2300 = stablehlo.broadcast_in_dim %2299, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %2301 = stablehlo.broadcast_in_dim %arg1090, dims = [1, 2, 3] : (tensor<1024x1x1xbf16>) -> tensor<2x1024x15x20xbf16>
      %2302 = stablehlo.add %2300, %2301 : tensor<2x1024x15x20xbf16>
      %2303 = stablehlo.broadcast_in_dim %2302, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %2304 = stablehlo.maximum %1662, %2303 : tensor<2x1024x15x20xbf16>
      %2305 = stablehlo.minimum %1662, %2303 : tensor<2x1024x15x20xbf16>
      %2306 = stablehlo.broadcast_in_dim %2305, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %2307 = stablehlo.multiply %2306, %1667 : tensor<2x1024x15x20xbf16>
      %2308 = stablehlo.add %2304, %2307 : tensor<2x1024x15x20xbf16>
      %2309 = stablehlo.convolution(%2308, %arg659) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x1024x15x20xbf16>, tensor<512x1024x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %2310 = stablehlo.broadcast_in_dim %2309, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %2311 = stablehlo.broadcast_in_dim %arg1091, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %2312 = stablehlo.subtract %2310, %2311 : tensor<2x512x15x20xbf16>
      %2313 = stablehlo.broadcast_in_dim %2312, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %2314 = stablehlo.broadcast_in_dim %arg1092, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %2315 = stablehlo.multiply %2313, %2314 : tensor<2x512x15x20xbf16>
      %2316 = stablehlo.broadcast_in_dim %2315, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %2317 = stablehlo.broadcast_in_dim %arg1093, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %2318 = stablehlo.multiply %2316, %2317 : tensor<2x512x15x20xbf16>
      %2319 = stablehlo.broadcast_in_dim %2318, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %2320 = stablehlo.broadcast_in_dim %arg1094, dims = [1, 2, 3] : (tensor<512x1x1xbf16>) -> tensor<2x512x15x20xbf16>
      %2321 = stablehlo.add %2319, %2320 : tensor<2x512x15x20xbf16>
      %2322 = stablehlo.broadcast_in_dim %2321, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %2323 = stablehlo.maximum %1639, %2322 : tensor<2x512x15x20xbf16>
      %2324 = stablehlo.minimum %1639, %2322 : tensor<2x512x15x20xbf16>
      %2325 = stablehlo.broadcast_in_dim %2324, dims = [0, 1, 2, 3] : (tensor<2x512x15x20xbf16>) -> tensor<2x512x15x20xbf16>
      %2326 = stablehlo.multiply %2325, %1646 : tensor<2x512x15x20xbf16>
      %2327 = stablehlo.add %2323, %2326 : tensor<2x512x15x20xbf16>
      %2328 = stablehlo.convolution(%2327, %arg660) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x512x15x20xbf16>, tensor<1024x512x3x3xbf16>) -> tensor<2x1024x15x20xbf16>
      %2329 = stablehlo.broadcast_in_dim %2328, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %2330 = stablehlo.broadcast_in_dim %arg1095, dims = [1, 2, 3] : (tensor<1024x1x1xbf16>) -> tensor<2x1024x15x20xbf16>
      %2331 = stablehlo.subtract %2329, %2330 : tensor<2x1024x15x20xbf16>
      %2332 = stablehlo.broadcast_in_dim %2331, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %2333 = stablehlo.broadcast_in_dim %arg1096, dims = [1, 2, 3] : (tensor<1024x1x1xbf16>) -> tensor<2x1024x15x20xbf16>
      %2334 = stablehlo.multiply %2332, %2333 : tensor<2x1024x15x20xbf16>
      %2335 = stablehlo.broadcast_in_dim %2334, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %2336 = stablehlo.broadcast_in_dim %arg1097, dims = [1, 2, 3] : (tensor<1024x1x1xbf16>) -> tensor<2x1024x15x20xbf16>
      %2337 = stablehlo.multiply %2335, %2336 : tensor<2x1024x15x20xbf16>
      %2338 = stablehlo.broadcast_in_dim %2337, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %2339 = stablehlo.broadcast_in_dim %arg1098, dims = [1, 2, 3] : (tensor<1024x1x1xbf16>) -> tensor<2x1024x15x20xbf16>
      %2340 = stablehlo.add %2338, %2339 : tensor<2x1024x15x20xbf16>
      %2341 = stablehlo.broadcast_in_dim %2340, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %2342 = stablehlo.maximum %1662, %2341 : tensor<2x1024x15x20xbf16>
      %2343 = stablehlo.minimum %1662, %2341 : tensor<2x1024x15x20xbf16>
      %2344 = stablehlo.broadcast_in_dim %2343, dims = [0, 1, 2, 3] : (tensor<2x1024x15x20xbf16>) -> tensor<2x1024x15x20xbf16>
      %2345 = stablehlo.multiply %2344, %1667 : tensor<2x1024x15x20xbf16>
      %2346 = stablehlo.add %2342, %2345 : tensor<2x1024x15x20xbf16>
      %2347 = stablehlo.convolution(%2346, %arg661) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x1024x15x20xbf16>, tensor<255x1024x1x1xbf16>) -> tensor<2x255x15x20xbf16>
      %2348 = stablehlo.reshape %arg662 : (tensor<255xbf16>) -> tensor<255x1x1xbf16>
      %2349 = stablehlo.broadcast_in_dim %2347, dims = [0, 1, 2, 3] : (tensor<2x255x15x20xbf16>) -> tensor<2x255x15x20xbf16>
      %2350 = stablehlo.broadcast_in_dim %2348, dims = [1, 2, 3] : (tensor<255x1x1xbf16>) -> tensor<2x255x15x20xbf16>
      %2351 = stablehlo.add %2349, %2350 : tensor<2x255x15x20xbf16>
      sdy.return %2073, %2212, %2351 : tensor<2x255x60x80xbf16>, tensor<2x255x30x40xbf16>, tensor<2x255x15x20xbf16>
    } : (tensor<32x3x3x3xbf16>, tensor<64x32x3x3xbf16>, tensor<64x64x1x1xbf16>, tensor<64x64x1x1xbf16>, tensor<32x64x1x1xbf16>, tensor<64x32x3x3xbf16>, tensor<64x64x1x1xbf16>, tensor<64x128x1x1xbf16>, tensor<128x64x3x3xbf16>, tensor<64x128x1x1xbf16>, tensor<64x128x1x1xbf16>, tensor<64x64x1x1xbf16>, tensor<64x64x3x3xbf16>, tensor<64x64x1x1xbf16>, tensor<64x64x3x3xbf16>, tensor<64x64x1x1xbf16>, tensor<128x128x1x1xbf16>, tensor<256x128x3x3xbf16>, tensor<128x256x1x1xbf16>, tensor<128x256x1x1xbf16>, tensor<128x128x1x1xbf16>, tensor<128x128x3x3xbf16>, tensor<128x128x1x1xbf16>, tensor<128x128x3x3xbf16>, tensor<128x128x1x1xbf16>, tensor<128x128x3x3xbf16>, tensor<128x128x1x1xbf16>, tensor<128x128x3x3xbf16>, tensor<128x128x1x1xbf16>, tensor<128x128x3x3xbf16>, tensor<128x128x1x1xbf16>, tensor<128x128x3x3xbf16>, tensor<128x128x1x1xbf16>, tensor<128x128x3x3xbf16>, tensor<128x128x1x1xbf16>, tensor<128x128x3x3xbf16>, tensor<128x128x1x1xbf16>, tensor<256x256x1x1xbf16>, tensor<512x256x3x3xbf16>, tensor<256x512x1x1xbf16>, tensor<256x512x1x1xbf16>, tensor<256x256x1x1xbf16>, tensor<256x256x3x3xbf16>, tensor<256x256x1x1xbf16>, tensor<256x256x3x3xbf16>, tensor<256x256x1x1xbf16>, tensor<256x256x3x3xbf16>, tensor<256x256x1x1xbf16>, tensor<256x256x3x3xbf16>, tensor<256x256x1x1xbf16>, tensor<256x256x3x3xbf16>, tensor<256x256x1x1xbf16>, tensor<256x256x3x3xbf16>, tensor<256x256x1x1xbf16>, tensor<256x256x3x3xbf16>, tensor<256x256x1x1xbf16>, tensor<256x256x3x3xbf16>, tensor<256x256x1x1xbf16>, tensor<512x512x1x1xbf16>, tensor<1024x512x3x3xbf16>, tensor<512x1024x1x1xbf16>, tensor<512x1024x1x1xbf16>, tensor<512x512x1x1xbf16>, tensor<512x512x3x3xbf16>, tensor<512x512x1x1xbf16>, tensor<512x512x3x3xbf16>, tensor<512x512x1x1xbf16>, tensor<512x512x3x3xbf16>, tensor<512x512x1x1xbf16>, tensor<512x512x3x3xbf16>, tensor<512x512x1x1xbf16>, tensor<1024x1024x1x1xbf16>, tensor<512x1024x1x1xbf16>, tensor<1024x512x3x3xbf16>, tensor<512x1024x1x1xbf16>, tensor<512x2048x1x1xbf16>, tensor<1024x512x3x3xbf16>, tensor<512x1024x1x1xbf16>, tensor<256x512x1x1xbf16>, tensor<256x512x1x1xbf16>, tensor<256x512x1x1xbf16>, tensor<512x256x3x3xbf16>, tensor<256x512x1x1xbf16>, tensor<512x256x3x3xbf16>, tensor<256x512x1x1xbf16>, tensor<128x256x1x1xbf16>, tensor<128x256x1x1xbf16>, tensor<128x256x1x1xbf16>, tensor<256x128x3x3xbf16>, tensor<128x256x1x1xbf16>, tensor<256x128x3x3xbf16>, tensor<128x256x1x1xbf16>, tensor<256x128x3x3xbf16>, tensor<255x256x1x1xbf16>, tensor<255xbf16>, tensor<256x128x3x3xbf16>, tensor<256x512x1x1xbf16>, tensor<512x256x3x3xbf16>, tensor<256x512x1x1xbf16>, tensor<512x256x3x3xbf16>, tensor<256x512x1x1xbf16>, tensor<512x256x3x3xbf16>, tensor<255x512x1x1xbf16>, tensor<255xbf16>, tensor<512x256x3x3xbf16>, tensor<512x1024x1x1xbf16>, tensor<1024x512x3x3xbf16>, tensor<512x1024x1x1xbf16>, tensor<1024x512x3x3xbf16>, tensor<512x1024x1x1xbf16>, tensor<1024x512x3x3xbf16>, tensor<255x1024x1x1xbf16>, tensor<255xbf16>, tensor<32x1x1xbf16>, tensor<32x1x1xbf16>, tensor<32x1x1xbf16>, tensor<32x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<32x1x1xbf16>, tensor<32x1x1xbf16>, tensor<32x1x1xbf16>, tensor<32x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<64x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<30xf32>, tensor<40xf32>, tensor<1024x15x30xbf16>, tensor<1024x20x40xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<60xf32>, tensor<80xf32>, tensor<512x30x60xbf16>, tensor<512x40x80xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<128x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<256x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<512x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<1024x1x1xbf16>, tensor<4x3x480x640xbf16>) -> (tensor<4x255x60x80xbf16>, tensor<4x255x30x40xbf16>, tensor<4x255x15x20xbf16>)
    return %0#0, %0#1, %0#2 : tensor<4x255x60x80xbf16>, tensor<4x255x30x40xbf16>, tensor<4x255x15x20xbf16>
  }
}

