WARNING:root:Defaulting to PJRT_DEVICE=CPU
============================= test session starts ==============================
platform linux -- Python 3.10.12, pytest-8.4.1, pluggy-1.6.0 -- /localdev/jameszianxu/gen_mc/tt-torch/env/venv/bin/python3.10
cachedir: .pytest_cache
rootdir: /localdev/jameszianxu/gen_mc/tt-torch
configfile: pyproject.toml
plugins: cov-6.2.1, forked-1.6.0, xdist-3.8.0, split-0.10.0
collecting ... collected 1 item

tests/models/llama/test_llama3_generative.py::test_llama3_generate Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 51.54it/s]
[James] Manually forwarding attention mask
Initial prompt: '<|begin_of_text|>I like taking walks in the'
<|begin_of_text|>I like taking walks in the2025-08-12 15:38:59.557 (   0.000s) [        C545D1C0]      dylib_platform.cc:47       1| DylibPlatform::SubclassInitialize
2025-08-12 15:38:59.558 (   0.001s) [        C545D1C0]     client_instance.cc:39       1| ClientInstance::ClientInstance
2025-08-12 15:38:59.558 (   0.001s) [        C545D1C0]              client.cc:18       1| TTClientInstance::TTClientInstance
2025-08-12 15:38:59.558 (   0.001s) [        C545D1C0]     client_instance.cc:60       1| ClientInstance::Initialize
2025-08-12 15:39:02.841 (   3.284s) [        C545D1C0]              stubs.inc:112   WARN| STUB: PJRT_Client_TopologyDescription
2025-08-12 15:39:02.841 (   3.284s) [        C545D1C0]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-08-12 15:39:02.841 (   3.284s) [        C545D1C0]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-08-12 15:39:02.841 (   3.284s) [        C545D1C0]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-08-12 15:39:02.841 (   3.284s) [        C545D1C0]     client_instance.cc:383      1| ClientInstance::PJRT_Client_PlatformVersion
2025-08-12 15:39:02.841 (   3.284s) [        C545D1C0]     client_instance.cc:363      1| ClientInstance::PJRT_Client_PlatformName
2025-08-12 15:39:02.842 (   3.284s) [        C545D1C0]     client_instance.cc:395      1| ClientInstance::PJRT_Client_Devices
2025-08-12 15:39:02.842 (   3.284s) [        C545D1C0]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-08-12 15:39:02.842 (   3.284s) [        C545D1C0]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-08-12 15:39:02.842 (   3.284s) [        C545D1C0]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-08-12 15:39:02.842 (   3.284s) [        C545D1C0]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-08-12 15:39:02.842 (   3.284s) [        C545D1C0]     client_instance.cc:408      1| ClientInstance::PJRT_Client_AddressableDevices
2025-08-12 15:39:02.842 (   3.284s) [        C545D1C0]     client_instance.cc:458      1| ClientInstance::PJRT_Client_AddressableMemories
2025-08-12 15:39:02.842 (   3.284s) [        C545D1C0]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-08-12 15:39:02.842 (   3.284s) [        C545D1C0]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-08-12 15:39:02.842 (   3.284s) [        C545D1C0]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-08-12 15:39:02.842 (   3.284s) [        C545D1C0]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-08-12 15:39:02.842 (   3.284s) [        C545D1C0]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-08-12 15:39:02.842 (   3.284s) [        C545D1C0]        api_bindings.cc:76       1| PJRT_Plugin_Attributes
2025-08-12 15:39:02.842198: W torch_xla/csrc/runtime/profiler.cpp:88] Profiler API not found for PJRT plugin
2025-08-12 15:39:02.842 (   3.285s) [        C545D1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-12 15:39:02.842 (   3.285s) [        C545D1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-12 15:39:02.842 (   3.285s) [        C545D1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-12 15:39:02.842 (   3.285s) [        C545D1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-12 15:39:02.842 (   3.285s) [        C545D1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-12 15:39:02.842 (   3.285s) [        C545D1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-12 15:39:02.842 (   3.285s) [        C545D1C0]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-08-12 15:39:02.842 (   3.285s) [        C545D1C0]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-08-12 15:39:02.842 (   3.285s) [        C545D1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-12 15:39:02.842 (   3.285s) [        C545D1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-12 15:39:02.842 (   3.285s) [        C545D1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-12 15:39:02.842 (   3.285s) [        C545D1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-12 15:39:02.842 (   3.285s) [        C545D1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-12 15:39:02.843 (   3.286s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:02.843 (   3.286s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:02.843 (   3.286s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:02.843 (   3.286s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:02.843 (   3.286s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:02.843 (   3.286s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:02.843 (   3.286s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:02.843 (   3.286s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:02.843 (   3.286s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:02.844 (   3.286s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:02.844 (   3.286s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:02.844 (   3.286s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:02.844 (   3.286s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:02.844 (   3.286s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:02.844 (   3.286s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:02.844 (   3.286s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:02.844 (   3.286s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:02.844 (   3.286s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:02.844 (   3.287s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:02.844 (   3.287s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:02.844 (   3.287s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:02.844 (   3.287s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:02.844 (   3.287s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:02.844 (   3.287s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:02.844 (   3.287s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:02.844 (   3.287s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:02.844 (   3.287s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:02.844 (   3.287s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:02.844 (   3.287s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:02.844 (   3.287s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:02.876 (   3.319s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:02.876 (   3.319s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:02.876 (   3.319s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:02.876 (   3.319s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [128256, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:02.876 (   3.319s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:02.876 (   3.319s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:02.877 (   3.320s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:02.877 (   3.320s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:02.877 (   3.320s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:02.877 (   3.320s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1, 64, 1] (semantics: ZeroCopy/other)
2025-08-12 15:39:02.877 (   3.320s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:02.877 (   3.320s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:02.892 (   3.335s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:02.892 (   3.335s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:02.892 (   3.335s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:02.892 (   3.335s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:02.892 (   3.335s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:02.892 (   3.335s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:02.896 (   3.339s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:02.896 (   3.339s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:02.896 (   3.339s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:02.897 (   3.339s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 1024] (semantics: ZeroCopy/other)
2025-08-12 15:39:02.897 (   3.339s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:02.897 (   3.339s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:02.901 (   3.344s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:02.901 (   3.344s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:02.901 (   3.344s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:02.901 (   3.344s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 1024] (semantics: ZeroCopy/other)
2025-08-12 15:39:02.901 (   3.344s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:02.901 (   3.344s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:02.916 (   3.358s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:02.916 (   3.358s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:02.916 (   3.358s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:02.916 (   3.359s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:02.916 (   3.359s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:02.916 (   3.359s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:02.959 (   3.402s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:02.959 (   3.402s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:02.959 (   3.402s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:02.959 (   3.402s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 8192] (semantics: ZeroCopy/other)
2025-08-12 15:39:02.959 (   3.402s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:02.959 (   3.402s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:03.002 (   3.445s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:03.002 (   3.445s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:03.002 (   3.445s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:03.002 (   3.445s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 8192] (semantics: ZeroCopy/other)
2025-08-12 15:39:03.002 (   3.445s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:03.002 (   3.445s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:03.043 (   3.485s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:03.043 (   3.486s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:03.043 (   3.486s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:03.043 (   3.486s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [8192, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:03.043 (   3.486s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:03.043 (   3.486s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:03.057 (   3.500s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:03.057 (   3.500s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:03.057 (   3.500s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:03.057 (   3.500s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:03.058 (   3.500s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:03.058 (   3.500s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:03.062 (   3.505s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:03.062 (   3.505s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:03.062 (   3.505s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:03.063 (   3.505s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 1024] (semantics: ZeroCopy/other)
2025-08-12 15:39:03.063 (   3.505s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:03.063 (   3.505s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:03.067 (   3.510s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:03.067 (   3.510s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:03.067 (   3.510s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:03.068 (   3.510s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 1024] (semantics: ZeroCopy/other)
2025-08-12 15:39:03.068 (   3.510s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:03.068 (   3.510s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:03.082 (   3.525s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:03.082 (   3.525s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:03.082 (   3.525s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:03.082 (   3.525s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:03.082 (   3.525s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:03.082 (   3.525s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:03.125 (   3.568s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:03.125 (   3.568s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:03.125 (   3.568s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:03.125 (   3.568s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 8192] (semantics: ZeroCopy/other)
2025-08-12 15:39:03.125 (   3.568s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:03.125 (   3.568s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:03.168 (   3.611s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:03.168 (   3.611s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:03.168 (   3.611s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:03.168 (   3.611s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 8192] (semantics: ZeroCopy/other)
2025-08-12 15:39:03.168 (   3.611s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:03.168 (   3.611s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:03.208 (   3.651s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:03.208 (   3.651s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:03.208 (   3.651s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:03.209 (   3.651s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [8192, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:03.209 (   3.651s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:03.209 (   3.651s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:03.912 (   4.355s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:03.912 (   4.355s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:03.912 (   4.355s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:03.912 (   4.355s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 128256] (semantics: ZeroCopy/other)
2025-08-12 15:39:03.912 (   4.355s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:03.912 (   4.355s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:03.914 (   4.356s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:03.914 (   4.356s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:03.914 (   4.356s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:03.914 (   4.356s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:03.914 (   4.356s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:03.914 (   4.356s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:03.914 (   4.357s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:03.914 (   4.357s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:03.914 (   4.357s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:03.914 (   4.357s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1024, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:03.914 (   4.357s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:03.914 (   4.357s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:03.915 (   4.358s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:03.915 (   4.358s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:03.915 (   4.358s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:03.915 (   4.358s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1024, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:03.915 (   4.358s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:03.915 (   4.358s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:03.916 (   4.359s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:03.916 (   4.359s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:03.916 (   4.359s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:03.916 (   4.359s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:03.916 (   4.359s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:03.916 (   4.359s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:03.919 (   4.361s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:03.919 (   4.361s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:03.919 (   4.361s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:03.919 (   4.361s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [8192, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:03.919 (   4.361s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:03.919 (   4.362s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:03.921 (   4.364s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:03.921 (   4.364s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:03.921 (   4.364s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:03.921 (   4.364s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [8192, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:03.921 (   4.364s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:03.921 (   4.364s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:03.923 (   4.366s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:03.923 (   4.366s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:03.923 (   4.366s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:03.923 (   4.366s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 8192] (semantics: ZeroCopy/other)
2025-08-12 15:39:03.923 (   4.366s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:03.924 (   4.366s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:03.924 (   4.367s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:03.924 (   4.367s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:03.924 (   4.367s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:03.925 (   4.367s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:03.925 (   4.367s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:03.925 (   4.367s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:03.925 (   4.368s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:03.925 (   4.368s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:03.925 (   4.368s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:03.925 (   4.368s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1024, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:03.925 (   4.368s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:03.925 (   4.368s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:03.926 (   4.368s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:03.926 (   4.368s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:03.926 (   4.368s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:03.926 (   4.368s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1024, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:03.926 (   4.368s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:03.926 (   4.369s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:03.927 (   4.369s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:03.927 (   4.369s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:03.927 (   4.369s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:03.927 (   4.370s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:03.927 (   4.370s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:03.927 (   4.370s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:03.933 (   4.376s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:03.933 (   4.376s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:03.933 (   4.376s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:03.934 (   4.376s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [8192, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:03.934 (   4.376s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:03.934 (   4.376s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:03.936 (   4.379s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:03.936 (   4.379s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:03.936 (   4.379s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:03.936 (   4.379s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [8192, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:03.936 (   4.379s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:03.936 (   4.379s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:03.938 (   4.381s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:03.938 (   4.381s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:03.938 (   4.381s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:03.938 (   4.381s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 8192] (semantics: ZeroCopy/other)
2025-08-12 15:39:03.938 (   4.381s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:03.938 (   4.381s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:03.970 (   4.413s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:03.970 (   4.413s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:03.970 (   4.413s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:03.970 (   4.413s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [128256, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:03.971 (   4.413s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:03.971 (   4.413s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:03.971 (   4.414s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:03.971 (   4.414s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:03.971 (   4.414s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:03.971 (   4.414s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1, 8, 128, 128] (semantics: ZeroCopy/other)
2025-08-12 15:39:03.971 (   4.414s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:03.971 (   4.414s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:03.971 (   4.414s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:03.971 (   4.414s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:03.971 (   4.414s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:03.971 (   4.414s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1, 8, 128, 128] (semantics: ZeroCopy/other)
2025-08-12 15:39:03.971 (   4.414s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:03.971 (   4.414s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:03.972 (   4.414s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:03.972 (   4.414s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:03.972 (   4.414s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:03.972 (   4.414s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1, 8, 128, 128] (semantics: ZeroCopy/other)
2025-08-12 15:39:03.972 (   4.414s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:03.972 (   4.414s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:03.972 (   4.415s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:03.972 (   4.415s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:03.972 (   4.415s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:03.972 (   4.415s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1, 8, 128, 128] (semantics: ZeroCopy/other)
2025-08-12 15:39:03.972 (   4.415s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:03.972 (   4.415s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:03.972 (   4.415s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:03.972 (   4.415s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:03.972 (   4.415s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:03.972 (   4.415s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [64] (semantics: ZeroCopy/other)
2025-08-12 15:39:03.972 (   4.415s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:03.972 (   4.415s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:04.031 (   4.473s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:04.031 (   4.473s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:04.031 (   4.473s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:04.031 (   4.473s) [        C545D1C0]     buffer_instance.cc:213      1| [BUFFER] Creating OWNED host tensor with shape [1, 7] (semantics: other, supported_dtype: false)
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-08-12 15:39:04.031 (   4.474s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:04.031 (   4.474s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:04.031 (   4.474s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:04.031 (   4.474s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:04.031 (   4.474s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:04.031 (   4.474s) [        C545D1C0]     buffer_instance.cc:213      1| [BUFFER] Creating OWNED host tensor with shape [7] (semantics: other, supported_dtype: false)
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-08-12 15:39:04.031 (   4.474s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:04.031 (   4.474s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:04.031 (   4.474s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:04.031 (   4.474s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:04.031 (   4.474s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:04.031 (   4.474s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1, 1, 7, 128] (semantics: ZeroCopy/other)
2025-08-12 15:39:04.031 (   4.474s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:04.031 (   4.474s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:04.033 (   4.476s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:04.033 (   4.476s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:04.033 (   4.476s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:04.033 (   4.476s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1] (semantics: ZeroCopy/other)
2025-08-12 15:39:04.033 (   4.476s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:04.033 (   4.476s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:04.035 (   4.477s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:04.035 (   4.477s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:04.035 (   4.477s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:04.035 (   4.477s) [        C545D1C0]     buffer_instance.cc:213      1| [BUFFER] Creating OWNED host tensor with shape [1] (semantics: other, supported_dtype: false)
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-08-12 15:39:04.035 (   4.477s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:04.035 (   4.477s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:04.035 (   4.478s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:04.035 (   4.478s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:04.035 (   4.478s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:04.035 (   4.478s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1] (semantics: ZeroCopy/other)
2025-08-12 15:39:04.035 (   4.478s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:04.035 (   4.478s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:04.041 (   4.483s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.483s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
Note: Using experimental XLA backend.
[James] disable rectify buffer inplace copy
Tensor id via xlac: 1 with shape torch.Size([3072])
Tensor id via xlac: 2 with shape torch.Size([3072])
Tensor id via xlac: 3 with shape torch.Size([3072])
Tensor id via xlac: 4 with shape torch.Size([3072])
Tensor id via xlac: 5 with shape torch.Size([3072])
Tensor id via xlac: 6 with shape torch.Size([128256, 3072])
Tensor id via xlac: 7 with shape torch.Size([1, 64, 1])
Tensor id via xlac: 8 with shape torch.Size([3072, 3072])
Tensor id via xlac: 9 with shape torch.Size([3072, 1024])
Tensor id via xlac: 10 with shape torch.Size([3072, 1024])
Tensor id via xlac: 11 with shape torch.Size([3072, 3072])
Tensor id via xlac: 12 with shape torch.Size([3072, 8192])
Tensor id via xlac: 13 with shape torch.Size([3072, 8192])
Tensor id via xlac: 14 with shape torch.Size([8192, 3072])
Tensor id via xlac: 15 with shape torch.Size([3072, 3072])
Tensor id via xlac: 16 with shape torch.Size([3072, 1024])
Tensor id via xlac: 17 with shape torch.Size([3072, 1024])
Tensor id via xlac: 18 with shape torch.Size([3072, 3072])
Tensor id via xlac: 19 with shape torch.Size([3072, 8192])
Tensor id via xlac: 20 with shape torch.Size([3072, 8192])
Tensor id via xlac: 21 with shape torch.Size([8192, 3072])
Tensor id via xlac: 22 with shape torch.Size([3072, 128256])
Tensor id via xlac: 23 with shape torch.Size([3072, 3072])
Tensor id via xlac: 24 with shape torch.Size([1024, 3072])
Tensor id via xlac: 25 with shape torch.Size([1024, 3072])
Tensor id via xlac: 26 with shape torch.Size([3072, 3072])
Tensor id via xlac: 27 with shape torch.Size([8192, 3072])
Tensor id via xlac: 28 with shape torch.Size([8192, 3072])
Tensor id via xlac: 29 with shape torch.Size([3072, 8192])
Tensor id via xlac: 30 with shape torch.Size([3072, 3072])
Tensor id via xlac: 31 with shape torch.Size([1024, 3072])
Tensor id via xlac: 32 with shape torch.Size([1024, 3072])
Tensor id via xlac: 33 with shape torch.Size([3072, 3072])
Tensor id via xlac: 34 with shape torch.Size([8192, 3072])
Tensor id via xlac: 35 with shape torch.Size([8192, 3072])
Tensor id via xlac: 36 with shape torch.Size([3072, 8192])
Tensor id via xlac: 37 with shape torch.Size([128256, 3072])
Tensor id via xlac: 38 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 39 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 40 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 41 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 42 with shape torch.Size([64])
Tensor id via xlac: 43 with shape torch.Size([1, 7])
Tensor id via xlac: 44 with shape torch.Size([7])
Tensor id via xlac: 45 with shape torch.Size([1, 1, 7, 128])
[XLA Cache] New inputs: ['input_0_shape_[3072]_dtype_torch.bfloat16 (idx=0)', 'input_1_shape_[3072]_dtype_torch.bfloat16 (idx=1)', 'input_2_shape_[3072]_dtype_torch.bfloat16 (idx=2)', 'input_3_shape_[3072]_dtype_torch.bfloat16 (idx=3)', 'input_4_shape_[3072]_dtype_torch.bfloat16 (idx=4)', 'input_5_shape_[128256, 3072]_dtype_torch.bfloat16 (idx=5)', 'input_6_shape_[1, 64, 1]_dtype_torch.float32 (idx=6)', 'input_7_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=7)', 'input_8_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=8)', 'input_9_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=9)', 'input_10_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=10)', 'input_11_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=11)', 'input_12_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=12)', 'input_13_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=13)', 'input_14_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=14)', 'input_15_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=15)', 'input_16_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=16)', 'input_17_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=17)', 'input_18_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=18)', 'input_19_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=19)', 'input_20_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=20)', 'input_21_shape_[3072, 128256]_dtype_torch.bfloat16 (idx=21)', 'input_22_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=22)', 'input_23_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=23)', 'input_24_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=24)', 'input_25_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=25)', 'input_26_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=26)', 'input_27_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=27)', 'input_28_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=28)', 'input_29_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=29)', 'input_30_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=30)', 'input_31_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=31)', 'input_32_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=32)', 'input_33_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=33)', 'input_34_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=34)', 'input_35_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=35)', 'input_36_shape_[128256, 3072]_dtype_torch.bfloat16 (idx=36)', 'input_37_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=37)', 'input_38_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=38)', 'input_39_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=39)', 'input_40_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=40)', 'input_41_shape_[64]_dtype_torch.float32 (idx=41)', 'input_42_shape_[1, 7]_dtype_torch.int64 (idx=42)', 'input_43_shape_[7]_dtype_torch.int64 (idx=43)', 'input_44_shape_[1, 1, 7, 128]_dtype_torch.bfloat16 (idx=44)']
[XLA Cache] Total cache size: 45 unique tensors
Hlo input positions pre normalization [22, 375, 21, 20, 18, 17, 14, 13, 11, 10, 43, 37, 1, 44, -1, 39, 45, 312, 7, 9, 38, 8, 2, 12, 3, 41, 16, 40, 15, 4, 19, 5]
Hlo input positions post normalization [23, 376, 22, 21, 19, 18, 15, 14, 12, 11, 44, 38, 2, 45, 0, 40, 46, 313, 8, 10, 39, 9, 3, 13, 4, 42, 17, 41, 16, 5, 20, 6]
match key in_spec.target L__self___model_layers__modules__0___input_layernorm_weight with ID 139658584088448 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_layers__modules__0___post_attention_layernorm_weight with ID 139658589162128 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_layers__modules__1___input_layernorm_weight with ID 139658589165728 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_layers__modules__1___post_attention_layernorm_weight with ID 139658581142368 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_norm_weight with ID 139658581024640 and kind InputKind.PARAMETER
match key in_spec.target L__self___lm_head.weight with ID 139658584091808 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.0 with ID 139654937127536 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.1 with ID 139659373550368 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.2 with ID 139654937229920 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.3 with ID 139654937230480 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.4 with ID 139654937230560 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.5 with ID 139654937229120 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.6 with ID 139654937229200 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.7 with ID 139654937230800 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.8 with ID 139654937228480 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.9 with ID 139654937230320 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.10 with ID 139654937231040 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.11 with ID 139654937228560 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.12 with ID 139654937228640 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.13 with ID 139654937228400 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.14 with ID 139654937229040 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.15 with ID 139654937228960 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_q_proj.weight with ID 139658589160768 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_k_proj.weight with ID 139658589162688 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_v_proj.weight with ID 139658589173008 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_o_proj.weight with ID 139658589164768 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___mlp_gate_proj.weight with ID 139658584087008 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___mlp_up_proj.weight with ID 139658589172608 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___mlp_down_proj.weight with ID 139658584095888 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_q_proj.weight with ID 139658581139488 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_k_proj.weight with ID 139658581145168 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_v_proj.weight with ID 139658581134368 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_o_proj.weight with ID 139658581144048 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___mlp_gate_proj.weight with ID 139658589162288 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___mlp_up_proj.weight with ID 139658589171488 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___mlp_down_proj.weight with ID 139658589163008 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___lm_head.weight with ID 139658584091808 and kind InputKind.PARAMETER
match key in_spec.target kwargs____past_key_values___key_cache_0 with ID 139659373973792 and kind InputKind.BUFFER
match key in_spec.target kwargs____past_key_values___value_cache_0 with ID 139659373983872 and kind InputKind.BUFFER
match key in_spec.target kwargs____past_key_values___key_cache_1 with ID 139659373976192 and kind InputKind.BUFFER
match key in_spec.target kwargs____past_key_values___value_cache_1 with ID 139659373976432 and kind InputKind.BUFFER
match key in_spec.target const_subgraph_module.L__self___model_rotary_emb_inv_freq with ID 139658581032480 and kind InputKind.BUFFER
[JAMES] setting arg ref map to  refs=139658589162688,constant_unknown,139658589160768,139654937228960,139654937228400,139654937228640,139654937230320,139654937228480,139654937229200,139654937229120,user_input,139659373983872,139658589165728,constant_unknown,139658584088448,139659373976432,constant_unknown,constant_unknown,139654937229920,139654937230560,139659373976192,139654937230480,139658581142368,139654937230800,139658581024640,user_input,139654937228560,139658581032480,139654937231040,139658584091808,139654937229040,139654937127536
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.041 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.484s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.485s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.485s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.485s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.485s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.485s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.485s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.485s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.485s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.485s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.485s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.485s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.485s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.485s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.485s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.485s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.485s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.485s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.485s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.485s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.485s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.485s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.485s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.485s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.485s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.042 (   4.485s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.043 (   4.485s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.043 (   4.485s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.043 (   4.486s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.043 (   4.486s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.043 (   4.486s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.043 (   4.486s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.043 (   4.486s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.043 (   4.486s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.043 (   4.486s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.043 (   4.486s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.043 (   4.486s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.043 (   4.486s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.043 (   4.486s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.043 (   4.486s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.043 (   4.486s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.043 (   4.486s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.044 (   4.486s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.044 (   4.486s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.044 (   4.487s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.044 (   4.487s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.044 (   4.487s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:04.077 (   4.520s) [        C545D1C0]     client_instance.cc:471      1| ClientInstance::PJRT_Client_Compile
2025-08-12 15:39:04.077 (   4.520s) [        C545D1C0]      module_builder.cc:101      1| ModuleBuilder::buildModule
2025-08-12 15:39:04.079 (   4.522s) [        C545D1C0]      module_builder.cc:155      1| VHLO Module:
module @SyncTensorsGraph.613 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<7x!vhlo.i64_v1>, %arg1: !vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>, %arg2: !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg4: !vhlo.tensor_v1<1x7x!vhlo.i64_v1>, %arg5: !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, %arg6: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg7: !vhlo.tensor_v1<!vhlo.i64_v1>, %arg8: !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, %arg9: !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>, %arg10: !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, %arg11: !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>, %arg13: !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>, %arg14: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg15: !vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>, %arg16: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg17: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg18: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg19: !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>, %arg20: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg21: !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, %arg22: !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>, %arg23: !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, %arg24: !vhlo.tensor_v1<3072x128256x!vhlo.bf16_v1>, %arg25: !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>, %arg26: !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>, %arg27: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg28: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg29: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg30: !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>, %arg31: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<3.25520843E-4> : tensor<1x7xf32>>}> : () -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2.000000e+00> : tensor<1x7x3072xf32>>}> : () -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF800000> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %3 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %4 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0> : tensor<7xi64>>}> : () -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %5 = "vhlo.compare_v1"(%arg0, %4) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 LT>}> : (!vhlo.tensor_v1<7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.bool_v1>
    %6 = "vhlo.broadcast_in_dim_v1"(%arg7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %7 = "vhlo.add_v1"(%arg0, %6) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %8 = "vhlo.select_v1"(%5, %7, %arg0) : (!vhlo.tensor_v1<7x!vhlo.bool_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %9 = "vhlo.reshape_v1"(%8) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x1x!vhlo.i64_v1>
    %10 = "vhlo.convert_v1"(%arg6) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %11 = "vhlo.broadcast_in_dim_v1"(%10) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %12 = "vhlo.reshape_v1"(%arg4) : (!vhlo.tensor_v1<1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %13 = "vhlo.convert_v1"(%12) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.ui32_v1>
    %14 = "vhlo.gather_v2"(%arg5, %13) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 3072]> : tensor<2xi64>>, start_index_map = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x!vhlo.ui32_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %15 = "vhlo.reshape_v1"(%14) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %16 = "vhlo.convert_v1"(%15) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %17 = "vhlo.power_v1"(%16, %1) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %18 = "vhlo.reduce_v1"(%17, %3) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg32: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg33: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %284 = "vhlo.add_v1"(%arg32, %arg33) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%284) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %19 = "vhlo.multiply_v1"(%18, %0) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %20 = "vhlo.reshape_v1"(%19) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %21 = "vhlo.broadcast_in_dim_v1"(%arg3) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %22 = "vhlo.add_v1"(%20, %21) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %23 = "vhlo.rsqrt_v2"(%22) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %24 = "vhlo.reshape_v1"(%23) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %25 = "vhlo.broadcast_in_dim_v1"(%24) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %26 = "vhlo.multiply_v1"(%16, %25) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %27 = "vhlo.convert_v1"(%26) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %28 = "vhlo.convert_v1"(%27) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %29 = "vhlo.multiply_v1"(%11, %28) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %30 = "vhlo.convert_v1"(%29) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %31 = "vhlo.reshape_v1"(%30) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %32 = "vhlo.dot_general_v2"(%31, %arg2) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>
    %33 = "vhlo.reshape_v1"(%32) : (!vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>
    %34 = "vhlo.transpose_v1"(%33) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %35 = "vhlo.convert_v1"(%34) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %36 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>
    %37 = "vhlo.convert_v1"(%36) : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.f32_v1>
    %38 = "vhlo.dot_general_v2"(%arg1, %37) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>
    %39 = "vhlo.transpose_v1"(%38) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1]> : tensor<3xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[1, 2, 0]> : tensor<3xindex>>, xla_shape = #vhlo.string_v1<"f32[1,7,64]{1,2,0}">} : (!vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>
    %40 = "vhlo.concatenate_v1"(%39, %39) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %41 = "vhlo.cosine_v2"(%40) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %42 = "vhlo.convert_v1"(%41) : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>
    %43 = "vhlo.reshape_v1"(%42) : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>
    %44 = "vhlo.convert_v1"(%43) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>
    %45 = "vhlo.reshape_v1"(%44) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %46 = "vhlo.broadcast_in_dim_v1"(%45) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %47 = "vhlo.multiply_v1"(%35, %46) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %48 = "vhlo.convert_v1"(%47) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %49 = "vhlo.slice_v1"(%34) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 7, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %50 = "vhlo.negate_v1"(%49) : (!vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %51 = "vhlo.slice_v1"(%34) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 7, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %52 = "vhlo.concatenate_v1"(%50, %51) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %53 = "vhlo.convert_v1"(%52) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %54 = "vhlo.sine_v2"(%40) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %55 = "vhlo.convert_v1"(%54) : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>
    %56 = "vhlo.reshape_v1"(%55) : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>
    %57 = "vhlo.convert_v1"(%56) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>
    %58 = "vhlo.reshape_v1"(%57) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %59 = "vhlo.broadcast_in_dim_v1"(%58) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %60 = "vhlo.multiply_v1"(%53, %59) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %61 = "vhlo.convert_v1"(%60) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %62 = "vhlo.add_v1"(%48, %61) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %63 = "vhlo.scatter_v2"(%arg8, %9, %62) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg33: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg33) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>
    %64 = "vhlo.dot_general_v2"(%31, %arg9) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>
    %65 = "vhlo.reshape_v1"(%64) : (!vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>
    %66 = "vhlo.transpose_v1"(%65) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %67 = "vhlo.scatter_v2"(%arg10, %9, %66) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg33: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg33) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>
    %68 = "vhlo.convert_v1"(%arg20) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %69 = "vhlo.broadcast_in_dim_v1"(%68) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %70 = "vhlo.dot_general_v2"(%31, %arg17) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %71 = "vhlo.reshape_v1"(%70) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>
    %72 = "vhlo.transpose_v1"(%71) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,24,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %73 = "vhlo.convert_v1"(%72) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,24,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %74 = "vhlo.broadcast_in_dim_v1"(%45) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %75 = "vhlo.multiply_v1"(%73, %74) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %76 = "vhlo.convert_v1"(%75) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %77 = "vhlo.slice_v1"(%72) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 7, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>
    %78 = "vhlo.negate_v1"(%77) : (!vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>
    %79 = "vhlo.slice_v1"(%72) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 7, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>
    %80 = "vhlo.concatenate_v1"(%78, %79) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %81 = "vhlo.convert_v1"(%80) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %82 = "vhlo.broadcast_in_dim_v1"(%58) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %83 = "vhlo.multiply_v1"(%81, %82) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %84 = "vhlo.convert_v1"(%83) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %85 = "vhlo.add_v1"(%76, %84) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %86 = "vhlo.reshape_v1"(%85) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>
    %87 = "vhlo.broadcast_in_dim_v1"(%63) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>
    %88 = "vhlo.reshape_v1"(%87) : (!vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>
    %89 = "vhlo.transpose_v1"(%88) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 3, 1, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,24,128,128]{2,3,1,0}">} : (!vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>
    %90 = "vhlo.reshape_v1"(%89) : (!vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>
    %91 = "vhlo.dot_general_v2"(%86, %90) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>
    %92 = "vhlo.reshape_v1"(%91) : (!vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %93 = "vhlo.convert_v1"(%92) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %94 = "vhlo.broadcast_in_dim_v1"(%arg16) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %95 = "vhlo.multiply_v1"(%93, %94) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %96 = "vhlo.convert_v1"(%95) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %97 = "vhlo.reshape_v1"(%arg15) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>
    %98 = "vhlo.broadcast_in_dim_v1"(%97) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %99 = "vhlo.add_v1"(%96, %98) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %100 = "vhlo.convert_v1"(%99) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %101 = "vhlo.reduce_v1"(%100, %2) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg32: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg33: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %284 = "vhlo.maximum_v1"(%arg32, %arg33) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%284) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>
    %102 = "vhlo.broadcast_in_dim_v1"(%101) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %103 = "vhlo.subtract_v1"(%100, %102) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %104 = "vhlo.exponential_v2"(%103) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %105 = "vhlo.reduce_v1"(%104, %3) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg32: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg33: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %284 = "vhlo.add_v1"(%arg32, %arg33) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%284) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>
    %106 = "vhlo.broadcast_in_dim_v1"(%105) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %107 = "vhlo.divide_v1"(%104, %106) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %108 = "vhlo.convert_v1"(%107) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %109 = "vhlo.reshape_v1"(%108) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>
    %110 = "vhlo.broadcast_in_dim_v1"(%67) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>
    %111 = "vhlo.reshape_v1"(%110) : (!vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>
    %112 = "vhlo.dot_general_v2"(%109, %111) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>
    %113 = "vhlo.reshape_v1"(%112) : (!vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %114 = "vhlo.transpose_v1"(%113) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,7,24,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>
    %115 = "vhlo.reshape_v1"(%114) : (!vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %116 = "vhlo.dot_general_v2"(%115, %arg14) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %117 = "vhlo.reshape_v1"(%116) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %118 = "vhlo.add_v1"(%15, %117) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %119 = "vhlo.convert_v1"(%arg18) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %120 = "vhlo.broadcast_in_dim_v1"(%119) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %121 = "vhlo.convert_v1"(%118) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %122 = "vhlo.power_v1"(%121, %1) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %123 = "vhlo.reduce_v1"(%122, %3) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg32: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg33: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %284 = "vhlo.add_v1"(%arg32, %arg33) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%284) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %124 = "vhlo.multiply_v1"(%123, %0) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %125 = "vhlo.reshape_v1"(%124) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %126 = "vhlo.add_v1"(%125, %21) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %127 = "vhlo.rsqrt_v2"(%126) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %128 = "vhlo.reshape_v1"(%127) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %129 = "vhlo.broadcast_in_dim_v1"(%128) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %130 = "vhlo.multiply_v1"(%121, %129) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %131 = "vhlo.convert_v1"(%130) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %132 = "vhlo.convert_v1"(%131) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %133 = "vhlo.multiply_v1"(%120, %132) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %134 = "vhlo.convert_v1"(%133) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %135 = "vhlo.reshape_v1"(%134) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %136 = "vhlo.dot_general_v2"(%135, %arg19) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %137 = "vhlo.reshape_v1"(%136) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %138 = "vhlo.convert_v1"(%137) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %139 = "vhlo.logistic_v2"(%137) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %140 = "vhlo.convert_v1"(%139) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %141 = "vhlo.multiply_v1"(%138, %140) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %142 = "vhlo.convert_v1"(%141) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %143 = "vhlo.convert_v1"(%142) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %144 = "vhlo.dot_general_v2"(%135, %arg13) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %145 = "vhlo.reshape_v1"(%144) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %146 = "vhlo.convert_v1"(%145) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %147 = "vhlo.multiply_v1"(%143, %146) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %148 = "vhlo.convert_v1"(%147) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %149 = "vhlo.reshape_v1"(%148) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %150 = "vhlo.dot_general_v2"(%149, %arg12) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %151 = "vhlo.reshape_v1"(%150) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %152 = "vhlo.add_v1"(%118, %151) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %153 = "vhlo.convert_v1"(%152) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %154 = "vhlo.power_v1"(%153, %1) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %155 = "vhlo.reduce_v1"(%154, %3) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg32: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg33: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %284 = "vhlo.add_v1"(%arg32, %arg33) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%284) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %156 = "vhlo.multiply_v1"(%155, %0) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %157 = "vhlo.reshape_v1"(%156) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %158 = "vhlo.add_v1"(%157, %21) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %159 = "vhlo.rsqrt_v2"(%158) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %160 = "vhlo.reshape_v1"(%159) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %161 = "vhlo.broadcast_in_dim_v1"(%160) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %162 = "vhlo.multiply_v1"(%153, %161) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %163 = "vhlo.convert_v1"(%162) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %164 = "vhlo.convert_v1"(%163) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %165 = "vhlo.multiply_v1"(%69, %164) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %166 = "vhlo.convert_v1"(%165) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %167 = "vhlo.reshape_v1"(%166) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %168 = "vhlo.dot_general_v2"(%167, %arg11) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>
    %169 = "vhlo.reshape_v1"(%168) : (!vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>
    %170 = "vhlo.transpose_v1"(%169) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %171 = "vhlo.convert_v1"(%170) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %172 = "vhlo.multiply_v1"(%171, %46) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %173 = "vhlo.convert_v1"(%172) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %174 = "vhlo.slice_v1"(%170) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 7, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %175 = "vhlo.negate_v1"(%174) : (!vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %176 = "vhlo.slice_v1"(%170) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 7, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %177 = "vhlo.concatenate_v1"(%175, %176) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %178 = "vhlo.convert_v1"(%177) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %179 = "vhlo.multiply_v1"(%178, %59) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %180 = "vhlo.convert_v1"(%179) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %181 = "vhlo.add_v1"(%173, %180) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %182 = "vhlo.scatter_v2"(%arg21, %9, %181) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg33: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg33) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>
    %183 = "vhlo.dot_general_v2"(%167, %arg22) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>
    %184 = "vhlo.reshape_v1"(%183) : (!vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>
    %185 = "vhlo.transpose_v1"(%184) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %186 = "vhlo.scatter_v2"(%arg23, %9, %185) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg33: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg33) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>
    %187 = "vhlo.convert_v1"(%arg31) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %188 = "vhlo.broadcast_in_dim_v1"(%187) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %189 = "vhlo.dot_general_v2"(%167, %arg28) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %190 = "vhlo.reshape_v1"(%189) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>
    %191 = "vhlo.transpose_v1"(%190) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,24,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %192 = "vhlo.convert_v1"(%191) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,24,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %193 = "vhlo.multiply_v1"(%192, %74) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %194 = "vhlo.convert_v1"(%193) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %195 = "vhlo.slice_v1"(%191) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 7, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>
    %196 = "vhlo.negate_v1"(%195) : (!vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>
    %197 = "vhlo.slice_v1"(%191) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 7, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>
    %198 = "vhlo.concatenate_v1"(%196, %197) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %199 = "vhlo.convert_v1"(%198) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %200 = "vhlo.multiply_v1"(%199, %82) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %201 = "vhlo.convert_v1"(%200) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %202 = "vhlo.add_v1"(%194, %201) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %203 = "vhlo.reshape_v1"(%202) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>
    %204 = "vhlo.broadcast_in_dim_v1"(%182) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>
    %205 = "vhlo.reshape_v1"(%204) : (!vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>
    %206 = "vhlo.transpose_v1"(%205) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 3, 1, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,24,128,128]{2,3,1,0}">} : (!vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>
    %207 = "vhlo.reshape_v1"(%206) : (!vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>
    %208 = "vhlo.dot_general_v2"(%203, %207) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>
    %209 = "vhlo.reshape_v1"(%208) : (!vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %210 = "vhlo.convert_v1"(%209) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %211 = "vhlo.multiply_v1"(%210, %94) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %212 = "vhlo.convert_v1"(%211) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %213 = "vhlo.add_v1"(%212, %98) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %214 = "vhlo.convert_v1"(%213) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %215 = "vhlo.reduce_v1"(%214, %2) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg32: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg33: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %284 = "vhlo.maximum_v1"(%arg32, %arg33) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%284) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>
    %216 = "vhlo.broadcast_in_dim_v1"(%215) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %217 = "vhlo.subtract_v1"(%214, %216) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %218 = "vhlo.exponential_v2"(%217) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %219 = "vhlo.reduce_v1"(%218, %3) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg32: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg33: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %284 = "vhlo.add_v1"(%arg32, %arg33) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%284) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>
    %220 = "vhlo.broadcast_in_dim_v1"(%219) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %221 = "vhlo.divide_v1"(%218, %220) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %222 = "vhlo.convert_v1"(%221) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %223 = "vhlo.reshape_v1"(%222) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>
    %224 = "vhlo.broadcast_in_dim_v1"(%186) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>
    %225 = "vhlo.reshape_v1"(%224) : (!vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>
    %226 = "vhlo.dot_general_v2"(%223, %225) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>
    %227 = "vhlo.reshape_v1"(%226) : (!vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %228 = "vhlo.transpose_v1"(%227) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,7,24,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>
    %229 = "vhlo.reshape_v1"(%228) : (!vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %230 = "vhlo.dot_general_v2"(%229, %arg27) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %231 = "vhlo.reshape_v1"(%230) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %232 = "vhlo.add_v1"(%152, %231) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %233 = "vhlo.convert_v1"(%arg29) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %234 = "vhlo.broadcast_in_dim_v1"(%233) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %235 = "vhlo.convert_v1"(%232) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %236 = "vhlo.power_v1"(%235, %1) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %237 = "vhlo.reduce_v1"(%236, %3) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg32: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg33: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %284 = "vhlo.add_v1"(%arg32, %arg33) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%284) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %238 = "vhlo.multiply_v1"(%237, %0) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %239 = "vhlo.reshape_v1"(%238) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %240 = "vhlo.add_v1"(%239, %21) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %241 = "vhlo.rsqrt_v2"(%240) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %242 = "vhlo.reshape_v1"(%241) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %243 = "vhlo.broadcast_in_dim_v1"(%242) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %244 = "vhlo.multiply_v1"(%235, %243) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %245 = "vhlo.convert_v1"(%244) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %246 = "vhlo.convert_v1"(%245) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %247 = "vhlo.multiply_v1"(%234, %246) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %248 = "vhlo.convert_v1"(%247) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %249 = "vhlo.reshape_v1"(%248) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %250 = "vhlo.dot_general_v2"(%249, %arg30) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %251 = "vhlo.reshape_v1"(%250) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %252 = "vhlo.convert_v1"(%251) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %253 = "vhlo.logistic_v2"(%251) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %254 = "vhlo.convert_v1"(%253) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %255 = "vhlo.multiply_v1"(%252, %254) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %256 = "vhlo.convert_v1"(%255) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %257 = "vhlo.convert_v1"(%256) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %258 = "vhlo.dot_general_v2"(%249, %arg26) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %259 = "vhlo.reshape_v1"(%258) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %260 = "vhlo.convert_v1"(%259) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %261 = "vhlo.multiply_v1"(%257, %260) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %262 = "vhlo.convert_v1"(%261) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %263 = "vhlo.reshape_v1"(%262) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %264 = "vhlo.dot_general_v2"(%263, %arg25) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %265 = "vhlo.reshape_v1"(%264) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %266 = "vhlo.add_v1"(%232, %265) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %267 = "vhlo.convert_v1"(%266) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %268 = "vhlo.power_v1"(%267, %1) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %269 = "vhlo.reduce_v1"(%268, %3) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg32: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg33: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %284 = "vhlo.add_v1"(%arg32, %arg33) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%284) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %270 = "vhlo.multiply_v1"(%269, %0) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %271 = "vhlo.reshape_v1"(%270) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %272 = "vhlo.add_v1"(%271, %21) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %273 = "vhlo.rsqrt_v2"(%272) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %274 = "vhlo.reshape_v1"(%273) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %275 = "vhlo.broadcast_in_dim_v1"(%274) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %276 = "vhlo.multiply_v1"(%267, %275) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %277 = "vhlo.convert_v1"(%276) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %278 = "vhlo.convert_v1"(%277) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %279 = "vhlo.multiply_v1"(%188, %278) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %280 = "vhlo.convert_v1"(%279) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %281 = "vhlo.reshape_v1"(%280) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %282 = "vhlo.dot_general_v2"(%281, %arg24) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>
    %283 = "vhlo.reshape_v1"(%282) : (!vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>
    "vhlo.return_v1"(%63, %67, %182, %186, %282, %283) : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}
2025-08-12 15:39:04.093 (   4.535s) [        C545D1C0]      module_builder.cc:188      1| SHLO Module:
module @SyncTensorsGraph.613 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<7xi64>, %arg1: tensor<1x64x1xf32>, %arg2: tensor<3072x1024xbf16>, %arg3: tensor<f32>, %arg4: tensor<1x7xi64>, %arg5: tensor<128256x3072xbf16>, %arg6: tensor<3072xbf16>, %arg7: tensor<i64>, %arg8: tensor<1x8x128x128xbf16>, %arg9: tensor<3072x1024xbf16>, %arg10: tensor<1x8x128x128xbf16>, %arg11: tensor<3072x1024xbf16>, %arg12: tensor<8192x3072xbf16>, %arg13: tensor<3072x8192xbf16>, %arg14: tensor<3072x3072xbf16>, %arg15: tensor<1x1x7x128xbf16>, %arg16: tensor<f32>, %arg17: tensor<3072x3072xbf16>, %arg18: tensor<3072xbf16>, %arg19: tensor<3072x8192xbf16>, %arg20: tensor<3072xbf16>, %arg21: tensor<1x8x128x128xbf16>, %arg22: tensor<3072x1024xbf16>, %arg23: tensor<1x8x128x128xbf16>, %arg24: tensor<3072x128256xbf16>, %arg25: tensor<8192x3072xbf16>, %arg26: tensor<3072x8192xbf16>, %arg27: tensor<3072x3072xbf16>, %arg28: tensor<3072x3072xbf16>, %arg29: tensor<3072xbf16>, %arg30: tensor<3072x8192xbf16>, %arg31: tensor<3072xbf16>) -> (tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>) {
    %cst = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<1x7x3072xf32>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<0> : tensor<7xi64>
    %0 = stablehlo.compare  LT, %arg0, %c : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %1 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %2 = stablehlo.add %arg0, %1 : tensor<7xi64>
    %3 = stablehlo.select %0, %2, %arg0 : tensor<7xi1>, tensor<7xi64>
    %4 = stablehlo.reshape %3 : (tensor<7xi64>) -> tensor<7x1xi64>
    %5 = stablehlo.convert %arg6 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %6 = stablehlo.broadcast_in_dim %5, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %7 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<7xi64>
    %8 = stablehlo.convert %7 : (tensor<7xi64>) -> tensor<7xui32>
    %9 = "stablehlo.gather"(%arg5, %8) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %10 = stablehlo.reshape %9 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %11 = stablehlo.convert %10 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %12 = stablehlo.power %11, %cst_0 : tensor<1x7x3072xf32>
    %13 = stablehlo.reduce(%12 init: %cst_2) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %14 = stablehlo.multiply %13, %cst : tensor<1x7xf32>
    %15 = stablehlo.reshape %14 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %16 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %17 = stablehlo.add %15, %16 : tensor<1x7x1xf32>
    %18 = stablehlo.rsqrt %17 : tensor<1x7x1xf32>
    %19 = stablehlo.reshape %18 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %20 = stablehlo.broadcast_in_dim %19, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %21 = stablehlo.multiply %11, %20 : tensor<1x7x3072xf32>
    %22 = stablehlo.convert %21 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %23 = stablehlo.convert %22 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %24 = stablehlo.multiply %6, %23 : tensor<1x7x3072xf32>
    %25 = stablehlo.convert %24 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %26 = stablehlo.reshape %25 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %27 = stablehlo.dot_general %26, %arg2, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %28 = stablehlo.reshape %27 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %29 = stablehlo.transpose %28, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %30 = stablehlo.convert %29 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %31 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %32 = stablehlo.convert %31 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %33 = stablehlo.dot_general %arg1, %32, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %34 = stablehlo.transpose %33, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %35 = stablehlo.concatenate %34, %34, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %36 = stablehlo.cosine %35 : tensor<1x7x128xf32>
    %37 = stablehlo.convert %36 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %38 = stablehlo.reshape %37 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %39 = stablehlo.convert %38 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %40 = stablehlo.reshape %39 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %41 = stablehlo.broadcast_in_dim %40, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %42 = stablehlo.multiply %30, %41 : tensor<1x8x7x128xf32>
    %43 = stablehlo.convert %42 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %44 = stablehlo.slice %29 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %45 = stablehlo.negate %44 : tensor<1x8x7x64xbf16>
    %46 = stablehlo.slice %29 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %47 = stablehlo.concatenate %45, %46, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %48 = stablehlo.convert %47 : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %49 = stablehlo.sine %35 : tensor<1x7x128xf32>
    %50 = stablehlo.convert %49 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %51 = stablehlo.reshape %50 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %52 = stablehlo.convert %51 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %53 = stablehlo.reshape %52 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %54 = stablehlo.broadcast_in_dim %53, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %55 = stablehlo.multiply %48, %54 : tensor<1x8x7x128xf32>
    %56 = stablehlo.convert %55 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %57 = stablehlo.add %43, %56 : tensor<1x8x7x128xbf16>
    %58 = "stablehlo.scatter"(%arg8, %4, %57) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg32: tensor<bf16>, %arg33: tensor<bf16>):
      stablehlo.return %arg33 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x128x128xbf16>
    %59 = stablehlo.dot_general %26, %arg9, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %60 = stablehlo.reshape %59 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %61 = stablehlo.transpose %60, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = "stablehlo.scatter"(%arg10, %4, %61) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg32: tensor<bf16>, %arg33: tensor<bf16>):
      stablehlo.return %arg33 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x128x128xbf16>
    %63 = stablehlo.convert %arg20 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %64 = stablehlo.broadcast_in_dim %63, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %65 = stablehlo.dot_general %26, %arg17, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %66 = stablehlo.reshape %65 : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %67 = stablehlo.transpose %66, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %68 = stablehlo.convert %67 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %69 = stablehlo.broadcast_in_dim %40, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %70 = stablehlo.multiply %68, %69 : tensor<1x24x7x128xf32>
    %71 = stablehlo.convert %70 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %72 = stablehlo.slice %67 [0:1, 0:24, 0:7, 64:128] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %73 = stablehlo.negate %72 : tensor<1x24x7x64xbf16>
    %74 = stablehlo.slice %67 [0:1, 0:24, 0:7, 0:64] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %75 = stablehlo.concatenate %73, %74, dim = 3 : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %76 = stablehlo.convert %75 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %77 = stablehlo.broadcast_in_dim %53, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %78 = stablehlo.multiply %76, %77 : tensor<1x24x7x128xf32>
    %79 = stablehlo.convert %78 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %80 = stablehlo.add %71, %79 : tensor<1x24x7x128xbf16>
    %81 = stablehlo.reshape %80 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %82 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x8x3x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %84 = stablehlo.transpose %83, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,128]{2,3,1,0}"} : (tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %85 = stablehlo.reshape %84 : (tensor<1x24x128x128xbf16>) -> tensor<24x128x128xbf16>
    %86 = stablehlo.dot_general %81, %85, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x7x128xbf16>
    %87 = stablehlo.reshape %86 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %88 = stablehlo.convert %87 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %89 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<1x24x7x128xf32>
    %90 = stablehlo.multiply %88, %89 : tensor<1x24x7x128xf32>
    %91 = stablehlo.convert %90 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.reshape %arg15 : (tensor<1x1x7x128xbf16>) -> tensor<1x7x128xbf16>
    %93 = stablehlo.broadcast_in_dim %92, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %94 = stablehlo.add %91, %93 : tensor<1x24x7x128xbf16>
    %95 = stablehlo.convert %94 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %96 = stablehlo.reduce(%95 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x7x128xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %97 = stablehlo.broadcast_in_dim %96, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x128xf32>
    %98 = stablehlo.subtract %95, %97 : tensor<1x24x7x128xf32>
    %99 = stablehlo.exponential %98 : tensor<1x24x7x128xf32>
    %100 = stablehlo.reduce(%99 init: %cst_2) applies stablehlo.add across dimensions = [3] : (tensor<1x24x7x128xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %101 = stablehlo.broadcast_in_dim %100, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.divide %99, %101 : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.reshape %103 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %105 = stablehlo.broadcast_in_dim %62, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %106 = stablehlo.reshape %105 : (tensor<1x8x3x128x128xbf16>) -> tensor<24x128x128xbf16>
    %107 = stablehlo.dot_general %104, %106, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x7x128xbf16>
    %108 = stablehlo.reshape %107 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %109 = stablehlo.transpose %108, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %110 = stablehlo.reshape %109 : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %111 = stablehlo.dot_general %110, %arg14, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %112 = stablehlo.reshape %111 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %113 = stablehlo.add %10, %112 : tensor<1x7x3072xbf16>
    %114 = stablehlo.convert %arg18 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %115 = stablehlo.broadcast_in_dim %114, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %116 = stablehlo.convert %113 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %117 = stablehlo.power %116, %cst_0 : tensor<1x7x3072xf32>
    %118 = stablehlo.reduce(%117 init: %cst_2) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %119 = stablehlo.multiply %118, %cst : tensor<1x7xf32>
    %120 = stablehlo.reshape %119 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %121 = stablehlo.add %120, %16 : tensor<1x7x1xf32>
    %122 = stablehlo.rsqrt %121 : tensor<1x7x1xf32>
    %123 = stablehlo.reshape %122 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %124 = stablehlo.broadcast_in_dim %123, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %125 = stablehlo.multiply %116, %124 : tensor<1x7x3072xf32>
    %126 = stablehlo.convert %125 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %127 = stablehlo.convert %126 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %128 = stablehlo.multiply %115, %127 : tensor<1x7x3072xf32>
    %129 = stablehlo.convert %128 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %131 = stablehlo.dot_general %130, %arg19, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %132 = stablehlo.reshape %131 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %133 = stablehlo.convert %132 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %134 = stablehlo.logistic %132 : tensor<1x7x8192xbf16>
    %135 = stablehlo.convert %134 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %136 = stablehlo.multiply %133, %135 : tensor<1x7x8192xf32>
    %137 = stablehlo.convert %136 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %138 = stablehlo.convert %137 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %139 = stablehlo.dot_general %130, %arg13, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %140 = stablehlo.reshape %139 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %141 = stablehlo.convert %140 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %142 = stablehlo.multiply %138, %141 : tensor<1x7x8192xf32>
    %143 = stablehlo.convert %142 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %144 = stablehlo.reshape %143 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %145 = stablehlo.dot_general %144, %arg12, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %146 = stablehlo.reshape %145 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %147 = stablehlo.add %113, %146 : tensor<1x7x3072xbf16>
    %148 = stablehlo.convert %147 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %149 = stablehlo.power %148, %cst_0 : tensor<1x7x3072xf32>
    %150 = stablehlo.reduce(%149 init: %cst_2) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %151 = stablehlo.multiply %150, %cst : tensor<1x7xf32>
    %152 = stablehlo.reshape %151 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %153 = stablehlo.add %152, %16 : tensor<1x7x1xf32>
    %154 = stablehlo.rsqrt %153 : tensor<1x7x1xf32>
    %155 = stablehlo.reshape %154 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %157 = stablehlo.multiply %148, %156 : tensor<1x7x3072xf32>
    %158 = stablehlo.convert %157 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %159 = stablehlo.convert %158 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %160 = stablehlo.multiply %64, %159 : tensor<1x7x3072xf32>
    %161 = stablehlo.convert %160 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %162 = stablehlo.reshape %161 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %163 = stablehlo.dot_general %162, %arg11, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %164 = stablehlo.reshape %163 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %165 = stablehlo.transpose %164, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %166 = stablehlo.convert %165 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %167 = stablehlo.multiply %166, %41 : tensor<1x8x7x128xf32>
    %168 = stablehlo.convert %167 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %169 = stablehlo.slice %165 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %170 = stablehlo.negate %169 : tensor<1x8x7x64xbf16>
    %171 = stablehlo.slice %165 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %172 = stablehlo.concatenate %170, %171, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %173 = stablehlo.convert %172 : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %174 = stablehlo.multiply %173, %54 : tensor<1x8x7x128xf32>
    %175 = stablehlo.convert %174 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %176 = stablehlo.add %168, %175 : tensor<1x8x7x128xbf16>
    %177 = "stablehlo.scatter"(%arg21, %4, %176) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg32: tensor<bf16>, %arg33: tensor<bf16>):
      stablehlo.return %arg33 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x128x128xbf16>
    %178 = stablehlo.dot_general %162, %arg22, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %179 = stablehlo.reshape %178 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %180 = stablehlo.transpose %179, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %181 = "stablehlo.scatter"(%arg23, %4, %180) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg32: tensor<bf16>, %arg33: tensor<bf16>):
      stablehlo.return %arg33 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x128x128xbf16>
    %182 = stablehlo.convert %arg31 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %183 = stablehlo.broadcast_in_dim %182, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %184 = stablehlo.dot_general %162, %arg28, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %185 = stablehlo.reshape %184 : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %186 = stablehlo.transpose %185, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %187 = stablehlo.convert %186 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %188 = stablehlo.multiply %187, %69 : tensor<1x24x7x128xf32>
    %189 = stablehlo.convert %188 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %190 = stablehlo.slice %186 [0:1, 0:24, 0:7, 64:128] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %191 = stablehlo.negate %190 : tensor<1x24x7x64xbf16>
    %192 = stablehlo.slice %186 [0:1, 0:24, 0:7, 0:64] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %193 = stablehlo.concatenate %191, %192, dim = 3 : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %194 = stablehlo.convert %193 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %195 = stablehlo.multiply %194, %77 : tensor<1x24x7x128xf32>
    %196 = stablehlo.convert %195 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %197 = stablehlo.add %189, %196 : tensor<1x24x7x128xbf16>
    %198 = stablehlo.reshape %197 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %199 = stablehlo.broadcast_in_dim %177, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %200 = stablehlo.reshape %199 : (tensor<1x8x3x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %201 = stablehlo.transpose %200, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,128]{2,3,1,0}"} : (tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %202 = stablehlo.reshape %201 : (tensor<1x24x128x128xbf16>) -> tensor<24x128x128xbf16>
    %203 = stablehlo.dot_general %198, %202, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x7x128xbf16>
    %204 = stablehlo.reshape %203 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %205 = stablehlo.convert %204 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %206 = stablehlo.multiply %205, %89 : tensor<1x24x7x128xf32>
    %207 = stablehlo.convert %206 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %208 = stablehlo.add %207, %93 : tensor<1x24x7x128xbf16>
    %209 = stablehlo.convert %208 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %210 = stablehlo.reduce(%209 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x7x128xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %211 = stablehlo.broadcast_in_dim %210, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x128xf32>
    %212 = stablehlo.subtract %209, %211 : tensor<1x24x7x128xf32>
    %213 = stablehlo.exponential %212 : tensor<1x24x7x128xf32>
    %214 = stablehlo.reduce(%213 init: %cst_2) applies stablehlo.add across dimensions = [3] : (tensor<1x24x7x128xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %215 = stablehlo.broadcast_in_dim %214, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x128xf32>
    %216 = stablehlo.divide %213, %215 : tensor<1x24x7x128xf32>
    %217 = stablehlo.convert %216 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %218 = stablehlo.reshape %217 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %219 = stablehlo.broadcast_in_dim %181, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %220 = stablehlo.reshape %219 : (tensor<1x8x3x128x128xbf16>) -> tensor<24x128x128xbf16>
    %221 = stablehlo.dot_general %218, %220, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x7x128xbf16>
    %222 = stablehlo.reshape %221 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %223 = stablehlo.transpose %222, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %224 = stablehlo.reshape %223 : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %225 = stablehlo.dot_general %224, %arg27, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %226 = stablehlo.reshape %225 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %227 = stablehlo.add %147, %226 : tensor<1x7x3072xbf16>
    %228 = stablehlo.convert %arg29 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %229 = stablehlo.broadcast_in_dim %228, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %230 = stablehlo.convert %227 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %231 = stablehlo.power %230, %cst_0 : tensor<1x7x3072xf32>
    %232 = stablehlo.reduce(%231 init: %cst_2) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %233 = stablehlo.multiply %232, %cst : tensor<1x7xf32>
    %234 = stablehlo.reshape %233 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %235 = stablehlo.add %234, %16 : tensor<1x7x1xf32>
    %236 = stablehlo.rsqrt %235 : tensor<1x7x1xf32>
    %237 = stablehlo.reshape %236 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %238 = stablehlo.broadcast_in_dim %237, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %239 = stablehlo.multiply %230, %238 : tensor<1x7x3072xf32>
    %240 = stablehlo.convert %239 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %241 = stablehlo.convert %240 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %242 = stablehlo.multiply %229, %241 : tensor<1x7x3072xf32>
    %243 = stablehlo.convert %242 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %244 = stablehlo.reshape %243 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %245 = stablehlo.dot_general %244, %arg30, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %246 = stablehlo.reshape %245 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %247 = stablehlo.convert %246 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %248 = stablehlo.logistic %246 : tensor<1x7x8192xbf16>
    %249 = stablehlo.convert %248 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %250 = stablehlo.multiply %247, %249 : tensor<1x7x8192xf32>
    %251 = stablehlo.convert %250 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %252 = stablehlo.convert %251 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %253 = stablehlo.dot_general %244, %arg26, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %254 = stablehlo.reshape %253 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %255 = stablehlo.convert %254 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %256 = stablehlo.multiply %252, %255 : tensor<1x7x8192xf32>
    %257 = stablehlo.convert %256 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %258 = stablehlo.reshape %257 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %259 = stablehlo.dot_general %258, %arg25, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %260 = stablehlo.reshape %259 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %261 = stablehlo.add %227, %260 : tensor<1x7x3072xbf16>
    %262 = stablehlo.convert %261 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %263 = stablehlo.power %262, %cst_0 : tensor<1x7x3072xf32>
    %264 = stablehlo.reduce(%263 init: %cst_2) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %265 = stablehlo.multiply %264, %cst : tensor<1x7xf32>
    %266 = stablehlo.reshape %265 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %267 = stablehlo.add %266, %16 : tensor<1x7x1xf32>
    %268 = stablehlo.rsqrt %267 : tensor<1x7x1xf32>
    %269 = stablehlo.reshape %268 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %270 = stablehlo.broadcast_in_dim %269, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %271 = stablehlo.multiply %262, %270 : tensor<1x7x3072xf32>
    %272 = stablehlo.convert %271 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %273 = stablehlo.convert %272 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %274 = stablehlo.multiply %183, %273 : tensor<1x7x3072xf32>
    %275 = stablehlo.convert %274 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %276 = stablehlo.reshape %275 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %277 = stablehlo.dot_general %276, %arg24, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %278 = stablehlo.reshape %277 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %58, %62, %177, %181, %277, %278 : tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}
2025-08-12 15:39:04.166 (   4.609s) [        C545D1C0]      module_builder.cc:205      1| SHLO StableHLO Pipeline Module:
module @SyncTensorsGraph.613 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=1]>
  func.func @main(%arg0: tensor<7xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<1x64x1xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg2: tensor<3072x1024xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg3: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg4: tensor<1x7xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg5: tensor<128256x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg6: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg7: tensor<i64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg8: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg9: tensor<3072x1024xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg10: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg11: tensor<3072x1024xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg12: tensor<8192x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg13: tensor<3072x8192xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg14: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg15: tensor<1x1x7x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg16: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg17: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg18: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg19: tensor<3072x8192xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg20: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg21: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg22: tensor<3072x1024xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg23: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg24: tensor<3072x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg25: tensor<8192x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg26: tensor<3072x8192xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg27: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg28: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg29: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg30: tensor<3072x8192xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg31: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) -> (tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<1x7x3072xf32>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<0> : tensor<7xi64>
    %0 = stablehlo.compare  LT, %arg0, %c : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %1 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %2 = stablehlo.add %arg0, %1 : tensor<7xi64>
    %3 = stablehlo.select %0, %2, %arg0 : tensor<7xi1>, tensor<7xi64>
    %4 = stablehlo.reshape %3 : (tensor<7xi64>) -> tensor<7x1xi64>
    %5 = stablehlo.convert %arg6 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %6 = stablehlo.broadcast_in_dim %5, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %7 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<7xi64>
    %8 = stablehlo.convert %7 : (tensor<7xi64>) -> tensor<7xui32>
    %9 = "stablehlo.gather"(%arg5, %8) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %10 = stablehlo.reshape %9 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %11 = stablehlo.convert %10 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %12 = stablehlo.power %11, %cst_0 : tensor<1x7x3072xf32>
    %13 = stablehlo.reduce(%12 init: %cst_2) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %14 = stablehlo.multiply %13, %cst : tensor<1x7xf32>
    %15 = stablehlo.reshape %14 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %16 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %17 = stablehlo.add %15, %16 : tensor<1x7x1xf32>
    %18 = stablehlo.rsqrt %17 : tensor<1x7x1xf32>
    %19 = stablehlo.reshape %18 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %20 = stablehlo.broadcast_in_dim %19, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %21 = stablehlo.multiply %11, %20 : tensor<1x7x3072xf32>
    %22 = stablehlo.convert %21 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %23 = stablehlo.convert %22 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %24 = stablehlo.multiply %6, %23 : tensor<1x7x3072xf32>
    %25 = stablehlo.convert %24 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %26 = stablehlo.reshape %25 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %27 = stablehlo.dot_general %26, %arg2, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %28 = stablehlo.reshape %27 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %29 = stablehlo.transpose %28, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %30 = stablehlo.convert %29 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %31 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %32 = stablehlo.convert %31 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %33 = stablehlo.dot_general %arg1, %32, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %34 = stablehlo.transpose %33, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %35 = stablehlo.concatenate %34, %34, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %36 = stablehlo.cosine %35 : tensor<1x7x128xf32>
    %37 = stablehlo.convert %36 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %38 = stablehlo.reshape %37 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %39 = stablehlo.convert %38 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %40 = stablehlo.reshape %39 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %41 = stablehlo.broadcast_in_dim %40, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %42 = stablehlo.multiply %30, %41 : tensor<1x8x7x128xf32>
    %43 = stablehlo.convert %42 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %44 = stablehlo.slice %29 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %45 = stablehlo.negate %44 : tensor<1x8x7x64xbf16>
    %46 = stablehlo.slice %29 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %47 = stablehlo.concatenate %45, %46, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %48 = stablehlo.convert %47 : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %49 = stablehlo.sine %35 : tensor<1x7x128xf32>
    %50 = stablehlo.convert %49 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %51 = stablehlo.reshape %50 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %52 = stablehlo.convert %51 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %53 = stablehlo.reshape %52 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %54 = stablehlo.broadcast_in_dim %53, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %55 = stablehlo.multiply %48, %54 : tensor<1x8x7x128xf32>
    %56 = stablehlo.convert %55 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %57 = stablehlo.add %43, %56 : tensor<1x8x7x128xbf16>
    %58 = "stablehlo.scatter"(%arg8, %4, %57) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg32: tensor<bf16>, %arg33: tensor<bf16>):
      stablehlo.return %arg33 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x128x128xbf16>
    %59 = stablehlo.dot_general %26, %arg9, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %60 = stablehlo.reshape %59 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %61 = stablehlo.transpose %60, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = "stablehlo.scatter"(%arg10, %4, %61) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg32: tensor<bf16>, %arg33: tensor<bf16>):
      stablehlo.return %arg33 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x128x128xbf16>
    %63 = stablehlo.convert %arg20 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %64 = stablehlo.broadcast_in_dim %63, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %65 = stablehlo.dot_general %26, %arg17, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %66 = stablehlo.reshape %65 : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %67 = stablehlo.transpose %66, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %68 = stablehlo.convert %67 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %69 = stablehlo.broadcast_in_dim %40, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %70 = stablehlo.multiply %68, %69 : tensor<1x24x7x128xf32>
    %71 = stablehlo.convert %70 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %72 = stablehlo.slice %67 [0:1, 0:24, 0:7, 64:128] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %73 = stablehlo.negate %72 : tensor<1x24x7x64xbf16>
    %74 = stablehlo.slice %67 [0:1, 0:24, 0:7, 0:64] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %75 = stablehlo.concatenate %73, %74, dim = 3 : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %76 = stablehlo.convert %75 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %77 = stablehlo.broadcast_in_dim %53, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %78 = stablehlo.multiply %76, %77 : tensor<1x24x7x128xf32>
    %79 = stablehlo.convert %78 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %80 = stablehlo.add %71, %79 : tensor<1x24x7x128xbf16>
    %81 = stablehlo.reshape %80 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %82 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x8x3x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %84 = stablehlo.transpose %83, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,128]{2,3,1,0}"} : (tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %85 = stablehlo.reshape %84 : (tensor<1x24x128x128xbf16>) -> tensor<24x128x128xbf16>
    %86 = stablehlo.dot_general %81, %85, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x7x128xbf16>
    %87 = stablehlo.reshape %86 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %88 = stablehlo.convert %87 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %89 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<1x24x7x128xf32>
    %90 = stablehlo.multiply %88, %89 : tensor<1x24x7x128xf32>
    %91 = stablehlo.convert %90 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.reshape %arg15 : (tensor<1x1x7x128xbf16>) -> tensor<1x7x128xbf16>
    %93 = stablehlo.broadcast_in_dim %92, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %94 = stablehlo.add %91, %93 : tensor<1x24x7x128xbf16>
    %95 = stablehlo.convert %94 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %96 = stablehlo.reduce(%95 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x7x128xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %97 = stablehlo.broadcast_in_dim %96, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x128xf32>
    %98 = stablehlo.subtract %95, %97 : tensor<1x24x7x128xf32>
    %99 = stablehlo.exponential %98 : tensor<1x24x7x128xf32>
    %100 = stablehlo.reduce(%99 init: %cst_2) applies stablehlo.add across dimensions = [3] : (tensor<1x24x7x128xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %101 = stablehlo.broadcast_in_dim %100, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.divide %99, %101 : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.reshape %103 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %105 = stablehlo.broadcast_in_dim %62, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %106 = stablehlo.reshape %105 : (tensor<1x8x3x128x128xbf16>) -> tensor<24x128x128xbf16>
    %107 = stablehlo.dot_general %104, %106, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x7x128xbf16>
    %108 = stablehlo.reshape %107 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %109 = stablehlo.transpose %108, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %110 = stablehlo.reshape %109 : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %111 = stablehlo.dot_general %110, %arg14, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %112 = stablehlo.reshape %111 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %113 = stablehlo.add %10, %112 : tensor<1x7x3072xbf16>
    %114 = stablehlo.convert %arg18 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %115 = stablehlo.broadcast_in_dim %114, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %116 = stablehlo.convert %113 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %117 = stablehlo.power %116, %cst_0 : tensor<1x7x3072xf32>
    %118 = stablehlo.reduce(%117 init: %cst_2) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %119 = stablehlo.multiply %118, %cst : tensor<1x7xf32>
    %120 = stablehlo.reshape %119 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %121 = stablehlo.add %120, %16 : tensor<1x7x1xf32>
    %122 = stablehlo.rsqrt %121 : tensor<1x7x1xf32>
    %123 = stablehlo.reshape %122 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %124 = stablehlo.broadcast_in_dim %123, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %125 = stablehlo.multiply %116, %124 : tensor<1x7x3072xf32>
    %126 = stablehlo.convert %125 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %127 = stablehlo.convert %126 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %128 = stablehlo.multiply %115, %127 : tensor<1x7x3072xf32>
    %129 = stablehlo.convert %128 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %131 = stablehlo.dot_general %130, %arg19, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %132 = stablehlo.reshape %131 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %133 = stablehlo.convert %132 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %134 = stablehlo.logistic %132 : tensor<1x7x8192xbf16>
    %135 = stablehlo.convert %134 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %136 = stablehlo.multiply %133, %135 : tensor<1x7x8192xf32>
    %137 = stablehlo.convert %136 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %138 = stablehlo.convert %137 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %139 = stablehlo.dot_general %130, %arg13, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %140 = stablehlo.reshape %139 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %141 = stablehlo.convert %140 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %142 = stablehlo.multiply %138, %141 : tensor<1x7x8192xf32>
    %143 = stablehlo.convert %142 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %144 = stablehlo.reshape %143 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %145 = stablehlo.dot_general %144, %arg12, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %146 = stablehlo.reshape %145 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %147 = stablehlo.add %113, %146 : tensor<1x7x3072xbf16>
    %148 = stablehlo.convert %147 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %149 = stablehlo.power %148, %cst_0 : tensor<1x7x3072xf32>
    %150 = stablehlo.reduce(%149 init: %cst_2) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %151 = stablehlo.multiply %150, %cst : tensor<1x7xf32>
    %152 = stablehlo.reshape %151 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %153 = stablehlo.add %152, %16 : tensor<1x7x1xf32>
    %154 = stablehlo.rsqrt %153 : tensor<1x7x1xf32>
    %155 = stablehlo.reshape %154 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %157 = stablehlo.multiply %148, %156 : tensor<1x7x3072xf32>
    %158 = stablehlo.convert %157 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %159 = stablehlo.convert %158 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %160 = stablehlo.multiply %64, %159 : tensor<1x7x3072xf32>
    %161 = stablehlo.convert %160 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %162 = stablehlo.reshape %161 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %163 = stablehlo.dot_general %162, %arg11, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %164 = stablehlo.reshape %163 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %165 = stablehlo.transpose %164, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %166 = stablehlo.convert %165 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %167 = stablehlo.multiply %166, %41 : tensor<1x8x7x128xf32>
    %168 = stablehlo.convert %167 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %169 = stablehlo.slice %165 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %170 = stablehlo.negate %169 : tensor<1x8x7x64xbf16>
    %171 = stablehlo.slice %165 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %172 = stablehlo.concatenate %170, %171, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %173 = stablehlo.convert %172 : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %174 = stablehlo.multiply %173, %54 : tensor<1x8x7x128xf32>
    %175 = stablehlo.convert %174 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %176 = stablehlo.add %168, %175 : tensor<1x8x7x128xbf16>
    %177 = "stablehlo.scatter"(%arg21, %4, %176) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg32: tensor<bf16>, %arg33: tensor<bf16>):
      stablehlo.return %arg33 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x128x128xbf16>
    %178 = stablehlo.dot_general %162, %arg22, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %179 = stablehlo.reshape %178 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %180 = stablehlo.transpose %179, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %181 = "stablehlo.scatter"(%arg23, %4, %180) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg32: tensor<bf16>, %arg33: tensor<bf16>):
      stablehlo.return %arg33 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x128x128xbf16>
    %182 = stablehlo.convert %arg31 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %183 = stablehlo.broadcast_in_dim %182, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %184 = stablehlo.dot_general %162, %arg28, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %185 = stablehlo.reshape %184 : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %186 = stablehlo.transpose %185, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %187 = stablehlo.convert %186 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %188 = stablehlo.multiply %187, %69 : tensor<1x24x7x128xf32>
    %189 = stablehlo.convert %188 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %190 = stablehlo.slice %186 [0:1, 0:24, 0:7, 64:128] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %191 = stablehlo.negate %190 : tensor<1x24x7x64xbf16>
    %192 = stablehlo.slice %186 [0:1, 0:24, 0:7, 0:64] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %193 = stablehlo.concatenate %191, %192, dim = 3 : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %194 = stablehlo.convert %193 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %195 = stablehlo.multiply %194, %77 : tensor<1x24x7x128xf32>
    %196 = stablehlo.convert %195 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %197 = stablehlo.add %189, %196 : tensor<1x24x7x128xbf16>
    %198 = stablehlo.reshape %197 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %199 = stablehlo.broadcast_in_dim %177, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %200 = stablehlo.reshape %199 : (tensor<1x8x3x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %201 = stablehlo.transpose %200, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,128]{2,3,1,0}"} : (tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %202 = stablehlo.reshape %201 : (tensor<1x24x128x128xbf16>) -> tensor<24x128x128xbf16>
    %203 = stablehlo.dot_general %198, %202, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x7x128xbf16>
    %204 = stablehlo.reshape %203 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %205 = stablehlo.convert %204 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %206 = stablehlo.multiply %205, %89 : tensor<1x24x7x128xf32>
    %207 = stablehlo.convert %206 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %208 = stablehlo.add %207, %93 : tensor<1x24x7x128xbf16>
    %209 = stablehlo.convert %208 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %210 = stablehlo.reduce(%209 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x7x128xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %211 = stablehlo.broadcast_in_dim %210, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x128xf32>
    %212 = stablehlo.subtract %209, %211 : tensor<1x24x7x128xf32>
    %213 = stablehlo.exponential %212 : tensor<1x24x7x128xf32>
    %214 = stablehlo.reduce(%213 init: %cst_2) applies stablehlo.add across dimensions = [3] : (tensor<1x24x7x128xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %215 = stablehlo.broadcast_in_dim %214, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x128xf32>
    %216 = stablehlo.divide %213, %215 : tensor<1x24x7x128xf32>
    %217 = stablehlo.convert %216 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %218 = stablehlo.reshape %217 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %219 = stablehlo.broadcast_in_dim %181, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %220 = stablehlo.reshape %219 : (tensor<1x8x3x128x128xbf16>) -> tensor<24x128x128xbf16>
    %221 = stablehlo.dot_general %218, %220, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x7x128xbf16>
    %222 = stablehlo.reshape %221 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %223 = stablehlo.transpose %222, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %224 = stablehlo.reshape %223 : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %225 = stablehlo.dot_general %224, %arg27, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %226 = stablehlo.reshape %225 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %227 = stablehlo.add %147, %226 : tensor<1x7x3072xbf16>
    %228 = stablehlo.convert %arg29 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %229 = stablehlo.broadcast_in_dim %228, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %230 = stablehlo.convert %227 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %231 = stablehlo.power %230, %cst_0 : tensor<1x7x3072xf32>
    %232 = stablehlo.reduce(%231 init: %cst_2) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %233 = stablehlo.multiply %232, %cst : tensor<1x7xf32>
    %234 = stablehlo.reshape %233 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %235 = stablehlo.add %234, %16 : tensor<1x7x1xf32>
    %236 = stablehlo.rsqrt %235 : tensor<1x7x1xf32>
    %237 = stablehlo.reshape %236 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %238 = stablehlo.broadcast_in_dim %237, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %239 = stablehlo.multiply %230, %238 : tensor<1x7x3072xf32>
    %240 = stablehlo.convert %239 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %241 = stablehlo.convert %240 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %242 = stablehlo.multiply %229, %241 : tensor<1x7x3072xf32>
    %243 = stablehlo.convert %242 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %244 = stablehlo.reshape %243 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %245 = stablehlo.dot_general %244, %arg30, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %246 = stablehlo.reshape %245 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %247 = stablehlo.convert %246 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %248 = stablehlo.logistic %246 : tensor<1x7x8192xbf16>
    %249 = stablehlo.convert %248 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %250 = stablehlo.multiply %247, %249 : tensor<1x7x8192xf32>
    %251 = stablehlo.convert %250 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %252 = stablehlo.convert %251 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %253 = stablehlo.dot_general %244, %arg26, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %254 = stablehlo.reshape %253 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %255 = stablehlo.convert %254 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %256 = stablehlo.multiply %252, %255 : tensor<1x7x8192xf32>
    %257 = stablehlo.convert %256 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %258 = stablehlo.reshape %257 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %259 = stablehlo.dot_general %258, %arg25, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %260 = stablehlo.reshape %259 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %261 = stablehlo.add %227, %260 : tensor<1x7x3072xbf16>
    %262 = stablehlo.convert %261 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %263 = stablehlo.power %262, %cst_0 : tensor<1x7x3072xf32>
    %264 = stablehlo.reduce(%263 init: %cst_2) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %265 = stablehlo.multiply %264, %cst : tensor<1x7xf32>
    %266 = stablehlo.reshape %265 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %267 = stablehlo.add %266, %16 : tensor<1x7x1xf32>
    %268 = stablehlo.rsqrt %267 : tensor<1x7x1xf32>
    %269 = stablehlo.reshape %268 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %270 = stablehlo.broadcast_in_dim %269, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %271 = stablehlo.multiply %262, %270 : tensor<1x7x3072xf32>
    %272 = stablehlo.convert %271 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %273 = stablehlo.convert %272 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %274 = stablehlo.multiply %183, %273 : tensor<1x7x3072xf32>
    %275 = stablehlo.convert %274 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %276 = stablehlo.reshape %275 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %277 = stablehlo.dot_general %276, %arg24, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %278 = stablehlo.reshape %277 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %58, %62, %177, %181, %277, %278 : tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}
2025-08-12 15:39:04.178 (   4.621s) [        C545D1C0]      module_builder.cc:452      1| TTIR Module:
module @SyncTensorsGraph.613 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  func.func @main(%arg0: tensor<7xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<1x64x1xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg2: tensor<3072x1024xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg3: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg4: tensor<1x7xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg5: tensor<128256x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg6: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg7: tensor<i64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg8: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg9: tensor<3072x1024xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg10: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg11: tensor<3072x1024xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg12: tensor<8192x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg13: tensor<3072x8192xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg14: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg15: tensor<1x1x7x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg16: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg17: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg18: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg19: tensor<3072x8192xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg20: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg21: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg22: tensor<3072x1024xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg23: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg24: tensor<3072x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg25: tensor<8192x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg26: tensor<3072x8192xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg27: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg28: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg29: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg30: tensor<3072x8192xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg31: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) -> (tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<3.25520843E-4> : tensor<1x7xf32>}> : () -> tensor<1x7xf32>
    %1 = "ttir.constant"() <{value = dense<2.000000e+00> : tensor<1x7x3072xf32>}> : () -> tensor<1x7x3072xf32>
    %2 = "ttir.constant"() <{value = dense<0xFF800000> : tensor<f32>}> : () -> tensor<f32>
    %3 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<f32>}> : () -> tensor<f32>
    %4 = "ttir.constant"() <{value = dense<0> : tensor<7xi64>}> : () -> tensor<7xi64>
    %5 = ttir.empty() : tensor<7xi1>
    %6 = "ttir.lt"(%arg0, %4, %5) : (tensor<7xi64>, tensor<7xi64>, tensor<7xi1>) -> tensor<7xi1>
    %7 = ttir.empty() : tensor<1xi64>
    %8 = "ttir.reshape"(%arg7, %7) <{shape = [1 : i32]}> : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9 = ttir.empty() : tensor<7xi64>
    %10 = "ttir.broadcast"(%8, %9) <{broadcast_dimensions = array<i64: 7>}> : (tensor<1xi64>, tensor<7xi64>) -> tensor<7xi64>
    %11 = ttir.empty() : tensor<7xi64>
    %12 = "ttir.add"(%arg0, %10, %11) : (tensor<7xi64>, tensor<7xi64>, tensor<7xi64>) -> tensor<7xi64>
    %13 = ttir.empty() : tensor<7xi64>
    %14 = "ttir.where"(%6, %12, %arg0, %13) : (tensor<7xi1>, tensor<7xi64>, tensor<7xi64>, tensor<7xi64>) -> tensor<7xi64>
    %15 = ttir.empty() : tensor<7x1xi64>
    %16 = "ttir.reshape"(%14, %15) <{shape = [7 : i32, 1 : i32]}> : (tensor<7xi64>, tensor<7x1xi64>) -> tensor<7x1xi64>
    %17 = ttir.empty() : tensor<3072xf32>
    %18 = "ttir.typecast"(%arg6, %17) <{conservative_folding = false}> : (tensor<3072xbf16>, tensor<3072xf32>) -> tensor<3072xf32>
    %19 = ttir.empty() : tensor<1x1x3072xf32>
    %20 = "ttir.reshape"(%18, %19) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %21 = ttir.empty() : tensor<1x7x3072xf32>
    %22 = "ttir.broadcast"(%20, %21) <{broadcast_dimensions = array<i64: 1, 7, 1>}> : (tensor<1x1x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %23 = ttir.empty() : tensor<7xi64>
    %24 = "ttir.reshape"(%arg4, %23) <{shape = [7 : i32]}> : (tensor<1x7xi64>, tensor<7xi64>) -> tensor<7xi64>
    %25 = ttir.empty() : tensor<7xui32>
    %26 = "ttir.typecast"(%24, %25) <{conservative_folding = false}> : (tensor<7xi64>, tensor<7xui32>) -> tensor<7xui32>
    %27 = ttir.empty() : tensor<7x3072xbf16>
    %28 = "ttir.gather"(%arg5, %26, %27) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 3072>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<128256x3072xbf16>, tensor<7xui32>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %29 = ttir.empty() : tensor<1x7x3072xbf16>
    %30 = "ttir.reshape"(%28, %29) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %31 = ttir.empty() : tensor<1x7x3072xf32>
    %32 = "ttir.typecast"(%30, %31) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %33 = ttir.empty() : tensor<1x7x3072xf32>
    %34 = "ttir.pow"(%32, %1, %33) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %35 = ttir.empty() : tensor<1x7xf32>
    %36 = "ttir.sum"(%34, %35) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %37 = ttir.empty() : tensor<1x7xf32>
    %38 = "ttir.multiply"(%36, %0, %37) : (tensor<1x7xf32>, tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %39 = ttir.empty() : tensor<1x7x1xf32>
    %40 = "ttir.reshape"(%38, %39) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %41 = ttir.empty() : tensor<1x1x1xf32>
    %42 = "ttir.reshape"(%arg3, %41) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %43 = ttir.empty() : tensor<1x7x1xf32>
    %44 = "ttir.broadcast"(%42, %43) <{broadcast_dimensions = array<i64: 1, 7, 1>}> : (tensor<1x1x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %45 = ttir.empty() : tensor<1x7x1xf32>
    %46 = "ttir.add"(%40, %44, %45) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %47 = ttir.empty() : tensor<1x7x1xf32>
    %48 = "ttir.rsqrt"(%46, %47) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %49 = ttir.empty() : tensor<1x7xf32>
    %50 = "ttir.reshape"(%48, %49) <{shape = [1 : i32, 7 : i32]}> : (tensor<1x7x1xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %51 = ttir.empty() : tensor<1x7x1xf32>
    %52 = "ttir.reshape"(%50, %51) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %53 = ttir.empty() : tensor<1x7x3072xf32>
    %54 = "ttir.broadcast"(%52, %53) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x7x1xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %55 = ttir.empty() : tensor<1x7x3072xf32>
    %56 = "ttir.multiply"(%32, %54, %55) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %57 = ttir.empty() : tensor<1x7x3072xbf16>
    %58 = "ttir.typecast"(%56, %57) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %59 = ttir.empty() : tensor<1x7x3072xf32>
    %60 = "ttir.typecast"(%58, %59) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %61 = ttir.empty() : tensor<1x7x3072xf32>
    %62 = "ttir.multiply"(%22, %60, %61) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %63 = ttir.empty() : tensor<1x7x3072xbf16>
    %64 = "ttir.typecast"(%62, %63) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %65 = ttir.empty() : tensor<7x3072xbf16>
    %66 = "ttir.reshape"(%64, %65) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x7x3072xbf16>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %67 = "ttir.dot_general"(%66, %arg2) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %68 = ttir.empty() : tensor<1x7x8x128xbf16>
    %69 = "ttir.reshape"(%67, %68) <{shape = [1 : i32, 7 : i32, 8 : i32, 128 : i32]}> : (tensor<7x1024xbf16>, tensor<1x7x8x128xbf16>) -> tensor<1x7x8x128xbf16>
    %70 = ttir.empty() : tensor<1x8x7x128xbf16>
    %71 = "ttir.permute"(%69, %70) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x8x128xbf16>, tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %72 = ttir.empty() : tensor<1x8x7x128xf32>
    %73 = "ttir.typecast"(%71, %72) <{conservative_folding = false}> : (tensor<1x8x7x128xbf16>, tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xf32>
    %74 = ttir.empty() : tensor<1x1x7xi64>
    %75 = "ttir.reshape"(%arg0, %74) <{shape = [1 : i32, 1 : i32, 7 : i32]}> : (tensor<7xi64>, tensor<1x1x7xi64>) -> tensor<1x1x7xi64>
    %76 = ttir.empty() : tensor<1x1x7xf32>
    %77 = "ttir.typecast"(%75, %76) <{conservative_folding = false}> : (tensor<1x1x7xi64>, tensor<1x1x7xf32>) -> tensor<1x1x7xf32>
    %78 = "ttir.dot_general"(%arg1, %77) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %79 = ttir.empty() : tensor<1x7x64xf32>
    %80 = "ttir.permute"(%78, %79) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x7xf32>, tensor<1x7x64xf32>) -> tensor<1x7x64xf32>
    %81 = ttir.empty() : tensor<1x7x128xf32>
    %82 = "ttir.concat"(%80, %80, %81) <{dim = 2 : si32}> : (tensor<1x7x64xf32>, tensor<1x7x64xf32>, tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
    %83 = ttir.empty() : tensor<1x7x128xf32>
    %84 = "ttir.cos"(%82, %83) : (tensor<1x7x128xf32>, tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
    %85 = ttir.empty() : tensor<1x7x128xbf16>
    %86 = "ttir.typecast"(%84, %85) <{conservative_folding = false}> : (tensor<1x7x128xf32>, tensor<1x7x128xbf16>) -> tensor<1x7x128xbf16>
    %87 = ttir.empty() : tensor<1x1x7x128xbf16>
    %88 = "ttir.reshape"(%86, %87) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xbf16>, tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %89 = ttir.empty() : tensor<1x1x7x128xf32>
    %90 = "ttir.typecast"(%88, %89) <{conservative_folding = false}> : (tensor<1x1x7x128xbf16>, tensor<1x1x7x128xf32>) -> tensor<1x1x7x128xf32>
    %91 = ttir.empty() : tensor<1x7x128xf32>
    %92 = "ttir.reshape"(%90, %91) <{shape = [1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x1x7x128xf32>, tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
    %93 = ttir.empty() : tensor<1x1x7x128xf32>
    %94 = "ttir.reshape"(%92, %93) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32>, tensor<1x1x7x128xf32>) -> tensor<1x1x7x128xf32>
    %95 = ttir.empty() : tensor<1x8x7x128xf32>
    %96 = "ttir.broadcast"(%94, %95) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x7x128xf32>, tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xf32>
    %97 = ttir.empty() : tensor<1x8x7x128xf32>
    %98 = "ttir.multiply"(%73, %96, %97) : (tensor<1x8x7x128xf32>, tensor<1x8x7x128xf32>, tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xf32>
    %99 = ttir.empty() : tensor<1x8x7x128xbf16>
    %100 = "ttir.typecast"(%98, %99) <{conservative_folding = false}> : (tensor<1x8x7x128xf32>, tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %101 = ttir.empty() : tensor<1x8x7x64xbf16>
    %102 = "ttir.slice"(%71, %101) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 8 : i32, 7 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x7x128xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x64xbf16>
    %103 = ttir.empty() : tensor<1x8x7x64xbf16>
    %104 = "ttir.neg"(%102, %103) : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x64xbf16>
    %105 = ttir.empty() : tensor<1x8x7x64xbf16>
    %106 = "ttir.slice"(%71, %105) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 7 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x7x128xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x64xbf16>
    %107 = ttir.empty() : tensor<1x8x7x128xbf16>
    %108 = "ttir.concat"(%104, %106, %107) <{dim = 3 : si32}> : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>, tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %109 = ttir.empty() : tensor<1x8x7x128xf32>
    %110 = "ttir.typecast"(%108, %109) <{conservative_folding = false}> : (tensor<1x8x7x128xbf16>, tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xf32>
    %111 = ttir.empty() : tensor<1x7x128xf32>
    %112 = "ttir.sin"(%82, %111) : (tensor<1x7x128xf32>, tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
    %113 = ttir.empty() : tensor<1x7x128xbf16>
    %114 = "ttir.typecast"(%112, %113) <{conservative_folding = false}> : (tensor<1x7x128xf32>, tensor<1x7x128xbf16>) -> tensor<1x7x128xbf16>
    %115 = ttir.empty() : tensor<1x1x7x128xbf16>
    %116 = "ttir.reshape"(%114, %115) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xbf16>, tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %117 = ttir.empty() : tensor<1x1x7x128xf32>
    %118 = "ttir.typecast"(%116, %117) <{conservative_folding = false}> : (tensor<1x1x7x128xbf16>, tensor<1x1x7x128xf32>) -> tensor<1x1x7x128xf32>
    %119 = ttir.empty() : tensor<1x7x128xf32>
    %120 = "ttir.reshape"(%118, %119) <{shape = [1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x1x7x128xf32>, tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
    %121 = ttir.empty() : tensor<1x1x7x128xf32>
    %122 = "ttir.reshape"(%120, %121) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32>, tensor<1x1x7x128xf32>) -> tensor<1x1x7x128xf32>
    %123 = ttir.empty() : tensor<1x8x7x128xf32>
    %124 = "ttir.broadcast"(%122, %123) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x7x128xf32>, tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xf32>
    %125 = ttir.empty() : tensor<1x8x7x128xf32>
    %126 = "ttir.multiply"(%110, %124, %125) : (tensor<1x8x7x128xf32>, tensor<1x8x7x128xf32>, tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xf32>
    %127 = ttir.empty() : tensor<1x8x7x128xbf16>
    %128 = "ttir.typecast"(%126, %127) <{conservative_folding = false}> : (tensor<1x8x7x128xf32>, tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %129 = ttir.empty() : tensor<1x8x7x128xbf16>
    %130 = "ttir.add"(%100, %128, %129) : (tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %131 = ttir.empty() : tensor<1x8x128x128xbf16>
    %132 = "ttir.scatter"(%arg8, %16, %130, %131) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x8x128x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>, tensor<1x8x128x128xbf16>) -> tensor<1x8x128x128xbf16>
    %133 = "ttir.dot_general"(%66, %arg9) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %134 = ttir.empty() : tensor<1x7x8x128xbf16>
    %135 = "ttir.reshape"(%133, %134) <{shape = [1 : i32, 7 : i32, 8 : i32, 128 : i32]}> : (tensor<7x1024xbf16>, tensor<1x7x8x128xbf16>) -> tensor<1x7x8x128xbf16>
    %136 = ttir.empty() : tensor<1x8x7x128xbf16>
    %137 = "ttir.permute"(%135, %136) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x8x128xbf16>, tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %138 = ttir.empty() : tensor<1x8x128x128xbf16>
    %139 = "ttir.scatter"(%arg10, %16, %137, %138) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x8x128x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>, tensor<1x8x128x128xbf16>) -> tensor<1x8x128x128xbf16>
    %140 = ttir.empty() : tensor<3072xf32>
    %141 = "ttir.typecast"(%arg20, %140) <{conservative_folding = false}> : (tensor<3072xbf16>, tensor<3072xf32>) -> tensor<3072xf32>
    %142 = ttir.empty() : tensor<1x1x3072xf32>
    %143 = "ttir.reshape"(%141, %142) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %144 = ttir.empty() : tensor<1x7x3072xf32>
    %145 = "ttir.broadcast"(%143, %144) <{broadcast_dimensions = array<i64: 1, 7, 1>}> : (tensor<1x1x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %146 = "ttir.dot_general"(%66, %arg17) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %147 = ttir.empty() : tensor<1x7x24x128xbf16>
    %148 = "ttir.reshape"(%146, %147) <{shape = [1 : i32, 7 : i32, 24 : i32, 128 : i32]}> : (tensor<7x3072xbf16>, tensor<1x7x24x128xbf16>) -> tensor<1x7x24x128xbf16>
    %149 = ttir.empty() : tensor<1x24x7x128xbf16>
    %150 = "ttir.permute"(%148, %149) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x24x128xbf16>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %151 = ttir.empty() : tensor<1x24x7x128xf32>
    %152 = "ttir.typecast"(%150, %151) <{conservative_folding = false}> : (tensor<1x24x7x128xbf16>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %153 = ttir.empty() : tensor<1x1x7x128xf32>
    %154 = "ttir.reshape"(%92, %153) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32>, tensor<1x1x7x128xf32>) -> tensor<1x1x7x128xf32>
    %155 = ttir.empty() : tensor<1x24x7x128xf32>
    %156 = "ttir.broadcast"(%154, %155) <{broadcast_dimensions = array<i64: 1, 24, 1, 1>}> : (tensor<1x1x7x128xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %157 = ttir.empty() : tensor<1x24x7x128xf32>
    %158 = "ttir.multiply"(%152, %156, %157) : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %159 = ttir.empty() : tensor<1x24x7x128xbf16>
    %160 = "ttir.typecast"(%158, %159) <{conservative_folding = false}> : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %161 = ttir.empty() : tensor<1x24x7x64xbf16>
    %162 = "ttir.slice"(%150, %161) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 24 : i32, 7 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x7x128xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x64xbf16>
    %163 = ttir.empty() : tensor<1x24x7x64xbf16>
    %164 = "ttir.neg"(%162, %163) : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x64xbf16>
    %165 = ttir.empty() : tensor<1x24x7x64xbf16>
    %166 = "ttir.slice"(%150, %165) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 24 : i32, 7 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x7x128xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x64xbf16>
    %167 = ttir.empty() : tensor<1x24x7x128xbf16>
    %168 = "ttir.concat"(%164, %166, %167) <{dim = 3 : si32}> : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %169 = ttir.empty() : tensor<1x24x7x128xf32>
    %170 = "ttir.typecast"(%168, %169) <{conservative_folding = false}> : (tensor<1x24x7x128xbf16>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %171 = ttir.empty() : tensor<1x1x7x128xf32>
    %172 = "ttir.reshape"(%120, %171) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32>, tensor<1x1x7x128xf32>) -> tensor<1x1x7x128xf32>
    %173 = ttir.empty() : tensor<1x24x7x128xf32>
    %174 = "ttir.broadcast"(%172, %173) <{broadcast_dimensions = array<i64: 1, 24, 1, 1>}> : (tensor<1x1x7x128xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %175 = ttir.empty() : tensor<1x24x7x128xf32>
    %176 = "ttir.multiply"(%170, %174, %175) : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %177 = ttir.empty() : tensor<1x24x7x128xbf16>
    %178 = "ttir.typecast"(%176, %177) <{conservative_folding = false}> : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %179 = ttir.empty() : tensor<1x24x7x128xbf16>
    %180 = "ttir.add"(%160, %178, %179) : (tensor<1x24x7x128xbf16>, tensor<1x24x7x128xbf16>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %181 = ttir.empty() : tensor<24x7x128xbf16>
    %182 = "ttir.reshape"(%180, %181) <{shape = [24 : i32, 7 : i32, 128 : i32]}> : (tensor<1x24x7x128xbf16>, tensor<24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %183 = ttir.empty() : tensor<1x8x1x128x128xbf16>
    %184 = "ttir.reshape"(%132, %183) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x128x128xbf16>, tensor<1x8x1x128x128xbf16>) -> tensor<1x8x1x128x128xbf16>
    %185 = ttir.empty() : tensor<1x8x3x128x128xbf16>
    %186 = "ttir.broadcast"(%184, %185) <{broadcast_dimensions = array<i64: 1, 1, 3, 1, 1>}> : (tensor<1x8x1x128x128xbf16>, tensor<1x8x3x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %187 = ttir.empty() : tensor<1x24x128x128xbf16>
    %188 = "ttir.reshape"(%186, %187) <{shape = [1 : i32, 24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x3x128x128xbf16>, tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %189 = ttir.empty() : tensor<1x24x128x128xbf16>
    %190 = "ttir.permute"(%188, %189) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x24x128x128xbf16>, tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %191 = ttir.empty() : tensor<24x128x128xbf16>
    %192 = "ttir.reshape"(%190, %191) <{shape = [24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x24x128x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x128x128xbf16>
    %193 = "ttir.dot_general"(%182, %192) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<24x7x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x7x128xbf16>
    %194 = ttir.empty() : tensor<1x24x7x128xbf16>
    %195 = "ttir.reshape"(%193, %194) <{shape = [1 : i32, 24 : i32, 7 : i32, 128 : i32]}> : (tensor<24x7x128xbf16>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %196 = ttir.empty() : tensor<1x24x7x128xf32>
    %197 = "ttir.typecast"(%195, %196) <{conservative_folding = false}> : (tensor<1x24x7x128xbf16>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %198 = ttir.empty() : tensor<1x1x1x1xf32>
    %199 = "ttir.reshape"(%arg16, %198) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %200 = ttir.empty() : tensor<1x24x7x128xf32>
    %201 = "ttir.broadcast"(%199, %200) <{broadcast_dimensions = array<i64: 1, 24, 7, 128>}> : (tensor<1x1x1x1xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %202 = ttir.empty() : tensor<1x24x7x128xf32>
    %203 = "ttir.multiply"(%197, %201, %202) : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %204 = ttir.empty() : tensor<1x24x7x128xbf16>
    %205 = "ttir.typecast"(%203, %204) <{conservative_folding = false}> : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %206 = ttir.empty() : tensor<1x7x128xbf16>
    %207 = "ttir.reshape"(%arg15, %206) <{shape = [1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x1x7x128xbf16>, tensor<1x7x128xbf16>) -> tensor<1x7x128xbf16>
    %208 = ttir.empty() : tensor<1x1x7x128xbf16>
    %209 = "ttir.reshape"(%207, %208) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xbf16>, tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %210 = ttir.empty() : tensor<1x24x7x128xbf16>
    %211 = "ttir.broadcast"(%209, %210) <{broadcast_dimensions = array<i64: 1, 24, 1, 1>}> : (tensor<1x1x7x128xbf16>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %212 = ttir.empty() : tensor<1x24x7x128xbf16>
    %213 = "ttir.add"(%205, %211, %212) : (tensor<1x24x7x128xbf16>, tensor<1x24x7x128xbf16>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %214 = ttir.empty() : tensor<1x24x7x128xf32>
    %215 = "ttir.typecast"(%213, %214) <{conservative_folding = false}> : (tensor<1x24x7x128xbf16>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %216 = ttir.empty() : tensor<1x24x7xf32>
    %217 = "ttir.max"(%215, %216) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x24x7x128xf32>, tensor<1x24x7xf32>) -> tensor<1x24x7xf32>
    %218 = ttir.empty() : tensor<1x24x7x1xf32>
    %219 = "ttir.reshape"(%217, %218) <{shape = [1 : i32, 24 : i32, 7 : i32, 1 : i32]}> : (tensor<1x24x7xf32>, tensor<1x24x7x1xf32>) -> tensor<1x24x7x1xf32>
    %220 = ttir.empty() : tensor<1x24x7x128xf32>
    %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<1x24x7x1xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %222 = ttir.empty() : tensor<1x24x7x128xf32>
    %223 = "ttir.subtract"(%215, %221, %222) : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %224 = ttir.empty() : tensor<1x24x7x128xf32>
    %225 = "ttir.exp"(%223, %224) : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %226 = ttir.empty() : tensor<1x24x7xf32>
    %227 = "ttir.sum"(%225, %226) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x24x7x128xf32>, tensor<1x24x7xf32>) -> tensor<1x24x7xf32>
    %228 = ttir.empty() : tensor<1x24x7x1xf32>
    %229 = "ttir.reshape"(%227, %228) <{shape = [1 : i32, 24 : i32, 7 : i32, 1 : i32]}> : (tensor<1x24x7xf32>, tensor<1x24x7x1xf32>) -> tensor<1x24x7x1xf32>
    %230 = ttir.empty() : tensor<1x24x7x128xf32>
    %231 = "ttir.broadcast"(%229, %230) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<1x24x7x1xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %232 = ttir.empty() : tensor<1x24x7x128xf32>
    %233 = "ttir.div"(%225, %231, %232) : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %234 = ttir.empty() : tensor<1x24x7x128xbf16>
    %235 = "ttir.typecast"(%233, %234) <{conservative_folding = false}> : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %236 = ttir.empty() : tensor<24x7x128xbf16>
    %237 = "ttir.reshape"(%235, %236) <{shape = [24 : i32, 7 : i32, 128 : i32]}> : (tensor<1x24x7x128xbf16>, tensor<24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %238 = ttir.empty() : tensor<1x8x1x128x128xbf16>
    %239 = "ttir.reshape"(%139, %238) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x128x128xbf16>, tensor<1x8x1x128x128xbf16>) -> tensor<1x8x1x128x128xbf16>
    %240 = ttir.empty() : tensor<1x8x3x128x128xbf16>
    %241 = "ttir.broadcast"(%239, %240) <{broadcast_dimensions = array<i64: 1, 1, 3, 1, 1>}> : (tensor<1x8x1x128x128xbf16>, tensor<1x8x3x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %242 = ttir.empty() : tensor<24x128x128xbf16>
    %243 = "ttir.reshape"(%241, %242) <{shape = [24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x3x128x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x128x128xbf16>
    %244 = "ttir.dot_general"(%237, %243) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<24x7x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x7x128xbf16>
    %245 = ttir.empty() : tensor<1x24x7x128xbf16>
    %246 = "ttir.reshape"(%244, %245) <{shape = [1 : i32, 24 : i32, 7 : i32, 128 : i32]}> : (tensor<24x7x128xbf16>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %247 = ttir.empty() : tensor<1x7x24x128xbf16>
    %248 = "ttir.permute"(%246, %247) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x24x7x128xbf16>, tensor<1x7x24x128xbf16>) -> tensor<1x7x24x128xbf16>
    %249 = ttir.empty() : tensor<7x3072xbf16>
    %250 = "ttir.reshape"(%248, %249) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x7x24x128xbf16>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %251 = "ttir.dot_general"(%250, %arg14) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %252 = ttir.empty() : tensor<1x7x3072xbf16>
    %253 = "ttir.reshape"(%251, %252) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %254 = ttir.empty() : tensor<1x7x3072xbf16>
    %255 = "ttir.add"(%30, %253, %254) : (tensor<1x7x3072xbf16>, tensor<1x7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %256 = ttir.empty() : tensor<3072xf32>
    %257 = "ttir.typecast"(%arg18, %256) <{conservative_folding = false}> : (tensor<3072xbf16>, tensor<3072xf32>) -> tensor<3072xf32>
    %258 = ttir.empty() : tensor<1x1x3072xf32>
    %259 = "ttir.reshape"(%257, %258) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %260 = ttir.empty() : tensor<1x7x3072xf32>
    %261 = "ttir.broadcast"(%259, %260) <{broadcast_dimensions = array<i64: 1, 7, 1>}> : (tensor<1x1x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %262 = ttir.empty() : tensor<1x7x3072xf32>
    %263 = "ttir.typecast"(%255, %262) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %264 = ttir.empty() : tensor<1x7x3072xf32>
    %265 = "ttir.pow"(%263, %1, %264) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %266 = ttir.empty() : tensor<1x7xf32>
    %267 = "ttir.sum"(%265, %266) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %268 = ttir.empty() : tensor<1x7xf32>
    %269 = "ttir.multiply"(%267, %0, %268) : (tensor<1x7xf32>, tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %270 = ttir.empty() : tensor<1x7x1xf32>
    %271 = "ttir.reshape"(%269, %270) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %272 = ttir.empty() : tensor<1x7x1xf32>
    %273 = "ttir.add"(%271, %44, %272) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %274 = ttir.empty() : tensor<1x7x1xf32>
    %275 = "ttir.rsqrt"(%273, %274) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %276 = ttir.empty() : tensor<1x7xf32>
    %277 = "ttir.reshape"(%275, %276) <{shape = [1 : i32, 7 : i32]}> : (tensor<1x7x1xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %278 = ttir.empty() : tensor<1x7x1xf32>
    %279 = "ttir.reshape"(%277, %278) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %280 = ttir.empty() : tensor<1x7x3072xf32>
    %281 = "ttir.broadcast"(%279, %280) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x7x1xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %282 = ttir.empty() : tensor<1x7x3072xf32>
    %283 = "ttir.multiply"(%263, %281, %282) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %284 = ttir.empty() : tensor<1x7x3072xbf16>
    %285 = "ttir.typecast"(%283, %284) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %286 = ttir.empty() : tensor<1x7x3072xf32>
    %287 = "ttir.typecast"(%285, %286) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %288 = ttir.empty() : tensor<1x7x3072xf32>
    %289 = "ttir.multiply"(%261, %287, %288) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %290 = ttir.empty() : tensor<1x7x3072xbf16>
    %291 = "ttir.typecast"(%289, %290) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %292 = ttir.empty() : tensor<7x3072xbf16>
    %293 = "ttir.reshape"(%291, %292) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x7x3072xbf16>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %294 = "ttir.dot_general"(%293, %arg19) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %295 = ttir.empty() : tensor<1x7x8192xbf16>
    %296 = "ttir.reshape"(%294, %295) <{shape = [1 : i32, 7 : i32, 8192 : i32]}> : (tensor<7x8192xbf16>, tensor<1x7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %297 = ttir.empty() : tensor<1x7x8192xf32>
    %298 = "ttir.typecast"(%296, %297) <{conservative_folding = false}> : (tensor<1x7x8192xbf16>, tensor<1x7x8192xf32>) -> tensor<1x7x8192xf32>
    %299 = ttir.empty() : tensor<1x7x8192xbf16>
    %300 = "ttir.sigmoid"(%296, %299) : (tensor<1x7x8192xbf16>, tensor<1x7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %301 = ttir.empty() : tensor<1x7x8192xf32>
    %302 = "ttir.typecast"(%300, %301) <{conservative_folding = false}> : (tensor<1x7x8192xbf16>, tensor<1x7x8192xf32>) -> tensor<1x7x8192xf32>
    %303 = ttir.empty() : tensor<1x7x8192xf32>
    %304 = "ttir.multiply"(%298, %302, %303) : (tensor<1x7x8192xf32>, tensor<1x7x8192xf32>, tensor<1x7x8192xf32>) -> tensor<1x7x8192xf32>
    %305 = ttir.empty() : tensor<1x7x8192xbf16>
    %306 = "ttir.typecast"(%304, %305) <{conservative_folding = false}> : (tensor<1x7x8192xf32>, tensor<1x7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %307 = ttir.empty() : tensor<1x7x8192xf32>
    %308 = "ttir.typecast"(%306, %307) <{conservative_folding = false}> : (tensor<1x7x8192xbf16>, tensor<1x7x8192xf32>) -> tensor<1x7x8192xf32>
    %309 = "ttir.dot_general"(%293, %arg13) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %310 = ttir.empty() : tensor<1x7x8192xbf16>
    %311 = "ttir.reshape"(%309, %310) <{shape = [1 : i32, 7 : i32, 8192 : i32]}> : (tensor<7x8192xbf16>, tensor<1x7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %312 = ttir.empty() : tensor<1x7x8192xf32>
    %313 = "ttir.typecast"(%311, %312) <{conservative_folding = false}> : (tensor<1x7x8192xbf16>, tensor<1x7x8192xf32>) -> tensor<1x7x8192xf32>
    %314 = ttir.empty() : tensor<1x7x8192xf32>
    %315 = "ttir.multiply"(%308, %313, %314) : (tensor<1x7x8192xf32>, tensor<1x7x8192xf32>, tensor<1x7x8192xf32>) -> tensor<1x7x8192xf32>
    %316 = ttir.empty() : tensor<1x7x8192xbf16>
    %317 = "ttir.typecast"(%315, %316) <{conservative_folding = false}> : (tensor<1x7x8192xf32>, tensor<1x7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %318 = ttir.empty() : tensor<7x8192xbf16>
    %319 = "ttir.reshape"(%317, %318) <{shape = [7 : i32, 8192 : i32]}> : (tensor<1x7x8192xbf16>, tensor<7x8192xbf16>) -> tensor<7x8192xbf16>
    %320 = "ttir.dot_general"(%319, %arg12) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %321 = ttir.empty() : tensor<1x7x3072xbf16>
    %322 = "ttir.reshape"(%320, %321) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %323 = ttir.empty() : tensor<1x7x3072xbf16>
    %324 = "ttir.add"(%255, %322, %323) : (tensor<1x7x3072xbf16>, tensor<1x7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %325 = ttir.empty() : tensor<1x7x3072xf32>
    %326 = "ttir.typecast"(%324, %325) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %327 = ttir.empty() : tensor<1x7x3072xf32>
    %328 = "ttir.pow"(%326, %1, %327) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %329 = ttir.empty() : tensor<1x7xf32>
    %330 = "ttir.sum"(%328, %329) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %331 = ttir.empty() : tensor<1x7xf32>
    %332 = "ttir.multiply"(%330, %0, %331) : (tensor<1x7xf32>, tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %333 = ttir.empty() : tensor<1x7x1xf32>
    %334 = "ttir.reshape"(%332, %333) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %335 = ttir.empty() : tensor<1x7x1xf32>
    %336 = "ttir.add"(%334, %44, %335) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %337 = ttir.empty() : tensor<1x7x1xf32>
    %338 = "ttir.rsqrt"(%336, %337) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %339 = ttir.empty() : tensor<1x7xf32>
    %340 = "ttir.reshape"(%338, %339) <{shape = [1 : i32, 7 : i32]}> : (tensor<1x7x1xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %341 = ttir.empty() : tensor<1x7x1xf32>
    %342 = "ttir.reshape"(%340, %341) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %343 = ttir.empty() : tensor<1x7x3072xf32>
    %344 = "ttir.broadcast"(%342, %343) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x7x1xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %345 = ttir.empty() : tensor<1x7x3072xf32>
    %346 = "ttir.multiply"(%326, %344, %345) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %347 = ttir.empty() : tensor<1x7x3072xbf16>
    %348 = "ttir.typecast"(%346, %347) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %349 = ttir.empty() : tensor<1x7x3072xf32>
    %350 = "ttir.typecast"(%348, %349) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %351 = ttir.empty() : tensor<1x7x3072xf32>
    %352 = "ttir.multiply"(%145, %350, %351) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %353 = ttir.empty() : tensor<1x7x3072xbf16>
    %354 = "ttir.typecast"(%352, %353) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %355 = ttir.empty() : tensor<7x3072xbf16>
    %356 = "ttir.reshape"(%354, %355) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x7x3072xbf16>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %357 = "ttir.dot_general"(%356, %arg11) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %358 = ttir.empty() : tensor<1x7x8x128xbf16>
    %359 = "ttir.reshape"(%357, %358) <{shape = [1 : i32, 7 : i32, 8 : i32, 128 : i32]}> : (tensor<7x1024xbf16>, tensor<1x7x8x128xbf16>) -> tensor<1x7x8x128xbf16>
    %360 = ttir.empty() : tensor<1x8x7x128xbf16>
    %361 = "ttir.permute"(%359, %360) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x8x128xbf16>, tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %362 = ttir.empty() : tensor<1x8x7x128xf32>
    %363 = "ttir.typecast"(%361, %362) <{conservative_folding = false}> : (tensor<1x8x7x128xbf16>, tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xf32>
    %364 = ttir.empty() : tensor<1x8x7x128xf32>
    %365 = "ttir.multiply"(%363, %96, %364) : (tensor<1x8x7x128xf32>, tensor<1x8x7x128xf32>, tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xf32>
    %366 = ttir.empty() : tensor<1x8x7x128xbf16>
    %367 = "ttir.typecast"(%365, %366) <{conservative_folding = false}> : (tensor<1x8x7x128xf32>, tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %368 = ttir.empty() : tensor<1x8x7x64xbf16>
    %369 = "ttir.slice"(%361, %368) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 8 : i32, 7 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x7x128xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x64xbf16>
    %370 = ttir.empty() : tensor<1x8x7x64xbf16>
    %371 = "ttir.neg"(%369, %370) : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x64xbf16>
    %372 = ttir.empty() : tensor<1x8x7x64xbf16>
    %373 = "ttir.slice"(%361, %372) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 7 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x7x128xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x64xbf16>
    %374 = ttir.empty() : tensor<1x8x7x128xbf16>
    %375 = "ttir.concat"(%371, %373, %374) <{dim = 3 : si32}> : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>, tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %376 = ttir.empty() : tensor<1x8x7x128xf32>
    %377 = "ttir.typecast"(%375, %376) <{conservative_folding = false}> : (tensor<1x8x7x128xbf16>, tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xf32>
    %378 = ttir.empty() : tensor<1x8x7x128xf32>
    %379 = "ttir.multiply"(%377, %124, %378) : (tensor<1x8x7x128xf32>, tensor<1x8x7x128xf32>, tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xf32>
    %380 = ttir.empty() : tensor<1x8x7x128xbf16>
    %381 = "ttir.typecast"(%379, %380) <{conservative_folding = false}> : (tensor<1x8x7x128xf32>, tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %382 = ttir.empty() : tensor<1x8x7x128xbf16>
    %383 = "ttir.add"(%367, %381, %382) : (tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %384 = ttir.empty() : tensor<1x8x128x128xbf16>
    %385 = "ttir.scatter"(%arg21, %16, %383, %384) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x8x128x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>, tensor<1x8x128x128xbf16>) -> tensor<1x8x128x128xbf16>
    %386 = "ttir.dot_general"(%356, %arg22) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %387 = ttir.empty() : tensor<1x7x8x128xbf16>
    %388 = "ttir.reshape"(%386, %387) <{shape = [1 : i32, 7 : i32, 8 : i32, 128 : i32]}> : (tensor<7x1024xbf16>, tensor<1x7x8x128xbf16>) -> tensor<1x7x8x128xbf16>
    %389 = ttir.empty() : tensor<1x8x7x128xbf16>
    %390 = "ttir.permute"(%388, %389) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x8x128xbf16>, tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %391 = ttir.empty() : tensor<1x8x128x128xbf16>
    %392 = "ttir.scatter"(%arg23, %16, %390, %391) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x8x128x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>, tensor<1x8x128x128xbf16>) -> tensor<1x8x128x128xbf16>
    %393 = ttir.empty() : tensor<3072xf32>
    %394 = "ttir.typecast"(%arg31, %393) <{conservative_folding = false}> : (tensor<3072xbf16>, tensor<3072xf32>) -> tensor<3072xf32>
    %395 = ttir.empty() : tensor<1x1x3072xf32>
    %396 = "ttir.reshape"(%394, %395) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %397 = ttir.empty() : tensor<1x7x3072xf32>
    %398 = "ttir.broadcast"(%396, %397) <{broadcast_dimensions = array<i64: 1, 7, 1>}> : (tensor<1x1x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %399 = "ttir.dot_general"(%356, %arg28) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %400 = ttir.empty() : tensor<1x7x24x128xbf16>
    %401 = "ttir.reshape"(%399, %400) <{shape = [1 : i32, 7 : i32, 24 : i32, 128 : i32]}> : (tensor<7x3072xbf16>, tensor<1x7x24x128xbf16>) -> tensor<1x7x24x128xbf16>
    %402 = ttir.empty() : tensor<1x24x7x128xbf16>
    %403 = "ttir.permute"(%401, %402) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x24x128xbf16>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %404 = ttir.empty() : tensor<1x24x7x128xf32>
    %405 = "ttir.typecast"(%403, %404) <{conservative_folding = false}> : (tensor<1x24x7x128xbf16>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %406 = ttir.empty() : tensor<1x24x7x128xf32>
    %407 = "ttir.multiply"(%405, %156, %406) : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %408 = ttir.empty() : tensor<1x24x7x128xbf16>
    %409 = "ttir.typecast"(%407, %408) <{conservative_folding = false}> : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %410 = ttir.empty() : tensor<1x24x7x64xbf16>
    %411 = "ttir.slice"(%403, %410) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 24 : i32, 7 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x7x128xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x64xbf16>
    %412 = ttir.empty() : tensor<1x24x7x64xbf16>
    %413 = "ttir.neg"(%411, %412) : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x64xbf16>
    %414 = ttir.empty() : tensor<1x24x7x64xbf16>
    %415 = "ttir.slice"(%403, %414) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 24 : i32, 7 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x7x128xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x64xbf16>
    %416 = ttir.empty() : tensor<1x24x7x128xbf16>
    %417 = "ttir.concat"(%413, %415, %416) <{dim = 3 : si32}> : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %418 = ttir.empty() : tensor<1x24x7x128xf32>
    %419 = "ttir.typecast"(%417, %418) <{conservative_folding = false}> : (tensor<1x24x7x128xbf16>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %420 = ttir.empty() : tensor<1x24x7x128xf32>
    %421 = "ttir.multiply"(%419, %174, %420) : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %422 = ttir.empty() : tensor<1x24x7x128xbf16>
    %423 = "ttir.typecast"(%421, %422) <{conservative_folding = false}> : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %424 = ttir.empty() : tensor<1x24x7x128xbf16>
    %425 = "ttir.add"(%409, %423, %424) : (tensor<1x24x7x128xbf16>, tensor<1x24x7x128xbf16>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %426 = ttir.empty() : tensor<24x7x128xbf16>
    %427 = "ttir.reshape"(%425, %426) <{shape = [24 : i32, 7 : i32, 128 : i32]}> : (tensor<1x24x7x128xbf16>, tensor<24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %428 = ttir.empty() : tensor<1x8x1x128x128xbf16>
    %429 = "ttir.reshape"(%385, %428) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x128x128xbf16>, tensor<1x8x1x128x128xbf16>) -> tensor<1x8x1x128x128xbf16>
    %430 = ttir.empty() : tensor<1x8x3x128x128xbf16>
    %431 = "ttir.broadcast"(%429, %430) <{broadcast_dimensions = array<i64: 1, 1, 3, 1, 1>}> : (tensor<1x8x1x128x128xbf16>, tensor<1x8x3x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %432 = ttir.empty() : tensor<1x24x128x128xbf16>
    %433 = "ttir.reshape"(%431, %432) <{shape = [1 : i32, 24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x3x128x128xbf16>, tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %434 = ttir.empty() : tensor<1x24x128x128xbf16>
    %435 = "ttir.permute"(%433, %434) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x24x128x128xbf16>, tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %436 = ttir.empty() : tensor<24x128x128xbf16>
    %437 = "ttir.reshape"(%435, %436) <{shape = [24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x24x128x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x128x128xbf16>
    %438 = "ttir.dot_general"(%427, %437) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<24x7x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x7x128xbf16>
    %439 = ttir.empty() : tensor<1x24x7x128xbf16>
    %440 = "ttir.reshape"(%438, %439) <{shape = [1 : i32, 24 : i32, 7 : i32, 128 : i32]}> : (tensor<24x7x128xbf16>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %441 = ttir.empty() : tensor<1x24x7x128xf32>
    %442 = "ttir.typecast"(%440, %441) <{conservative_folding = false}> : (tensor<1x24x7x128xbf16>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %443 = ttir.empty() : tensor<1x24x7x128xf32>
    %444 = "ttir.multiply"(%442, %201, %443) : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %445 = ttir.empty() : tensor<1x24x7x128xbf16>
    %446 = "ttir.typecast"(%444, %445) <{conservative_folding = false}> : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %447 = ttir.empty() : tensor<1x24x7x128xbf16>
    %448 = "ttir.add"(%446, %211, %447) : (tensor<1x24x7x128xbf16>, tensor<1x24x7x128xbf16>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %449 = ttir.empty() : tensor<1x24x7x128xf32>
    %450 = "ttir.typecast"(%448, %449) <{conservative_folding = false}> : (tensor<1x24x7x128xbf16>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %451 = ttir.empty() : tensor<1x24x7xf32>
    %452 = "ttir.max"(%450, %451) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x24x7x128xf32>, tensor<1x24x7xf32>) -> tensor<1x24x7xf32>
    %453 = ttir.empty() : tensor<1x24x7x1xf32>
    %454 = "ttir.reshape"(%452, %453) <{shape = [1 : i32, 24 : i32, 7 : i32, 1 : i32]}> : (tensor<1x24x7xf32>, tensor<1x24x7x1xf32>) -> tensor<1x24x7x1xf32>
    %455 = ttir.empty() : tensor<1x24x7x128xf32>
    %456 = "ttir.broadcast"(%454, %455) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<1x24x7x1xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %457 = ttir.empty() : tensor<1x24x7x128xf32>
    %458 = "ttir.subtract"(%450, %456, %457) : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %459 = ttir.empty() : tensor<1x24x7x128xf32>
    %460 = "ttir.exp"(%458, %459) : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %461 = ttir.empty() : tensor<1x24x7xf32>
    %462 = "ttir.sum"(%460, %461) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x24x7x128xf32>, tensor<1x24x7xf32>) -> tensor<1x24x7xf32>
    %463 = ttir.empty() : tensor<1x24x7x1xf32>
    %464 = "ttir.reshape"(%462, %463) <{shape = [1 : i32, 24 : i32, 7 : i32, 1 : i32]}> : (tensor<1x24x7xf32>, tensor<1x24x7x1xf32>) -> tensor<1x24x7x1xf32>
    %465 = ttir.empty() : tensor<1x24x7x128xf32>
    %466 = "ttir.broadcast"(%464, %465) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<1x24x7x1xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %467 = ttir.empty() : tensor<1x24x7x128xf32>
    %468 = "ttir.div"(%460, %466, %467) : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %469 = ttir.empty() : tensor<1x24x7x128xbf16>
    %470 = "ttir.typecast"(%468, %469) <{conservative_folding = false}> : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %471 = ttir.empty() : tensor<24x7x128xbf16>
    %472 = "ttir.reshape"(%470, %471) <{shape = [24 : i32, 7 : i32, 128 : i32]}> : (tensor<1x24x7x128xbf16>, tensor<24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %473 = ttir.empty() : tensor<1x8x1x128x128xbf16>
    %474 = "ttir.reshape"(%392, %473) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x128x128xbf16>, tensor<1x8x1x128x128xbf16>) -> tensor<1x8x1x128x128xbf16>
    %475 = ttir.empty() : tensor<1x8x3x128x128xbf16>
    %476 = "ttir.broadcast"(%474, %475) <{broadcast_dimensions = array<i64: 1, 1, 3, 1, 1>}> : (tensor<1x8x1x128x128xbf16>, tensor<1x8x3x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %477 = ttir.empty() : tensor<24x128x128xbf16>
    %478 = "ttir.reshape"(%476, %477) <{shape = [24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x3x128x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x128x128xbf16>
    %479 = "ttir.dot_general"(%472, %478) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<24x7x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x7x128xbf16>
    %480 = ttir.empty() : tensor<1x24x7x128xbf16>
    %481 = "ttir.reshape"(%479, %480) <{shape = [1 : i32, 24 : i32, 7 : i32, 128 : i32]}> : (tensor<24x7x128xbf16>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %482 = ttir.empty() : tensor<1x7x24x128xbf16>
    %483 = "ttir.permute"(%481, %482) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x24x7x128xbf16>, tensor<1x7x24x128xbf16>) -> tensor<1x7x24x128xbf16>
    %484 = ttir.empty() : tensor<7x3072xbf16>
    %485 = "ttir.reshape"(%483, %484) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x7x24x128xbf16>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %486 = "ttir.dot_general"(%485, %arg27) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %487 = ttir.empty() : tensor<1x7x3072xbf16>
    %488 = "ttir.reshape"(%486, %487) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %489 = ttir.empty() : tensor<1x7x3072xbf16>
    %490 = "ttir.add"(%324, %488, %489) : (tensor<1x7x3072xbf16>, tensor<1x7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %491 = ttir.empty() : tensor<3072xf32>
    %492 = "ttir.typecast"(%arg29, %491) <{conservative_folding = false}> : (tensor<3072xbf16>, tensor<3072xf32>) -> tensor<3072xf32>
    %493 = ttir.empty() : tensor<1x1x3072xf32>
    %494 = "ttir.reshape"(%492, %493) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %495 = ttir.empty() : tensor<1x7x3072xf32>
    %496 = "ttir.broadcast"(%494, %495) <{broadcast_dimensions = array<i64: 1, 7, 1>}> : (tensor<1x1x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %497 = ttir.empty() : tensor<1x7x3072xf32>
    %498 = "ttir.typecast"(%490, %497) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %499 = ttir.empty() : tensor<1x7x3072xf32>
    %500 = "ttir.pow"(%498, %1, %499) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %501 = ttir.empty() : tensor<1x7xf32>
    %502 = "ttir.sum"(%500, %501) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %503 = ttir.empty() : tensor<1x7xf32>
    %504 = "ttir.multiply"(%502, %0, %503) : (tensor<1x7xf32>, tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %505 = ttir.empty() : tensor<1x7x1xf32>
    %506 = "ttir.reshape"(%504, %505) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %507 = ttir.empty() : tensor<1x7x1xf32>
    %508 = "ttir.add"(%506, %44, %507) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %509 = ttir.empty() : tensor<1x7x1xf32>
    %510 = "ttir.rsqrt"(%508, %509) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %511 = ttir.empty() : tensor<1x7xf32>
    %512 = "ttir.reshape"(%510, %511) <{shape = [1 : i32, 7 : i32]}> : (tensor<1x7x1xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %513 = ttir.empty() : tensor<1x7x1xf32>
    %514 = "ttir.reshape"(%512, %513) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %515 = ttir.empty() : tensor<1x7x3072xf32>
    %516 = "ttir.broadcast"(%514, %515) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x7x1xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %517 = ttir.empty() : tensor<1x7x3072xf32>
    %518 = "ttir.multiply"(%498, %516, %517) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %519 = ttir.empty() : tensor<1x7x3072xbf16>
    %520 = "ttir.typecast"(%518, %519) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %521 = ttir.empty() : tensor<1x7x3072xf32>
    %522 = "ttir.typecast"(%520, %521) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %523 = ttir.empty() : tensor<1x7x3072xf32>
    %524 = "ttir.multiply"(%496, %522, %523) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %525 = ttir.empty() : tensor<1x7x3072xbf16>
    %526 = "ttir.typecast"(%524, %525) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %527 = ttir.empty() : tensor<7x3072xbf16>
    %528 = "ttir.reshape"(%526, %527) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x7x3072xbf16>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %529 = "ttir.dot_general"(%528, %arg30) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %530 = ttir.empty() : tensor<1x7x8192xbf16>
    %531 = "ttir.reshape"(%529, %530) <{shape = [1 : i32, 7 : i32, 8192 : i32]}> : (tensor<7x8192xbf16>, tensor<1x7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %532 = ttir.empty() : tensor<1x7x8192xf32>
    %533 = "ttir.typecast"(%531, %532) <{conservative_folding = false}> : (tensor<1x7x8192xbf16>, tensor<1x7x8192xf32>) -> tensor<1x7x8192xf32>
    %534 = ttir.empty() : tensor<1x7x8192xbf16>
    %535 = "ttir.sigmoid"(%531, %534) : (tensor<1x7x8192xbf16>, tensor<1x7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %536 = ttir.empty() : tensor<1x7x8192xf32>
    %537 = "ttir.typecast"(%535, %536) <{conservative_folding = false}> : (tensor<1x7x8192xbf16>, tensor<1x7x8192xf32>) -> tensor<1x7x8192xf32>
    %538 = ttir.empty() : tensor<1x7x8192xf32>
    %539 = "ttir.multiply"(%533, %537, %538) : (tensor<1x7x8192xf32>, tensor<1x7x8192xf32>, tensor<1x7x8192xf32>) -> tensor<1x7x8192xf32>
    %540 = ttir.empty() : tensor<1x7x8192xbf16>
    %541 = "ttir.typecast"(%539, %540) <{conservative_folding = false}> : (tensor<1x7x8192xf32>, tensor<1x7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %542 = ttir.empty() : tensor<1x7x8192xf32>
    %543 = "ttir.typecast"(%541, %542) <{conservative_folding = false}> : (tensor<1x7x8192xbf16>, tensor<1x7x8192xf32>) -> tensor<1x7x8192xf32>
    %544 = "ttir.dot_general"(%528, %arg26) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %545 = ttir.empty() : tensor<1x7x8192xbf16>
    %546 = "ttir.reshape"(%544, %545) <{shape = [1 : i32, 7 : i32, 8192 : i32]}> : (tensor<7x8192xbf16>, tensor<1x7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %547 = ttir.empty() : tensor<1x7x8192xf32>
    %548 = "ttir.typecast"(%546, %547) <{conservative_folding = false}> : (tensor<1x7x8192xbf16>, tensor<1x7x8192xf32>) -> tensor<1x7x8192xf32>
    %549 = ttir.empty() : tensor<1x7x8192xf32>
    %550 = "ttir.multiply"(%543, %548, %549) : (tensor<1x7x8192xf32>, tensor<1x7x8192xf32>, tensor<1x7x8192xf32>) -> tensor<1x7x8192xf32>
    %551 = ttir.empty() : tensor<1x7x8192xbf16>
    %552 = "ttir.typecast"(%550, %551) <{conservative_folding = false}> : (tensor<1x7x8192xf32>, tensor<1x7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %553 = ttir.empty() : tensor<7x8192xbf16>
    %554 = "ttir.reshape"(%552, %553) <{shape = [7 : i32, 8192 : i32]}> : (tensor<1x7x8192xbf16>, tensor<7x8192xbf16>) -> tensor<7x8192xbf16>
    %555 = "ttir.dot_general"(%554, %arg25) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %556 = ttir.empty() : tensor<1x7x3072xbf16>
    %557 = "ttir.reshape"(%555, %556) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %558 = ttir.empty() : tensor<1x7x3072xbf16>
    %559 = "ttir.add"(%490, %557, %558) : (tensor<1x7x3072xbf16>, tensor<1x7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %560 = ttir.empty() : tensor<1x7x3072xf32>
    %561 = "ttir.typecast"(%559, %560) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %562 = ttir.empty() : tensor<1x7x3072xf32>
    %563 = "ttir.pow"(%561, %1, %562) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %564 = ttir.empty() : tensor<1x7xf32>
    %565 = "ttir.sum"(%563, %564) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %566 = ttir.empty() : tensor<1x7xf32>
    %567 = "ttir.multiply"(%565, %0, %566) : (tensor<1x7xf32>, tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %568 = ttir.empty() : tensor<1x7x1xf32>
    %569 = "ttir.reshape"(%567, %568) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %570 = ttir.empty() : tensor<1x7x1xf32>
    %571 = "ttir.add"(%569, %44, %570) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %572 = ttir.empty() : tensor<1x7x1xf32>
    %573 = "ttir.rsqrt"(%571, %572) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %574 = ttir.empty() : tensor<1x7xf32>
    %575 = "ttir.reshape"(%573, %574) <{shape = [1 : i32, 7 : i32]}> : (tensor<1x7x1xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %576 = ttir.empty() : tensor<1x7x1xf32>
    %577 = "ttir.reshape"(%575, %576) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %578 = ttir.empty() : tensor<1x7x3072xf32>
    %579 = "ttir.broadcast"(%577, %578) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x7x1xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %580 = ttir.empty() : tensor<1x7x3072xf32>
    %581 = "ttir.multiply"(%561, %579, %580) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %582 = ttir.empty() : tensor<1x7x3072xbf16>
    %583 = "ttir.typecast"(%581, %582) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %584 = ttir.empty() : tensor<1x7x3072xf32>
    %585 = "ttir.typecast"(%583, %584) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %586 = ttir.empty() : tensor<1x7x3072xf32>
    %587 = "ttir.multiply"(%398, %585, %586) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %588 = ttir.empty() : tensor<1x7x3072xbf16>
    %589 = "ttir.typecast"(%587, %588) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %590 = ttir.empty() : tensor<7x3072xbf16>
    %591 = "ttir.reshape"(%589, %590) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x7x3072xbf16>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %592 = "ttir.dot_general"(%591, %arg24) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %593 = ttir.empty() : tensor<1x7x128256xbf16>
    %594 = "ttir.reshape"(%592, %593) <{shape = [1 : i32, 7 : i32, 128256 : i32]}> : (tensor<7x128256xbf16>, tensor<1x7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %132, %139, %385, %392, %592, %594 : tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}
2025-08-12 15:39:04.189 (   4.632s) [        C545D1C0]      module_builder.cc:506   WARN| `mhlo.num_partitions` attribute not found, assuming default number of partitions: 1
2025-08-12 15:39:04.189 (   4.632s) [        C545D1C0]      module_builder.cc:520   WARN| `mhlo.num_replicas` attribute not found, assuming default number of replicas: 1
2025-08-12 15:39:04.189 (   4.632s) [        C545D1C0]      module_builder.cc:528   WARN| Num replicas and num partitions are not set, inferring the number of devices from mesh shape
TTIRFusingPass: Starting pass on operation
TTIRFusingPass: Successfully applied Conv2dTagWeights patterns
CacheFillUpdatePattern: Attempting to match ScatterOp at loc("scatter.111")
getCacheUpdatePositions: Checking ScatterOp at loc("scatter.111")
CacheFillUpdatePattern: Successfully fusing ScatterOp into FillCacheOp
CacheFillUpdatePattern: Attempting to match ScatterOp at loc("scatter.135")
getCacheUpdatePositions: Checking ScatterOp at loc("scatter.135")
CacheFillUpdatePattern: Successfully fusing ScatterOp into FillCacheOp
CacheFillUpdatePattern: Attempting to match ScatterOp at loc("scatter.381")
getCacheUpdatePositions: Checking ScatterOp at loc("scatter.381")
CacheFillUpdatePattern: Successfully fusing ScatterOp into FillCacheOp
CacheFillUpdatePattern: Attempting to match ScatterOp at loc("scatter.405")
getCacheUpdatePositions: Checking ScatterOp at loc("scatter.405")
CacheFillUpdatePattern: Successfully fusing ScatterOp into FillCacheOp
TTIRFusingPass: Successfully applied all fusion patterns
TTIRFusingPass: Completed pass on operation
TTIRFusingPass: Starting pass on operation
TTIRFusingPass: Successfully applied Conv2dTagWeights patterns
TTIRFusingPass: Successfully applied all fusion patterns
TTIRFusingPass: Completed pass on operation
TTIRFusingPass: Starting pass on operation
TTIRFusingPass: Successfully applied Conv2dTagWeights patterns
TTIRFusingPass: Successfully applied all fusion patterns
TTIRFusingPass: Completed pass on operation
2025-08-12 15:39:04.280 (   4.723s) [        C545D1C0]      module_builder.cc:588      1| TTNN Module:
module @SyncTensorsGraph.613 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.613 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073184896, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073193216, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = 1x1, chipIds = [0]>
      func.func @main(%arg0: tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg2: tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg3: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg4: tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg5: tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg6: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg7: tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg8: tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg9: tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg10: tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg11: tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg12: tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg13: tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg14: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg15: tensor<1x1x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg16: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg17: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg18: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg19: tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg20: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg21: tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg22: tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg23: tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg24: tensor<3072x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg25: tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg26: tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg27: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg28: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg29: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg30: tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg31: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) -> (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device
        "ttnn.deallocate"(%arg7) <{force = false}> : (tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 3.25520843E-4 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x7>}> : (!ttnn.device) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x7x3072>}> : (!ttnn.device) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %3 = "ttnn.typecast"(%arg6) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg6) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %5 = "ttnn.typecast"(%arg4) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %6 = "ttnn.reshape"(%5) <{shape = [7 : i32]}> : (tensor<1x7xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x7xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %7 = "ttnn.from_device"(%6) : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %8 = "ttnn.to_layout"(%7) <{layout = #ttnn.layout<row_major>}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> ()
        %9 = "ttnn.to_device"(%8, %0) <{memory_config = #ttnn.memory_config<<dram>, <interleaved>>}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<system_memory>>>>) -> ()
        %10 = "ttnn.embedding"(%9, %arg5) : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%arg5) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %11 = "ttnn.typecast"(%10) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %12 = "ttnn.reshape"(%11) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %13 = "ttnn.pow"(%12, %2) : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %14 = "ttnn.sum"(%13) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %15 = "ttnn.multiply"(%14, %1) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %16 = "ttnn.reshape"(%15) <{shape = [7 : i32, 1 : i32]}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %17 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %18 = "ttnn.add"(%16, %17) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %19 = "ttnn.rsqrt"(%18) : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %20 = "ttnn.multiply"(%11, %19) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %21 = "ttnn.multiply"(%4, %20) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %22 = "ttnn.typecast"(%21) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%21) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %23 = "ttnn.matmul"(%22, %arg2) <{transpose_a = false, transpose_b = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %24 = "ttnn.reshape"(%23) <{shape = [1 : i32, 7 : i32, 8 : i32, 128 : i32]}> : (tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x8x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%23) <{force = false}> : (tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %25 = "ttnn.permute"(%24) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x8x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%24) <{force = false}> : (tensor<1x7x8x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %26 = "ttnn.typecast"(%25) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %27 = "ttnn.typecast"(%arg0) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %28 = "ttnn.reshape"(%27) <{shape = [1 : i32, 1 : i32, 7 : i32]}> : (tensor<7xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%27) <{force = false}> : (tensor<7xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %29 = "ttnn.matmul"(%arg1, %28) <{transpose_a = false, transpose_b = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%28) <{force = false}> : (tensor<1x1x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %30 = "ttnn.permute"(%29) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%29) <{force = false}> : (tensor<1x64x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %31 = "ttnn.reshape"(%30) <{shape = [1 : i32, 1 : i32, 7 : i32, 64 : i32]}> : (tensor<1x7x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %32 = "ttnn.reshape"(%30) <{shape = [1 : i32, 1 : i32, 7 : i32, 64 : i32]}> : (tensor<1x7x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%30) <{force = false}> : (tensor<1x7x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %33 = "ttnn.concat"(%31, %32) <{dim = 3 : si32}> : (tensor<1x1x7x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%32) <{force = false}> : (tensor<1x1x7x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%31) <{force = false}> : (tensor<1x1x7x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %34 = "ttnn.cos"(%33) : (tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %35 = "ttnn.multiply"(%26, %34) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%26) <{force = false}> : (tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %36 = "ttnn.typecast"(%35) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%35) <{force = false}> : (tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %37 = "ttnn.slice"(%25) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 8 : i32, 7 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %38 = "ttnn.neg"(%37) : (tensor<1x8x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%37) <{force = false}> : (tensor<1x8x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %39 = "ttnn.slice"(%25) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 7 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%25) <{force = false}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %40 = "ttnn.concat"(%38, %39) <{dim = 3 : si32}> : (tensor<1x8x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%39) <{force = false}> : (tensor<1x8x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%38) <{force = false}> : (tensor<1x8x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %41 = "ttnn.typecast"(%40) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%40) <{force = false}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %42 = "ttnn.sin"(%33) : (tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%33) <{force = false}> : (tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %43 = "ttnn.multiply"(%41, %42) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%41) <{force = false}> : (tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %44 = "ttnn.typecast"(%43) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%43) <{force = false}> : (tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %45 = "ttnn.add"(%36, %44) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%44) <{force = false}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%36) <{force = false}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.fill_cache"(%arg8, %45) <{batch_offset = 0 : i32}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%45) <{force = false}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %46 = "ttnn.matmul"(%22, %arg9) <{transpose_a = false, transpose_b = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg9) <{force = false}> : (tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %47 = "ttnn.reshape"(%46) <{shape = [1 : i32, 7 : i32, 8 : i32, 128 : i32]}> : (tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x8x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%46) <{force = false}> : (tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %48 = "ttnn.permute"(%47) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x8x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%47) <{force = false}> : (tensor<1x7x8x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.fill_cache"(%arg10, %48) <{batch_offset = 0 : i32}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%48) <{force = false}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %49 = "ttnn.typecast"(%arg20) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg20) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %50 = "ttnn.reshape"(%49) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%49) <{force = false}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %51 = "ttnn.matmul"(%22, %arg17) <{transpose_a = false, transpose_b = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%22) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%arg17) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %52 = "ttnn.reshape"(%51) <{shape = [1 : i32, 7 : i32, 24 : i32, 128 : i32]}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x24x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%51) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %53 = "ttnn.permute"(%52) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x24x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%52) <{force = false}> : (tensor<1x7x24x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %54 = "ttnn.typecast"(%53) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %55 = "ttnn.reshape"(%54) <{shape = [24 : i32, 7 : i32, 128 : i32]}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%54) <{force = false}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %56 = "ttnn.reshape"(%34) <{shape = [1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %57 = "ttnn.multiply"(%55, %56) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%55) <{force = false}> : (tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %58 = "ttnn.typecast"(%57) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%57) <{force = false}> : (tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %59 = "ttnn.slice"(%53) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 24 : i32, 7 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %60 = "ttnn.neg"(%59) : (tensor<1x24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%59) <{force = false}> : (tensor<1x24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %61 = "ttnn.reshape"(%60) <{shape = [24 : i32, 7 : i32, 64 : i32]}> : (tensor<1x24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%60) <{force = false}> : (tensor<1x24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %62 = "ttnn.slice"(%53) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 24 : i32, 7 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%53) <{force = false}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %63 = "ttnn.reshape"(%62) <{shape = [24 : i32, 7 : i32, 64 : i32]}> : (tensor<1x24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%62) <{force = false}> : (tensor<1x24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %64 = "ttnn.concat"(%61, %63) <{dim = 2 : si32}> : (tensor<24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%63) <{force = false}> : (tensor<24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%61) <{force = false}> : (tensor<24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %65 = "ttnn.typecast"(%64) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%64) <{force = false}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %66 = "ttnn.reshape"(%42) <{shape = [1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %67 = "ttnn.multiply"(%65, %66) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%65) <{force = false}> : (tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %68 = "ttnn.typecast"(%67) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%67) <{force = false}> : (tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %69 = "ttnn.add"(%58, %68) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%68) <{force = false}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%58) <{force = false}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %70 = "ttnn.reshape"(%arg8) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %71 = "ttnn.repeat"(%70) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%70) <{force = false}> : (tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %72 = "ttnn.reshape"(%71) <{shape = [1 : i32, 24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%71) <{force = false}> : (tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %73 = "ttnn.permute"(%72) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%72) <{force = false}> : (tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %74 = "ttnn.reshape"(%73) <{shape = [24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%73) <{force = false}> : (tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %75 = "ttnn.matmul"(%69, %74) <{transpose_a = false, transpose_b = false}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%74) <{force = false}> : (tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%69) <{force = false}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %76 = "ttnn.typecast"(%75) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%75) <{force = false}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %77 = "ttnn.reshape"(%76) <{shape = [1 : i32, 24 : i32, 7 : i32, 128 : i32]}> : (tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%76) <{force = false}> : (tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %78 = "ttnn.reshape"(%arg16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg16) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %79 = "ttnn.multiply"(%77, %78) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%77) <{force = false}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %80 = "ttnn.typecast"(%79) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%79) <{force = false}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %81 = "ttnn.add"(%80, %arg15) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%80) <{force = false}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %82 = "ttnn.typecast"(%81) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%81) <{force = false}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %83 = "ttnn.max"(%82) <{dim_arg = [3 : i32], keep_dim = true}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %84 = "ttnn.neg"(%83) : (tensor<1x24x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%83) <{force = false}> : (tensor<1x24x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %85 = "ttnn.add"(%82, %84) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x24x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%84) <{force = false}> : (tensor<1x24x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%82) <{force = false}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %86 = "ttnn.softmax"(%85) <{dimension = 3 : si32}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%85) <{force = false}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %87 = "ttnn.typecast"(%86) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%86) <{force = false}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %88 = "ttnn.reshape"(%87) <{shape = [24 : i32, 7 : i32, 128 : i32]}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%87) <{force = false}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %89 = "ttnn.reshape"(%arg10) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %90 = "ttnn.repeat"(%89) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%89) <{force = false}> : (tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %91 = "ttnn.reshape"(%90) <{shape = [24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%90) <{force = false}> : (tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %92 = "ttnn.matmul"(%88, %91) <{transpose_a = false, transpose_b = false}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%91) <{force = false}> : (tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%88) <{force = false}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %93 = "ttnn.reshape"(%92) <{shape = [1 : i32, 24 : i32, 7 : i32, 128 : i32]}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%92) <{force = false}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %94 = "ttnn.permute"(%93) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x24x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%93) <{force = false}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %95 = "ttnn.reshape"(%94) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x7x24x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%94) <{force = false}> : (tensor<1x7x24x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %96 = "ttnn.matmul"(%95, %arg14) <{transpose_a = false, transpose_b = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%95) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%arg14) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %97 = "ttnn.add"(%10, %96) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%96) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %98 = "ttnn.typecast"(%arg18) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg18) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %99 = "ttnn.reshape"(%98) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%98) <{force = false}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %100 = "ttnn.typecast"(%97) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %101 = "ttnn.reshape"(%100) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %102 = "ttnn.pow"(%101, %2) : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%101) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %103 = "ttnn.sum"(%102) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%102) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %104 = "ttnn.multiply"(%103, %1) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%103) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %105 = "ttnn.reshape"(%104) <{shape = [7 : i32, 1 : i32]}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%104) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %106 = "ttnn.add"(%105, %17) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%105) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %107 = "ttnn.rsqrt"(%106) : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%106) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %108 = "ttnn.multiply"(%100, %107) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%107) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%100) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %109 = "ttnn.multiply"(%99, %108) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%108) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%99) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %110 = "ttnn.typecast"(%109) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%109) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %111 = "ttnn.matmul"(%110, %arg19) <{transpose_a = false, transpose_b = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg19) <{force = false}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %112 = "ttnn.typecast"(%111) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %113 = "ttnn.sigmoid"(%111) : (tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%111) <{force = false}> : (tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %114 = "ttnn.typecast"(%113) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%113) <{force = false}> : (tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %115 = "ttnn.multiply"(%112, %114) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%114) <{force = false}> : (tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%112) <{force = false}> : (tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %116 = "ttnn.matmul"(%110, %arg13) <{transpose_a = false, transpose_b = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%110) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%arg13) <{force = false}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %117 = "ttnn.typecast"(%116) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%116) <{force = false}> : (tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %118 = "ttnn.multiply"(%115, %117) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%117) <{force = false}> : (tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%115) <{force = false}> : (tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %119 = "ttnn.typecast"(%118) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%118) <{force = false}> : (tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %120 = "ttnn.matmul"(%119, %arg12) <{transpose_a = false, transpose_b = false}> : (tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%119) <{force = false}> : (tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%arg12) <{force = false}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %121 = "ttnn.add"(%97, %120) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%120) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%97) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %122 = "ttnn.typecast"(%121) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %123 = "ttnn.reshape"(%122) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %124 = "ttnn.pow"(%123, %2) : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%123) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %125 = "ttnn.sum"(%124) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%124) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %126 = "ttnn.multiply"(%125, %1) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%125) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %127 = "ttnn.reshape"(%126) <{shape = [7 : i32, 1 : i32]}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%126) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %128 = "ttnn.add"(%127, %17) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%127) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %129 = "ttnn.rsqrt"(%128) : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%128) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %130 = "ttnn.multiply"(%122, %129) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%129) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%122) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %131 = "ttnn.multiply"(%50, %130) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%130) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%50) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %132 = "ttnn.typecast"(%131) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%131) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %133 = "ttnn.matmul"(%132, %arg11) <{transpose_a = false, transpose_b = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg11) <{force = false}> : (tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %134 = "ttnn.reshape"(%133) <{shape = [1 : i32, 7 : i32, 8 : i32, 128 : i32]}> : (tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x8x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%133) <{force = false}> : (tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %135 = "ttnn.permute"(%134) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x8x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%134) <{force = false}> : (tensor<1x7x8x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %136 = "ttnn.typecast"(%135) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %137 = "ttnn.multiply"(%136, %34) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%136) <{force = false}> : (tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%34) <{force = false}> : (tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %138 = "ttnn.typecast"(%137) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%137) <{force = false}> : (tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %139 = "ttnn.slice"(%135) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 8 : i32, 7 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %140 = "ttnn.neg"(%139) : (tensor<1x8x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%139) <{force = false}> : (tensor<1x8x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %141 = "ttnn.slice"(%135) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 7 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%135) <{force = false}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %142 = "ttnn.concat"(%140, %141) <{dim = 3 : si32}> : (tensor<1x8x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%141) <{force = false}> : (tensor<1x8x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%140) <{force = false}> : (tensor<1x8x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %143 = "ttnn.typecast"(%142) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%142) <{force = false}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %144 = "ttnn.multiply"(%143, %42) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%143) <{force = false}> : (tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%42) <{force = false}> : (tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %145 = "ttnn.typecast"(%144) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%144) <{force = false}> : (tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %146 = "ttnn.add"(%138, %145) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%145) <{force = false}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%138) <{force = false}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.fill_cache"(%arg21, %146) <{batch_offset = 0 : i32}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%146) <{force = false}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %147 = "ttnn.matmul"(%132, %arg22) <{transpose_a = false, transpose_b = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg22) <{force = false}> : (tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %148 = "ttnn.reshape"(%147) <{shape = [1 : i32, 7 : i32, 8 : i32, 128 : i32]}> : (tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x8x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%147) <{force = false}> : (tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %149 = "ttnn.permute"(%148) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x8x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%148) <{force = false}> : (tensor<1x7x8x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.fill_cache"(%arg23, %149) <{batch_offset = 0 : i32}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%149) <{force = false}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %150 = "ttnn.typecast"(%arg31) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg31) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %151 = "ttnn.reshape"(%150) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%150) <{force = false}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %152 = "ttnn.matmul"(%132, %arg28) <{transpose_a = false, transpose_b = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%132) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%arg28) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %153 = "ttnn.reshape"(%152) <{shape = [1 : i32, 7 : i32, 24 : i32, 128 : i32]}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x24x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%152) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %154 = "ttnn.permute"(%153) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x24x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%153) <{force = false}> : (tensor<1x7x24x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %155 = "ttnn.typecast"(%154) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %156 = "ttnn.reshape"(%155) <{shape = [24 : i32, 7 : i32, 128 : i32]}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%155) <{force = false}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %157 = "ttnn.multiply"(%156, %56) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%156) <{force = false}> : (tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%56) <{force = false}> : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %158 = "ttnn.typecast"(%157) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%157) <{force = false}> : (tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %159 = "ttnn.slice"(%154) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 24 : i32, 7 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %160 = "ttnn.neg"(%159) : (tensor<1x24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%159) <{force = false}> : (tensor<1x24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %161 = "ttnn.reshape"(%160) <{shape = [24 : i32, 7 : i32, 64 : i32]}> : (tensor<1x24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%160) <{force = false}> : (tensor<1x24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %162 = "ttnn.slice"(%154) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 24 : i32, 7 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%154) <{force = false}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %163 = "ttnn.reshape"(%162) <{shape = [24 : i32, 7 : i32, 64 : i32]}> : (tensor<1x24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%162) <{force = false}> : (tensor<1x24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %164 = "ttnn.concat"(%161, %163) <{dim = 2 : si32}> : (tensor<24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%163) <{force = false}> : (tensor<24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%161) <{force = false}> : (tensor<24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %165 = "ttnn.typecast"(%164) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%164) <{force = false}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %166 = "ttnn.multiply"(%165, %66) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%165) <{force = false}> : (tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%66) <{force = false}> : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %167 = "ttnn.typecast"(%166) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%166) <{force = false}> : (tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %168 = "ttnn.add"(%158, %167) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%167) <{force = false}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%158) <{force = false}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %169 = "ttnn.reshape"(%arg21) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %170 = "ttnn.repeat"(%169) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%169) <{force = false}> : (tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %171 = "ttnn.reshape"(%170) <{shape = [1 : i32, 24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%170) <{force = false}> : (tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %172 = "ttnn.permute"(%171) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%171) <{force = false}> : (tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %173 = "ttnn.reshape"(%172) <{shape = [24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%172) <{force = false}> : (tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %174 = "ttnn.matmul"(%168, %173) <{transpose_a = false, transpose_b = false}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%173) <{force = false}> : (tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%168) <{force = false}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %175 = "ttnn.typecast"(%174) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%174) <{force = false}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %176 = "ttnn.reshape"(%175) <{shape = [1 : i32, 24 : i32, 7 : i32, 128 : i32]}> : (tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%175) <{force = false}> : (tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %177 = "ttnn.multiply"(%176, %78) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%176) <{force = false}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%78) <{force = false}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %178 = "ttnn.typecast"(%177) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%177) <{force = false}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %179 = "ttnn.add"(%178, %arg15) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%178) <{force = false}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%arg15) <{force = false}> : (tensor<1x1x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %180 = "ttnn.typecast"(%179) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%179) <{force = false}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %181 = "ttnn.max"(%180) <{dim_arg = [3 : i32], keep_dim = true}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %182 = "ttnn.neg"(%181) : (tensor<1x24x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%181) <{force = false}> : (tensor<1x24x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %183 = "ttnn.add"(%180, %182) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x24x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%182) <{force = false}> : (tensor<1x24x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%180) <{force = false}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %184 = "ttnn.softmax"(%183) <{dimension = 3 : si32}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%183) <{force = false}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %185 = "ttnn.typecast"(%184) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%184) <{force = false}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %186 = "ttnn.reshape"(%185) <{shape = [24 : i32, 7 : i32, 128 : i32]}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%185) <{force = false}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %187 = "ttnn.reshape"(%arg23) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %188 = "ttnn.repeat"(%187) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%187) <{force = false}> : (tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %189 = "ttnn.reshape"(%188) <{shape = [24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%188) <{force = false}> : (tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %190 = "ttnn.matmul"(%186, %189) <{transpose_a = false, transpose_b = false}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%189) <{force = false}> : (tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%186) <{force = false}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %191 = "ttnn.reshape"(%190) <{shape = [1 : i32, 24 : i32, 7 : i32, 128 : i32]}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%190) <{force = false}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %192 = "ttnn.permute"(%191) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x24x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%191) <{force = false}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %193 = "ttnn.reshape"(%192) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x7x24x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%192) <{force = false}> : (tensor<1x7x24x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %194 = "ttnn.matmul"(%193, %arg27) <{transpose_a = false, transpose_b = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%193) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%arg27) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %195 = "ttnn.add"(%121, %194) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%194) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%121) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %196 = "ttnn.typecast"(%arg29) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg29) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %197 = "ttnn.reshape"(%196) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%196) <{force = false}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %198 = "ttnn.typecast"(%195) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %199 = "ttnn.reshape"(%198) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %200 = "ttnn.pow"(%199, %2) : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%199) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %201 = "ttnn.sum"(%200) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%200) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %202 = "ttnn.multiply"(%201, %1) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%201) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %203 = "ttnn.reshape"(%202) <{shape = [7 : i32, 1 : i32]}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%202) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %204 = "ttnn.add"(%203, %17) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%203) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %205 = "ttnn.rsqrt"(%204) : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%204) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %206 = "ttnn.multiply"(%198, %205) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%205) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%198) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %207 = "ttnn.multiply"(%197, %206) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%206) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%197) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %208 = "ttnn.typecast"(%207) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%207) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %209 = "ttnn.matmul"(%208, %arg30) <{transpose_a = false, transpose_b = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg30) <{force = false}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %210 = "ttnn.typecast"(%209) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %211 = "ttnn.sigmoid"(%209) : (tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%209) <{force = false}> : (tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %212 = "ttnn.typecast"(%211) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%211) <{force = false}> : (tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %213 = "ttnn.multiply"(%210, %212) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%212) <{force = false}> : (tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%210) <{force = false}> : (tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %214 = "ttnn.matmul"(%208, %arg26) <{transpose_a = false, transpose_b = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%208) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%arg26) <{force = false}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %215 = "ttnn.typecast"(%214) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%214) <{force = false}> : (tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %216 = "ttnn.multiply"(%213, %215) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%215) <{force = false}> : (tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%213) <{force = false}> : (tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %217 = "ttnn.typecast"(%216) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%216) <{force = false}> : (tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %218 = "ttnn.matmul"(%217, %arg25) <{transpose_a = false, transpose_b = false}> : (tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%217) <{force = false}> : (tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%arg25) <{force = false}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %219 = "ttnn.add"(%195, %218) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%218) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%195) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %220 = "ttnn.typecast"(%219) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%219) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %221 = "ttnn.reshape"(%220) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %222 = "ttnn.pow"(%221, %2) : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%221) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %223 = "ttnn.sum"(%222) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%222) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %224 = "ttnn.multiply"(%223, %1) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%223) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %225 = "ttnn.reshape"(%224) <{shape = [7 : i32, 1 : i32]}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%224) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %226 = "ttnn.add"(%225, %17) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%225) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %227 = "ttnn.rsqrt"(%226) : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%226) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %228 = "ttnn.multiply"(%220, %227) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%227) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%220) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %229 = "ttnn.multiply"(%151, %228) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%228) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%151) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %230 = "ttnn.typecast"(%229) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%229) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %231 = "ttnn.matmul"(%230, %arg24) <{transpose_a = false, transpose_b = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%230) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%arg24) <{force = false}> : (tensor<3072x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %232 = "ttnn.reshape"(%231) <{shape = [1 : i32, 7 : i32, 128256 : i32]}> : (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %arg8, %arg10, %arg21, %arg23, %231, %232 : tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
    }
  }
}
2025-08-12 15:39:04.314 (   4.756s) [        C545D1C0]loaded_executable_insta:98       1| [LIFECYCLE] LoadedExecutableInstance constructor - instance created: 0x56544d18cb70
2025-08-12 15:39:04.314 (   4.757s) [        C545D1C0]loaded_executable_insta:516      1| LoadedExecutableInstance::PJRT_LoadedExecutable_GetExecutable
2025-08-12 15:39:04.314 (   4.757s) [        C545D1C0]loaded_executable_insta:535      1| LoadedExecutableInstance::PJRT_LoadedExecutable_AddressableDevices
2025-08-12 15:39:04.314 (   4.757s) [        C545D1C0]              stubs.inc:76    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2025-08-12 15:39:04.314 (   4.757s) [        C545D1C0]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-08-12 15:39:04.314 (   4.757s) [        C545D1C0]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-08-12 15:39:04.314 (   4.757s) [        C545D1C0]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-08-12 15:39:04.314 (   4.757s) [        C545D1C0] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-08-12 15:39:04.315 (   4.757s) [        C545D1C0] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-08-12 15:39:04.329 (   4.772s) [        C545D1C0] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-08-12 15:39:04.329 (   4.772s) [        C545D1C0] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640] executable_instance.cc:138      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]loaded_executable_insta:571      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]loaded_executable_insta:95       1| LoadedExecutableInstance::Execute
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]loaded_executable_insta:114      1| [DEVICE] Runtime device not opened, opening devices...
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]loaded_executable_insta:204      1| [DEVICE] Starting device opening process with 32 args on 1 devices
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]loaded_executable_insta:207      1| [DEVICE] Found 1 unique device IDs from arguments
2025-08-12 15:39:04.348 (   4.791s) [        CAFFD640]loaded_executable_insta:246      1| [DEVICE] Opening mesh device with shape [1, 1]
2025-08-12 15:39:05.434 (   5.877s) [        CAFFD640]loaded_executable_insta:250      1| [DEVICE] Mesh device opened successfully
2025-08-12 15:39:05.434 (   5.877s) [        CAFFD640]loaded_executable_insta:121      1| [DEVICE] Successfully opened runtime device
2025-08-12 15:39:05.435 (   5.877s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544d14c4c0 (arg 0)
2025-08-12 15:39:05.435 (   5.877s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442e7f3e0 (arg 1)
2025-08-12 15:39:05.435 (   5.878s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442e63540 (arg 2)
2025-08-12 15:39:05.927 (   6.370s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442df5210 (arg 3)
2025-08-12 15:39:05.927 (   6.370s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x5654194a3660 (arg 4)
2025-08-12 15:39:05.927 (   6.370s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442d5fb30 (arg 5)
2025-08-12 15:39:06.509 (   6.952s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544d10d520 (arg 6)
2025-08-12 15:39:07.007 (   7.450s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ff46020 (arg 7)
2025-08-12 15:39:07.007 (   7.450s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442df6310 (arg 8)
2025-08-12 15:39:07.500 (   7.943s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x5654443afe50 (arg 9)
2025-08-12 15:39:07.501 (   7.944s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442db7c30 (arg 10)
2025-08-12 15:39:07.502 (   7.944s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x5654193690f0 (arg 11)
2025-08-12 15:39:07.503 (   7.946s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442caa890 (arg 12)
2025-08-12 15:39:07.991 (   8.434s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442ddf2a0 (arg 13)
2025-08-12 15:39:08.525 (   8.968s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442d8ac80 (arg 14)
2025-08-12 15:39:09.070 (   9.513s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x5654400acf50 (arg 15)
2025-08-12 15:39:09.516 (   9.959s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544d2685d0 (arg 16)
2025-08-12 15:39:09.517 (   9.959s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442d1ba90 (arg 17)
2025-08-12 15:39:09.520 (   9.963s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x5654442fbe30 (arg 18)
2025-08-12 15:39:09.521 (   9.964s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442db2fb0 (arg 19)
2025-08-12 15:39:09.529 (   9.972s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543fdd9280 (arg 20)
2025-08-12 15:39:09.530 (   9.973s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442e258a0 (arg 21)
2025-08-12 15:39:09.531 (   9.974s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565445938000 (arg 22)
2025-08-12 15:39:09.532 (   9.975s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442e21350 (arg 23)
2025-08-12 15:39:09.533 (   9.975s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56541937cf20 (arg 24)
2025-08-12 15:39:10.121 (  10.563s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442dac6b0 (arg 25)
2025-08-12 15:39:10.135 (  10.578s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442ea72c0 (arg 26)
2025-08-12 15:39:10.143 (  10.586s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442e3bda0 (arg 27)
2025-08-12 15:39:10.146 (  10.589s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442e9b490 (arg 28)
2025-08-12 15:39:10.149 (  10.592s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544d1e8ff0 (arg 29)
2025-08-12 15:39:10.150 (  10.593s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565445a80c40 (arg 30)
2025-08-12 15:39:10.158 (  10.601s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565444604f10 (arg 31)
2025-08-12 15:39:37.241 (  37.684s) [        CAFFD640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:37.241 (  37.684s) [        CAFFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:37.241 (  37.684s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:37.241 (  37.684s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:37.241 (  37.684s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:37.241 (  37.684s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:37.241 (  37.684s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:37.241 (  37.684s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:37.241 (  37.684s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:37.241 (  37.684s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:37.241 (  37.684s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:37.241 (  37.684s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:37.241 (  37.684s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:37.241 (  37.684s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:37.241 (  37.684s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:37.241 (  37.684s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:37.241 (  37.684s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:37.241 (  37.684s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:37.241 (  37.684s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:37.241 (  37.684s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:37.241 (  37.684s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:37.241 (  37.684s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:37.242 (  37.684s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:37.242 (  37.684s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:37.242 (  37.684s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:37.242 (  37.684s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:37.242 (  37.684s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:37.242 (  37.685s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:37.242 (  37.685s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:37.242 (  37.685s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:37.242 (  37.685s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:37.242 (  37.685s) [        C545D1C0]     buffer_instance.cc:454      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-08-12 15:39:37.242 (  37.685s) [        C545D1C0]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:37.242 (  37.685s) [        C545D1C0]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:37.243 (  37.685s) [        C545D1C0]     buffer_instance.cc:476      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-08-12 15:39:37.243 (  37.686s) [        C545D1C0]     buffer_instance.cc:497      1| [BUFFER_TO_HOST] Source buffer shape: [1, 7, 128256], data_type: 13, required_size: 1795584 bytes
2025-08-12 15:39:37.243 (  37.686s) [        C545D1C0]     buffer_instance.cc:508      1| [BUFFER_TO_HOST] Destination buffer: dst_size=1795584 bytes, dst_ptr=0x5654454d2900
2025-08-12 15:39:37.243 (  37.686s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:37.243 (  37.686s) [        CC95F640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:37.245 (  37.687s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:37.245 (  37.687s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
alink2025-08-12 15:39:39.049 (  39.492s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:39.049 (  39.492s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:39.049 (  39.492s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:39.049 (  39.492s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:39.049 (  39.492s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:39.049 (  39.492s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:39.049 (  39.492s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:39.049 (  39.492s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:39.049 (  39.492s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:39.049 (  39.492s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:39.049 (  39.492s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:39.049 (  39.492s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:39.049 (  39.492s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:39.049 (  39.492s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:39.049 (  39.492s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:39.049 (  39.492s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:39.049 (  39.492s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:39.049 (  39.492s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:39.050 (  39.492s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:39.050 (  39.492s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:39.050 (  39.492s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:39.050 (  39.492s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:39.050 (  39.492s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:39.050 (  39.492s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:39.050 (  39.492s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:39.050 (  39.492s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:39.050 (  39.492s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:39.050 (  39.493s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:39.050 (  39.493s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:39.050 (  39.493s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:39.082 (  39.525s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:39.082 (  39.525s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:39.082 (  39.525s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:39.083 (  39.525s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [128256, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:39.083 (  39.525s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:39.083 (  39.525s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:39.083 (  39.526s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:39.083 (  39.526s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:39.083 (  39.526s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:39.083 (  39.526s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1, 64, 1] (semantics: ZeroCopy/other)
2025-08-12 15:39:39.083 (  39.526s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:39.083 (  39.526s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:39.098 (  39.540s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:39.098 (  39.540s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:39.098 (  39.540s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:39.098 (  39.541s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:39.098 (  39.541s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:39.098 (  39.541s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:39.103 (  39.545s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:39.103 (  39.545s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:39.103 (  39.545s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:39.103 (  39.546s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 1024] (semantics: ZeroCopy/other)
2025-08-12 15:39:39.103 (  39.546s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:39.103 (  39.546s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:39.108 (  39.550s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:39.108 (  39.550s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:39.108 (  39.550s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:39.108 (  39.551s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 1024] (semantics: ZeroCopy/other)
2025-08-12 15:39:39.108 (  39.551s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:39.108 (  39.551s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:39.122 (  39.565s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:39.122 (  39.565s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:39.122 (  39.565s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:39.122 (  39.565s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:39.122 (  39.565s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:39.122 (  39.565s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:39.164 (  39.607s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:39.164 (  39.607s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:39.164 (  39.607s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:39.164 (  39.607s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 8192] (semantics: ZeroCopy/other)
2025-08-12 15:39:39.164 (  39.607s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:39.164 (  39.607s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:39.206 (  39.648s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:39.206 (  39.648s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:39.206 (  39.648s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:39.206 (  39.649s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 8192] (semantics: ZeroCopy/other)
2025-08-12 15:39:39.206 (  39.649s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:39.206 (  39.649s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:39.245 (  39.688s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:39.245 (  39.688s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:39.245 (  39.688s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:39.246 (  39.688s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [8192, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:39.246 (  39.689s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:39.246 (  39.689s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:39.260 (  39.703s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:39.260 (  39.703s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:39.260 (  39.703s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:39.260 (  39.703s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:39.260 (  39.703s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:39.260 (  39.703s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:39.265 (  39.708s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:39.265 (  39.708s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:39.265 (  39.708s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:39.265 (  39.708s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 1024] (semantics: ZeroCopy/other)
2025-08-12 15:39:39.265 (  39.708s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:39.265 (  39.708s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:39.270 (  39.713s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:39.270 (  39.713s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:39.270 (  39.713s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:39.270 (  39.713s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 1024] (semantics: ZeroCopy/other)
2025-08-12 15:39:39.270 (  39.713s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:39.270 (  39.713s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:39.284 (  39.727s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:39.284 (  39.727s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:39.284 (  39.727s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:39.284 (  39.727s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:39.285 (  39.727s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:39.285 (  39.727s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:39.326 (  39.769s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:39.326 (  39.769s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:39.326 (  39.769s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:39.327 (  39.769s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 8192] (semantics: ZeroCopy/other)
2025-08-12 15:39:39.327 (  39.769s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:39.327 (  39.769s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:39.368 (  39.811s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:39.369 (  39.811s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:39.369 (  39.811s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:39.369 (  39.812s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 8192] (semantics: ZeroCopy/other)
2025-08-12 15:39:39.369 (  39.812s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:39.369 (  39.812s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:39.408 (  39.851s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:39.408 (  39.851s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:39.408 (  39.851s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:39.408 (  39.851s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [8192, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:39.408 (  39.851s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:39.408 (  39.851s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:40.090 (  40.533s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:40.091 (  40.533s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:40.091 (  40.533s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:40.091 (  40.534s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 128256] (semantics: ZeroCopy/other)
2025-08-12 15:39:40.091 (  40.534s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:40.091 (  40.534s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:40.092 (  40.535s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:40.092 (  40.535s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:40.092 (  40.535s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:40.093 (  40.535s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:40.093 (  40.535s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:40.093 (  40.535s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:40.093 (  40.536s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:40.093 (  40.536s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:40.093 (  40.536s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:40.093 (  40.536s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1024, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:40.093 (  40.536s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:40.093 (  40.536s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:40.094 (  40.536s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:40.094 (  40.536s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:40.094 (  40.536s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:40.094 (  40.537s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1024, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:40.094 (  40.537s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:40.094 (  40.537s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:40.095 (  40.538s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:40.095 (  40.538s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:40.095 (  40.538s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:40.095 (  40.538s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:40.095 (  40.538s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:40.095 (  40.538s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:40.098 (  40.540s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:40.098 (  40.540s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:40.098 (  40.540s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:40.098 (  40.540s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [8192, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:40.098 (  40.541s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:40.098 (  40.541s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:40.100 (  40.543s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:40.100 (  40.543s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:40.100 (  40.543s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:40.100 (  40.543s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [8192, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:40.100 (  40.543s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:40.100 (  40.543s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:40.103 (  40.545s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:40.103 (  40.545s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:40.103 (  40.545s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:40.103 (  40.545s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 8192] (semantics: ZeroCopy/other)
2025-08-12 15:39:40.103 (  40.546s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:40.103 (  40.546s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:40.104 (  40.547s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:40.104 (  40.547s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:40.104 (  40.547s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:40.104 (  40.547s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:40.104 (  40.547s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:40.104 (  40.547s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:40.104 (  40.547s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:40.104 (  40.547s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:40.105 (  40.547s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:40.105 (  40.547s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1024, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:40.105 (  40.547s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:40.105 (  40.547s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:40.105 (  40.548s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:40.105 (  40.548s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:40.105 (  40.548s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:40.105 (  40.548s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1024, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:40.105 (  40.548s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:40.105 (  40.548s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:40.106 (  40.549s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:40.106 (  40.549s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:40.106 (  40.549s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:40.106 (  40.549s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:40.106 (  40.549s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:40.106 (  40.549s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:40.109 (  40.552s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:40.109 (  40.552s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:40.109 (  40.552s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:40.109 (  40.552s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [8192, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:40.109 (  40.552s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:40.109 (  40.552s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:40.111 (  40.554s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:40.111 (  40.554s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:40.111 (  40.554s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:40.111 (  40.554s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [8192, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:40.111 (  40.554s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:40.111 (  40.554s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:40.114 (  40.557s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:40.114 (  40.557s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:40.114 (  40.557s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:40.114 (  40.557s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 8192] (semantics: ZeroCopy/other)
2025-08-12 15:39:40.114 (  40.557s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:40.114 (  40.557s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:40.149 (  40.592s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:40.149 (  40.592s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:40.149 (  40.592s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:40.149 (  40.592s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [128256, 3072] (semantics: ZeroCopy/other)
2025-08-12 15:39:40.150 (  40.592s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:40.150 (  40.592s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:40.150 (  40.593s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:40.150 (  40.593s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:40.150 (  40.593s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:40.150 (  40.593s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1, 8, 128, 128] (semantics: ZeroCopy/other)
2025-08-12 15:39:40.150 (  40.593s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:40.150 (  40.593s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:40.150 (  40.593s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:40.150 (  40.593s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:40.150 (  40.593s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:40.150 (  40.593s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1, 8, 128, 128] (semantics: ZeroCopy/other)
2025-08-12 15:39:40.150 (  40.593s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:40.150 (  40.593s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:40.151 (  40.593s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:40.151 (  40.593s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:40.151 (  40.593s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:40.151 (  40.593s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1, 8, 128, 128] (semantics: ZeroCopy/other)
2025-08-12 15:39:40.151 (  40.594s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:40.151 (  40.594s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:40.151 (  40.594s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:40.151 (  40.594s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:40.151 (  40.594s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:40.151 (  40.594s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1, 8, 128, 128] (semantics: ZeroCopy/other)
2025-08-12 15:39:40.151 (  40.594s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:40.151 (  40.594s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:40.151 (  40.594s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:40.151 (  40.594s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:40.151 (  40.594s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:40.151 (  40.594s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [64] (semantics: ZeroCopy/other)
2025-08-12 15:39:40.151 (  40.594s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:40.151 (  40.594s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:40.210 (  40.653s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:40.210 (  40.653s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:40.210 (  40.653s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:40.210 (  40.653s) [        C545D1C0]     buffer_instance.cc:213      1| [BUFFER] Creating OWNED host tensor with shape [1, 1] (semantics: other, supported_dtype: false)
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-08-12 15:39:40.210 (  40.653s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:40.210 (  40.653s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:40.211 (  40.653s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:40.211 (  40.653s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:40.211 (  40.653s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:40.211 (  40.653s) [        C545D1C0]     buffer_instance.cc:213      1| [BUFFER] Creating OWNED host tensor with shape [1] (semantics: other, supported_dtype: false)
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-08-12 15:39:40.211 (  40.653s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:40.211 (  40.653s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:40.211 (  40.654s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:40.211 (  40.654s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:40.211 (  40.654s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:40.211 (  40.654s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1, 1, 1, 128] (semantics: ZeroCopy/other)
2025-08-12 15:39:40.211 (  40.654s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:40.211 (  40.654s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:40.219 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.219 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.219 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.219 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.219 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.219 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.219 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.219 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.219 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.219 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.219 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.219 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.219 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.219 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.219 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.219 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.219 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.219 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.219 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.219 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.219 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.219 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.219 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.219 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.219 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.219 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.219 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.219 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.219 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.662s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
Note: Using experimental XLA backend.
[James] disable rectify buffer inplace copy
Tensor id via xlac: 423 with shape torch.Size([3072])
Tensor id via xlac: 424 with shape torch.Size([3072])
Tensor id via xlac: 425 with shape torch.Size([3072])
Tensor id via xlac: 426 with shape torch.Size([3072])
Tensor id via xlac: 427 with shape torch.Size([3072])
Tensor id via xlac: 428 with shape torch.Size([128256, 3072])
Tensor id via xlac: 429 with shape torch.Size([1, 64, 1])
Tensor id via xlac: 430 with shape torch.Size([3072, 3072])
Tensor id via xlac: 431 with shape torch.Size([3072, 1024])
Tensor id via xlac: 432 with shape torch.Size([3072, 1024])
Tensor id via xlac: 433 with shape torch.Size([3072, 3072])
Tensor id via xlac: 434 with shape torch.Size([3072, 8192])
Tensor id via xlac: 435 with shape torch.Size([3072, 8192])
Tensor id via xlac: 436 with shape torch.Size([8192, 3072])
Tensor id via xlac: 437 with shape torch.Size([3072, 3072])
Tensor id via xlac: 438 with shape torch.Size([3072, 1024])
Tensor id via xlac: 439 with shape torch.Size([3072, 1024])
Tensor id via xlac: 440 with shape torch.Size([3072, 3072])
Tensor id via xlac: 441 with shape torch.Size([3072, 8192])
Tensor id via xlac: 442 with shape torch.Size([3072, 8192])
Tensor id via xlac: 443 with shape torch.Size([8192, 3072])
Tensor id via xlac: 444 with shape torch.Size([3072, 128256])
Tensor id via xlac: 445 with shape torch.Size([3072, 3072])
Tensor id via xlac: 446 with shape torch.Size([1024, 3072])
Tensor id via xlac: 447 with shape torch.Size([1024, 3072])
Tensor id via xlac: 448 with shape torch.Size([3072, 3072])
Tensor id via xlac: 449 with shape torch.Size([8192, 3072])
Tensor id via xlac: 450 with shape torch.Size([8192, 3072])
Tensor id via xlac: 451 with shape torch.Size([3072, 8192])
Tensor id via xlac: 452 with shape torch.Size([3072, 3072])
Tensor id via xlac: 453 with shape torch.Size([1024, 3072])
Tensor id via xlac: 454 with shape torch.Size([1024, 3072])
Tensor id via xlac: 455 with shape torch.Size([3072, 3072])
Tensor id via xlac: 456 with shape torch.Size([8192, 3072])
Tensor id via xlac: 457 with shape torch.Size([8192, 3072])
Tensor id via xlac: 458 with shape torch.Size([3072, 8192])
Tensor id via xlac: 459 with shape torch.Size([128256, 3072])
Tensor id via xlac: 460 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 461 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 462 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 463 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 464 with shape torch.Size([64])
Tensor id via xlac: 465 with shape torch.Size([1, 1])
Tensor id via xlac: 466 with shape torch.Size([1])
Tensor id via xlac: 467 with shape torch.Size([1, 1, 1, 128])
[XLA Cache] New inputs: ['input_0_shape_[3072]_dtype_torch.bfloat16 (idx=0)', 'input_1_shape_[3072]_dtype_torch.bfloat16 (idx=1)', 'input_2_shape_[3072]_dtype_torch.bfloat16 (idx=2)', 'input_3_shape_[3072]_dtype_torch.bfloat16 (idx=3)', 'input_4_shape_[3072]_dtype_torch.bfloat16 (idx=4)', 'input_5_shape_[128256, 3072]_dtype_torch.bfloat16 (idx=5)', 'input_6_shape_[1, 64, 1]_dtype_torch.float32 (idx=6)', 'input_7_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=7)', 'input_8_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=8)', 'input_9_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=9)', 'input_10_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=10)', 'input_11_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=11)', 'input_12_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=12)', 'input_13_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=13)', 'input_14_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=14)', 'input_15_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=15)', 'input_16_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=16)', 'input_17_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=17)', 'input_18_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=18)', 'input_19_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=19)', 'input_20_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=20)', 'input_21_shape_[3072, 128256]_dtype_torch.bfloat16 (idx=21)', 'input_22_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=22)', 'input_23_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=23)', 'input_24_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=24)', 'input_25_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=25)', 'input_26_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=26)', 'input_27_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=27)', 'input_28_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=28)', 'input_29_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=29)', 'input_30_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=30)', 'input_31_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=31)', 'input_32_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=32)', 'input_33_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=33)', 'input_34_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=34)', 'input_35_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=35)', 'input_36_shape_[128256, 3072]_dtype_torch.bfloat16 (idx=36)', 'input_37_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=37)', 'input_38_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=38)', 'input_39_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=39)', 'input_40_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=40)', 'input_41_shape_[64]_dtype_torch.float32 (idx=41)', 'input_42_shape_[1, 1]_dtype_torch.int64 (idx=42)', 'input_43_shape_[1]_dtype_torch.int64 (idx=43)', 'input_44_shape_[1, 1, 1, 128]_dtype_torch.bfloat16 (idx=44)']
[XLA Cache] Total cache size: 45 unique tensors
Hlo input positions pre normalization [444, 795, 443, 442, 440, 439, 436, 435, 433, 432, 465, 459, 423, 466, -1, 461, 467, 733, 429, 431, 460, 430, 424, 434, 425, 463, 438, 462, 437, 426, 441, 427]
Hlo input positions post normalization [445, 796, 444, 443, 441, 440, 437, 436, 434, 433, 466, 460, 424, 467, 0, 462, 468, 734, 430, 432, 461, 431, 425, 435, 426, 464, 439, 463, 438, 427, 442, 428]
match key in_spec.target L__self___model_layers__modules__0___input_layernorm_weight with ID 139658584088448 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_layers__modules__0___post_attention_layernorm_weight with ID 139658589162128 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_layers__modules__1___input_layernorm_weight with ID 139658589165728 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_layers__modules__1___post_attention_layernorm_weight with ID 139658581142368 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_norm_weight with ID 139658581024640 and kind InputKind.PARAMETER
match key in_spec.target L__self___lm_head.weight with ID 139658584091808 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.0 with ID 139654952040096 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.1 with ID 139654952040496 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.2 with ID 139654952042416 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.3 with ID 139654952041616 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.4 with ID 139654952040336 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.5 with ID 139654952041056 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.6 with ID 139654952040816 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.7 with ID 139654952040576 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.8 with ID 139654952038416 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.9 with ID 139654952041216 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.10 with ID 139654952030496 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.11 with ID 139654952040656 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.12 with ID 139654952039776 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.13 with ID 139654952041696 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.14 with ID 139654952040976 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.15 with ID 139654952041856 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_q_proj.weight with ID 139658589160768 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_k_proj.weight with ID 139658589162688 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_v_proj.weight with ID 139658589173008 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_o_proj.weight with ID 139658589164768 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___mlp_gate_proj.weight with ID 139658584087008 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___mlp_up_proj.weight with ID 139658589172608 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___mlp_down_proj.weight with ID 139658584095888 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_q_proj.weight with ID 139658581139488 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_k_proj.weight with ID 139658581145168 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_v_proj.weight with ID 139658581134368 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_o_proj.weight with ID 139658581144048 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___mlp_gate_proj.weight with ID 139658589162288 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___mlp_up_proj.weight with ID 139658589171488 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___mlp_down_proj.weight with ID 139658589163008 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___lm_head.weight with ID 139658584091808 and kind InputKind.PARAMETER
match key in_spec.target kwargs____past_key_values___key_cache_0 with ID 139659373973792 and kind InputKind.BUFFER
match key in_spec.target kwargs____past_key_values___value_cache_0 with ID 139659373983872 and kind InputKind.BUFFER
match key in_spec.target kwargs____past_key_values___key_cache_1 with ID 139659373976192 and kind InputKind.BUFFER
match key in_spec.target kwargs____past_key_values___value_cache_1 with ID 139659373976432 and kind InputKind.BUFFER
match key in_spec.target const_subgraph_module.L__self___model_rotary_emb_inv_freq with ID 139658581032480 and kind InputKind.BUFFER
[JAMES] setting arg ref map to  refs=constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,139658584088448,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.220 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.221 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.221 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.221 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.221 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.221 (  40.663s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.221 (  40.664s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.221 (  40.664s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.221 (  40.664s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.221 (  40.664s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.221 (  40.664s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.221 (  40.664s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.221 (  40.664s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.221 (  40.664s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.221 (  40.664s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.221 (  40.664s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.221 (  40.664s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.221 (  40.664s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.221 (  40.664s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.221 (  40.664s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.222 (  40.664s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.222 (  40.664s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.222 (  40.665s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.222 (  40.665s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.222 (  40.665s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.222 (  40.665s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.222 (  40.665s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.222 (  40.665s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.222 (  40.665s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.222 (  40.665s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.222 (  40.665s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.222 (  40.665s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.222 (  40.665s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.222 (  40.665s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.222 (  40.665s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.222 (  40.665s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.223 (  40.665s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.223 (  40.665s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.223 (  40.666s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:40.254 (  40.697s) [        C545D1C0]     client_instance.cc:471      1| ClientInstance::PJRT_Client_Compile
2025-08-12 15:39:40.254 (  40.697s) [        C545D1C0]      module_builder.cc:101      1| ModuleBuilder::buildModule
2025-08-12 15:39:40.255 (  40.698s) [        C545D1C0]      module_builder.cc:155      1| VHLO Module:
module @SyncTensorsGraph.605 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<1x!vhlo.i64_v1>, %arg1: !vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>, %arg2: !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg4: !vhlo.tensor_v1<1x1x!vhlo.i64_v1>, %arg5: !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, %arg6: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg7: !vhlo.tensor_v1<!vhlo.i64_v1>, %arg8: !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, %arg9: !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>, %arg10: !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, %arg11: !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>, %arg13: !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>, %arg14: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg15: !vhlo.tensor_v1<1x1x1x128x!vhlo.bf16_v1>, %arg16: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg17: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg18: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg19: !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>, %arg20: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg21: !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, %arg22: !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>, %arg23: !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, %arg24: !vhlo.tensor_v1<3072x128256x!vhlo.bf16_v1>, %arg25: !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>, %arg26: !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>, %arg27: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg28: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg29: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg30: !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>, %arg31: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128256x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x128256x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<3.25520843E-4> : tensor<1x1xf32>>}> : () -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2.000000e+00> : tensor<1x1x3072xf32>>}> : () -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF800000> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %3 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %4 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : () -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %5 = "vhlo.compare_v1"(%arg0, %4) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 LT>}> : (!vhlo.tensor_v1<1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.bool_v1>
    %6 = "vhlo.reshape_v1"(%arg7) : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %7 = "vhlo.add_v1"(%arg0, %6) : (!vhlo.tensor_v1<1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %8 = "vhlo.select_v1"(%5, %7, %arg0) : (!vhlo.tensor_v1<1x!vhlo.bool_v1>, !vhlo.tensor_v1<1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %9 = "vhlo.reshape_v1"(%8) : (!vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.i64_v1>
    %10 = "vhlo.convert_v1"(%arg6) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %12 = "vhlo.reshape_v1"(%arg4) : (!vhlo.tensor_v1<1x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %13 = "vhlo.convert_v1"(%12) : (!vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.ui32_v1>
    %14 = "vhlo.gather_v2"(%arg5, %13) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 3072]> : tensor<2xi64>>, start_index_map = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x!vhlo.ui32_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %15 = "vhlo.reshape_v1"(%14) : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %16 = "vhlo.convert_v1"(%15) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %17 = "vhlo.power_v1"(%16, %1) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %18 = "vhlo.reduce_v1"(%17, %3) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg32: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg33: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %274 = "vhlo.add_v1"(%arg32, %arg33) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%274) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %19 = "vhlo.multiply_v1"(%18, %0) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %20 = "vhlo.reshape_v1"(%19) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %21 = "vhlo.reshape_v1"(%arg3) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %22 = "vhlo.add_v1"(%20, %21) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %23 = "vhlo.rsqrt_v2"(%22) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %24 = "vhlo.reshape_v1"(%23) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %25 = "vhlo.broadcast_in_dim_v1"(%24) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %26 = "vhlo.multiply_v1"(%16, %25) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %27 = "vhlo.convert_v1"(%26) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %28 = "vhlo.convert_v1"(%27) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %29 = "vhlo.multiply_v1"(%11, %28) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %30 = "vhlo.convert_v1"(%29) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %31 = "vhlo.reshape_v1"(%30) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %32 = "vhlo.dot_general_v2"(%31, %arg2) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>
    %33 = "vhlo.reshape_v1"(%32) : (!vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %34 = "vhlo.convert_v1"(%33) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,8,1,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %35 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.i64_v1>
    %36 = "vhlo.convert_v1"(%35) : (!vhlo.tensor_v1<1x1x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %37 = "vhlo.dot_general_v2"(%arg1, %36) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>
    %38 = "vhlo.reshape_v1"(%37) : (!vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>
    %39 = "vhlo.concatenate_v1"(%38, %38) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>
    %40 = "vhlo.cosine_v2"(%39) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>
    %41 = "vhlo.convert_v1"(%40) : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>
    %42 = "vhlo.reshape_v1"(%41) : (!vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1x128x!vhlo.bf16_v1>
    %43 = "vhlo.convert_v1"(%42) : (!vhlo.tensor_v1<1x1x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1x128x!vhlo.f32_v1>
    %44 = "vhlo.reshape_v1"(%43) : (!vhlo.tensor_v1<1x1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>
    %45 = "vhlo.broadcast_in_dim_v1"(%44) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %46 = "vhlo.multiply_v1"(%34, %45) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %47 = "vhlo.convert_v1"(%46) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %48 = "vhlo.slice_v1"(%33) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 1, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>
    %49 = "vhlo.negate_v1"(%48) : (!vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>
    %50 = "vhlo.slice_v1"(%33) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 1, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>
    %51 = "vhlo.concatenate_v1"(%49, %50) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %52 = "vhlo.convert_v1"(%51) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %53 = "vhlo.sine_v2"(%39) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>
    %54 = "vhlo.convert_v1"(%53) : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>
    %55 = "vhlo.reshape_v1"(%54) : (!vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1x128x!vhlo.bf16_v1>
    %56 = "vhlo.convert_v1"(%55) : (!vhlo.tensor_v1<1x1x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1x128x!vhlo.f32_v1>
    %57 = "vhlo.reshape_v1"(%56) : (!vhlo.tensor_v1<1x1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>
    %58 = "vhlo.broadcast_in_dim_v1"(%57) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %59 = "vhlo.multiply_v1"(%52, %58) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %60 = "vhlo.convert_v1"(%59) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %61 = "vhlo.add_v1"(%47, %60) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %62 = "vhlo.scatter_v2"(%arg8, %9, %61) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg33: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg33) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>
    %63 = "vhlo.dot_general_v2"(%31, %arg9) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>
    %64 = "vhlo.reshape_v1"(%63) : (!vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %65 = "vhlo.scatter_v2"(%arg10, %9, %64) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg33: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg33) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>
    %66 = "vhlo.convert_v1"(%arg20) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %67 = "vhlo.reshape_v1"(%66) : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %68 = "vhlo.dot_general_v2"(%31, %arg17) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %69 = "vhlo.reshape_v1"(%68) : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %70 = "vhlo.convert_v1"(%69) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,24,1,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %71 = "vhlo.broadcast_in_dim_v1"(%44) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %72 = "vhlo.multiply_v1"(%70, %71) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %73 = "vhlo.convert_v1"(%72) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %74 = "vhlo.slice_v1"(%69) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 1, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>
    %75 = "vhlo.negate_v1"(%74) : (!vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>
    %76 = "vhlo.slice_v1"(%69) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 1, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>
    %77 = "vhlo.concatenate_v1"(%75, %76) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %78 = "vhlo.convert_v1"(%77) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %79 = "vhlo.broadcast_in_dim_v1"(%57) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %80 = "vhlo.multiply_v1"(%78, %79) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %81 = "vhlo.convert_v1"(%80) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %82 = "vhlo.add_v1"(%73, %81) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %83 = "vhlo.reshape_v1"(%82) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>
    %84 = "vhlo.broadcast_in_dim_v1"(%62) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>
    %85 = "vhlo.reshape_v1"(%84) : (!vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>
    %86 = "vhlo.transpose_v1"(%85) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 3, 1, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,24,128,128]{2,3,1,0}">} : (!vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>
    %87 = "vhlo.reshape_v1"(%86) : (!vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>
    %88 = "vhlo.dot_general_v2"(%83, %87) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>
    %89 = "vhlo.reshape_v1"(%88) : (!vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %90 = "vhlo.convert_v1"(%89) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %91 = "vhlo.broadcast_in_dim_v1"(%arg16) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %92 = "vhlo.multiply_v1"(%90, %91) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %93 = "vhlo.convert_v1"(%92) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %94 = "vhlo.reshape_v1"(%arg15) : (!vhlo.tensor_v1<1x1x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>
    %95 = "vhlo.broadcast_in_dim_v1"(%94) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %96 = "vhlo.add_v1"(%93, %95) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %97 = "vhlo.convert_v1"(%96) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %98 = "vhlo.reduce_v1"(%97, %2) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg32: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg33: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %274 = "vhlo.maximum_v1"(%arg32, %arg33) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%274) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x!vhlo.f32_v1>
    %99 = "vhlo.broadcast_in_dim_v1"(%98) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %100 = "vhlo.subtract_v1"(%97, %99) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %101 = "vhlo.exponential_v2"(%100) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %102 = "vhlo.reduce_v1"(%101, %3) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg32: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg33: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %274 = "vhlo.add_v1"(%arg32, %arg33) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%274) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x!vhlo.f32_v1>
    %103 = "vhlo.broadcast_in_dim_v1"(%102) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %104 = "vhlo.divide_v1"(%101, %103) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %105 = "vhlo.convert_v1"(%104) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %106 = "vhlo.reshape_v1"(%105) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>
    %107 = "vhlo.broadcast_in_dim_v1"(%65) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>
    %108 = "vhlo.reshape_v1"(%107) : (!vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>
    %109 = "vhlo.dot_general_v2"(%106, %108) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>
    %110 = "vhlo.reshape_v1"(%109) : (!vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %111 = "vhlo.dot_general_v2"(%110, %arg14) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %112 = "vhlo.reshape_v1"(%111) : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %113 = "vhlo.add_v1"(%15, %112) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %114 = "vhlo.convert_v1"(%arg18) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %115 = "vhlo.reshape_v1"(%114) : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %116 = "vhlo.convert_v1"(%113) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %117 = "vhlo.power_v1"(%116, %1) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %118 = "vhlo.reduce_v1"(%117, %3) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg32: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg33: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %274 = "vhlo.add_v1"(%arg32, %arg33) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%274) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %119 = "vhlo.multiply_v1"(%118, %0) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %120 = "vhlo.reshape_v1"(%119) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %121 = "vhlo.add_v1"(%120, %21) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %122 = "vhlo.rsqrt_v2"(%121) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %123 = "vhlo.reshape_v1"(%122) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %124 = "vhlo.broadcast_in_dim_v1"(%123) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %125 = "vhlo.multiply_v1"(%116, %124) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %126 = "vhlo.convert_v1"(%125) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %127 = "vhlo.convert_v1"(%126) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %128 = "vhlo.multiply_v1"(%115, %127) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %129 = "vhlo.convert_v1"(%128) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %130 = "vhlo.reshape_v1"(%129) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %131 = "vhlo.dot_general_v2"(%130, %arg19) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>
    %132 = "vhlo.reshape_v1"(%131) : (!vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %133 = "vhlo.convert_v1"(%132) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %134 = "vhlo.logistic_v2"(%132) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %135 = "vhlo.convert_v1"(%134) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %136 = "vhlo.multiply_v1"(%133, %135) : (!vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %137 = "vhlo.convert_v1"(%136) : (!vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %138 = "vhlo.convert_v1"(%137) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %139 = "vhlo.dot_general_v2"(%130, %arg13) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>
    %140 = "vhlo.reshape_v1"(%139) : (!vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %141 = "vhlo.convert_v1"(%140) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %142 = "vhlo.multiply_v1"(%138, %141) : (!vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %143 = "vhlo.convert_v1"(%142) : (!vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %144 = "vhlo.reshape_v1"(%143) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>
    %145 = "vhlo.dot_general_v2"(%144, %arg12) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %146 = "vhlo.reshape_v1"(%145) : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %147 = "vhlo.add_v1"(%113, %146) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %148 = "vhlo.convert_v1"(%147) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %149 = "vhlo.power_v1"(%148, %1) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %150 = "vhlo.reduce_v1"(%149, %3) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg32: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg33: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %274 = "vhlo.add_v1"(%arg32, %arg33) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%274) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %151 = "vhlo.multiply_v1"(%150, %0) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %152 = "vhlo.reshape_v1"(%151) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %153 = "vhlo.add_v1"(%152, %21) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %154 = "vhlo.rsqrt_v2"(%153) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %155 = "vhlo.reshape_v1"(%154) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %156 = "vhlo.broadcast_in_dim_v1"(%155) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %157 = "vhlo.multiply_v1"(%148, %156) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %158 = "vhlo.convert_v1"(%157) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %159 = "vhlo.convert_v1"(%158) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %160 = "vhlo.multiply_v1"(%67, %159) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %161 = "vhlo.convert_v1"(%160) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %162 = "vhlo.reshape_v1"(%161) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %163 = "vhlo.dot_general_v2"(%162, %arg11) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>
    %164 = "vhlo.reshape_v1"(%163) : (!vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %165 = "vhlo.convert_v1"(%164) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,8,1,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %166 = "vhlo.multiply_v1"(%165, %45) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %167 = "vhlo.convert_v1"(%166) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %168 = "vhlo.slice_v1"(%164) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 1, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>
    %169 = "vhlo.negate_v1"(%168) : (!vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>
    %170 = "vhlo.slice_v1"(%164) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 1, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>
    %171 = "vhlo.concatenate_v1"(%169, %170) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %172 = "vhlo.convert_v1"(%171) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %173 = "vhlo.multiply_v1"(%172, %58) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %174 = "vhlo.convert_v1"(%173) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %175 = "vhlo.add_v1"(%167, %174) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %176 = "vhlo.scatter_v2"(%arg21, %9, %175) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg33: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg33) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>
    %177 = "vhlo.dot_general_v2"(%162, %arg22) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>
    %178 = "vhlo.reshape_v1"(%177) : (!vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %179 = "vhlo.scatter_v2"(%arg23, %9, %178) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg33: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg33) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>
    %180 = "vhlo.convert_v1"(%arg31) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %181 = "vhlo.reshape_v1"(%180) : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %182 = "vhlo.dot_general_v2"(%162, %arg28) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %183 = "vhlo.reshape_v1"(%182) : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %184 = "vhlo.convert_v1"(%183) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,24,1,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %185 = "vhlo.multiply_v1"(%184, %71) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %186 = "vhlo.convert_v1"(%185) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %187 = "vhlo.slice_v1"(%183) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 1, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>
    %188 = "vhlo.negate_v1"(%187) : (!vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>
    %189 = "vhlo.slice_v1"(%183) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 1, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>
    %190 = "vhlo.concatenate_v1"(%188, %189) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %191 = "vhlo.convert_v1"(%190) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %192 = "vhlo.multiply_v1"(%191, %79) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %193 = "vhlo.convert_v1"(%192) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %194 = "vhlo.add_v1"(%186, %193) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %195 = "vhlo.reshape_v1"(%194) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>
    %196 = "vhlo.broadcast_in_dim_v1"(%176) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>
    %197 = "vhlo.reshape_v1"(%196) : (!vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>
    %198 = "vhlo.transpose_v1"(%197) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 3, 1, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,24,128,128]{2,3,1,0}">} : (!vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>
    %199 = "vhlo.reshape_v1"(%198) : (!vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>
    %200 = "vhlo.dot_general_v2"(%195, %199) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>
    %201 = "vhlo.reshape_v1"(%200) : (!vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %202 = "vhlo.convert_v1"(%201) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %203 = "vhlo.multiply_v1"(%202, %91) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %204 = "vhlo.convert_v1"(%203) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %205 = "vhlo.add_v1"(%204, %95) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %206 = "vhlo.convert_v1"(%205) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %207 = "vhlo.reduce_v1"(%206, %2) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg32: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg33: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %274 = "vhlo.maximum_v1"(%arg32, %arg33) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%274) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x!vhlo.f32_v1>
    %208 = "vhlo.broadcast_in_dim_v1"(%207) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %209 = "vhlo.subtract_v1"(%206, %208) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %210 = "vhlo.exponential_v2"(%209) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %211 = "vhlo.reduce_v1"(%210, %3) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg32: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg33: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %274 = "vhlo.add_v1"(%arg32, %arg33) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%274) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x!vhlo.f32_v1>
    %212 = "vhlo.broadcast_in_dim_v1"(%211) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %213 = "vhlo.divide_v1"(%210, %212) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %214 = "vhlo.convert_v1"(%213) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %215 = "vhlo.reshape_v1"(%214) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>
    %216 = "vhlo.broadcast_in_dim_v1"(%179) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>
    %217 = "vhlo.reshape_v1"(%216) : (!vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>
    %218 = "vhlo.dot_general_v2"(%215, %217) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>
    %219 = "vhlo.reshape_v1"(%218) : (!vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %220 = "vhlo.dot_general_v2"(%219, %arg27) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %221 = "vhlo.reshape_v1"(%220) : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %222 = "vhlo.add_v1"(%147, %221) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %223 = "vhlo.convert_v1"(%arg29) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %224 = "vhlo.reshape_v1"(%223) : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %225 = "vhlo.convert_v1"(%222) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %226 = "vhlo.power_v1"(%225, %1) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %227 = "vhlo.reduce_v1"(%226, %3) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg32: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg33: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %274 = "vhlo.add_v1"(%arg32, %arg33) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%274) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %228 = "vhlo.multiply_v1"(%227, %0) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %229 = "vhlo.reshape_v1"(%228) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %230 = "vhlo.add_v1"(%229, %21) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %231 = "vhlo.rsqrt_v2"(%230) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %232 = "vhlo.reshape_v1"(%231) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %233 = "vhlo.broadcast_in_dim_v1"(%232) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %234 = "vhlo.multiply_v1"(%225, %233) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %235 = "vhlo.convert_v1"(%234) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %236 = "vhlo.convert_v1"(%235) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %237 = "vhlo.multiply_v1"(%224, %236) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %238 = "vhlo.convert_v1"(%237) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %239 = "vhlo.reshape_v1"(%238) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %240 = "vhlo.dot_general_v2"(%239, %arg30) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>
    %241 = "vhlo.reshape_v1"(%240) : (!vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %242 = "vhlo.convert_v1"(%241) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %243 = "vhlo.logistic_v2"(%241) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %244 = "vhlo.convert_v1"(%243) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %245 = "vhlo.multiply_v1"(%242, %244) : (!vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %246 = "vhlo.convert_v1"(%245) : (!vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %247 = "vhlo.convert_v1"(%246) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %248 = "vhlo.dot_general_v2"(%239, %arg26) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>
    %249 = "vhlo.reshape_v1"(%248) : (!vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %250 = "vhlo.convert_v1"(%249) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %251 = "vhlo.multiply_v1"(%247, %250) : (!vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %252 = "vhlo.convert_v1"(%251) : (!vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %253 = "vhlo.reshape_v1"(%252) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>
    %254 = "vhlo.dot_general_v2"(%253, %arg25) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %255 = "vhlo.reshape_v1"(%254) : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %256 = "vhlo.add_v1"(%222, %255) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %257 = "vhlo.convert_v1"(%256) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %258 = "vhlo.power_v1"(%257, %1) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %259 = "vhlo.reduce_v1"(%258, %3) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg32: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg33: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %274 = "vhlo.add_v1"(%arg32, %arg33) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%274) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %260 = "vhlo.multiply_v1"(%259, %0) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %261 = "vhlo.reshape_v1"(%260) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %262 = "vhlo.add_v1"(%261, %21) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %263 = "vhlo.rsqrt_v2"(%262) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %264 = "vhlo.reshape_v1"(%263) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %265 = "vhlo.broadcast_in_dim_v1"(%264) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %266 = "vhlo.multiply_v1"(%257, %265) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %267 = "vhlo.convert_v1"(%266) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %268 = "vhlo.convert_v1"(%267) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %269 = "vhlo.multiply_v1"(%181, %268) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %270 = "vhlo.convert_v1"(%269) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %271 = "vhlo.reshape_v1"(%270) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %272 = "vhlo.dot_general_v2"(%271, %arg24) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x!vhlo.bf16_v1>
    %273 = "vhlo.reshape_v1"(%272) : (!vhlo.tensor_v1<1x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x128256x!vhlo.bf16_v1>
    "vhlo.return_v1"(%62, %65, %176, %179, %272, %273) : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128256x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x128256x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}
2025-08-12 15:39:40.268 (  40.711s) [        C545D1C0]      module_builder.cc:188      1| SHLO Module:
module @SyncTensorsGraph.605 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<1xi64>, %arg1: tensor<1x64x1xf32>, %arg2: tensor<3072x1024xbf16>, %arg3: tensor<f32>, %arg4: tensor<1x1xi64>, %arg5: tensor<128256x3072xbf16>, %arg6: tensor<3072xbf16>, %arg7: tensor<i64>, %arg8: tensor<1x8x128x128xbf16>, %arg9: tensor<3072x1024xbf16>, %arg10: tensor<1x8x128x128xbf16>, %arg11: tensor<3072x1024xbf16>, %arg12: tensor<8192x3072xbf16>, %arg13: tensor<3072x8192xbf16>, %arg14: tensor<3072x3072xbf16>, %arg15: tensor<1x1x1x128xbf16>, %arg16: tensor<f32>, %arg17: tensor<3072x3072xbf16>, %arg18: tensor<3072xbf16>, %arg19: tensor<3072x8192xbf16>, %arg20: tensor<3072xbf16>, %arg21: tensor<1x8x128x128xbf16>, %arg22: tensor<3072x1024xbf16>, %arg23: tensor<1x8x128x128xbf16>, %arg24: tensor<3072x128256xbf16>, %arg25: tensor<8192x3072xbf16>, %arg26: tensor<3072x8192xbf16>, %arg27: tensor<3072x3072xbf16>, %arg28: tensor<3072x3072xbf16>, %arg29: tensor<3072xbf16>, %arg30: tensor<3072x8192xbf16>, %arg31: tensor<3072xbf16>) -> (tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x128256xbf16>, tensor<1x1x128256xbf16>) {
    %cst = stablehlo.constant dense<3.25520843E-4> : tensor<1x1xf32>
    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<1x1x3072xf32>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<0> : tensor<1xi64>
    %0 = stablehlo.compare  LT, %arg0, %c : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
    %1 = stablehlo.reshape %arg7 : (tensor<i64>) -> tensor<1xi64>
    %2 = stablehlo.add %arg0, %1 : tensor<1xi64>
    %3 = stablehlo.select %0, %2, %arg0 : tensor<1xi1>, tensor<1xi64>
    %4 = stablehlo.reshape %3 : (tensor<1xi64>) -> tensor<1x1xi64>
    %5 = stablehlo.convert %arg6 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %6 = stablehlo.reshape %5 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
    %7 = stablehlo.reshape %arg4 : (tensor<1x1xi64>) -> tensor<1xi64>
    %8 = stablehlo.convert %7 : (tensor<1xi64>) -> tensor<1xui32>
    %9 = "stablehlo.gather"(%arg5, %8) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<1xui32>) -> tensor<1x3072xbf16>
    %10 = stablehlo.reshape %9 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.convert %10 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %12 = stablehlo.power %11, %cst_0 : tensor<1x1x3072xf32>
    %13 = stablehlo.reduce(%12 init: %cst_2) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %14 = stablehlo.multiply %13, %cst : tensor<1x1xf32>
    %15 = stablehlo.reshape %14 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %16 = stablehlo.reshape %arg3 : (tensor<f32>) -> tensor<1x1x1xf32>
    %17 = stablehlo.add %15, %16 : tensor<1x1x1xf32>
    %18 = stablehlo.rsqrt %17 : tensor<1x1x1xf32>
    %19 = stablehlo.reshape %18 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %20 = stablehlo.broadcast_in_dim %19, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %21 = stablehlo.multiply %11, %20 : tensor<1x1x3072xf32>
    %22 = stablehlo.convert %21 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %23 = stablehlo.convert %22 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %24 = stablehlo.multiply %6, %23 : tensor<1x1x3072xf32>
    %25 = stablehlo.convert %24 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %26 = stablehlo.reshape %25 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %27 = stablehlo.dot_general %26, %arg2, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %28 = stablehlo.reshape %27 : (tensor<1x1024xbf16>) -> tensor<1x8x1x128xbf16>
    %29 = stablehlo.convert %28 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,1,128]{3,1,2,0}"} : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xf32>
    %30 = stablehlo.reshape %arg0 : (tensor<1xi64>) -> tensor<1x1x1xi64>
    %31 = stablehlo.convert %30 : (tensor<1x1x1xi64>) -> tensor<1x1x1xf32>
    %32 = stablehlo.dot_general %arg1, %31, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x1xf32>) -> tensor<1x64x1xf32>
    %33 = stablehlo.reshape %32 : (tensor<1x64x1xf32>) -> tensor<1x1x64xf32>
    %34 = stablehlo.concatenate %33, %33, dim = 2 : (tensor<1x1x64xf32>, tensor<1x1x64xf32>) -> tensor<1x1x128xf32>
    %35 = stablehlo.cosine %34 : tensor<1x1x128xf32>
    %36 = stablehlo.convert %35 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
    %37 = stablehlo.reshape %36 : (tensor<1x1x128xbf16>) -> tensor<1x1x1x128xbf16>
    %38 = stablehlo.convert %37 : (tensor<1x1x1x128xbf16>) -> tensor<1x1x1x128xf32>
    %39 = stablehlo.reshape %38 : (tensor<1x1x1x128xf32>) -> tensor<1x1x128xf32>
    %40 = stablehlo.broadcast_in_dim %39, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x8x1x128xf32>
    %41 = stablehlo.multiply %29, %40 : tensor<1x8x1x128xf32>
    %42 = stablehlo.convert %41 : (tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xbf16>
    %43 = stablehlo.slice %28 [0:1, 0:8, 0:1, 64:128] : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x64xbf16>
    %44 = stablehlo.negate %43 : tensor<1x8x1x64xbf16>
    %45 = stablehlo.slice %28 [0:1, 0:8, 0:1, 0:64] : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x64xbf16>
    %46 = stablehlo.concatenate %44, %45, dim = 3 : (tensor<1x8x1x64xbf16>, tensor<1x8x1x64xbf16>) -> tensor<1x8x1x128xbf16>
    %47 = stablehlo.convert %46 : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xf32>
    %48 = stablehlo.sine %34 : tensor<1x1x128xf32>
    %49 = stablehlo.convert %48 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
    %50 = stablehlo.reshape %49 : (tensor<1x1x128xbf16>) -> tensor<1x1x1x128xbf16>
    %51 = stablehlo.convert %50 : (tensor<1x1x1x128xbf16>) -> tensor<1x1x1x128xf32>
    %52 = stablehlo.reshape %51 : (tensor<1x1x1x128xf32>) -> tensor<1x1x128xf32>
    %53 = stablehlo.broadcast_in_dim %52, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x8x1x128xf32>
    %54 = stablehlo.multiply %47, %53 : tensor<1x8x1x128xf32>
    %55 = stablehlo.convert %54 : (tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xbf16>
    %56 = stablehlo.add %42, %55 : tensor<1x8x1x128xbf16>
    %57 = "stablehlo.scatter"(%arg8, %4, %56) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg32: tensor<bf16>, %arg33: tensor<bf16>):
      stablehlo.return %arg33 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>) -> tensor<1x8x128x128xbf16>
    %58 = stablehlo.dot_general %26, %arg9, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %59 = stablehlo.reshape %58 : (tensor<1x1024xbf16>) -> tensor<1x8x1x128xbf16>
    %60 = "stablehlo.scatter"(%arg10, %4, %59) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg32: tensor<bf16>, %arg33: tensor<bf16>):
      stablehlo.return %arg33 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>) -> tensor<1x8x128x128xbf16>
    %61 = stablehlo.convert %arg20 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %62 = stablehlo.reshape %61 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
    %63 = stablehlo.dot_general %26, %arg17, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %64 = stablehlo.reshape %63 : (tensor<1x3072xbf16>) -> tensor<1x24x1x128xbf16>
    %65 = stablehlo.convert %64 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,1,128]{3,1,2,0}"} : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %66 = stablehlo.broadcast_in_dim %39, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x24x1x128xf32>
    %67 = stablehlo.multiply %65, %66 : tensor<1x24x1x128xf32>
    %68 = stablehlo.convert %67 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %69 = stablehlo.slice %64 [0:1, 0:24, 0:1, 64:128] : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x64xbf16>
    %70 = stablehlo.negate %69 : tensor<1x24x1x64xbf16>
    %71 = stablehlo.slice %64 [0:1, 0:24, 0:1, 0:64] : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x64xbf16>
    %72 = stablehlo.concatenate %70, %71, dim = 3 : (tensor<1x24x1x64xbf16>, tensor<1x24x1x64xbf16>) -> tensor<1x24x1x128xbf16>
    %73 = stablehlo.convert %72 : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %74 = stablehlo.broadcast_in_dim %52, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x24x1x128xf32>
    %75 = stablehlo.multiply %73, %74 : tensor<1x24x1x128xf32>
    %76 = stablehlo.convert %75 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %77 = stablehlo.add %68, %76 : tensor<1x24x1x128xbf16>
    %78 = stablehlo.reshape %77 : (tensor<1x24x1x128xbf16>) -> tensor<24x1x128xbf16>
    %79 = stablehlo.broadcast_in_dim %57, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %80 = stablehlo.reshape %79 : (tensor<1x8x3x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %81 = stablehlo.transpose %80, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,128]{2,3,1,0}"} : (tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %82 = stablehlo.reshape %81 : (tensor<1x24x128x128xbf16>) -> tensor<24x128x128xbf16>
    %83 = stablehlo.dot_general %78, %82, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x1x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x1x128xbf16>
    %84 = stablehlo.reshape %83 : (tensor<24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %85 = stablehlo.convert %84 : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %86 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<1x24x1x128xf32>
    %87 = stablehlo.multiply %85, %86 : tensor<1x24x1x128xf32>
    %88 = stablehlo.convert %87 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %89 = stablehlo.reshape %arg15 : (tensor<1x1x1x128xbf16>) -> tensor<1x1x128xbf16>
    %90 = stablehlo.broadcast_in_dim %89, dims = [0, 2, 3] : (tensor<1x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %91 = stablehlo.add %88, %90 : tensor<1x24x1x128xbf16>
    %92 = stablehlo.convert %91 : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %93 = stablehlo.reduce(%92 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x1x128xf32>, tensor<f32>) -> tensor<1x24x1xf32>
    %94 = stablehlo.broadcast_in_dim %93, dims = [0, 1, 2] : (tensor<1x24x1xf32>) -> tensor<1x24x1x128xf32>
    %95 = stablehlo.subtract %92, %94 : tensor<1x24x1x128xf32>
    %96 = stablehlo.exponential %95 : tensor<1x24x1x128xf32>
    %97 = stablehlo.reduce(%96 init: %cst_2) applies stablehlo.add across dimensions = [3] : (tensor<1x24x1x128xf32>, tensor<f32>) -> tensor<1x24x1xf32>
    %98 = stablehlo.broadcast_in_dim %97, dims = [0, 1, 2] : (tensor<1x24x1xf32>) -> tensor<1x24x1x128xf32>
    %99 = stablehlo.divide %96, %98 : tensor<1x24x1x128xf32>
    %100 = stablehlo.convert %99 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %101 = stablehlo.reshape %100 : (tensor<1x24x1x128xbf16>) -> tensor<24x1x128xbf16>
    %102 = stablehlo.broadcast_in_dim %60, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %103 = stablehlo.reshape %102 : (tensor<1x8x3x128x128xbf16>) -> tensor<24x128x128xbf16>
    %104 = stablehlo.dot_general %101, %103, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x1x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x1x128xbf16>
    %105 = stablehlo.reshape %104 : (tensor<24x1x128xbf16>) -> tensor<1x3072xbf16>
    %106 = stablehlo.dot_general %105, %arg14, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %107 = stablehlo.reshape %106 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %108 = stablehlo.add %10, %107 : tensor<1x1x3072xbf16>
    %109 = stablehlo.convert %arg18 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %110 = stablehlo.reshape %109 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
    %111 = stablehlo.convert %108 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %112 = stablehlo.power %111, %cst_0 : tensor<1x1x3072xf32>
    %113 = stablehlo.reduce(%112 init: %cst_2) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %114 = stablehlo.multiply %113, %cst : tensor<1x1xf32>
    %115 = stablehlo.reshape %114 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %116 = stablehlo.add %115, %16 : tensor<1x1x1xf32>
    %117 = stablehlo.rsqrt %116 : tensor<1x1x1xf32>
    %118 = stablehlo.reshape %117 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %119 = stablehlo.broadcast_in_dim %118, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %120 = stablehlo.multiply %111, %119 : tensor<1x1x3072xf32>
    %121 = stablehlo.convert %120 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %122 = stablehlo.convert %121 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %123 = stablehlo.multiply %110, %122 : tensor<1x1x3072xf32>
    %124 = stablehlo.convert %123 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %125 = stablehlo.reshape %124 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %126 = stablehlo.dot_general %125, %arg19, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %127 = stablehlo.reshape %126 : (tensor<1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %128 = stablehlo.convert %127 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %129 = stablehlo.logistic %127 : tensor<1x1x8192xbf16>
    %130 = stablehlo.convert %129 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %131 = stablehlo.multiply %128, %130 : tensor<1x1x8192xf32>
    %132 = stablehlo.convert %131 : (tensor<1x1x8192xf32>) -> tensor<1x1x8192xbf16>
    %133 = stablehlo.convert %132 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %134 = stablehlo.dot_general %125, %arg13, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %135 = stablehlo.reshape %134 : (tensor<1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %136 = stablehlo.convert %135 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %137 = stablehlo.multiply %133, %136 : tensor<1x1x8192xf32>
    %138 = stablehlo.convert %137 : (tensor<1x1x8192xf32>) -> tensor<1x1x8192xbf16>
    %139 = stablehlo.reshape %138 : (tensor<1x1x8192xbf16>) -> tensor<1x8192xbf16>
    %140 = stablehlo.dot_general %139, %arg12, contracting_dims = [1] x [0] : (tensor<1x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<1x3072xbf16>
    %141 = stablehlo.reshape %140 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %142 = stablehlo.add %108, %141 : tensor<1x1x3072xbf16>
    %143 = stablehlo.convert %142 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %144 = stablehlo.power %143, %cst_0 : tensor<1x1x3072xf32>
    %145 = stablehlo.reduce(%144 init: %cst_2) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %146 = stablehlo.multiply %145, %cst : tensor<1x1xf32>
    %147 = stablehlo.reshape %146 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %148 = stablehlo.add %147, %16 : tensor<1x1x1xf32>
    %149 = stablehlo.rsqrt %148 : tensor<1x1x1xf32>
    %150 = stablehlo.reshape %149 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %151 = stablehlo.broadcast_in_dim %150, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %152 = stablehlo.multiply %143, %151 : tensor<1x1x3072xf32>
    %153 = stablehlo.convert %152 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %154 = stablehlo.convert %153 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %155 = stablehlo.multiply %62, %154 : tensor<1x1x3072xf32>
    %156 = stablehlo.convert %155 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %157 = stablehlo.reshape %156 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %158 = stablehlo.dot_general %157, %arg11, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %159 = stablehlo.reshape %158 : (tensor<1x1024xbf16>) -> tensor<1x8x1x128xbf16>
    %160 = stablehlo.convert %159 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,1,128]{3,1,2,0}"} : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xf32>
    %161 = stablehlo.multiply %160, %40 : tensor<1x8x1x128xf32>
    %162 = stablehlo.convert %161 : (tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xbf16>
    %163 = stablehlo.slice %159 [0:1, 0:8, 0:1, 64:128] : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x64xbf16>
    %164 = stablehlo.negate %163 : tensor<1x8x1x64xbf16>
    %165 = stablehlo.slice %159 [0:1, 0:8, 0:1, 0:64] : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x64xbf16>
    %166 = stablehlo.concatenate %164, %165, dim = 3 : (tensor<1x8x1x64xbf16>, tensor<1x8x1x64xbf16>) -> tensor<1x8x1x128xbf16>
    %167 = stablehlo.convert %166 : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xf32>
    %168 = stablehlo.multiply %167, %53 : tensor<1x8x1x128xf32>
    %169 = stablehlo.convert %168 : (tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xbf16>
    %170 = stablehlo.add %162, %169 : tensor<1x8x1x128xbf16>
    %171 = "stablehlo.scatter"(%arg21, %4, %170) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg32: tensor<bf16>, %arg33: tensor<bf16>):
      stablehlo.return %arg33 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>) -> tensor<1x8x128x128xbf16>
    %172 = stablehlo.dot_general %157, %arg22, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x1024xbf16>) -> tensor<1x8x1x128xbf16>
    %174 = "stablehlo.scatter"(%arg23, %4, %173) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg32: tensor<bf16>, %arg33: tensor<bf16>):
      stablehlo.return %arg33 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>) -> tensor<1x8x128x128xbf16>
    %175 = stablehlo.convert %arg31 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %176 = stablehlo.reshape %175 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
    %177 = stablehlo.dot_general %157, %arg28, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %178 = stablehlo.reshape %177 : (tensor<1x3072xbf16>) -> tensor<1x24x1x128xbf16>
    %179 = stablehlo.convert %178 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,1,128]{3,1,2,0}"} : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %180 = stablehlo.multiply %179, %66 : tensor<1x24x1x128xf32>
    %181 = stablehlo.convert %180 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %182 = stablehlo.slice %178 [0:1, 0:24, 0:1, 64:128] : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x64xbf16>
    %183 = stablehlo.negate %182 : tensor<1x24x1x64xbf16>
    %184 = stablehlo.slice %178 [0:1, 0:24, 0:1, 0:64] : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x64xbf16>
    %185 = stablehlo.concatenate %183, %184, dim = 3 : (tensor<1x24x1x64xbf16>, tensor<1x24x1x64xbf16>) -> tensor<1x24x1x128xbf16>
    %186 = stablehlo.convert %185 : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %187 = stablehlo.multiply %186, %74 : tensor<1x24x1x128xf32>
    %188 = stablehlo.convert %187 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %189 = stablehlo.add %181, %188 : tensor<1x24x1x128xbf16>
    %190 = stablehlo.reshape %189 : (tensor<1x24x1x128xbf16>) -> tensor<24x1x128xbf16>
    %191 = stablehlo.broadcast_in_dim %171, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %192 = stablehlo.reshape %191 : (tensor<1x8x3x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %193 = stablehlo.transpose %192, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,128]{2,3,1,0}"} : (tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %194 = stablehlo.reshape %193 : (tensor<1x24x128x128xbf16>) -> tensor<24x128x128xbf16>
    %195 = stablehlo.dot_general %190, %194, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x1x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x1x128xbf16>
    %196 = stablehlo.reshape %195 : (tensor<24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %197 = stablehlo.convert %196 : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %198 = stablehlo.multiply %197, %86 : tensor<1x24x1x128xf32>
    %199 = stablehlo.convert %198 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %200 = stablehlo.add %199, %90 : tensor<1x24x1x128xbf16>
    %201 = stablehlo.convert %200 : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %202 = stablehlo.reduce(%201 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x1x128xf32>, tensor<f32>) -> tensor<1x24x1xf32>
    %203 = stablehlo.broadcast_in_dim %202, dims = [0, 1, 2] : (tensor<1x24x1xf32>) -> tensor<1x24x1x128xf32>
    %204 = stablehlo.subtract %201, %203 : tensor<1x24x1x128xf32>
    %205 = stablehlo.exponential %204 : tensor<1x24x1x128xf32>
    %206 = stablehlo.reduce(%205 init: %cst_2) applies stablehlo.add across dimensions = [3] : (tensor<1x24x1x128xf32>, tensor<f32>) -> tensor<1x24x1xf32>
    %207 = stablehlo.broadcast_in_dim %206, dims = [0, 1, 2] : (tensor<1x24x1xf32>) -> tensor<1x24x1x128xf32>
    %208 = stablehlo.divide %205, %207 : tensor<1x24x1x128xf32>
    %209 = stablehlo.convert %208 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %210 = stablehlo.reshape %209 : (tensor<1x24x1x128xbf16>) -> tensor<24x1x128xbf16>
    %211 = stablehlo.broadcast_in_dim %174, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %212 = stablehlo.reshape %211 : (tensor<1x8x3x128x128xbf16>) -> tensor<24x128x128xbf16>
    %213 = stablehlo.dot_general %210, %212, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x1x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x1x128xbf16>
    %214 = stablehlo.reshape %213 : (tensor<24x1x128xbf16>) -> tensor<1x3072xbf16>
    %215 = stablehlo.dot_general %214, %arg27, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %216 = stablehlo.reshape %215 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %217 = stablehlo.add %142, %216 : tensor<1x1x3072xbf16>
    %218 = stablehlo.convert %arg29 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %219 = stablehlo.reshape %218 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
    %220 = stablehlo.convert %217 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %221 = stablehlo.power %220, %cst_0 : tensor<1x1x3072xf32>
    %222 = stablehlo.reduce(%221 init: %cst_2) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %223 = stablehlo.multiply %222, %cst : tensor<1x1xf32>
    %224 = stablehlo.reshape %223 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %225 = stablehlo.add %224, %16 : tensor<1x1x1xf32>
    %226 = stablehlo.rsqrt %225 : tensor<1x1x1xf32>
    %227 = stablehlo.reshape %226 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %228 = stablehlo.broadcast_in_dim %227, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %229 = stablehlo.multiply %220, %228 : tensor<1x1x3072xf32>
    %230 = stablehlo.convert %229 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %231 = stablehlo.convert %230 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %232 = stablehlo.multiply %219, %231 : tensor<1x1x3072xf32>
    %233 = stablehlo.convert %232 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %234 = stablehlo.reshape %233 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %235 = stablehlo.dot_general %234, %arg30, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %236 = stablehlo.reshape %235 : (tensor<1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %237 = stablehlo.convert %236 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %238 = stablehlo.logistic %236 : tensor<1x1x8192xbf16>
    %239 = stablehlo.convert %238 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %240 = stablehlo.multiply %237, %239 : tensor<1x1x8192xf32>
    %241 = stablehlo.convert %240 : (tensor<1x1x8192xf32>) -> tensor<1x1x8192xbf16>
    %242 = stablehlo.convert %241 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %243 = stablehlo.dot_general %234, %arg26, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %244 = stablehlo.reshape %243 : (tensor<1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %245 = stablehlo.convert %244 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %246 = stablehlo.multiply %242, %245 : tensor<1x1x8192xf32>
    %247 = stablehlo.convert %246 : (tensor<1x1x8192xf32>) -> tensor<1x1x8192xbf16>
    %248 = stablehlo.reshape %247 : (tensor<1x1x8192xbf16>) -> tensor<1x8192xbf16>
    %249 = stablehlo.dot_general %248, %arg25, contracting_dims = [1] x [0] : (tensor<1x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<1x3072xbf16>
    %250 = stablehlo.reshape %249 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %251 = stablehlo.add %217, %250 : tensor<1x1x3072xbf16>
    %252 = stablehlo.convert %251 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %253 = stablehlo.power %252, %cst_0 : tensor<1x1x3072xf32>
    %254 = stablehlo.reduce(%253 init: %cst_2) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %255 = stablehlo.multiply %254, %cst : tensor<1x1xf32>
    %256 = stablehlo.reshape %255 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %257 = stablehlo.add %256, %16 : tensor<1x1x1xf32>
    %258 = stablehlo.rsqrt %257 : tensor<1x1x1xf32>
    %259 = stablehlo.reshape %258 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %260 = stablehlo.broadcast_in_dim %259, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %261 = stablehlo.multiply %252, %260 : tensor<1x1x3072xf32>
    %262 = stablehlo.convert %261 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %263 = stablehlo.convert %262 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %264 = stablehlo.multiply %176, %263 : tensor<1x1x3072xf32>
    %265 = stablehlo.convert %264 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %266 = stablehlo.reshape %265 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %267 = stablehlo.dot_general %266, %arg24, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<1x128256xbf16>
    %268 = stablehlo.reshape %267 : (tensor<1x128256xbf16>) -> tensor<1x1x128256xbf16>
    return %57, %60, %171, %174, %267, %268 : tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x128256xbf16>, tensor<1x1x128256xbf16>
  }
}
2025-08-12 15:39:40.338 (  40.781s) [        C545D1C0]      module_builder.cc:205      1| SHLO StableHLO Pipeline Module:
module @SyncTensorsGraph.605 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=1]>
  func.func @main(%arg0: tensor<1xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<1x64x1xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg2: tensor<3072x1024xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg3: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg4: tensor<1x1xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg5: tensor<128256x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg6: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg7: tensor<i64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg8: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg9: tensor<3072x1024xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg10: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg11: tensor<3072x1024xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg12: tensor<8192x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg13: tensor<3072x8192xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg14: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg15: tensor<1x1x1x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg16: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg17: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg18: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg19: tensor<3072x8192xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg20: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg21: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg22: tensor<3072x1024xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg23: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg24: tensor<3072x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg25: tensor<8192x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg26: tensor<3072x8192xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg27: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg28: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg29: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg30: tensor<3072x8192xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg31: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) -> (tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x1x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<3.25520843E-4> : tensor<1x1xf32>
    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<1x1x3072xf32>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<0> : tensor<1xi64>
    %0 = stablehlo.compare  LT, %arg0, %c : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
    %1 = stablehlo.reshape %arg7 : (tensor<i64>) -> tensor<1xi64>
    %2 = stablehlo.add %arg0, %1 : tensor<1xi64>
    %3 = stablehlo.select %0, %2, %arg0 : tensor<1xi1>, tensor<1xi64>
    %4 = stablehlo.reshape %3 : (tensor<1xi64>) -> tensor<1x1xi64>
    %5 = stablehlo.convert %arg6 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %6 = stablehlo.reshape %5 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
    %7 = stablehlo.reshape %arg4 : (tensor<1x1xi64>) -> tensor<1xi64>
    %8 = stablehlo.convert %7 : (tensor<1xi64>) -> tensor<1xui32>
    %9 = "stablehlo.gather"(%arg5, %8) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<1xui32>) -> tensor<1x3072xbf16>
    %10 = stablehlo.reshape %9 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.convert %10 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %12 = stablehlo.power %11, %cst_0 : tensor<1x1x3072xf32>
    %13 = stablehlo.reduce(%12 init: %cst_2) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %14 = stablehlo.multiply %13, %cst : tensor<1x1xf32>
    %15 = stablehlo.reshape %14 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %16 = stablehlo.reshape %arg3 : (tensor<f32>) -> tensor<1x1x1xf32>
    %17 = stablehlo.add %15, %16 : tensor<1x1x1xf32>
    %18 = stablehlo.rsqrt %17 : tensor<1x1x1xf32>
    %19 = stablehlo.reshape %18 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %20 = stablehlo.broadcast_in_dim %19, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %21 = stablehlo.multiply %11, %20 : tensor<1x1x3072xf32>
    %22 = stablehlo.convert %21 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %23 = stablehlo.convert %22 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %24 = stablehlo.multiply %6, %23 : tensor<1x1x3072xf32>
    %25 = stablehlo.convert %24 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %26 = stablehlo.reshape %25 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %27 = stablehlo.dot_general %26, %arg2, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %28 = stablehlo.reshape %27 : (tensor<1x1024xbf16>) -> tensor<1x8x1x128xbf16>
    %29 = stablehlo.convert %28 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,1,128]{3,1,2,0}"} : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xf32>
    %30 = stablehlo.reshape %arg0 : (tensor<1xi64>) -> tensor<1x1x1xi64>
    %31 = stablehlo.convert %30 : (tensor<1x1x1xi64>) -> tensor<1x1x1xf32>
    %32 = stablehlo.dot_general %arg1, %31, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x1xf32>) -> tensor<1x64x1xf32>
    %33 = stablehlo.reshape %32 : (tensor<1x64x1xf32>) -> tensor<1x1x64xf32>
    %34 = stablehlo.concatenate %33, %33, dim = 2 : (tensor<1x1x64xf32>, tensor<1x1x64xf32>) -> tensor<1x1x128xf32>
    %35 = stablehlo.cosine %34 : tensor<1x1x128xf32>
    %36 = stablehlo.convert %35 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
    %37 = stablehlo.reshape %36 : (tensor<1x1x128xbf16>) -> tensor<1x1x1x128xbf16>
    %38 = stablehlo.convert %37 : (tensor<1x1x1x128xbf16>) -> tensor<1x1x1x128xf32>
    %39 = stablehlo.reshape %38 : (tensor<1x1x1x128xf32>) -> tensor<1x1x128xf32>
    %40 = stablehlo.broadcast_in_dim %39, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x8x1x128xf32>
    %41 = stablehlo.multiply %29, %40 : tensor<1x8x1x128xf32>
    %42 = stablehlo.convert %41 : (tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xbf16>
    %43 = stablehlo.slice %28 [0:1, 0:8, 0:1, 64:128] : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x64xbf16>
    %44 = stablehlo.negate %43 : tensor<1x8x1x64xbf16>
    %45 = stablehlo.slice %28 [0:1, 0:8, 0:1, 0:64] : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x64xbf16>
    %46 = stablehlo.concatenate %44, %45, dim = 3 : (tensor<1x8x1x64xbf16>, tensor<1x8x1x64xbf16>) -> tensor<1x8x1x128xbf16>
    %47 = stablehlo.convert %46 : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xf32>
    %48 = stablehlo.sine %34 : tensor<1x1x128xf32>
    %49 = stablehlo.convert %48 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
    %50 = stablehlo.reshape %49 : (tensor<1x1x128xbf16>) -> tensor<1x1x1x128xbf16>
    %51 = stablehlo.convert %50 : (tensor<1x1x1x128xbf16>) -> tensor<1x1x1x128xf32>
    %52 = stablehlo.reshape %51 : (tensor<1x1x1x128xf32>) -> tensor<1x1x128xf32>
    %53 = stablehlo.broadcast_in_dim %52, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x8x1x128xf32>
    %54 = stablehlo.multiply %47, %53 : tensor<1x8x1x128xf32>
    %55 = stablehlo.convert %54 : (tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xbf16>
    %56 = stablehlo.add %42, %55 : tensor<1x8x1x128xbf16>
    %57 = "stablehlo.scatter"(%arg8, %4, %56) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg32: tensor<bf16>, %arg33: tensor<bf16>):
      stablehlo.return %arg33 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>) -> tensor<1x8x128x128xbf16>
    %58 = stablehlo.dot_general %26, %arg9, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %59 = stablehlo.reshape %58 : (tensor<1x1024xbf16>) -> tensor<1x8x1x128xbf16>
    %60 = "stablehlo.scatter"(%arg10, %4, %59) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg32: tensor<bf16>, %arg33: tensor<bf16>):
      stablehlo.return %arg33 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>) -> tensor<1x8x128x128xbf16>
    %61 = stablehlo.convert %arg20 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %62 = stablehlo.reshape %61 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
    %63 = stablehlo.dot_general %26, %arg17, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %64 = stablehlo.reshape %63 : (tensor<1x3072xbf16>) -> tensor<1x24x1x128xbf16>
    %65 = stablehlo.convert %64 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,1,128]{3,1,2,0}"} : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %66 = stablehlo.broadcast_in_dim %39, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x24x1x128xf32>
    %67 = stablehlo.multiply %65, %66 : tensor<1x24x1x128xf32>
    %68 = stablehlo.convert %67 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %69 = stablehlo.slice %64 [0:1, 0:24, 0:1, 64:128] : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x64xbf16>
    %70 = stablehlo.negate %69 : tensor<1x24x1x64xbf16>
    %71 = stablehlo.slice %64 [0:1, 0:24, 0:1, 0:64] : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x64xbf16>
    %72 = stablehlo.concatenate %70, %71, dim = 3 : (tensor<1x24x1x64xbf16>, tensor<1x24x1x64xbf16>) -> tensor<1x24x1x128xbf16>
    %73 = stablehlo.convert %72 : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %74 = stablehlo.broadcast_in_dim %52, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x24x1x128xf32>
    %75 = stablehlo.multiply %73, %74 : tensor<1x24x1x128xf32>
    %76 = stablehlo.convert %75 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %77 = stablehlo.add %68, %76 : tensor<1x24x1x128xbf16>
    %78 = stablehlo.reshape %77 : (tensor<1x24x1x128xbf16>) -> tensor<24x1x128xbf16>
    %79 = stablehlo.broadcast_in_dim %57, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %80 = stablehlo.reshape %79 : (tensor<1x8x3x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %81 = stablehlo.transpose %80, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,128]{2,3,1,0}"} : (tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %82 = stablehlo.reshape %81 : (tensor<1x24x128x128xbf16>) -> tensor<24x128x128xbf16>
    %83 = stablehlo.dot_general %78, %82, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x1x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x1x128xbf16>
    %84 = stablehlo.reshape %83 : (tensor<24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %85 = stablehlo.convert %84 : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %86 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<1x24x1x128xf32>
    %87 = stablehlo.multiply %85, %86 : tensor<1x24x1x128xf32>
    %88 = stablehlo.convert %87 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %89 = stablehlo.reshape %arg15 : (tensor<1x1x1x128xbf16>) -> tensor<1x1x128xbf16>
    %90 = stablehlo.broadcast_in_dim %89, dims = [0, 2, 3] : (tensor<1x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %91 = stablehlo.add %88, %90 : tensor<1x24x1x128xbf16>
    %92 = stablehlo.convert %91 : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %93 = stablehlo.reduce(%92 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x1x128xf32>, tensor<f32>) -> tensor<1x24x1xf32>
    %94 = stablehlo.broadcast_in_dim %93, dims = [0, 1, 2] : (tensor<1x24x1xf32>) -> tensor<1x24x1x128xf32>
    %95 = stablehlo.subtract %92, %94 : tensor<1x24x1x128xf32>
    %96 = stablehlo.exponential %95 : tensor<1x24x1x128xf32>
    %97 = stablehlo.reduce(%96 init: %cst_2) applies stablehlo.add across dimensions = [3] : (tensor<1x24x1x128xf32>, tensor<f32>) -> tensor<1x24x1xf32>
    %98 = stablehlo.broadcast_in_dim %97, dims = [0, 1, 2] : (tensor<1x24x1xf32>) -> tensor<1x24x1x128xf32>
    %99 = stablehlo.divide %96, %98 : tensor<1x24x1x128xf32>
    %100 = stablehlo.convert %99 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %101 = stablehlo.reshape %100 : (tensor<1x24x1x128xbf16>) -> tensor<24x1x128xbf16>
    %102 = stablehlo.broadcast_in_dim %60, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %103 = stablehlo.reshape %102 : (tensor<1x8x3x128x128xbf16>) -> tensor<24x128x128xbf16>
    %104 = stablehlo.dot_general %101, %103, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x1x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x1x128xbf16>
    %105 = stablehlo.reshape %104 : (tensor<24x1x128xbf16>) -> tensor<1x3072xbf16>
    %106 = stablehlo.dot_general %105, %arg14, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %107 = stablehlo.reshape %106 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %108 = stablehlo.add %10, %107 : tensor<1x1x3072xbf16>
    %109 = stablehlo.convert %arg18 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %110 = stablehlo.reshape %109 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
    %111 = stablehlo.convert %108 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %112 = stablehlo.power %111, %cst_0 : tensor<1x1x3072xf32>
    %113 = stablehlo.reduce(%112 init: %cst_2) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %114 = stablehlo.multiply %113, %cst : tensor<1x1xf32>
    %115 = stablehlo.reshape %114 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %116 = stablehlo.add %115, %16 : tensor<1x1x1xf32>
    %117 = stablehlo.rsqrt %116 : tensor<1x1x1xf32>
    %118 = stablehlo.reshape %117 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %119 = stablehlo.broadcast_in_dim %118, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %120 = stablehlo.multiply %111, %119 : tensor<1x1x3072xf32>
    %121 = stablehlo.convert %120 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %122 = stablehlo.convert %121 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %123 = stablehlo.multiply %110, %122 : tensor<1x1x3072xf32>
    %124 = stablehlo.convert %123 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %125 = stablehlo.reshape %124 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %126 = stablehlo.dot_general %125, %arg19, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %127 = stablehlo.reshape %126 : (tensor<1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %128 = stablehlo.convert %127 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %129 = stablehlo.logistic %127 : tensor<1x1x8192xbf16>
    %130 = stablehlo.convert %129 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %131 = stablehlo.multiply %128, %130 : tensor<1x1x8192xf32>
    %132 = stablehlo.convert %131 : (tensor<1x1x8192xf32>) -> tensor<1x1x8192xbf16>
    %133 = stablehlo.convert %132 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %134 = stablehlo.dot_general %125, %arg13, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %135 = stablehlo.reshape %134 : (tensor<1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %136 = stablehlo.convert %135 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %137 = stablehlo.multiply %133, %136 : tensor<1x1x8192xf32>
    %138 = stablehlo.convert %137 : (tensor<1x1x8192xf32>) -> tensor<1x1x8192xbf16>
    %139 = stablehlo.reshape %138 : (tensor<1x1x8192xbf16>) -> tensor<1x8192xbf16>
    %140 = stablehlo.dot_general %139, %arg12, contracting_dims = [1] x [0] : (tensor<1x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<1x3072xbf16>
    %141 = stablehlo.reshape %140 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %142 = stablehlo.add %108, %141 : tensor<1x1x3072xbf16>
    %143 = stablehlo.convert %142 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %144 = stablehlo.power %143, %cst_0 : tensor<1x1x3072xf32>
    %145 = stablehlo.reduce(%144 init: %cst_2) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %146 = stablehlo.multiply %145, %cst : tensor<1x1xf32>
    %147 = stablehlo.reshape %146 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %148 = stablehlo.add %147, %16 : tensor<1x1x1xf32>
    %149 = stablehlo.rsqrt %148 : tensor<1x1x1xf32>
    %150 = stablehlo.reshape %149 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %151 = stablehlo.broadcast_in_dim %150, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %152 = stablehlo.multiply %143, %151 : tensor<1x1x3072xf32>
    %153 = stablehlo.convert %152 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %154 = stablehlo.convert %153 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %155 = stablehlo.multiply %62, %154 : tensor<1x1x3072xf32>
    %156 = stablehlo.convert %155 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %157 = stablehlo.reshape %156 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %158 = stablehlo.dot_general %157, %arg11, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %159 = stablehlo.reshape %158 : (tensor<1x1024xbf16>) -> tensor<1x8x1x128xbf16>
    %160 = stablehlo.convert %159 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,1,128]{3,1,2,0}"} : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xf32>
    %161 = stablehlo.multiply %160, %40 : tensor<1x8x1x128xf32>
    %162 = stablehlo.convert %161 : (tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xbf16>
    %163 = stablehlo.slice %159 [0:1, 0:8, 0:1, 64:128] : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x64xbf16>
    %164 = stablehlo.negate %163 : tensor<1x8x1x64xbf16>
    %165 = stablehlo.slice %159 [0:1, 0:8, 0:1, 0:64] : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x64xbf16>
    %166 = stablehlo.concatenate %164, %165, dim = 3 : (tensor<1x8x1x64xbf16>, tensor<1x8x1x64xbf16>) -> tensor<1x8x1x128xbf16>
    %167 = stablehlo.convert %166 : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xf32>
    %168 = stablehlo.multiply %167, %53 : tensor<1x8x1x128xf32>
    %169 = stablehlo.convert %168 : (tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xbf16>
    %170 = stablehlo.add %162, %169 : tensor<1x8x1x128xbf16>
    %171 = "stablehlo.scatter"(%arg21, %4, %170) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg32: tensor<bf16>, %arg33: tensor<bf16>):
      stablehlo.return %arg33 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>) -> tensor<1x8x128x128xbf16>
    %172 = stablehlo.dot_general %157, %arg22, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x1024xbf16>) -> tensor<1x8x1x128xbf16>
    %174 = "stablehlo.scatter"(%arg23, %4, %173) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg32: tensor<bf16>, %arg33: tensor<bf16>):
      stablehlo.return %arg33 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>) -> tensor<1x8x128x128xbf16>
    %175 = stablehlo.convert %arg31 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %176 = stablehlo.reshape %175 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
    %177 = stablehlo.dot_general %157, %arg28, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %178 = stablehlo.reshape %177 : (tensor<1x3072xbf16>) -> tensor<1x24x1x128xbf16>
    %179 = stablehlo.convert %178 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,1,128]{3,1,2,0}"} : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %180 = stablehlo.multiply %179, %66 : tensor<1x24x1x128xf32>
    %181 = stablehlo.convert %180 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %182 = stablehlo.slice %178 [0:1, 0:24, 0:1, 64:128] : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x64xbf16>
    %183 = stablehlo.negate %182 : tensor<1x24x1x64xbf16>
    %184 = stablehlo.slice %178 [0:1, 0:24, 0:1, 0:64] : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x64xbf16>
    %185 = stablehlo.concatenate %183, %184, dim = 3 : (tensor<1x24x1x64xbf16>, tensor<1x24x1x64xbf16>) -> tensor<1x24x1x128xbf16>
    %186 = stablehlo.convert %185 : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %187 = stablehlo.multiply %186, %74 : tensor<1x24x1x128xf32>
    %188 = stablehlo.convert %187 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %189 = stablehlo.add %181, %188 : tensor<1x24x1x128xbf16>
    %190 = stablehlo.reshape %189 : (tensor<1x24x1x128xbf16>) -> tensor<24x1x128xbf16>
    %191 = stablehlo.broadcast_in_dim %171, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %192 = stablehlo.reshape %191 : (tensor<1x8x3x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %193 = stablehlo.transpose %192, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,128]{2,3,1,0}"} : (tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %194 = stablehlo.reshape %193 : (tensor<1x24x128x128xbf16>) -> tensor<24x128x128xbf16>
    %195 = stablehlo.dot_general %190, %194, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x1x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x1x128xbf16>
    %196 = stablehlo.reshape %195 : (tensor<24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %197 = stablehlo.convert %196 : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %198 = stablehlo.multiply %197, %86 : tensor<1x24x1x128xf32>
    %199 = stablehlo.convert %198 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %200 = stablehlo.add %199, %90 : tensor<1x24x1x128xbf16>
    %201 = stablehlo.convert %200 : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %202 = stablehlo.reduce(%201 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x1x128xf32>, tensor<f32>) -> tensor<1x24x1xf32>
    %203 = stablehlo.broadcast_in_dim %202, dims = [0, 1, 2] : (tensor<1x24x1xf32>) -> tensor<1x24x1x128xf32>
    %204 = stablehlo.subtract %201, %203 : tensor<1x24x1x128xf32>
    %205 = stablehlo.exponential %204 : tensor<1x24x1x128xf32>
    %206 = stablehlo.reduce(%205 init: %cst_2) applies stablehlo.add across dimensions = [3] : (tensor<1x24x1x128xf32>, tensor<f32>) -> tensor<1x24x1xf32>
    %207 = stablehlo.broadcast_in_dim %206, dims = [0, 1, 2] : (tensor<1x24x1xf32>) -> tensor<1x24x1x128xf32>
    %208 = stablehlo.divide %205, %207 : tensor<1x24x1x128xf32>
    %209 = stablehlo.convert %208 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %210 = stablehlo.reshape %209 : (tensor<1x24x1x128xbf16>) -> tensor<24x1x128xbf16>
    %211 = stablehlo.broadcast_in_dim %174, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %212 = stablehlo.reshape %211 : (tensor<1x8x3x128x128xbf16>) -> tensor<24x128x128xbf16>
    %213 = stablehlo.dot_general %210, %212, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x1x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x1x128xbf16>
    %214 = stablehlo.reshape %213 : (tensor<24x1x128xbf16>) -> tensor<1x3072xbf16>
    %215 = stablehlo.dot_general %214, %arg27, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %216 = stablehlo.reshape %215 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %217 = stablehlo.add %142, %216 : tensor<1x1x3072xbf16>
    %218 = stablehlo.convert %arg29 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %219 = stablehlo.reshape %218 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
    %220 = stablehlo.convert %217 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %221 = stablehlo.power %220, %cst_0 : tensor<1x1x3072xf32>
    %222 = stablehlo.reduce(%221 init: %cst_2) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %223 = stablehlo.multiply %222, %cst : tensor<1x1xf32>
    %224 = stablehlo.reshape %223 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %225 = stablehlo.add %224, %16 : tensor<1x1x1xf32>
    %226 = stablehlo.rsqrt %225 : tensor<1x1x1xf32>
    %227 = stablehlo.reshape %226 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %228 = stablehlo.broadcast_in_dim %227, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %229 = stablehlo.multiply %220, %228 : tensor<1x1x3072xf32>
    %230 = stablehlo.convert %229 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %231 = stablehlo.convert %230 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %232 = stablehlo.multiply %219, %231 : tensor<1x1x3072xf32>
    %233 = stablehlo.convert %232 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %234 = stablehlo.reshape %233 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %235 = stablehlo.dot_general %234, %arg30, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %236 = stablehlo.reshape %235 : (tensor<1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %237 = stablehlo.convert %236 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %238 = stablehlo.logistic %236 : tensor<1x1x8192xbf16>
    %239 = stablehlo.convert %238 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %240 = stablehlo.multiply %237, %239 : tensor<1x1x8192xf32>
    %241 = stablehlo.convert %240 : (tensor<1x1x8192xf32>) -> tensor<1x1x8192xbf16>
    %242 = stablehlo.convert %241 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %243 = stablehlo.dot_general %234, %arg26, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %244 = stablehlo.reshape %243 : (tensor<1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %245 = stablehlo.convert %244 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %246 = stablehlo.multiply %242, %245 : tensor<1x1x8192xf32>
    %247 = stablehlo.convert %246 : (tensor<1x1x8192xf32>) -> tensor<1x1x8192xbf16>
    %248 = stablehlo.reshape %247 : (tensor<1x1x8192xbf16>) -> tensor<1x8192xbf16>
    %249 = stablehlo.dot_general %248, %arg25, contracting_dims = [1] x [0] : (tensor<1x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<1x3072xbf16>
    %250 = stablehlo.reshape %249 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %251 = stablehlo.add %217, %250 : tensor<1x1x3072xbf16>
    %252 = stablehlo.convert %251 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %253 = stablehlo.power %252, %cst_0 : tensor<1x1x3072xf32>
    %254 = stablehlo.reduce(%253 init: %cst_2) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %255 = stablehlo.multiply %254, %cst : tensor<1x1xf32>
    %256 = stablehlo.reshape %255 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %257 = stablehlo.add %256, %16 : tensor<1x1x1xf32>
    %258 = stablehlo.rsqrt %257 : tensor<1x1x1xf32>
    %259 = stablehlo.reshape %258 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %260 = stablehlo.broadcast_in_dim %259, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %261 = stablehlo.multiply %252, %260 : tensor<1x1x3072xf32>
    %262 = stablehlo.convert %261 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %263 = stablehlo.convert %262 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %264 = stablehlo.multiply %176, %263 : tensor<1x1x3072xf32>
    %265 = stablehlo.convert %264 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %266 = stablehlo.reshape %265 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %267 = stablehlo.dot_general %266, %arg24, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<1x128256xbf16>
    %268 = stablehlo.reshape %267 : (tensor<1x128256xbf16>) -> tensor<1x1x128256xbf16>
    return %57, %60, %171, %174, %267, %268 : tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x128256xbf16>, tensor<1x1x128256xbf16>
  }
}
2025-08-12 15:39:40.350 (  40.793s) [        C545D1C0]      module_builder.cc:452      1| TTIR Module:
module @SyncTensorsGraph.605 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  func.func @main(%arg0: tensor<1xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<1x64x1xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg2: tensor<3072x1024xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg3: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg4: tensor<1x1xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg5: tensor<128256x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg6: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg7: tensor<i64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg8: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg9: tensor<3072x1024xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg10: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg11: tensor<3072x1024xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg12: tensor<8192x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg13: tensor<3072x8192xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg14: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg15: tensor<1x1x1x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg16: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg17: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg18: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg19: tensor<3072x8192xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg20: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg21: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg22: tensor<3072x1024xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg23: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg24: tensor<3072x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg25: tensor<8192x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg26: tensor<3072x8192xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg27: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg28: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg29: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg30: tensor<3072x8192xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg31: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) -> (tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x1x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<3.25520843E-4> : tensor<1x1xf32>}> : () -> tensor<1x1xf32>
    %1 = "ttir.constant"() <{value = dense<2.000000e+00> : tensor<1x1x3072xf32>}> : () -> tensor<1x1x3072xf32>
    %2 = "ttir.constant"() <{value = dense<0xFF800000> : tensor<f32>}> : () -> tensor<f32>
    %3 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<f32>}> : () -> tensor<f32>
    %4 = "ttir.constant"() <{value = dense<0> : tensor<1xi64>}> : () -> tensor<1xi64>
    %5 = ttir.empty() : tensor<1xi1>
    %6 = "ttir.lt"(%arg0, %4, %5) : (tensor<1xi64>, tensor<1xi64>, tensor<1xi1>) -> tensor<1xi1>
    %7 = ttir.empty() : tensor<1xi64>
    %8 = "ttir.reshape"(%arg7, %7) <{shape = [1 : i32]}> : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9 = ttir.empty() : tensor<1xi64>
    %10 = "ttir.add"(%arg0, %8, %9) : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %11 = ttir.empty() : tensor<1xi64>
    %12 = "ttir.where"(%6, %10, %arg0, %11) : (tensor<1xi1>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %13 = ttir.empty() : tensor<1x1xi64>
    %14 = "ttir.reshape"(%12, %13) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xi64>, tensor<1x1xi64>) -> tensor<1x1xi64>
    %15 = ttir.empty() : tensor<3072xf32>
    %16 = "ttir.typecast"(%arg6, %15) <{conservative_folding = false}> : (tensor<3072xbf16>, tensor<3072xf32>) -> tensor<3072xf32>
    %17 = ttir.empty() : tensor<1x1x3072xf32>
    %18 = "ttir.reshape"(%16, %17) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %19 = ttir.empty() : tensor<1xi64>
    %20 = "ttir.reshape"(%arg4, %19) <{shape = [1 : i32]}> : (tensor<1x1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %21 = ttir.empty() : tensor<1xui32>
    %22 = "ttir.typecast"(%20, %21) <{conservative_folding = false}> : (tensor<1xi64>, tensor<1xui32>) -> tensor<1xui32>
    %23 = ttir.empty() : tensor<1x3072xbf16>
    %24 = "ttir.gather"(%arg5, %22, %23) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 3072>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<128256x3072xbf16>, tensor<1xui32>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %25 = ttir.empty() : tensor<1x1x3072xbf16>
    %26 = "ttir.reshape"(%24, %25) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %27 = ttir.empty() : tensor<1x1x3072xf32>
    %28 = "ttir.typecast"(%26, %27) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %29 = ttir.empty() : tensor<1x1x3072xf32>
    %30 = "ttir.pow"(%28, %1, %29) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %31 = ttir.empty() : tensor<1x1xf32>
    %32 = "ttir.sum"(%30, %31) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %33 = ttir.empty() : tensor<1x1xf32>
    %34 = "ttir.multiply"(%32, %0, %33) : (tensor<1x1xf32>, tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %35 = ttir.empty() : tensor<1x1x1xf32>
    %36 = "ttir.reshape"(%34, %35) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %37 = ttir.empty() : tensor<1x1x1xf32>
    %38 = "ttir.reshape"(%arg3, %37) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %39 = ttir.empty() : tensor<1x1x1xf32>
    %40 = "ttir.add"(%36, %38, %39) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %41 = ttir.empty() : tensor<1x1x1xf32>
    %42 = "ttir.rsqrt"(%40, %41) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %43 = ttir.empty() : tensor<1x1xf32>
    %44 = "ttir.reshape"(%42, %43) <{shape = [1 : i32, 1 : i32]}> : (tensor<1x1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %45 = ttir.empty() : tensor<1x1x1xf32>
    %46 = "ttir.reshape"(%44, %45) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %47 = ttir.empty() : tensor<1x1x3072xf32>
    %48 = "ttir.broadcast"(%46, %47) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x1x1xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %49 = ttir.empty() : tensor<1x1x3072xf32>
    %50 = "ttir.multiply"(%28, %48, %49) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %51 = ttir.empty() : tensor<1x1x3072xbf16>
    %52 = "ttir.typecast"(%50, %51) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %53 = ttir.empty() : tensor<1x1x3072xf32>
    %54 = "ttir.typecast"(%52, %53) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %55 = ttir.empty() : tensor<1x1x3072xf32>
    %56 = "ttir.multiply"(%18, %54, %55) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %57 = ttir.empty() : tensor<1x1x3072xbf16>
    %58 = "ttir.typecast"(%56, %57) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %59 = ttir.empty() : tensor<1x3072xbf16>
    %60 = "ttir.reshape"(%58, %59) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x3072xbf16>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %61 = "ttir.dot_general"(%60, %arg2) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %62 = ttir.empty() : tensor<1x8x1x128xbf16>
    %63 = "ttir.reshape"(%61, %62) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1024xbf16>, tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xbf16>
    %64 = ttir.empty() : tensor<1x8x1x128xf32>
    %65 = "ttir.typecast"(%63, %64) <{conservative_folding = false}> : (tensor<1x8x1x128xbf16>, tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xf32>
    %66 = ttir.empty() : tensor<1x1x1xi64>
    %67 = "ttir.reshape"(%arg0, %66) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xi64>, tensor<1x1x1xi64>) -> tensor<1x1x1xi64>
    %68 = ttir.empty() : tensor<1x1x1xf32>
    %69 = "ttir.typecast"(%67, %68) <{conservative_folding = false}> : (tensor<1x1x1xi64>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %70 = "ttir.dot_general"(%arg1, %69) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x64x1xf32>, tensor<1x1x1xf32>) -> tensor<1x64x1xf32>
    %71 = ttir.empty() : tensor<1x1x64xf32>
    %72 = "ttir.reshape"(%70, %71) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<1x64x1xf32>, tensor<1x1x64xf32>) -> tensor<1x1x64xf32>
    %73 = ttir.empty() : tensor<1x1x128xf32>
    %74 = "ttir.concat"(%72, %72, %73) <{dim = 2 : si32}> : (tensor<1x1x64xf32>, tensor<1x1x64xf32>, tensor<1x1x128xf32>) -> tensor<1x1x128xf32>
    %75 = ttir.empty() : tensor<1x1x128xf32>
    %76 = "ttir.cos"(%74, %75) : (tensor<1x1x128xf32>, tensor<1x1x128xf32>) -> tensor<1x1x128xf32>
    %77 = ttir.empty() : tensor<1x1x128xbf16>
    %78 = "ttir.typecast"(%76, %77) <{conservative_folding = false}> : (tensor<1x1x128xf32>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16>
    %79 = ttir.empty() : tensor<1x1x1x128xbf16>
    %80 = "ttir.reshape"(%78, %79) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xbf16>, tensor<1x1x1x128xbf16>) -> tensor<1x1x1x128xbf16>
    %81 = ttir.empty() : tensor<1x1x1x128xf32>
    %82 = "ttir.typecast"(%80, %81) <{conservative_folding = false}> : (tensor<1x1x1x128xbf16>, tensor<1x1x1x128xf32>) -> tensor<1x1x1x128xf32>
    %83 = ttir.empty() : tensor<1x1x128xf32>
    %84 = "ttir.reshape"(%82, %83) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x1x128xf32>, tensor<1x1x128xf32>) -> tensor<1x1x128xf32>
    %85 = ttir.empty() : tensor<1x1x1x128xf32>
    %86 = "ttir.reshape"(%84, %85) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32>, tensor<1x1x1x128xf32>) -> tensor<1x1x1x128xf32>
    %87 = ttir.empty() : tensor<1x8x1x128xf32>
    %88 = "ttir.broadcast"(%86, %87) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x1x128xf32>, tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xf32>
    %89 = ttir.empty() : tensor<1x8x1x128xf32>
    %90 = "ttir.multiply"(%65, %88, %89) : (tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xf32>
    %91 = ttir.empty() : tensor<1x8x1x128xbf16>
    %92 = "ttir.typecast"(%90, %91) <{conservative_folding = false}> : (tensor<1x8x1x128xf32>, tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xbf16>
    %93 = ttir.empty() : tensor<1x8x1x64xbf16>
    %94 = "ttir.slice"(%63, %93) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 8 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x1x128xbf16>, tensor<1x8x1x64xbf16>) -> tensor<1x8x1x64xbf16>
    %95 = ttir.empty() : tensor<1x8x1x64xbf16>
    %96 = "ttir.neg"(%94, %95) : (tensor<1x8x1x64xbf16>, tensor<1x8x1x64xbf16>) -> tensor<1x8x1x64xbf16>
    %97 = ttir.empty() : tensor<1x8x1x64xbf16>
    %98 = "ttir.slice"(%63, %97) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x1x128xbf16>, tensor<1x8x1x64xbf16>) -> tensor<1x8x1x64xbf16>
    %99 = ttir.empty() : tensor<1x8x1x128xbf16>
    %100 = "ttir.concat"(%96, %98, %99) <{dim = 3 : si32}> : (tensor<1x8x1x64xbf16>, tensor<1x8x1x64xbf16>, tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xbf16>
    %101 = ttir.empty() : tensor<1x8x1x128xf32>
    %102 = "ttir.typecast"(%100, %101) <{conservative_folding = false}> : (tensor<1x8x1x128xbf16>, tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xf32>
    %103 = ttir.empty() : tensor<1x1x128xf32>
    %104 = "ttir.sin"(%74, %103) : (tensor<1x1x128xf32>, tensor<1x1x128xf32>) -> tensor<1x1x128xf32>
    %105 = ttir.empty() : tensor<1x1x128xbf16>
    %106 = "ttir.typecast"(%104, %105) <{conservative_folding = false}> : (tensor<1x1x128xf32>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16>
    %107 = ttir.empty() : tensor<1x1x1x128xbf16>
    %108 = "ttir.reshape"(%106, %107) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xbf16>, tensor<1x1x1x128xbf16>) -> tensor<1x1x1x128xbf16>
    %109 = ttir.empty() : tensor<1x1x1x128xf32>
    %110 = "ttir.typecast"(%108, %109) <{conservative_folding = false}> : (tensor<1x1x1x128xbf16>, tensor<1x1x1x128xf32>) -> tensor<1x1x1x128xf32>
    %111 = ttir.empty() : tensor<1x1x128xf32>
    %112 = "ttir.reshape"(%110, %111) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x1x128xf32>, tensor<1x1x128xf32>) -> tensor<1x1x128xf32>
    %113 = ttir.empty() : tensor<1x1x1x128xf32>
    %114 = "ttir.reshape"(%112, %113) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32>, tensor<1x1x1x128xf32>) -> tensor<1x1x1x128xf32>
    %115 = ttir.empty() : tensor<1x8x1x128xf32>
    %116 = "ttir.broadcast"(%114, %115) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x1x128xf32>, tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xf32>
    %117 = ttir.empty() : tensor<1x8x1x128xf32>
    %118 = "ttir.multiply"(%102, %116, %117) : (tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xf32>
    %119 = ttir.empty() : tensor<1x8x1x128xbf16>
    %120 = "ttir.typecast"(%118, %119) <{conservative_folding = false}> : (tensor<1x8x1x128xf32>, tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xbf16>
    %121 = ttir.empty() : tensor<1x8x1x128xbf16>
    %122 = "ttir.add"(%92, %120, %121) : (tensor<1x8x1x128xbf16>, tensor<1x8x1x128xbf16>, tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xbf16>
    %123 = ttir.empty() : tensor<1x8x128x128xbf16>
    %124 = "ttir.scatter"(%arg8, %14, %122, %123) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x8x128x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>, tensor<1x8x128x128xbf16>) -> tensor<1x8x128x128xbf16>
    %125 = "ttir.dot_general"(%60, %arg9) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %126 = ttir.empty() : tensor<1x8x1x128xbf16>
    %127 = "ttir.reshape"(%125, %126) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1024xbf16>, tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xbf16>
    %128 = ttir.empty() : tensor<1x8x128x128xbf16>
    %129 = "ttir.scatter"(%arg10, %14, %127, %128) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x8x128x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>, tensor<1x8x128x128xbf16>) -> tensor<1x8x128x128xbf16>
    %130 = ttir.empty() : tensor<3072xf32>
    %131 = "ttir.typecast"(%arg20, %130) <{conservative_folding = false}> : (tensor<3072xbf16>, tensor<3072xf32>) -> tensor<3072xf32>
    %132 = ttir.empty() : tensor<1x1x3072xf32>
    %133 = "ttir.reshape"(%131, %132) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %134 = "ttir.dot_general"(%60, %arg17) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %135 = ttir.empty() : tensor<1x24x1x128xbf16>
    %136 = "ttir.reshape"(%134, %135) <{shape = [1 : i32, 24 : i32, 1 : i32, 128 : i32]}> : (tensor<1x3072xbf16>, tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %137 = ttir.empty() : tensor<1x24x1x128xf32>
    %138 = "ttir.typecast"(%136, %137) <{conservative_folding = false}> : (tensor<1x24x1x128xbf16>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %139 = ttir.empty() : tensor<1x1x1x128xf32>
    %140 = "ttir.reshape"(%84, %139) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32>, tensor<1x1x1x128xf32>) -> tensor<1x1x1x128xf32>
    %141 = ttir.empty() : tensor<1x24x1x128xf32>
    %142 = "ttir.broadcast"(%140, %141) <{broadcast_dimensions = array<i64: 1, 24, 1, 1>}> : (tensor<1x1x1x128xf32>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %143 = ttir.empty() : tensor<1x24x1x128xf32>
    %144 = "ttir.multiply"(%138, %142, %143) : (tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %145 = ttir.empty() : tensor<1x24x1x128xbf16>
    %146 = "ttir.typecast"(%144, %145) <{conservative_folding = false}> : (tensor<1x24x1x128xf32>, tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %147 = ttir.empty() : tensor<1x24x1x64xbf16>
    %148 = "ttir.slice"(%136, %147) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 24 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x1x128xbf16>, tensor<1x24x1x64xbf16>) -> tensor<1x24x1x64xbf16>
    %149 = ttir.empty() : tensor<1x24x1x64xbf16>
    %150 = "ttir.neg"(%148, %149) : (tensor<1x24x1x64xbf16>, tensor<1x24x1x64xbf16>) -> tensor<1x24x1x64xbf16>
    %151 = ttir.empty() : tensor<1x24x1x64xbf16>
    %152 = "ttir.slice"(%136, %151) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 24 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x1x128xbf16>, tensor<1x24x1x64xbf16>) -> tensor<1x24x1x64xbf16>
    %153 = ttir.empty() : tensor<1x24x1x128xbf16>
    %154 = "ttir.concat"(%150, %152, %153) <{dim = 3 : si32}> : (tensor<1x24x1x64xbf16>, tensor<1x24x1x64xbf16>, tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %155 = ttir.empty() : tensor<1x24x1x128xf32>
    %156 = "ttir.typecast"(%154, %155) <{conservative_folding = false}> : (tensor<1x24x1x128xbf16>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %157 = ttir.empty() : tensor<1x1x1x128xf32>
    %158 = "ttir.reshape"(%112, %157) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32>, tensor<1x1x1x128xf32>) -> tensor<1x1x1x128xf32>
    %159 = ttir.empty() : tensor<1x24x1x128xf32>
    %160 = "ttir.broadcast"(%158, %159) <{broadcast_dimensions = array<i64: 1, 24, 1, 1>}> : (tensor<1x1x1x128xf32>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %161 = ttir.empty() : tensor<1x24x1x128xf32>
    %162 = "ttir.multiply"(%156, %160, %161) : (tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %163 = ttir.empty() : tensor<1x24x1x128xbf16>
    %164 = "ttir.typecast"(%162, %163) <{conservative_folding = false}> : (tensor<1x24x1x128xf32>, tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %165 = ttir.empty() : tensor<1x24x1x128xbf16>
    %166 = "ttir.add"(%146, %164, %165) : (tensor<1x24x1x128xbf16>, tensor<1x24x1x128xbf16>, tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %167 = ttir.empty() : tensor<24x1x128xbf16>
    %168 = "ttir.reshape"(%166, %167) <{shape = [24 : i32, 1 : i32, 128 : i32]}> : (tensor<1x24x1x128xbf16>, tensor<24x1x128xbf16>) -> tensor<24x1x128xbf16>
    %169 = ttir.empty() : tensor<1x8x1x128x128xbf16>
    %170 = "ttir.reshape"(%124, %169) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x128x128xbf16>, tensor<1x8x1x128x128xbf16>) -> tensor<1x8x1x128x128xbf16>
    %171 = ttir.empty() : tensor<1x8x3x128x128xbf16>
    %172 = "ttir.broadcast"(%170, %171) <{broadcast_dimensions = array<i64: 1, 1, 3, 1, 1>}> : (tensor<1x8x1x128x128xbf16>, tensor<1x8x3x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %173 = ttir.empty() : tensor<1x24x128x128xbf16>
    %174 = "ttir.reshape"(%172, %173) <{shape = [1 : i32, 24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x3x128x128xbf16>, tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %175 = ttir.empty() : tensor<1x24x128x128xbf16>
    %176 = "ttir.permute"(%174, %175) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x24x128x128xbf16>, tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %177 = ttir.empty() : tensor<24x128x128xbf16>
    %178 = "ttir.reshape"(%176, %177) <{shape = [24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x24x128x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x128x128xbf16>
    %179 = "ttir.dot_general"(%168, %178) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<24x1x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x1x128xbf16>
    %180 = ttir.empty() : tensor<1x24x1x128xbf16>
    %181 = "ttir.reshape"(%179, %180) <{shape = [1 : i32, 24 : i32, 1 : i32, 128 : i32]}> : (tensor<24x1x128xbf16>, tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %182 = ttir.empty() : tensor<1x24x1x128xf32>
    %183 = "ttir.typecast"(%181, %182) <{conservative_folding = false}> : (tensor<1x24x1x128xbf16>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %184 = ttir.empty() : tensor<1x1x1x1xf32>
    %185 = "ttir.reshape"(%arg16, %184) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %186 = ttir.empty() : tensor<1x24x1x128xf32>
    %187 = "ttir.broadcast"(%185, %186) <{broadcast_dimensions = array<i64: 1, 24, 1, 128>}> : (tensor<1x1x1x1xf32>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %188 = ttir.empty() : tensor<1x24x1x128xf32>
    %189 = "ttir.multiply"(%183, %187, %188) : (tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %190 = ttir.empty() : tensor<1x24x1x128xbf16>
    %191 = "ttir.typecast"(%189, %190) <{conservative_folding = false}> : (tensor<1x24x1x128xf32>, tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %192 = ttir.empty() : tensor<1x1x128xbf16>
    %193 = "ttir.reshape"(%arg15, %192) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x1x128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16>
    %194 = ttir.empty() : tensor<1x1x1x128xbf16>
    %195 = "ttir.reshape"(%193, %194) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xbf16>, tensor<1x1x1x128xbf16>) -> tensor<1x1x1x128xbf16>
    %196 = ttir.empty() : tensor<1x24x1x128xbf16>
    %197 = "ttir.broadcast"(%195, %196) <{broadcast_dimensions = array<i64: 1, 24, 1, 1>}> : (tensor<1x1x1x128xbf16>, tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %198 = ttir.empty() : tensor<1x24x1x128xbf16>
    %199 = "ttir.add"(%191, %197, %198) : (tensor<1x24x1x128xbf16>, tensor<1x24x1x128xbf16>, tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %200 = ttir.empty() : tensor<1x24x1x128xf32>
    %201 = "ttir.typecast"(%199, %200) <{conservative_folding = false}> : (tensor<1x24x1x128xbf16>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %202 = ttir.empty() : tensor<1x24x1xf32>
    %203 = "ttir.max"(%201, %202) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x24x1x128xf32>, tensor<1x24x1xf32>) -> tensor<1x24x1xf32>
    %204 = ttir.empty() : tensor<1x24x1x1xf32>
    %205 = "ttir.reshape"(%203, %204) <{shape = [1 : i32, 24 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x1xf32>, tensor<1x24x1x1xf32>) -> tensor<1x24x1x1xf32>
    %206 = ttir.empty() : tensor<1x24x1x128xf32>
    %207 = "ttir.broadcast"(%205, %206) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<1x24x1x1xf32>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %208 = ttir.empty() : tensor<1x24x1x128xf32>
    %209 = "ttir.subtract"(%201, %207, %208) : (tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %210 = ttir.empty() : tensor<1x24x1x128xf32>
    %211 = "ttir.exp"(%209, %210) : (tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %212 = ttir.empty() : tensor<1x24x1xf32>
    %213 = "ttir.sum"(%211, %212) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x24x1x128xf32>, tensor<1x24x1xf32>) -> tensor<1x24x1xf32>
    %214 = ttir.empty() : tensor<1x24x1x1xf32>
    %215 = "ttir.reshape"(%213, %214) <{shape = [1 : i32, 24 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x1xf32>, tensor<1x24x1x1xf32>) -> tensor<1x24x1x1xf32>
    %216 = ttir.empty() : tensor<1x24x1x128xf32>
    %217 = "ttir.broadcast"(%215, %216) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<1x24x1x1xf32>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %218 = ttir.empty() : tensor<1x24x1x128xf32>
    %219 = "ttir.div"(%211, %217, %218) : (tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %220 = ttir.empty() : tensor<1x24x1x128xbf16>
    %221 = "ttir.typecast"(%219, %220) <{conservative_folding = false}> : (tensor<1x24x1x128xf32>, tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %222 = ttir.empty() : tensor<24x1x128xbf16>
    %223 = "ttir.reshape"(%221, %222) <{shape = [24 : i32, 1 : i32, 128 : i32]}> : (tensor<1x24x1x128xbf16>, tensor<24x1x128xbf16>) -> tensor<24x1x128xbf16>
    %224 = ttir.empty() : tensor<1x8x1x128x128xbf16>
    %225 = "ttir.reshape"(%129, %224) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x128x128xbf16>, tensor<1x8x1x128x128xbf16>) -> tensor<1x8x1x128x128xbf16>
    %226 = ttir.empty() : tensor<1x8x3x128x128xbf16>
    %227 = "ttir.broadcast"(%225, %226) <{broadcast_dimensions = array<i64: 1, 1, 3, 1, 1>}> : (tensor<1x8x1x128x128xbf16>, tensor<1x8x3x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %228 = ttir.empty() : tensor<24x128x128xbf16>
    %229 = "ttir.reshape"(%227, %228) <{shape = [24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x3x128x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x128x128xbf16>
    %230 = "ttir.dot_general"(%223, %229) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<24x1x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x1x128xbf16>
    %231 = ttir.empty() : tensor<1x3072xbf16>
    %232 = "ttir.reshape"(%230, %231) <{shape = [1 : i32, 3072 : i32]}> : (tensor<24x1x128xbf16>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %233 = "ttir.dot_general"(%232, %arg14) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %234 = ttir.empty() : tensor<1x1x3072xbf16>
    %235 = "ttir.reshape"(%233, %234) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %236 = ttir.empty() : tensor<1x1x3072xbf16>
    %237 = "ttir.add"(%26, %235, %236) : (tensor<1x1x3072xbf16>, tensor<1x1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %238 = ttir.empty() : tensor<3072xf32>
    %239 = "ttir.typecast"(%arg18, %238) <{conservative_folding = false}> : (tensor<3072xbf16>, tensor<3072xf32>) -> tensor<3072xf32>
    %240 = ttir.empty() : tensor<1x1x3072xf32>
    %241 = "ttir.reshape"(%239, %240) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %242 = ttir.empty() : tensor<1x1x3072xf32>
    %243 = "ttir.typecast"(%237, %242) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %244 = ttir.empty() : tensor<1x1x3072xf32>
    %245 = "ttir.pow"(%243, %1, %244) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %246 = ttir.empty() : tensor<1x1xf32>
    %247 = "ttir.sum"(%245, %246) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %248 = ttir.empty() : tensor<1x1xf32>
    %249 = "ttir.multiply"(%247, %0, %248) : (tensor<1x1xf32>, tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %250 = ttir.empty() : tensor<1x1x1xf32>
    %251 = "ttir.reshape"(%249, %250) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %252 = ttir.empty() : tensor<1x1x1xf32>
    %253 = "ttir.add"(%251, %38, %252) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %254 = ttir.empty() : tensor<1x1x1xf32>
    %255 = "ttir.rsqrt"(%253, %254) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %256 = ttir.empty() : tensor<1x1xf32>
    %257 = "ttir.reshape"(%255, %256) <{shape = [1 : i32, 1 : i32]}> : (tensor<1x1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %258 = ttir.empty() : tensor<1x1x1xf32>
    %259 = "ttir.reshape"(%257, %258) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %260 = ttir.empty() : tensor<1x1x3072xf32>
    %261 = "ttir.broadcast"(%259, %260) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x1x1xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %262 = ttir.empty() : tensor<1x1x3072xf32>
    %263 = "ttir.multiply"(%243, %261, %262) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %264 = ttir.empty() : tensor<1x1x3072xbf16>
    %265 = "ttir.typecast"(%263, %264) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %266 = ttir.empty() : tensor<1x1x3072xf32>
    %267 = "ttir.typecast"(%265, %266) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %268 = ttir.empty() : tensor<1x1x3072xf32>
    %269 = "ttir.multiply"(%241, %267, %268) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %270 = ttir.empty() : tensor<1x1x3072xbf16>
    %271 = "ttir.typecast"(%269, %270) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %272 = ttir.empty() : tensor<1x3072xbf16>
    %273 = "ttir.reshape"(%271, %272) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x3072xbf16>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %274 = "ttir.dot_general"(%273, %arg19) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %275 = ttir.empty() : tensor<1x1x8192xbf16>
    %276 = "ttir.reshape"(%274, %275) <{shape = [1 : i32, 1 : i32, 8192 : i32]}> : (tensor<1x8192xbf16>, tensor<1x1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %277 = ttir.empty() : tensor<1x1x8192xf32>
    %278 = "ttir.typecast"(%276, %277) <{conservative_folding = false}> : (tensor<1x1x8192xbf16>, tensor<1x1x8192xf32>) -> tensor<1x1x8192xf32>
    %279 = ttir.empty() : tensor<1x1x8192xbf16>
    %280 = "ttir.sigmoid"(%276, %279) : (tensor<1x1x8192xbf16>, tensor<1x1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %281 = ttir.empty() : tensor<1x1x8192xf32>
    %282 = "ttir.typecast"(%280, %281) <{conservative_folding = false}> : (tensor<1x1x8192xbf16>, tensor<1x1x8192xf32>) -> tensor<1x1x8192xf32>
    %283 = ttir.empty() : tensor<1x1x8192xf32>
    %284 = "ttir.multiply"(%278, %282, %283) : (tensor<1x1x8192xf32>, tensor<1x1x8192xf32>, tensor<1x1x8192xf32>) -> tensor<1x1x8192xf32>
    %285 = ttir.empty() : tensor<1x1x8192xbf16>
    %286 = "ttir.typecast"(%284, %285) <{conservative_folding = false}> : (tensor<1x1x8192xf32>, tensor<1x1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %287 = ttir.empty() : tensor<1x1x8192xf32>
    %288 = "ttir.typecast"(%286, %287) <{conservative_folding = false}> : (tensor<1x1x8192xbf16>, tensor<1x1x8192xf32>) -> tensor<1x1x8192xf32>
    %289 = "ttir.dot_general"(%273, %arg13) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %290 = ttir.empty() : tensor<1x1x8192xbf16>
    %291 = "ttir.reshape"(%289, %290) <{shape = [1 : i32, 1 : i32, 8192 : i32]}> : (tensor<1x8192xbf16>, tensor<1x1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %292 = ttir.empty() : tensor<1x1x8192xf32>
    %293 = "ttir.typecast"(%291, %292) <{conservative_folding = false}> : (tensor<1x1x8192xbf16>, tensor<1x1x8192xf32>) -> tensor<1x1x8192xf32>
    %294 = ttir.empty() : tensor<1x1x8192xf32>
    %295 = "ttir.multiply"(%288, %293, %294) : (tensor<1x1x8192xf32>, tensor<1x1x8192xf32>, tensor<1x1x8192xf32>) -> tensor<1x1x8192xf32>
    %296 = ttir.empty() : tensor<1x1x8192xbf16>
    %297 = "ttir.typecast"(%295, %296) <{conservative_folding = false}> : (tensor<1x1x8192xf32>, tensor<1x1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %298 = ttir.empty() : tensor<1x8192xbf16>
    %299 = "ttir.reshape"(%297, %298) <{shape = [1 : i32, 8192 : i32]}> : (tensor<1x1x8192xbf16>, tensor<1x8192xbf16>) -> tensor<1x8192xbf16>
    %300 = "ttir.dot_general"(%299, %arg12) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<1x3072xbf16>
    %301 = ttir.empty() : tensor<1x1x3072xbf16>
    %302 = "ttir.reshape"(%300, %301) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %303 = ttir.empty() : tensor<1x1x3072xbf16>
    %304 = "ttir.add"(%237, %302, %303) : (tensor<1x1x3072xbf16>, tensor<1x1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %305 = ttir.empty() : tensor<1x1x3072xf32>
    %306 = "ttir.typecast"(%304, %305) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %307 = ttir.empty() : tensor<1x1x3072xf32>
    %308 = "ttir.pow"(%306, %1, %307) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %309 = ttir.empty() : tensor<1x1xf32>
    %310 = "ttir.sum"(%308, %309) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %311 = ttir.empty() : tensor<1x1xf32>
    %312 = "ttir.multiply"(%310, %0, %311) : (tensor<1x1xf32>, tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %313 = ttir.empty() : tensor<1x1x1xf32>
    %314 = "ttir.reshape"(%312, %313) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %315 = ttir.empty() : tensor<1x1x1xf32>
    %316 = "ttir.add"(%314, %38, %315) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %317 = ttir.empty() : tensor<1x1x1xf32>
    %318 = "ttir.rsqrt"(%316, %317) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %319 = ttir.empty() : tensor<1x1xf32>
    %320 = "ttir.reshape"(%318, %319) <{shape = [1 : i32, 1 : i32]}> : (tensor<1x1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %321 = ttir.empty() : tensor<1x1x1xf32>
    %322 = "ttir.reshape"(%320, %321) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %323 = ttir.empty() : tensor<1x1x3072xf32>
    %324 = "ttir.broadcast"(%322, %323) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x1x1xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %325 = ttir.empty() : tensor<1x1x3072xf32>
    %326 = "ttir.multiply"(%306, %324, %325) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %327 = ttir.empty() : tensor<1x1x3072xbf16>
    %328 = "ttir.typecast"(%326, %327) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %329 = ttir.empty() : tensor<1x1x3072xf32>
    %330 = "ttir.typecast"(%328, %329) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %331 = ttir.empty() : tensor<1x1x3072xf32>
    %332 = "ttir.multiply"(%133, %330, %331) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %333 = ttir.empty() : tensor<1x1x3072xbf16>
    %334 = "ttir.typecast"(%332, %333) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %335 = ttir.empty() : tensor<1x3072xbf16>
    %336 = "ttir.reshape"(%334, %335) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x3072xbf16>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %337 = "ttir.dot_general"(%336, %arg11) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %338 = ttir.empty() : tensor<1x8x1x128xbf16>
    %339 = "ttir.reshape"(%337, %338) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1024xbf16>, tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xbf16>
    %340 = ttir.empty() : tensor<1x8x1x128xf32>
    %341 = "ttir.typecast"(%339, %340) <{conservative_folding = false}> : (tensor<1x8x1x128xbf16>, tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xf32>
    %342 = ttir.empty() : tensor<1x8x1x128xf32>
    %343 = "ttir.multiply"(%341, %88, %342) : (tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xf32>
    %344 = ttir.empty() : tensor<1x8x1x128xbf16>
    %345 = "ttir.typecast"(%343, %344) <{conservative_folding = false}> : (tensor<1x8x1x128xf32>, tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xbf16>
    %346 = ttir.empty() : tensor<1x8x1x64xbf16>
    %347 = "ttir.slice"(%339, %346) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 8 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x1x128xbf16>, tensor<1x8x1x64xbf16>) -> tensor<1x8x1x64xbf16>
    %348 = ttir.empty() : tensor<1x8x1x64xbf16>
    %349 = "ttir.neg"(%347, %348) : (tensor<1x8x1x64xbf16>, tensor<1x8x1x64xbf16>) -> tensor<1x8x1x64xbf16>
    %350 = ttir.empty() : tensor<1x8x1x64xbf16>
    %351 = "ttir.slice"(%339, %350) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x1x128xbf16>, tensor<1x8x1x64xbf16>) -> tensor<1x8x1x64xbf16>
    %352 = ttir.empty() : tensor<1x8x1x128xbf16>
    %353 = "ttir.concat"(%349, %351, %352) <{dim = 3 : si32}> : (tensor<1x8x1x64xbf16>, tensor<1x8x1x64xbf16>, tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xbf16>
    %354 = ttir.empty() : tensor<1x8x1x128xf32>
    %355 = "ttir.typecast"(%353, %354) <{conservative_folding = false}> : (tensor<1x8x1x128xbf16>, tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xf32>
    %356 = ttir.empty() : tensor<1x8x1x128xf32>
    %357 = "ttir.multiply"(%355, %116, %356) : (tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xf32>
    %358 = ttir.empty() : tensor<1x8x1x128xbf16>
    %359 = "ttir.typecast"(%357, %358) <{conservative_folding = false}> : (tensor<1x8x1x128xf32>, tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xbf16>
    %360 = ttir.empty() : tensor<1x8x1x128xbf16>
    %361 = "ttir.add"(%345, %359, %360) : (tensor<1x8x1x128xbf16>, tensor<1x8x1x128xbf16>, tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xbf16>
    %362 = ttir.empty() : tensor<1x8x128x128xbf16>
    %363 = "ttir.scatter"(%arg21, %14, %361, %362) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x8x128x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>, tensor<1x8x128x128xbf16>) -> tensor<1x8x128x128xbf16>
    %364 = "ttir.dot_general"(%336, %arg22) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %365 = ttir.empty() : tensor<1x8x1x128xbf16>
    %366 = "ttir.reshape"(%364, %365) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1024xbf16>, tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xbf16>
    %367 = ttir.empty() : tensor<1x8x128x128xbf16>
    %368 = "ttir.scatter"(%arg23, %14, %366, %367) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x8x128x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>, tensor<1x8x128x128xbf16>) -> tensor<1x8x128x128xbf16>
    %369 = ttir.empty() : tensor<3072xf32>
    %370 = "ttir.typecast"(%arg31, %369) <{conservative_folding = false}> : (tensor<3072xbf16>, tensor<3072xf32>) -> tensor<3072xf32>
    %371 = ttir.empty() : tensor<1x1x3072xf32>
    %372 = "ttir.reshape"(%370, %371) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %373 = "ttir.dot_general"(%336, %arg28) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %374 = ttir.empty() : tensor<1x24x1x128xbf16>
    %375 = "ttir.reshape"(%373, %374) <{shape = [1 : i32, 24 : i32, 1 : i32, 128 : i32]}> : (tensor<1x3072xbf16>, tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %376 = ttir.empty() : tensor<1x24x1x128xf32>
    %377 = "ttir.typecast"(%375, %376) <{conservative_folding = false}> : (tensor<1x24x1x128xbf16>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %378 = ttir.empty() : tensor<1x24x1x128xf32>
    %379 = "ttir.multiply"(%377, %142, %378) : (tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %380 = ttir.empty() : tensor<1x24x1x128xbf16>
    %381 = "ttir.typecast"(%379, %380) <{conservative_folding = false}> : (tensor<1x24x1x128xf32>, tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %382 = ttir.empty() : tensor<1x24x1x64xbf16>
    %383 = "ttir.slice"(%375, %382) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 24 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x1x128xbf16>, tensor<1x24x1x64xbf16>) -> tensor<1x24x1x64xbf16>
    %384 = ttir.empty() : tensor<1x24x1x64xbf16>
    %385 = "ttir.neg"(%383, %384) : (tensor<1x24x1x64xbf16>, tensor<1x24x1x64xbf16>) -> tensor<1x24x1x64xbf16>
    %386 = ttir.empty() : tensor<1x24x1x64xbf16>
    %387 = "ttir.slice"(%375, %386) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 24 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x1x128xbf16>, tensor<1x24x1x64xbf16>) -> tensor<1x24x1x64xbf16>
    %388 = ttir.empty() : tensor<1x24x1x128xbf16>
    %389 = "ttir.concat"(%385, %387, %388) <{dim = 3 : si32}> : (tensor<1x24x1x64xbf16>, tensor<1x24x1x64xbf16>, tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %390 = ttir.empty() : tensor<1x24x1x128xf32>
    %391 = "ttir.typecast"(%389, %390) <{conservative_folding = false}> : (tensor<1x24x1x128xbf16>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %392 = ttir.empty() : tensor<1x24x1x128xf32>
    %393 = "ttir.multiply"(%391, %160, %392) : (tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %394 = ttir.empty() : tensor<1x24x1x128xbf16>
    %395 = "ttir.typecast"(%393, %394) <{conservative_folding = false}> : (tensor<1x24x1x128xf32>, tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %396 = ttir.empty() : tensor<1x24x1x128xbf16>
    %397 = "ttir.add"(%381, %395, %396) : (tensor<1x24x1x128xbf16>, tensor<1x24x1x128xbf16>, tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %398 = ttir.empty() : tensor<24x1x128xbf16>
    %399 = "ttir.reshape"(%397, %398) <{shape = [24 : i32, 1 : i32, 128 : i32]}> : (tensor<1x24x1x128xbf16>, tensor<24x1x128xbf16>) -> tensor<24x1x128xbf16>
    %400 = ttir.empty() : tensor<1x8x1x128x128xbf16>
    %401 = "ttir.reshape"(%363, %400) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x128x128xbf16>, tensor<1x8x1x128x128xbf16>) -> tensor<1x8x1x128x128xbf16>
    %402 = ttir.empty() : tensor<1x8x3x128x128xbf16>
    %403 = "ttir.broadcast"(%401, %402) <{broadcast_dimensions = array<i64: 1, 1, 3, 1, 1>}> : (tensor<1x8x1x128x128xbf16>, tensor<1x8x3x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %404 = ttir.empty() : tensor<1x24x128x128xbf16>
    %405 = "ttir.reshape"(%403, %404) <{shape = [1 : i32, 24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x3x128x128xbf16>, tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %406 = ttir.empty() : tensor<1x24x128x128xbf16>
    %407 = "ttir.permute"(%405, %406) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x24x128x128xbf16>, tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %408 = ttir.empty() : tensor<24x128x128xbf16>
    %409 = "ttir.reshape"(%407, %408) <{shape = [24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x24x128x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x128x128xbf16>
    %410 = "ttir.dot_general"(%399, %409) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<24x1x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x1x128xbf16>
    %411 = ttir.empty() : tensor<1x24x1x128xbf16>
    %412 = "ttir.reshape"(%410, %411) <{shape = [1 : i32, 24 : i32, 1 : i32, 128 : i32]}> : (tensor<24x1x128xbf16>, tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %413 = ttir.empty() : tensor<1x24x1x128xf32>
    %414 = "ttir.typecast"(%412, %413) <{conservative_folding = false}> : (tensor<1x24x1x128xbf16>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %415 = ttir.empty() : tensor<1x24x1x128xf32>
    %416 = "ttir.multiply"(%414, %187, %415) : (tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %417 = ttir.empty() : tensor<1x24x1x128xbf16>
    %418 = "ttir.typecast"(%416, %417) <{conservative_folding = false}> : (tensor<1x24x1x128xf32>, tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %419 = ttir.empty() : tensor<1x24x1x128xbf16>
    %420 = "ttir.add"(%418, %197, %419) : (tensor<1x24x1x128xbf16>, tensor<1x24x1x128xbf16>, tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %421 = ttir.empty() : tensor<1x24x1x128xf32>
    %422 = "ttir.typecast"(%420, %421) <{conservative_folding = false}> : (tensor<1x24x1x128xbf16>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %423 = ttir.empty() : tensor<1x24x1xf32>
    %424 = "ttir.max"(%422, %423) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x24x1x128xf32>, tensor<1x24x1xf32>) -> tensor<1x24x1xf32>
    %425 = ttir.empty() : tensor<1x24x1x1xf32>
    %426 = "ttir.reshape"(%424, %425) <{shape = [1 : i32, 24 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x1xf32>, tensor<1x24x1x1xf32>) -> tensor<1x24x1x1xf32>
    %427 = ttir.empty() : tensor<1x24x1x128xf32>
    %428 = "ttir.broadcast"(%426, %427) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<1x24x1x1xf32>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %429 = ttir.empty() : tensor<1x24x1x128xf32>
    %430 = "ttir.subtract"(%422, %428, %429) : (tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %431 = ttir.empty() : tensor<1x24x1x128xf32>
    %432 = "ttir.exp"(%430, %431) : (tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %433 = ttir.empty() : tensor<1x24x1xf32>
    %434 = "ttir.sum"(%432, %433) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x24x1x128xf32>, tensor<1x24x1xf32>) -> tensor<1x24x1xf32>
    %435 = ttir.empty() : tensor<1x24x1x1xf32>
    %436 = "ttir.reshape"(%434, %435) <{shape = [1 : i32, 24 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x1xf32>, tensor<1x24x1x1xf32>) -> tensor<1x24x1x1xf32>
    %437 = ttir.empty() : tensor<1x24x1x128xf32>
    %438 = "ttir.broadcast"(%436, %437) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<1x24x1x1xf32>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %439 = ttir.empty() : tensor<1x24x1x128xf32>
    %440 = "ttir.div"(%432, %438, %439) : (tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %441 = ttir.empty() : tensor<1x24x1x128xbf16>
    %442 = "ttir.typecast"(%440, %441) <{conservative_folding = false}> : (tensor<1x24x1x128xf32>, tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %443 = ttir.empty() : tensor<24x1x128xbf16>
    %444 = "ttir.reshape"(%442, %443) <{shape = [24 : i32, 1 : i32, 128 : i32]}> : (tensor<1x24x1x128xbf16>, tensor<24x1x128xbf16>) -> tensor<24x1x128xbf16>
    %445 = ttir.empty() : tensor<1x8x1x128x128xbf16>
    %446 = "ttir.reshape"(%368, %445) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x128x128xbf16>, tensor<1x8x1x128x128xbf16>) -> tensor<1x8x1x128x128xbf16>
    %447 = ttir.empty() : tensor<1x8x3x128x128xbf16>
    %448 = "ttir.broadcast"(%446, %447) <{broadcast_dimensions = array<i64: 1, 1, 3, 1, 1>}> : (tensor<1x8x1x128x128xbf16>, tensor<1x8x3x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %449 = ttir.empty() : tensor<24x128x128xbf16>
    %450 = "ttir.reshape"(%448, %449) <{shape = [24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x3x128x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x128x128xbf16>
    %451 = "ttir.dot_general"(%444, %450) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<24x1x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x1x128xbf16>
    %452 = ttir.empty() : tensor<1x3072xbf16>
    %453 = "ttir.reshape"(%451, %452) <{shape = [1 : i32, 3072 : i32]}> : (tensor<24x1x128xbf16>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %454 = "ttir.dot_general"(%453, %arg27) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %455 = ttir.empty() : tensor<1x1x3072xbf16>
    %456 = "ttir.reshape"(%454, %455) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %457 = ttir.empty() : tensor<1x1x3072xbf16>
    %458 = "ttir.add"(%304, %456, %457) : (tensor<1x1x3072xbf16>, tensor<1x1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %459 = ttir.empty() : tensor<3072xf32>
    %460 = "ttir.typecast"(%arg29, %459) <{conservative_folding = false}> : (tensor<3072xbf16>, tensor<3072xf32>) -> tensor<3072xf32>
    %461 = ttir.empty() : tensor<1x1x3072xf32>
    %462 = "ttir.reshape"(%460, %461) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %463 = ttir.empty() : tensor<1x1x3072xf32>
    %464 = "ttir.typecast"(%458, %463) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %465 = ttir.empty() : tensor<1x1x3072xf32>
    %466 = "ttir.pow"(%464, %1, %465) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %467 = ttir.empty() : tensor<1x1xf32>
    %468 = "ttir.sum"(%466, %467) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %469 = ttir.empty() : tensor<1x1xf32>
    %470 = "ttir.multiply"(%468, %0, %469) : (tensor<1x1xf32>, tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %471 = ttir.empty() : tensor<1x1x1xf32>
    %472 = "ttir.reshape"(%470, %471) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %473 = ttir.empty() : tensor<1x1x1xf32>
    %474 = "ttir.add"(%472, %38, %473) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %475 = ttir.empty() : tensor<1x1x1xf32>
    %476 = "ttir.rsqrt"(%474, %475) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %477 = ttir.empty() : tensor<1x1xf32>
    %478 = "ttir.reshape"(%476, %477) <{shape = [1 : i32, 1 : i32]}> : (tensor<1x1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %479 = ttir.empty() : tensor<1x1x1xf32>
    %480 = "ttir.reshape"(%478, %479) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %481 = ttir.empty() : tensor<1x1x3072xf32>
    %482 = "ttir.broadcast"(%480, %481) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x1x1xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %483 = ttir.empty() : tensor<1x1x3072xf32>
    %484 = "ttir.multiply"(%464, %482, %483) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %485 = ttir.empty() : tensor<1x1x3072xbf16>
    %486 = "ttir.typecast"(%484, %485) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %487 = ttir.empty() : tensor<1x1x3072xf32>
    %488 = "ttir.typecast"(%486, %487) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %489 = ttir.empty() : tensor<1x1x3072xf32>
    %490 = "ttir.multiply"(%462, %488, %489) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %491 = ttir.empty() : tensor<1x1x3072xbf16>
    %492 = "ttir.typecast"(%490, %491) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %493 = ttir.empty() : tensor<1x3072xbf16>
    %494 = "ttir.reshape"(%492, %493) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x3072xbf16>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %495 = "ttir.dot_general"(%494, %arg30) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %496 = ttir.empty() : tensor<1x1x8192xbf16>
    %497 = "ttir.reshape"(%495, %496) <{shape = [1 : i32, 1 : i32, 8192 : i32]}> : (tensor<1x8192xbf16>, tensor<1x1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %498 = ttir.empty() : tensor<1x1x8192xf32>
    %499 = "ttir.typecast"(%497, %498) <{conservative_folding = false}> : (tensor<1x1x8192xbf16>, tensor<1x1x8192xf32>) -> tensor<1x1x8192xf32>
    %500 = ttir.empty() : tensor<1x1x8192xbf16>
    %501 = "ttir.sigmoid"(%497, %500) : (tensor<1x1x8192xbf16>, tensor<1x1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %502 = ttir.empty() : tensor<1x1x8192xf32>
    %503 = "ttir.typecast"(%501, %502) <{conservative_folding = false}> : (tensor<1x1x8192xbf16>, tensor<1x1x8192xf32>) -> tensor<1x1x8192xf32>
    %504 = ttir.empty() : tensor<1x1x8192xf32>
    %505 = "ttir.multiply"(%499, %503, %504) : (tensor<1x1x8192xf32>, tensor<1x1x8192xf32>, tensor<1x1x8192xf32>) -> tensor<1x1x8192xf32>
    %506 = ttir.empty() : tensor<1x1x8192xbf16>
    %507 = "ttir.typecast"(%505, %506) <{conservative_folding = false}> : (tensor<1x1x8192xf32>, tensor<1x1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %508 = ttir.empty() : tensor<1x1x8192xf32>
    %509 = "ttir.typecast"(%507, %508) <{conservative_folding = false}> : (tensor<1x1x8192xbf16>, tensor<1x1x8192xf32>) -> tensor<1x1x8192xf32>
    %510 = "ttir.dot_general"(%494, %arg26) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %511 = ttir.empty() : tensor<1x1x8192xbf16>
    %512 = "ttir.reshape"(%510, %511) <{shape = [1 : i32, 1 : i32, 8192 : i32]}> : (tensor<1x8192xbf16>, tensor<1x1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %513 = ttir.empty() : tensor<1x1x8192xf32>
    %514 = "ttir.typecast"(%512, %513) <{conservative_folding = false}> : (tensor<1x1x8192xbf16>, tensor<1x1x8192xf32>) -> tensor<1x1x8192xf32>
    %515 = ttir.empty() : tensor<1x1x8192xf32>
    %516 = "ttir.multiply"(%509, %514, %515) : (tensor<1x1x8192xf32>, tensor<1x1x8192xf32>, tensor<1x1x8192xf32>) -> tensor<1x1x8192xf32>
    %517 = ttir.empty() : tensor<1x1x8192xbf16>
    %518 = "ttir.typecast"(%516, %517) <{conservative_folding = false}> : (tensor<1x1x8192xf32>, tensor<1x1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %519 = ttir.empty() : tensor<1x8192xbf16>
    %520 = "ttir.reshape"(%518, %519) <{shape = [1 : i32, 8192 : i32]}> : (tensor<1x1x8192xbf16>, tensor<1x8192xbf16>) -> tensor<1x8192xbf16>
    %521 = "ttir.dot_general"(%520, %arg25) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<1x3072xbf16>
    %522 = ttir.empty() : tensor<1x1x3072xbf16>
    %523 = "ttir.reshape"(%521, %522) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %524 = ttir.empty() : tensor<1x1x3072xbf16>
    %525 = "ttir.add"(%458, %523, %524) : (tensor<1x1x3072xbf16>, tensor<1x1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %526 = ttir.empty() : tensor<1x1x3072xf32>
    %527 = "ttir.typecast"(%525, %526) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %528 = ttir.empty() : tensor<1x1x3072xf32>
    %529 = "ttir.pow"(%527, %1, %528) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %530 = ttir.empty() : tensor<1x1xf32>
    %531 = "ttir.sum"(%529, %530) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %532 = ttir.empty() : tensor<1x1xf32>
    %533 = "ttir.multiply"(%531, %0, %532) : (tensor<1x1xf32>, tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %534 = ttir.empty() : tensor<1x1x1xf32>
    %535 = "ttir.reshape"(%533, %534) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %536 = ttir.empty() : tensor<1x1x1xf32>
    %537 = "ttir.add"(%535, %38, %536) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %538 = ttir.empty() : tensor<1x1x1xf32>
    %539 = "ttir.rsqrt"(%537, %538) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %540 = ttir.empty() : tensor<1x1xf32>
    %541 = "ttir.reshape"(%539, %540) <{shape = [1 : i32, 1 : i32]}> : (tensor<1x1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %542 = ttir.empty() : tensor<1x1x1xf32>
    %543 = "ttir.reshape"(%541, %542) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %544 = ttir.empty() : tensor<1x1x3072xf32>
    %545 = "ttir.broadcast"(%543, %544) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x1x1xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %546 = ttir.empty() : tensor<1x1x3072xf32>
    %547 = "ttir.multiply"(%527, %545, %546) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %548 = ttir.empty() : tensor<1x1x3072xbf16>
    %549 = "ttir.typecast"(%547, %548) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %550 = ttir.empty() : tensor<1x1x3072xf32>
    %551 = "ttir.typecast"(%549, %550) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %552 = ttir.empty() : tensor<1x1x3072xf32>
    %553 = "ttir.multiply"(%372, %551, %552) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %554 = ttir.empty() : tensor<1x1x3072xbf16>
    %555 = "ttir.typecast"(%553, %554) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %556 = ttir.empty() : tensor<1x3072xbf16>
    %557 = "ttir.reshape"(%555, %556) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x3072xbf16>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %558 = "ttir.dot_general"(%557, %arg24) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<1x128256xbf16>
    %559 = ttir.empty() : tensor<1x1x128256xbf16>
    %560 = "ttir.reshape"(%558, %559) <{shape = [1 : i32, 1 : i32, 128256 : i32]}> : (tensor<1x128256xbf16>, tensor<1x1x128256xbf16>) -> tensor<1x1x128256xbf16>
    return %124, %129, %363, %368, %558, %560 : tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x128256xbf16>, tensor<1x1x128256xbf16>
  }
}
2025-08-12 15:39:40.364 (  40.807s) [        C545D1C0]      module_builder.cc:506   WARN| `mhlo.num_partitions` attribute not found, assuming default number of partitions: 1
2025-08-12 15:39:40.364 (  40.807s) [        C545D1C0]      module_builder.cc:520   WARN| `mhlo.num_replicas` attribute not found, assuming default number of replicas: 1
2025-08-12 15:39:40.364 (  40.807s) [        C545D1C0]      module_builder.cc:528   WARN| Num replicas and num partitions are not set, inferring the number of devices from mesh shape
TTIRFusingPass: Starting pass on operation
TTIRFusingPass: Successfully applied Conv2dTagWeights patterns
CacheFillUpdatePattern: Attempting to match ScatterOp at loc("scatter.109")
getCacheUpdatePositions: Checking ScatterOp at loc("scatter.109")
CacheFillUpdatePattern: Successfully fusing ScatterOp into UpdateCacheOp
CacheFillUpdatePattern: Attempting to match ScatterOp at loc("scatter.131")
getCacheUpdatePositions: Checking ScatterOp at loc("scatter.131")
CacheFillUpdatePattern: Successfully fusing ScatterOp into UpdateCacheOp
CacheFillUpdatePattern: Attempting to match ScatterOp at loc("scatter.375")
getCacheUpdatePositions: Checking ScatterOp at loc("scatter.375")
CacheFillUpdatePattern: Successfully fusing ScatterOp into UpdateCacheOp
CacheFillUpdatePattern: Attempting to match ScatterOp at loc("scatter.397")
getCacheUpdatePositions: Checking ScatterOp at loc("scatter.397")
CacheFillUpdatePattern: Successfully fusing ScatterOp into UpdateCacheOp
TTIRFusingPass: Successfully applied all fusion patterns
TTIRFusingPass: Completed pass on operation
TTIRFusingPass: Starting pass on operation
TTIRFusingPass: Successfully applied Conv2dTagWeights patterns
TTIRFusingPass: Successfully applied all fusion patterns
TTIRFusingPass: Completed pass on operation
TTIRFusingPass: Starting pass on operation
TTIRFusingPass: Successfully applied Conv2dTagWeights patterns
TTIRFusingPass: Successfully applied all fusion patterns
TTIRFusingPass: Completed pass on operation
2025-08-12 15:39:40.447 (  40.890s) [        C545D1C0]      module_builder.cc:588      1| TTNN Module:
module @SyncTensorsGraph.605 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.605 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073184896, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073193216, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = 1x1, chipIds = [0]>
      func.func @main(%arg0: tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg2: tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg3: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg4: tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg5: tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg6: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg7: tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg8: tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg9: tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg10: tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg11: tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg12: tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg13: tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg14: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg15: tensor<1x1x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg16: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg17: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg18: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg19: tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg20: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg21: tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg22: tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg23: tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg24: tensor<3072x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg25: tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg26: tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg27: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg28: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg29: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg30: tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg31: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) -> (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device
        "ttnn.deallocate"(%arg7) <{force = false}> : (tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 3.25520843E-4 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x1x3072>}> : (!ttnn.device) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %3 = "ttnn.typecast"(%arg6) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg6) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %5 = "ttnn.typecast"(%arg4) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %6 = "ttnn.reshape"(%5) <{shape = [1 : i32]}> : (tensor<1x1xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x1xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %7 = "ttnn.from_device"(%6) : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %8 = "ttnn.to_layout"(%7) <{layout = #ttnn.layout<row_major>}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> ()
        %9 = "ttnn.to_device"(%8, %0) <{memory_config = #ttnn.memory_config<<dram>, <interleaved>>}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<system_memory>>>>) -> ()
        %10 = "ttnn.embedding"(%9, %arg5) : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%arg5) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %11 = "ttnn.typecast"(%10) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %12 = "ttnn.reshape"(%11) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %13 = "ttnn.pow"(%12, %2) : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %14 = "ttnn.sum"(%13) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %15 = "ttnn.multiply"(%14, %1) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %16 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %17 = "ttnn.add"(%15, %16) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %18 = "ttnn.rsqrt"(%17) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %19 = "ttnn.multiply"(%11, %18) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %20 = "ttnn.multiply"(%4, %19) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %21 = "ttnn.typecast"(%20) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %22 = "ttnn.matmul"(%21, %arg2) <{transpose_a = false, transpose_b = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %23 = "ttnn.reshape"(%22) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%22) <{force = false}> : (tensor<1x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %24 = "ttnn.typecast"(%23) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %25 = "ttnn.typecast"(%arg0) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %26 = "ttnn.reshape"(%25) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%25) <{force = false}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %27 = "ttnn.matmul"(%arg1, %26) <{transpose_a = false, transpose_b = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%26) <{force = false}> : (tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %28 = "ttnn.reshape"(%27) <{shape = [1 : i32, 1 : i32, 1 : i32, 64 : i32]}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %29 = "ttnn.reshape"(%27) <{shape = [1 : i32, 1 : i32, 1 : i32, 64 : i32]}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%27) <{force = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %30 = "ttnn.concat"(%28, %29) <{dim = 3 : si32}> : (tensor<1x1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%29) <{force = false}> : (tensor<1x1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%28) <{force = false}> : (tensor<1x1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %31 = "ttnn.cos"(%30) : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %32 = "ttnn.multiply"(%24, %31) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%24) <{force = false}> : (tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %33 = "ttnn.typecast"(%32) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%32) <{force = false}> : (tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %34 = "ttnn.slice"(%23) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 8 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %35 = "ttnn.neg"(%34) : (tensor<1x8x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%34) <{force = false}> : (tensor<1x8x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %36 = "ttnn.slice"(%23) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%23) <{force = false}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %37 = "ttnn.concat"(%35, %36) <{dim = 3 : si32}> : (tensor<1x8x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%36) <{force = false}> : (tensor<1x8x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%35) <{force = false}> : (tensor<1x8x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %38 = "ttnn.typecast"(%37) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%37) <{force = false}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %39 = "ttnn.sin"(%30) : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%30) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %40 = "ttnn.multiply"(%38, %39) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%38) <{force = false}> : (tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %41 = "ttnn.typecast"(%40) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%40) <{force = false}> : (tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %42 = "ttnn.add"(%33, %41) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%41) <{force = false}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%33) <{force = false}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %43 = "ttnn.typecast"(%arg0) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.update_cache"(%arg8, %42, %43) <{batch_offset = 0 : i32}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%43) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%42) <{force = false}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %44 = "ttnn.matmul"(%21, %arg9) <{transpose_a = false, transpose_b = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg9) <{force = false}> : (tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %45 = "ttnn.reshape"(%44) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%44) <{force = false}> : (tensor<1x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %46 = "ttnn.typecast"(%arg0) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.update_cache"(%arg10, %45, %46) <{batch_offset = 0 : i32}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%46) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%45) <{force = false}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %47 = "ttnn.typecast"(%arg20) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg20) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %48 = "ttnn.reshape"(%47) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%47) <{force = false}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %49 = "ttnn.matmul"(%21, %arg17) <{transpose_a = false, transpose_b = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%21) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%arg17) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %50 = "ttnn.reshape"(%49) <{shape = [1 : i32, 24 : i32, 1 : i32, 128 : i32]}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %51 = "ttnn.typecast"(%49) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%49) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %52 = "ttnn.reshape"(%51) <{shape = [24 : i32, 1 : i32, 128 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%51) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %53 = "ttnn.reshape"(%31) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %54 = "ttnn.multiply"(%52, %53) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%52) <{force = false}> : (tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %55 = "ttnn.typecast"(%54) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%54) <{force = false}> : (tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %56 = "ttnn.slice"(%50) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 24 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %57 = "ttnn.neg"(%56) : (tensor<1x24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%56) <{force = false}> : (tensor<1x24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %58 = "ttnn.reshape"(%57) <{shape = [24 : i32, 1 : i32, 64 : i32]}> : (tensor<1x24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%57) <{force = false}> : (tensor<1x24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %59 = "ttnn.slice"(%50) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 24 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%50) <{force = false}> : (tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %60 = "ttnn.reshape"(%59) <{shape = [24 : i32, 1 : i32, 64 : i32]}> : (tensor<1x24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%59) <{force = false}> : (tensor<1x24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %61 = "ttnn.concat"(%58, %60) <{dim = 2 : si32}> : (tensor<24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%60) <{force = false}> : (tensor<24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%58) <{force = false}> : (tensor<24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %62 = "ttnn.typecast"(%61) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%61) <{force = false}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %63 = "ttnn.reshape"(%39) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %64 = "ttnn.multiply"(%62, %63) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%62) <{force = false}> : (tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %65 = "ttnn.typecast"(%64) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%64) <{force = false}> : (tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %66 = "ttnn.add"(%55, %65) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%65) <{force = false}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%55) <{force = false}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %67 = "ttnn.reshape"(%arg8) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %68 = "ttnn.repeat"(%67) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%67) <{force = false}> : (tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %69 = "ttnn.reshape"(%68) <{shape = [1 : i32, 24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%68) <{force = false}> : (tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %70 = "ttnn.permute"(%69) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%69) <{force = false}> : (tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %71 = "ttnn.reshape"(%70) <{shape = [24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%70) <{force = false}> : (tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %72 = "ttnn.matmul"(%66, %71) <{transpose_a = false, transpose_b = false}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%71) <{force = false}> : (tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%66) <{force = false}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %73 = "ttnn.typecast"(%72) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%72) <{force = false}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %74 = "ttnn.reshape"(%73) <{shape = [1 : i32, 24 : i32, 1 : i32, 128 : i32]}> : (tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%73) <{force = false}> : (tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %75 = "ttnn.reshape"(%arg16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg16) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %76 = "ttnn.multiply"(%74, %75) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%74) <{force = false}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %77 = "ttnn.typecast"(%76) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%76) <{force = false}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %78 = "ttnn.add"(%77, %arg15) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%77) <{force = false}> : (tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %79 = "ttnn.typecast"(%78) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%78) <{force = false}> : (tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %80 = "ttnn.max"(%79) <{dim_arg = [3 : i32], keep_dim = true}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %81 = "ttnn.neg"(%80) : (tensor<1x24x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%80) <{force = false}> : (tensor<1x24x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %82 = "ttnn.add"(%79, %81) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x24x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%81) <{force = false}> : (tensor<1x24x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%79) <{force = false}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %83 = "ttnn.softmax"(%82) <{dimension = 3 : si32}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%82) <{force = false}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %84 = "ttnn.typecast"(%83) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%83) <{force = false}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %85 = "ttnn.reshape"(%84) <{shape = [24 : i32, 1 : i32, 128 : i32]}> : (tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%84) <{force = false}> : (tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %86 = "ttnn.reshape"(%arg10) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %87 = "ttnn.repeat"(%86) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%86) <{force = false}> : (tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %88 = "ttnn.reshape"(%87) <{shape = [24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%87) <{force = false}> : (tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %89 = "ttnn.matmul"(%85, %88) <{transpose_a = false, transpose_b = false}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%88) <{force = false}> : (tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%85) <{force = false}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %90 = "ttnn.reshape"(%89) <{shape = [1 : i32, 3072 : i32]}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%89) <{force = false}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %91 = "ttnn.matmul"(%90, %arg14) <{transpose_a = false, transpose_b = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%90) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%arg14) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %92 = "ttnn.add"(%10, %91) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%91) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %93 = "ttnn.typecast"(%arg18) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg18) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %94 = "ttnn.reshape"(%93) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%93) <{force = false}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %95 = "ttnn.typecast"(%92) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %96 = "ttnn.reshape"(%95) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %97 = "ttnn.pow"(%96, %2) : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%96) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %98 = "ttnn.sum"(%97) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%97) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %99 = "ttnn.multiply"(%98, %1) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%98) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %100 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %101 = "ttnn.add"(%99, %100) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%100) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%99) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %102 = "ttnn.rsqrt"(%101) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%101) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %103 = "ttnn.multiply"(%95, %102) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%102) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%95) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %104 = "ttnn.multiply"(%94, %103) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%103) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%94) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %105 = "ttnn.typecast"(%104) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%104) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %106 = "ttnn.matmul"(%105, %arg19) <{transpose_a = false, transpose_b = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg19) <{force = false}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %107 = "ttnn.typecast"(%106) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %108 = "ttnn.sigmoid"(%106) : (tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%106) <{force = false}> : (tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %109 = "ttnn.typecast"(%108) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%108) <{force = false}> : (tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %110 = "ttnn.multiply"(%107, %109) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%109) <{force = false}> : (tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%107) <{force = false}> : (tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %111 = "ttnn.matmul"(%105, %arg13) <{transpose_a = false, transpose_b = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%105) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%arg13) <{force = false}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %112 = "ttnn.typecast"(%111) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%111) <{force = false}> : (tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %113 = "ttnn.multiply"(%110, %112) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%112) <{force = false}> : (tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%110) <{force = false}> : (tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %114 = "ttnn.typecast"(%113) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%113) <{force = false}> : (tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %115 = "ttnn.matmul"(%114, %arg12) <{transpose_a = false, transpose_b = false}> : (tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%114) <{force = false}> : (tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%arg12) <{force = false}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %116 = "ttnn.add"(%92, %115) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%115) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%92) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %117 = "ttnn.typecast"(%116) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %118 = "ttnn.reshape"(%117) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %119 = "ttnn.pow"(%118, %2) : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%118) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %120 = "ttnn.sum"(%119) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%119) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %121 = "ttnn.multiply"(%120, %1) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%120) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %122 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %123 = "ttnn.add"(%121, %122) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%122) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%121) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %124 = "ttnn.rsqrt"(%123) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%123) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %125 = "ttnn.multiply"(%117, %124) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%124) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%117) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %126 = "ttnn.multiply"(%48, %125) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%125) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%48) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %127 = "ttnn.typecast"(%126) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%126) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %128 = "ttnn.matmul"(%127, %arg11) <{transpose_a = false, transpose_b = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg11) <{force = false}> : (tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %129 = "ttnn.reshape"(%128) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%128) <{force = false}> : (tensor<1x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %130 = "ttnn.typecast"(%129) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %131 = "ttnn.multiply"(%130, %31) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%130) <{force = false}> : (tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%31) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %132 = "ttnn.typecast"(%131) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%131) <{force = false}> : (tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %133 = "ttnn.slice"(%129) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 8 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %134 = "ttnn.neg"(%133) : (tensor<1x8x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%133) <{force = false}> : (tensor<1x8x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %135 = "ttnn.slice"(%129) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%129) <{force = false}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %136 = "ttnn.concat"(%134, %135) <{dim = 3 : si32}> : (tensor<1x8x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%135) <{force = false}> : (tensor<1x8x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%134) <{force = false}> : (tensor<1x8x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %137 = "ttnn.typecast"(%136) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%136) <{force = false}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %138 = "ttnn.multiply"(%137, %39) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%137) <{force = false}> : (tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%39) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %139 = "ttnn.typecast"(%138) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%138) <{force = false}> : (tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %140 = "ttnn.add"(%132, %139) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%139) <{force = false}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%132) <{force = false}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %141 = "ttnn.typecast"(%arg0) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.update_cache"(%arg21, %140, %141) <{batch_offset = 0 : i32}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%141) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%140) <{force = false}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %142 = "ttnn.matmul"(%127, %arg22) <{transpose_a = false, transpose_b = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg22) <{force = false}> : (tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %143 = "ttnn.reshape"(%142) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%142) <{force = false}> : (tensor<1x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %144 = "ttnn.typecast"(%arg0) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.update_cache"(%arg23, %143, %144) <{batch_offset = 0 : i32}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%144) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%143) <{force = false}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %145 = "ttnn.typecast"(%arg31) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg31) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %146 = "ttnn.reshape"(%145) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%145) <{force = false}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %147 = "ttnn.matmul"(%127, %arg28) <{transpose_a = false, transpose_b = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%127) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%arg28) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %148 = "ttnn.reshape"(%147) <{shape = [1 : i32, 24 : i32, 1 : i32, 128 : i32]}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %149 = "ttnn.typecast"(%147) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%147) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %150 = "ttnn.reshape"(%149) <{shape = [24 : i32, 1 : i32, 128 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%149) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %151 = "ttnn.multiply"(%150, %53) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%150) <{force = false}> : (tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%53) <{force = false}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %152 = "ttnn.typecast"(%151) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%151) <{force = false}> : (tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %153 = "ttnn.slice"(%148) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 24 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %154 = "ttnn.neg"(%153) : (tensor<1x24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%153) <{force = false}> : (tensor<1x24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %155 = "ttnn.reshape"(%154) <{shape = [24 : i32, 1 : i32, 64 : i32]}> : (tensor<1x24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%154) <{force = false}> : (tensor<1x24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %156 = "ttnn.slice"(%148) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 24 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%148) <{force = false}> : (tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %157 = "ttnn.reshape"(%156) <{shape = [24 : i32, 1 : i32, 64 : i32]}> : (tensor<1x24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%156) <{force = false}> : (tensor<1x24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %158 = "ttnn.concat"(%155, %157) <{dim = 2 : si32}> : (tensor<24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%157) <{force = false}> : (tensor<24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%155) <{force = false}> : (tensor<24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %159 = "ttnn.typecast"(%158) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%158) <{force = false}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %160 = "ttnn.multiply"(%159, %63) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%159) <{force = false}> : (tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%63) <{force = false}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %161 = "ttnn.typecast"(%160) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%160) <{force = false}> : (tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %162 = "ttnn.add"(%152, %161) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%161) <{force = false}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%152) <{force = false}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %163 = "ttnn.reshape"(%arg21) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %164 = "ttnn.repeat"(%163) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%163) <{force = false}> : (tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %165 = "ttnn.reshape"(%164) <{shape = [1 : i32, 24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%164) <{force = false}> : (tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %166 = "ttnn.permute"(%165) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%165) <{force = false}> : (tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %167 = "ttnn.reshape"(%166) <{shape = [24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%166) <{force = false}> : (tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %168 = "ttnn.matmul"(%162, %167) <{transpose_a = false, transpose_b = false}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%167) <{force = false}> : (tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%162) <{force = false}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %169 = "ttnn.typecast"(%168) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%168) <{force = false}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %170 = "ttnn.reshape"(%169) <{shape = [1 : i32, 24 : i32, 1 : i32, 128 : i32]}> : (tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%169) <{force = false}> : (tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %171 = "ttnn.multiply"(%170, %75) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%170) <{force = false}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%75) <{force = false}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %172 = "ttnn.typecast"(%171) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%171) <{force = false}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %173 = "ttnn.add"(%172, %arg15) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%172) <{force = false}> : (tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%arg15) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %174 = "ttnn.typecast"(%173) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%173) <{force = false}> : (tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %175 = "ttnn.max"(%174) <{dim_arg = [3 : i32], keep_dim = true}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %176 = "ttnn.neg"(%175) : (tensor<1x24x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%175) <{force = false}> : (tensor<1x24x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %177 = "ttnn.add"(%174, %176) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x24x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%176) <{force = false}> : (tensor<1x24x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%174) <{force = false}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %178 = "ttnn.softmax"(%177) <{dimension = 3 : si32}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%177) <{force = false}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %179 = "ttnn.typecast"(%178) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%178) <{force = false}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %180 = "ttnn.reshape"(%179) <{shape = [24 : i32, 1 : i32, 128 : i32]}> : (tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%179) <{force = false}> : (tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %181 = "ttnn.reshape"(%arg23) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %182 = "ttnn.repeat"(%181) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%181) <{force = false}> : (tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %183 = "ttnn.reshape"(%182) <{shape = [24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%182) <{force = false}> : (tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %184 = "ttnn.matmul"(%180, %183) <{transpose_a = false, transpose_b = false}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%183) <{force = false}> : (tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%180) <{force = false}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %185 = "ttnn.reshape"(%184) <{shape = [1 : i32, 3072 : i32]}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%184) <{force = false}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %186 = "ttnn.matmul"(%185, %arg27) <{transpose_a = false, transpose_b = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%185) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%arg27) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %187 = "ttnn.add"(%116, %186) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%186) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%116) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %188 = "ttnn.typecast"(%arg29) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg29) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %189 = "ttnn.reshape"(%188) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%188) <{force = false}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %190 = "ttnn.typecast"(%187) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %191 = "ttnn.reshape"(%190) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %192 = "ttnn.pow"(%191, %2) : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%191) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %193 = "ttnn.sum"(%192) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%192) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %194 = "ttnn.multiply"(%193, %1) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%193) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %195 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %196 = "ttnn.add"(%194, %195) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%195) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%194) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %197 = "ttnn.rsqrt"(%196) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%196) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %198 = "ttnn.multiply"(%190, %197) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%197) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%190) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %199 = "ttnn.multiply"(%189, %198) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%198) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%189) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %200 = "ttnn.typecast"(%199) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%199) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %201 = "ttnn.matmul"(%200, %arg30) <{transpose_a = false, transpose_b = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg30) <{force = false}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %202 = "ttnn.typecast"(%201) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %203 = "ttnn.sigmoid"(%201) : (tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%201) <{force = false}> : (tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %204 = "ttnn.typecast"(%203) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%203) <{force = false}> : (tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %205 = "ttnn.multiply"(%202, %204) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%204) <{force = false}> : (tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%202) <{force = false}> : (tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %206 = "ttnn.matmul"(%200, %arg26) <{transpose_a = false, transpose_b = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%200) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%arg26) <{force = false}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %207 = "ttnn.typecast"(%206) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%206) <{force = false}> : (tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %208 = "ttnn.multiply"(%205, %207) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%207) <{force = false}> : (tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%205) <{force = false}> : (tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %209 = "ttnn.typecast"(%208) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%208) <{force = false}> : (tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %210 = "ttnn.matmul"(%209, %arg25) <{transpose_a = false, transpose_b = false}> : (tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%209) <{force = false}> : (tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%arg25) <{force = false}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %211 = "ttnn.add"(%187, %210) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%210) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%187) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %212 = "ttnn.typecast"(%211) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%211) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %213 = "ttnn.reshape"(%212) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %214 = "ttnn.pow"(%213, %2) : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%213) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %215 = "ttnn.sum"(%214) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%214) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %216 = "ttnn.multiply"(%215, %1) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%215) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %217 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %218 = "ttnn.add"(%216, %217) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%217) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%216) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %219 = "ttnn.rsqrt"(%218) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%218) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %220 = "ttnn.multiply"(%212, %219) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%219) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%212) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %221 = "ttnn.multiply"(%146, %220) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%220) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%146) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %222 = "ttnn.typecast"(%221) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%221) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %223 = "ttnn.matmul"(%222, %arg24) <{transpose_a = false, transpose_b = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%222) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%arg24) <{force = false}> : (tensor<3072x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %224 = "ttnn.reshape"(%223) <{shape = [1 : i32, 1 : i32, 128256 : i32]}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %arg8, %arg10, %arg21, %arg23, %223, %224 : tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
    }
  }
}
2025-08-12 15:39:40.484 (  40.927s) [        C545D1C0]loaded_executable_insta:98       1| [LIFECYCLE] LoadedExecutableInstance constructor - instance created: 0x565432da9d50
2025-08-12 15:39:40.484 (  40.927s) [        C545D1C0]loaded_executable_insta:516      1| LoadedExecutableInstance::PJRT_LoadedExecutable_GetExecutable
2025-08-12 15:39:40.484 (  40.927s) [        C545D1C0]loaded_executable_insta:535      1| LoadedExecutableInstance::PJRT_LoadedExecutable_AddressableDevices
2025-08-12 15:39:40.485 (  40.928s) [        C545D1C0]              stubs.inc:76    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2025-08-12 15:39:40.485 (  40.928s) [        C545D1C0]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-08-12 15:39:40.485 (  40.928s) [        C545D1C0]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-08-12 15:39:40.485 (  40.928s) [        C545D1C0]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-08-12 15:39:40.485 (  40.928s) [        C545D1C0] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-08-12 15:39:40.485 (  40.928s) [        C545D1C0] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-08-12 15:39:40.498 (  40.941s) [        C545D1C0] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-08-12 15:39:40.498 (  40.941s) [        C545D1C0] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640] executable_instance.cc:138      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]loaded_executable_insta:571      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]loaded_executable_insta:95       1| LoadedExecutableInstance::Execute
2025-08-12 15:39:40.509 (  40.952s) [        CAFFD640]loaded_executable_insta:125      1| [DEVICE] Runtime device already opened, reusing existing device
2025-08-12 15:39:40.510 (  40.952s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442e56f80 (arg 0)
2025-08-12 15:39:40.510 (  40.953s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56541956fb70 (arg 1)
2025-08-12 15:39:40.510 (  40.953s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae08a20 (arg 2)
2025-08-12 15:39:40.512 (  40.954s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442df5210 (arg 3)
2025-08-12 15:39:40.512 (  40.954s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565445829370 (arg 4)
2025-08-12 15:39:40.512 (  40.954s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae22a60 (arg 5)
2025-08-12 15:39:40.595 (  41.038s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565433e38450 (arg 6)
2025-08-12 15:39:40.596 (  41.039s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ff46020 (arg 7)
2025-08-12 15:39:40.596 (  41.039s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae149e0 (arg 8)
2025-08-12 15:39:40.597 (  41.039s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae1a5b0 (arg 9)
2025-08-12 15:39:40.605 (  41.048s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442b35e90 (arg 10)
2025-08-12 15:39:40.605 (  41.048s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544d19c270 (arg 11)
2025-08-12 15:39:40.607 (  41.049s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565419544220 (arg 12)
2025-08-12 15:39:40.613 (  41.055s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ff47ba0 (arg 13)
2025-08-12 15:39:40.621 (  41.064s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x5654070aebb0 (arg 14)
2025-08-12 15:39:40.625 (  41.068s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543feae890 (arg 15)
2025-08-12 15:39:40.625 (  41.068s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544d2685d0 (arg 16)
2025-08-12 15:39:40.626 (  41.068s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae3cb00 (arg 17)
2025-08-12 15:39:40.629 (  41.071s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544438c610 (arg 18)
2025-08-12 15:39:40.629 (  41.072s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544d22fa40 (arg 19)
2025-08-12 15:39:40.638 (  41.080s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c218220 (arg 20)
2025-08-12 15:39:40.638 (  41.081s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae16e30 (arg 21)
2025-08-12 15:39:40.639 (  41.082s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae84780 (arg 22)
2025-08-12 15:39:40.640 (  41.083s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae0b740 (arg 23)
2025-08-12 15:39:40.641 (  41.083s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x5654447af160 (arg 24)
2025-08-12 15:39:40.727 (  41.170s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544584e8c0 (arg 25)
2025-08-12 15:39:40.741 (  41.184s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442b58f60 (arg 26)
2025-08-12 15:39:40.750 (  41.192s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae7e8c0 (arg 27)
2025-08-12 15:39:40.753 (  41.196s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544572c160 (arg 28)
2025-08-12 15:39:40.757 (  41.199s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c2222e0 (arg 29)
2025-08-12 15:39:40.758 (  41.200s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x5654447c4260 (arg 30)
2025-08-12 15:39:40.766 (  41.209s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56541967fae0 (arg 31)
2025-08-12 15:39:44.952 (  45.395s) [        CAFFD640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:44.952 (  45.395s) [        CAFFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:44.952 (  45.395s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:44.952 (  45.395s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:44.952 (  45.395s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:44.952 (  45.395s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:44.952 (  45.395s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:44.952 (  45.395s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:44.952 (  45.395s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:44.952 (  45.395s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:44.952 (  45.395s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:44.952 (  45.395s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:44.952 (  45.395s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:44.952 (  45.395s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:44.952 (  45.395s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:44.952 (  45.395s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:44.952 (  45.395s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:44.952 (  45.395s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:44.952 (  45.395s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:44.952 (  45.395s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:44.952 (  45.395s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:44.952 (  45.395s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:44.952 (  45.395s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:44.952 (  45.395s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:44.952 (  45.395s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:44.952 (  45.395s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:44.952 (  45.395s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:44.953 (  45.396s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:44.953 (  45.396s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:44.953 (  45.396s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:44.953 (  45.396s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.954 (  45.396s) [        C545D1C0]     buffer_instance.cc:454      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-08-12 15:39:44.954 (  45.396s) [        C545D1C0]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:44.954 (  45.396s) [        C545D1C0]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:44.954 (  45.397s) [        C545D1C0]     buffer_instance.cc:476      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-08-12 15:39:44.954 (  45.397s) [        C545D1C0]     buffer_instance.cc:497      1| [BUFFER_TO_HOST] Source buffer shape: [1, 1, 128256], data_type: 13, required_size: 256512 bytes
2025-08-12 15:39:44.954 (  45.397s) [        C545D1C0]     buffer_instance.cc:508      1| [BUFFER_TO_HOST] Destination buffer: dst_size=256512 bytes, dst_ptr=0x565442b9ad00
2025-08-12 15:39:44.954 (  45.397s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:44.955 (  45.397s) [        121D0640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:44.955 (  45.398s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:44.955 (  45.398s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
alink2025-08-12 15:39:44.957 (  45.400s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:44.957 (  45.400s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:44.957 (  45.400s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:44.957 (  45.400s) [        C545D1C0]     buffer_instance.cc:213      1| [BUFFER] Creating OWNED host tensor with shape [1, 1] (semantics: other, supported_dtype: false)
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-08-12 15:39:44.957 (  45.400s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:44.957 (  45.400s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:44.957 (  45.400s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:44.957 (  45.400s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:44.957 (  45.400s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:44.957 (  45.400s) [        C545D1C0]     buffer_instance.cc:213      1| [BUFFER] Creating OWNED host tensor with shape [1] (semantics: other, supported_dtype: false)
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-08-12 15:39:44.957 (  45.400s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:44.957 (  45.400s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:44.957 (  45.400s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:44.957 (  45.400s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:44.957 (  45.400s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:44.957 (  45.400s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1, 1, 1, 128] (semantics: ZeroCopy/other)
2025-08-12 15:39:44.957 (  45.400s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:44.957 (  45.400s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:44.957 (  45.400s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:44.957 (  45.400s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:44.957 (  45.400s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.964 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
Tensor id via xlac: 423 with shape torch.Size([3072])
Tensor id via xlac: 424 with shape torch.Size([3072])
Tensor id via xlac: 425 with shape torch.Size([3072])
Tensor id via xlac: 426 with shape torch.Size([3072])
Tensor id via xlac: 427 with shape torch.Size([3072])
Tensor id via xlac: 428 with shape torch.Size([128256, 3072])
Tensor id via xlac: 429 with shape torch.Size([1, 64, 1])
Tensor id via xlac: 430 with shape torch.Size([3072, 3072])
Tensor id via xlac: 431 with shape torch.Size([3072, 1024])
Tensor id via xlac: 432 with shape torch.Size([3072, 1024])
Tensor id via xlac: 433 with shape torch.Size([3072, 3072])
Tensor id via xlac: 434 with shape torch.Size([3072, 8192])
Tensor id via xlac: 435 with shape torch.Size([3072, 8192])
Tensor id via xlac: 436 with shape torch.Size([8192, 3072])
Tensor id via xlac: 437 with shape torch.Size([3072, 3072])
Tensor id via xlac: 438 with shape torch.Size([3072, 1024])
Tensor id via xlac: 439 with shape torch.Size([3072, 1024])
Tensor id via xlac: 440 with shape torch.Size([3072, 3072])
Tensor id via xlac: 441 with shape torch.Size([3072, 8192])
Tensor id via xlac: 442 with shape torch.Size([3072, 8192])
Tensor id via xlac: 443 with shape torch.Size([8192, 3072])
Tensor id via xlac: 444 with shape torch.Size([3072, 128256])
Tensor id via xlac: 445 with shape torch.Size([3072, 3072])
Tensor id via xlac: 446 with shape torch.Size([1024, 3072])
Tensor id via xlac: 447 with shape torch.Size([1024, 3072])
Tensor id via xlac: 448 with shape torch.Size([3072, 3072])
Tensor id via xlac: 449 with shape torch.Size([8192, 3072])
Tensor id via xlac: 450 with shape torch.Size([8192, 3072])
Tensor id via xlac: 451 with shape torch.Size([3072, 8192])
Tensor id via xlac: 452 with shape torch.Size([3072, 3072])
Tensor id via xlac: 453 with shape torch.Size([1024, 3072])
Tensor id via xlac: 454 with shape torch.Size([1024, 3072])
Tensor id via xlac: 455 with shape torch.Size([3072, 3072])
Tensor id via xlac: 456 with shape torch.Size([8192, 3072])
Tensor id via xlac: 457 with shape torch.Size([8192, 3072])
Tensor id via xlac: 458 with shape torch.Size([3072, 8192])
Tensor id via xlac: 459 with shape torch.Size([128256, 3072])
Tensor id via xlac: 460 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 461 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 462 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 463 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 464 with shape torch.Size([64])
Tensor id via xlac: 843 with shape torch.Size([1, 1])
Tensor id via xlac: 844 with shape torch.Size([1])
Tensor id via xlac: 845 with shape torch.Size([1, 1, 1, 128])
[XLA Cache] Previously seen inputs: ['input_0_shape_[3072]_dtype_torch.bfloat16 (idx=0)', 'input_1_shape_[3072]_dtype_torch.bfloat16 (idx=1)', 'input_2_shape_[3072]_dtype_torch.bfloat16 (idx=2)', 'input_3_shape_[3072]_dtype_torch.bfloat16 (idx=3)', 'input_4_shape_[3072]_dtype_torch.bfloat16 (idx=4)', 'input_5_shape_[128256, 3072]_dtype_torch.bfloat16 (idx=5)', 'input_6_shape_[1, 64, 1]_dtype_torch.float32 (idx=6)', 'input_7_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=7)', 'input_8_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=8)', 'input_9_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=9)', 'input_10_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=10)', 'input_11_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=11)', 'input_12_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=12)', 'input_13_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=13)', 'input_14_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=14)', 'input_15_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=15)', 'input_16_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=16)', 'input_17_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=17)', 'input_18_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=18)', 'input_19_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=19)', 'input_20_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=20)', 'input_21_shape_[3072, 128256]_dtype_torch.bfloat16 (idx=21)', 'input_22_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=22)', 'input_23_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=23)', 'input_24_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=24)', 'input_25_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=25)', 'input_26_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=26)', 'input_27_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=27)', 'input_28_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=28)', 'input_29_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=29)', 'input_30_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=30)', 'input_31_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=31)', 'input_32_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=32)', 'input_33_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=33)', 'input_34_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=34)', 'input_35_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=35)', 'input_36_shape_[128256, 3072]_dtype_torch.bfloat16 (idx=36)', 'input_37_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=37)', 'input_38_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=38)', 'input_39_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=39)', 'input_40_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=40)', 'input_41_shape_[64]_dtype_torch.float32 (idx=41)']
[XLA Cache] New inputs: ['input_42_shape_[1, 1]_dtype_torch.int64 (idx=42)', 'input_43_shape_[1]_dtype_torch.int64 (idx=43)', 'input_44_shape_[1, 1, 1, 128]_dtype_torch.bfloat16 (idx=44)']
[XLA Cache] Total cache size: 48 unique tensors
Hlo input positions pre normalization [444, 1173, 443, 442, 440, 439, 436, 435, 433, 432, 843, 459, 423, 844, -1, 461, 845, 1111, 429, 431, 460, 430, 424, 434, 425, 463, 438, 462, 437, 426, 441, 427]
Hlo input positions post normalization [445, 1174, 444, 443, 441, 440, 437, 436, 434, 433, 844, 460, 424, 845, 0, 462, 846, 1112, 430, 432, 461, 431, 425, 435, 426, 464, 439, 463, 438, 427, 442, 428]
match key in_spec.target L__self___model_layers__modules__0___input_layernorm_weight with ID 139658584088448 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_layers__modules__0___post_attention_layernorm_weight with ID 139658589162128 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_layers__modules__1___input_layernorm_weight with ID 139658589165728 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_layers__modules__1___post_attention_layernorm_weight with ID 139658581142368 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_norm_weight with ID 139658581024640 and kind InputKind.PARAMETER
match key in_spec.target L__self___lm_head.weight with ID 139658584091808 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.0 with ID 139654952040096 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.1 with ID 139654952040496 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.2 with ID 139654952042416 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.3 with ID 139654952041616 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.4 with ID 139654952040336 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.5 with ID 139654952041056 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.6 with ID 139654952040816 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.7 with ID 139654952040576 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.8 with ID 139654952038416 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.9 with ID 139654952041216 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.10 with ID 139654952030496 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.11 with ID 139654952040656 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.12 with ID 139654952039776 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.13 with ID 139654952041696 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.14 with ID 139654952040976 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.15 with ID 139654952041856 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_q_proj.weight with ID 139658589160768 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_k_proj.weight with ID 139658589162688 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_v_proj.weight with ID 139658589173008 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_o_proj.weight with ID 139658589164768 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___mlp_gate_proj.weight with ID 139658584087008 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___mlp_up_proj.weight with ID 139658589172608 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___mlp_down_proj.weight with ID 139658584095888 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_q_proj.weight with ID 139658581139488 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_k_proj.weight with ID 139658581145168 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_v_proj.weight with ID 139658581134368 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_o_proj.weight with ID 139658581144048 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___mlp_gate_proj.weight with ID 139658589162288 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___mlp_up_proj.weight with ID 139658589171488 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___mlp_down_proj.weight with ID 139658589163008 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___lm_head.weight with ID 139658584091808 and kind InputKind.PARAMETER
match key in_spec.target kwargs____past_key_values___key_cache_0 with ID 139659373973792 and kind InputKind.BUFFER
match key in_spec.target kwargs____past_key_values___value_cache_0 with ID 139659373983872 and kind InputKind.BUFFER
match key in_spec.target kwargs____past_key_values___key_cache_1 with ID 139659373976192 and kind InputKind.BUFFER
match key in_spec.target kwargs____past_key_values___value_cache_1 with ID 139659373976432 and kind InputKind.BUFFER
match key in_spec.target const_subgraph_module.L__self___model_rotary_emb_inv_freq with ID 139658581032480 and kind InputKind.BUFFER
[JAMES] setting arg ref map to  refs=constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,139658584088448,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown
2025-08-12 15:39:44.965 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.407s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640] executable_instance.cc:138      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]loaded_executable_insta:571      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]loaded_executable_insta:95       1| LoadedExecutableInstance::Execute
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]loaded_executable_insta:125      1| [DEVICE] Runtime device already opened, reusing existing device
2025-08-12 15:39:44.965 (  45.408s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565445888ef0 (arg 0)
2025-08-12 15:39:44.966 (  45.408s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56541956fb70 (arg 1)
2025-08-12 15:39:44.966 (  45.408s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae08a20 (arg 2)
2025-08-12 15:39:44.967 (  45.410s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442df5210 (arg 3)
2025-08-12 15:39:44.967 (  45.410s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442d65330 (arg 4)
2025-08-12 15:39:44.967 (  45.410s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae22a60 (arg 5)
2025-08-12 15:39:45.050 (  45.493s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565433e38450 (arg 6)
2025-08-12 15:39:45.051 (  45.494s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ff46020 (arg 7)
2025-08-12 15:39:45.051 (  45.494s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c241b00 (arg 8)
2025-08-12 15:39:45.052 (  45.495s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae1a5b0 (arg 9)
2025-08-12 15:39:45.060 (  45.503s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c1fb680 (arg 10)
2025-08-12 15:39:45.061 (  45.504s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544d19c270 (arg 11)
2025-08-12 15:39:45.062 (  45.505s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565419544220 (arg 12)
2025-08-12 15:39:45.068 (  45.511s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ff47ba0 (arg 13)
2025-08-12 15:39:45.077 (  45.520s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x5654070aebb0 (arg 14)
2025-08-12 15:39:45.081 (  45.523s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ff22d90 (arg 15)
2025-08-12 15:39:45.081 (  45.524s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544d2685d0 (arg 16)
2025-08-12 15:39:45.081 (  45.524s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae3cb00 (arg 17)
2025-08-12 15:39:45.084 (  45.527s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544438c610 (arg 18)
2025-08-12 15:39:45.085 (  45.527s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544d22fa40 (arg 19)
2025-08-12 15:39:45.093 (  45.536s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c218220 (arg 20)
2025-08-12 15:39:45.094 (  45.536s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c1f8700 (arg 21)
2025-08-12 15:39:45.094 (  45.537s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae84780 (arg 22)
2025-08-12 15:39:45.095 (  45.538s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c1f62b0 (arg 23)
2025-08-12 15:39:45.096 (  45.539s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x5654447af160 (arg 24)
2025-08-12 15:39:45.182 (  45.625s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544584e8c0 (arg 25)
2025-08-12 15:39:45.196 (  45.639s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442b58f60 (arg 26)
2025-08-12 15:39:45.205 (  45.647s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae7e8c0 (arg 27)
2025-08-12 15:39:45.208 (  45.651s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544572c160 (arg 28)
2025-08-12 15:39:45.212 (  45.654s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c2222e0 (arg 29)
2025-08-12 15:39:45.212 (  45.655s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x5654447c4260 (arg 30)
2025-08-12 15:39:45.220 (  45.663s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56541967fae0 (arg 31)
2025-08-12 15:39:45.348 (  45.791s) [        CAFFD640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:45.348 (  45.791s) [        CAFFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:45.348 (  45.791s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:45.348 (  45.791s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:45.348 (  45.791s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:45.348 (  45.791s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:45.348 (  45.791s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:45.348 (  45.791s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:45.348 (  45.791s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:45.348 (  45.791s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:45.348 (  45.791s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:45.348 (  45.791s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:45.348 (  45.791s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:45.348 (  45.791s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:45.348 (  45.791s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:45.348 (  45.791s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:45.348 (  45.791s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:45.348 (  45.791s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:45.348 (  45.791s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:45.348 (  45.791s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:45.348 (  45.791s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:45.348 (  45.791s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:45.348 (  45.791s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:45.348 (  45.791s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:45.348 (  45.791s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:45.348 (  45.791s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:45.348 (  45.791s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:45.348 (  45.791s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:45.348 (  45.791s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:45.348 (  45.791s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:45.348 (  45.791s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.348 (  45.791s) [        C545D1C0]     buffer_instance.cc:454      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-08-12 15:39:45.348 (  45.791s) [        C545D1C0]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:45.348 (  45.791s) [        C545D1C0]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:45.348 (  45.791s) [        C545D1C0]     buffer_instance.cc:476      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-08-12 15:39:45.348 (  45.791s) [        C545D1C0]     buffer_instance.cc:497      1| [BUFFER_TO_HOST] Source buffer shape: [1, 1, 128256], data_type: 13, required_size: 256512 bytes
2025-08-12 15:39:45.348 (  45.791s) [        C545D1C0]     buffer_instance.cc:508      1| [BUFFER_TO_HOST] Destination buffer: dst_size=256512 bytes, dst_ptr=0x56544cf25bc0
2025-08-12 15:39:45.348 (  45.791s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:45.349 (  45.791s) [        111CE640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:45.353 (  45.795s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:45.353 (  45.796s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
alink2025-08-12 15:39:45.354 (  45.796s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:45.354 (  45.796s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:45.354 (  45.797s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:45.354 (  45.797s) [        C545D1C0]     buffer_instance.cc:213      1| [BUFFER] Creating OWNED host tensor with shape [1, 1] (semantics: other, supported_dtype: false)
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-08-12 15:39:45.354 (  45.797s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:45.354 (  45.797s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:45.354 (  45.797s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:45.354 (  45.797s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:45.354 (  45.797s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:45.354 (  45.797s) [        C545D1C0]     buffer_instance.cc:213      1| [BUFFER] Creating OWNED host tensor with shape [1] (semantics: other, supported_dtype: false)
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-08-12 15:39:45.354 (  45.797s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:45.354 (  45.797s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:45.359 (  45.802s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:45.359 (  45.802s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:45.359 (  45.802s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:45.359 (  45.802s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1, 1, 1, 128] (semantics: ZeroCopy/other)
2025-08-12 15:39:45.359 (  45.802s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:45.359 (  45.802s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:45.359 (  45.802s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:45.359 (  45.802s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:45.359 (  45.802s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.364 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.807s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
Tensor id via xlac: 423 with shape torch.Size([3072])
Tensor id via xlac: 424 with shape torch.Size([3072])
Tensor id via xlac: 425 with shape torch.Size([3072])
Tensor id via xlac: 426 with shape torch.Size([3072])
Tensor id via xlac: 427 with shape torch.Size([3072])
Tensor id via xlac: 428 with shape torch.Size([128256, 3072])
Tensor id via xlac: 429 with shape torch.Size([1, 64, 1])
Tensor id via xlac: 430 with shape torch.Size([3072, 3072])
Tensor id via xlac: 431 with shape torch.Size([3072, 1024])
Tensor id via xlac: 432 with shape torch.Size([3072, 1024])
Tensor id via xlac: 433 with shape torch.Size([3072, 3072])
Tensor id via xlac: 434 with shape torch.Size([3072, 8192])
Tensor id via xlac: 435 with shape torch.Size([3072, 8192])
Tensor id via xlac: 436 with shape torch.Size([8192, 3072])
Tensor id via xlac: 437 with shape torch.Size([3072, 3072])
Tensor id via xlac: 438 with shape torch.Size([3072, 1024])
Tensor id via xlac: 439 with shape torch.Size([3072, 1024])
Tensor id via xlac: 440 with shape torch.Size([3072, 3072])
Tensor id via xlac: 441 with shape torch.Size([3072, 8192])
Tensor id via xlac: 442 with shape torch.Size([3072, 8192])
Tensor id via xlac: 443 with shape torch.Size([8192, 3072])
Tensor id via xlac: 444 with shape torch.Size([3072, 128256])
Tensor id via xlac: 445 with shape torch.Size([3072, 3072])
Tensor id via xlac: 446 with shape torch.Size([1024, 3072])
Tensor id via xlac: 447 with shape torch.Size([1024, 3072])
Tensor id via xlac: 448 with shape torch.Size([3072, 3072])
Tensor id via xlac: 449 with shape torch.Size([8192, 3072])
Tensor id via xlac: 450 with shape torch.Size([8192, 3072])
Tensor id via xlac: 451 with shape torch.Size([3072, 8192])
Tensor id via xlac: 452 with shape torch.Size([3072, 3072])
Tensor id via xlac: 453 with shape torch.Size([1024, 3072])
Tensor id via xlac: 454 with shape torch.Size([1024, 3072])
Tensor id via xlac: 455 with shape torch.Size([3072, 3072])
Tensor id via xlac: 456 with shape torch.Size([8192, 3072])
Tensor id via xlac: 457 with shape torch.Size([8192, 3072])
Tensor id via xlac: 458 with shape torch.Size([3072, 8192])
Tensor id via xlac: 459 with shape torch.Size([128256, 3072])
Tensor id via xlac: 460 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 461 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 462 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 463 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 464 with shape torch.Size([64])
Tensor id via xlac: 1221 with shape torch.Size([1, 1])
Tensor id via xlac: 1222 with shape torch.Size([1])
Tensor id via xlac: 1223 with shape torch.Size([1, 1, 1, 128])
[XLA Cache] Previously seen inputs: ['input_0_shape_[3072]_dtype_torch.bfloat16 (idx=0)', 'input_1_shape_[3072]_dtype_torch.bfloat16 (idx=1)', 'input_2_shape_[3072]_dtype_torch.bfloat16 (idx=2)', 'input_3_shape_[3072]_dtype_torch.bfloat16 (idx=3)', 'input_4_shape_[3072]_dtype_torch.bfloat16 (idx=4)', 'input_5_shape_[128256, 3072]_dtype_torch.bfloat16 (idx=5)', 'input_6_shape_[1, 64, 1]_dtype_torch.float32 (idx=6)', 'input_7_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=7)', 'input_8_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=8)', 'input_9_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=9)', 'input_10_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=10)', 'input_11_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=11)', 'input_12_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=12)', 'input_13_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=13)', 'input_14_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=14)', 'input_15_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=15)', 'input_16_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=16)', 'input_17_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=17)', 'input_18_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=18)', 'input_19_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=19)', 'input_20_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=20)', 'input_21_shape_[3072, 128256]_dtype_torch.bfloat16 (idx=21)', 'input_22_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=22)', 'input_23_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=23)', 'input_24_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=24)', 'input_25_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=25)', 'input_26_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=26)', 'input_27_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=27)', 'input_28_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=28)', 'input_29_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=29)', 'input_30_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=30)', 'input_31_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=31)', 'input_32_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=32)', 'input_33_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=33)', 'input_34_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=34)', 'input_35_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=35)', 'input_36_shape_[128256, 3072]_dtype_torch.bfloat16 (idx=36)', 'input_37_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=37)', 'input_38_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=38)', 'input_39_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=39)', 'input_40_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=40)', 'input_41_shape_[64]_dtype_torch.float32 (idx=41)']
[XLA Cache] New inputs: ['input_42_shape_[1, 1]_dtype_torch.int64 (idx=42)', 'input_43_shape_[1]_dtype_torch.int64 (idx=43)', 'input_44_shape_[1, 1, 1, 128]_dtype_torch.bfloat16 (idx=44)']
[XLA Cache] Total cache size: 51 unique tensors
Hlo input positions pre normalization [444, 1551, 443, 442, 440, 439, 436, 435, 433, 432, 1221, 459, 423, 1222, -1, 461, 1223, 1489, 429, 431, 460, 430, 424, 434, 425, 463, 438, 462, 437, 426, 441, 427]
Hlo input positions post normalization [445, 1552, 444, 443, 441, 440, 437, 436, 434, 433, 1222, 460, 424, 1223, 0, 462, 1224, 1490, 430, 432, 461, 431, 425, 435, 426, 464, 439, 463, 438, 427, 442, 428]
match key in_spec.target L__self___model_layers__modules__0___input_layernorm_weight with ID 139658584088448 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_layers__modules__0___post_attention_layernorm_weight with ID 139658589162128 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_layers__modules__1___input_layernorm_weight with ID 139658589165728 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_layers__modules__1___post_attention_layernorm_weight with ID 139658581142368 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_norm_weight with ID 139658581024640 and kind InputKind.PARAMETER
match key in_spec.target L__self___lm_head.weight with ID 139658584091808 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.0 with ID 139654952040096 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.1 with ID 139654952040496 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.2 with ID 139654952042416 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.3 with ID 139654952041616 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.4 with ID 139654952040336 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.5 with ID 139654952041056 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.6 with ID 139654952040816 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.7 with ID 139654952040576 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.8 with ID 139654952038416 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.9 with ID 139654952041216 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.10 with ID 139654952030496 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.11 with ID 139654952040656 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.12 with ID 139654952039776 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.13 with ID 139654952041696 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.14 with ID 139654952040976 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.15 with ID 139654952041856 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_q_proj.weight with ID 139658589160768 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_k_proj.weight with ID 139658589162688 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_v_proj.weight with ID 139658589173008 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_o_proj.weight with ID 139658589164768 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___mlp_gate_proj.weight with ID 139658584087008 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___mlp_up_proj.weight with ID 139658589172608 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___mlp_down_proj.weight with ID 139658584095888 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_q_proj.weight with ID 139658581139488 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_k_proj.weight with ID 139658581145168 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_v_proj.weight with ID 139658581134368 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_o_proj.weight with ID 139658581144048 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___mlp_gate_proj.weight with ID 139658589162288 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___mlp_up_proj.weight with ID 139658589171488 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___mlp_down_proj.weight with ID 139658589163008 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___lm_head.weight with ID 139658584091808 and kind InputKind.PARAMETER
match key in_spec.target kwargs____past_key_values___key_cache_0 with ID 139659373973792 and kind InputKind.BUFFER
match key in_spec.target kwargs____past_key_values___value_cache_0 with ID 139659373983872 and kind InputKind.BUFFER
match key in_spec.target kwargs____past_key_values___key_cache_1 with ID 139659373976192 and kind InputKind.BUFFER
match key in_spec.target kwargs____past_key_values___value_cache_1 with ID 139659373976432 and kind InputKind.BUFFER
match key in_spec.target const_subgraph_module.L__self___model_rotary_emb_inv_freq with ID 139658581032480 and kind InputKind.BUFFER
[JAMES] setting arg ref map to  refs=constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,139658584088448,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640] executable_instance.cc:138      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]loaded_executable_insta:571      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]loaded_executable_insta:95       1| LoadedExecutableInstance::Execute
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]loaded_executable_insta:125      1| [DEVICE] Runtime device already opened, reusing existing device
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442e56f80 (arg 0)
2025-08-12 15:39:45.365 (  45.808s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56541956fb70 (arg 1)
2025-08-12 15:39:45.366 (  45.808s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae08a20 (arg 2)
2025-08-12 15:39:45.367 (  45.810s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442df5210 (arg 3)
2025-08-12 15:39:45.367 (  45.810s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c46d430 (arg 4)
2025-08-12 15:39:45.367 (  45.810s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae22a60 (arg 5)
2025-08-12 15:39:45.450 (  45.893s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565433e38450 (arg 6)
2025-08-12 15:39:45.451 (  45.894s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ff46020 (arg 7)
2025-08-12 15:39:45.451 (  45.894s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c348740 (arg 8)
2025-08-12 15:39:45.451 (  45.894s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae1a5b0 (arg 9)
2025-08-12 15:39:45.460 (  45.903s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c34d780 (arg 10)
2025-08-12 15:39:45.461 (  45.903s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544d19c270 (arg 11)
2025-08-12 15:39:45.462 (  45.904s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565419544220 (arg 12)
2025-08-12 15:39:45.468 (  45.910s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ff47ba0 (arg 13)
2025-08-12 15:39:45.477 (  45.919s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x5654070aebb0 (arg 14)
2025-08-12 15:39:45.480 (  45.923s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565445829370 (arg 15)
2025-08-12 15:39:45.480 (  45.923s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544d2685d0 (arg 16)
2025-08-12 15:39:45.481 (  45.923s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae3cb00 (arg 17)
2025-08-12 15:39:45.483 (  45.926s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544438c610 (arg 18)
2025-08-12 15:39:45.484 (  45.927s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544d22fa40 (arg 19)
2025-08-12 15:39:45.492 (  45.935s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c218220 (arg 20)
2025-08-12 15:39:45.493 (  45.936s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c211fe0 (arg 21)
2025-08-12 15:39:45.493 (  45.936s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae84780 (arg 22)
2025-08-12 15:39:45.494 (  45.937s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c23d2a0 (arg 23)
2025-08-12 15:39:45.495 (  45.938s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x5654447af160 (arg 24)
2025-08-12 15:39:45.582 (  46.025s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544584e8c0 (arg 25)
2025-08-12 15:39:45.596 (  46.038s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442b58f60 (arg 26)
2025-08-12 15:39:45.604 (  46.047s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae7e8c0 (arg 27)
2025-08-12 15:39:45.607 (  46.050s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544572c160 (arg 28)
2025-08-12 15:39:45.611 (  46.054s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c2222e0 (arg 29)
2025-08-12 15:39:45.612 (  46.055s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x5654447c4260 (arg 30)
2025-08-12 15:39:45.620 (  46.063s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56541967fae0 (arg 31)
2025-08-12 15:39:45.742 (  46.185s) [        CAFFD640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:45.742 (  46.185s) [        CAFFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:45.742 (  46.185s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:45.742 (  46.185s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:45.742 (  46.185s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:45.742 (  46.185s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:45.742 (  46.185s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:45.742 (  46.185s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:45.742 (  46.185s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:45.742 (  46.185s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:45.742 (  46.185s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:45.742 (  46.185s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:45.742 (  46.185s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:45.742 (  46.185s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:45.742 (  46.185s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:45.742 (  46.185s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:45.742 (  46.185s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:45.742 (  46.185s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:45.742 (  46.185s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:45.742 (  46.185s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:45.742 (  46.185s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:45.742 (  46.185s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:45.742 (  46.185s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:45.742 (  46.185s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:45.742 (  46.185s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:45.742 (  46.185s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:45.742 (  46.185s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:45.742 (  46.185s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:45.742 (  46.185s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:45.742 (  46.185s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:45.742 (  46.185s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.742 (  46.185s) [        C545D1C0]     buffer_instance.cc:454      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-08-12 15:39:45.742 (  46.185s) [        C545D1C0]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:45.742 (  46.185s) [        C545D1C0]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:45.742 (  46.185s) [        C545D1C0]     buffer_instance.cc:476      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-08-12 15:39:45.743 (  46.185s) [        C545D1C0]     buffer_instance.cc:497      1| [BUFFER_TO_HOST] Source buffer shape: [1, 1, 128256], data_type: 13, required_size: 256512 bytes
2025-08-12 15:39:45.743 (  46.185s) [        C545D1C0]     buffer_instance.cc:508      1| [BUFFER_TO_HOST] Destination buffer: dst_size=256512 bytes, dst_ptr=0x5654446f37c0
2025-08-12 15:39:45.743 (  46.185s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:45.743 (  46.185s) [        129D1640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:45.743 (  46.186s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:45.743 (  46.186s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
alink2025-08-12 15:39:45.744 (  46.187s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:45.744 (  46.187s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:45.744 (  46.187s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:45.744 (  46.187s) [        C545D1C0]     buffer_instance.cc:213      1| [BUFFER] Creating OWNED host tensor with shape [1, 1] (semantics: other, supported_dtype: false)
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-08-12 15:39:45.744 (  46.187s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:45.744 (  46.187s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:45.744 (  46.187s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:45.744 (  46.187s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:45.744 (  46.187s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:45.744 (  46.187s) [        C545D1C0]     buffer_instance.cc:213      1| [BUFFER] Creating OWNED host tensor with shape [1] (semantics: other, supported_dtype: false)
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-08-12 15:39:45.744 (  46.187s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:45.744 (  46.187s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:45.745 (  46.187s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:45.745 (  46.187s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:45.745 (  46.187s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:45.745 (  46.187s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1, 1, 1, 128] (semantics: ZeroCopy/other)
2025-08-12 15:39:45.745 (  46.187s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:45.745 (  46.187s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:45.745 (  46.187s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:45.745 (  46.187s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:45.745 (  46.187s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:45.750 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.750 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.750 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.750 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.750 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.750 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.750 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.750 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.750 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.750 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.750 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.750 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.750 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.750 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.750 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.750 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.750 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.750 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.750 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.750 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.750 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.750 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.750 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.750 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.750 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.750 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.750 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.750 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.750 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.750 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.193s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
Tensor id via xlac: 423 with shape torch.Size([3072])
Tensor id via xlac: 424 with shape torch.Size([3072])
Tensor id via xlac: 425 with shape torch.Size([3072])
Tensor id via xlac: 426 with shape torch.Size([3072])
Tensor id via xlac: 427 with shape torch.Size([3072])
Tensor id via xlac: 428 with shape torch.Size([128256, 3072])
Tensor id via xlac: 429 with shape torch.Size([1, 64, 1])
Tensor id via xlac: 430 with shape torch.Size([3072, 3072])
Tensor id via xlac: 431 with shape torch.Size([3072, 1024])
Tensor id via xlac: 432 with shape torch.Size([3072, 1024])
Tensor id via xlac: 433 with shape torch.Size([3072, 3072])
Tensor id via xlac: 434 with shape torch.Size([3072, 8192])
Tensor id via xlac: 435 with shape torch.Size([3072, 8192])
Tensor id via xlac: 436 with shape torch.Size([8192, 3072])
Tensor id via xlac: 437 with shape torch.Size([3072, 3072])
Tensor id via xlac: 438 with shape torch.Size([3072, 1024])
Tensor id via xlac: 439 with shape torch.Size([3072, 1024])
Tensor id via xlac: 440 with shape torch.Size([3072, 3072])
Tensor id via xlac: 441 with shape torch.Size([3072, 8192])
Tensor id via xlac: 442 with shape torch.Size([3072, 8192])
Tensor id via xlac: 443 with shape torch.Size([8192, 3072])
Tensor id via xlac: 444 with shape torch.Size([3072, 128256])
Tensor id via xlac: 445 with shape torch.Size([3072, 3072])
Tensor id via xlac: 446 with shape torch.Size([1024, 3072])
Tensor id via xlac: 447 with shape torch.Size([1024, 3072])
Tensor id via xlac: 448 with shape torch.Size([3072, 3072])
Tensor id via xlac: 449 with shape torch.Size([8192, 3072])
Tensor id via xlac: 450 with shape torch.Size([8192, 3072])
Tensor id via xlac: 451 with shape torch.Size([3072, 8192])
Tensor id via xlac: 452 with shape torch.Size([3072, 3072])
Tensor id via xlac: 453 with shape torch.Size([1024, 3072])
Tensor id via xlac: 454 with shape torch.Size([1024, 3072])
Tensor id via xlac: 455 with shape torch.Size([3072, 3072])
Tensor id via xlac: 456 with shape torch.Size([8192, 3072])
Tensor id via xlac: 457 with shape torch.Size([8192, 3072])
Tensor id via xlac: 458 with shape torch.Size([3072, 8192])
Tensor id via xlac: 459 with shape torch.Size([128256, 3072])
Tensor id via xlac: 460 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 461 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 462 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 463 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 464 with shape torch.Size([64])
Tensor id via xlac: 1599 with shape torch.Size([1, 1])
Tensor id via xlac: 1600 with shape torch.Size([1])
Tensor id via xlac: 1601 with shape torch.Size([1, 1, 1, 128])
[XLA Cache] Previously seen inputs: ['input_0_shape_[3072]_dtype_torch.bfloat16 (idx=0)', 'input_1_shape_[3072]_dtype_torch.bfloat16 (idx=1)', 'input_2_shape_[3072]_dtype_torch.bfloat16 (idx=2)', 'input_3_shape_[3072]_dtype_torch.bfloat16 (idx=3)', 'input_4_shape_[3072]_dtype_torch.bfloat16 (idx=4)', 'input_5_shape_[128256, 3072]_dtype_torch.bfloat16 (idx=5)', 'input_6_shape_[1, 64, 1]_dtype_torch.float32 (idx=6)', 'input_7_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=7)', 'input_8_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=8)', 'input_9_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=9)', 'input_10_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=10)', 'input_11_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=11)', 'input_12_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=12)', 'input_13_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=13)', 'input_14_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=14)', 'input_15_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=15)', 'input_16_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=16)', 'input_17_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=17)', 'input_18_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=18)', 'input_19_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=19)', 'input_20_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=20)', 'input_21_shape_[3072, 128256]_dtype_torch.bfloat16 (idx=21)', 'input_22_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=22)', 'input_23_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=23)', 'input_24_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=24)', 'input_25_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=25)', 'input_26_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=26)', 'input_27_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=27)', 'input_28_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=28)', 'input_29_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=29)', 'input_30_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=30)', 'input_31_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=31)', 'input_32_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=32)', 'input_33_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=33)', 'input_34_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=34)', 'input_35_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=35)', 'input_36_shape_[128256, 3072]_dtype_torch.bfloat16 (idx=36)', 'input_37_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=37)', 'input_38_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=38)', 'input_39_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=39)', 'input_40_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=40)', 'input_41_shape_[64]_dtype_torch.float32 (idx=41)']
[XLA Cache] New inputs: ['input_42_shape_[1, 1]_dtype_torch.int64 (idx=42)', 'input_43_shape_[1]_dtype_torch.int64 (idx=43)', 'input_44_shape_[1, 1, 1, 128]_dtype_torch.bfloat16 (idx=44)']
[XLA Cache] Total cache size: 54 unique tensors
Hlo input positions pre normalization [444, 1929, 443, 442, 440, 439, 436, 435, 433, 432, 1599, 459, 423, 1600, -1, 461, 1601, 1867, 429, 431, 460, 430, 424, 434, 425, 463, 438, 462, 437, 426, 441, 427]
Hlo input positions post normalization [445, 1930, 444, 443, 441, 440, 437, 436, 434, 433, 1600, 460, 424, 1601, 0, 462, 1602, 1868, 430, 432, 461, 431, 425, 435, 426, 464, 439, 463, 438, 427, 442, 428]
match key in_spec.target L__self___model_layers__modules__0___input_layernorm_weight with ID 139658584088448 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_layers__modules__0___post_attention_layernorm_weight with ID 139658589162128 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_layers__modules__1___input_layernorm_weight with ID 139658589165728 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_layers__modules__1___post_attention_layernorm_weight with ID 139658581142368 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_norm_weight with ID 139658581024640 and kind InputKind.PARAMETER
match key in_spec.target L__self___lm_head.weight with ID 139658584091808 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.0 with ID 139654952040096 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.1 with ID 139654952040496 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.2 with ID 139654952042416 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.3 with ID 139654952041616 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.4 with ID 139654952040336 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.5 with ID 139654952041056 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.6 with ID 139654952040816 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.7 with ID 139654952040576 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.8 with ID 139654952038416 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.9 with ID 139654952041216 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.10 with ID 139654952030496 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.11 with ID 139654952040656 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.12 with ID 139654952039776 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.13 with ID 139654952041696 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.14 with ID 139654952040976 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.15 with ID 139654952041856 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_q_proj.weight with ID 139658589160768 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_k_proj.weight with ID 139658589162688 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_v_proj.weight with ID 139658589173008 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_o_proj.weight with ID 139658589164768 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___mlp_gate_proj.weight with ID 139658584087008 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___mlp_up_proj.weight with ID 139658589172608 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___mlp_down_proj.weight with ID 139658584095888 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_q_proj.weight with ID 139658581139488 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_k_proj.weight with ID 139658581145168 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_v_proj.weight with ID 139658581134368 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_o_proj.weight with ID 139658581144048 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___mlp_gate_proj.weight with ID 139658589162288 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___mlp_up_proj.weight with ID 139658589171488 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___mlp_down_proj.weight with ID 139658589163008 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___lm_head.weight with ID 139658584091808 and kind InputKind.PARAMETER
match key in_spec.target kwargs____past_key_values___key_cache_0 with ID 139659373973792 and kind InputKind.BUFFER
match key in_spec.target kwargs____past_key_values___value_cache_0 with ID 139659373983872 and kind InputKind.BUFFER
match key in_spec.target kwargs____past_key_values___key_cache_1 with ID 139659373976192 and kind InputKind.BUFFER
match key in_spec.target kwargs____past_key_values___value_cache_1 with ID 139659373976432 and kind InputKind.BUFFER
match key in_spec.target const_subgraph_module.L__self___model_rotary_emb_inv_freq with ID 139658581032480 and kind InputKind.BUFFER
[JAMES] setting arg ref map to  refs=constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,139658584088448,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.751 (  46.194s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:45.752 (  46.194s) [        CAFFD640] executable_instance.cc:138      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-08-12 15:39:45.752 (  46.194s) [        CAFFD640]loaded_executable_insta:571      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-08-12 15:39:45.752 (  46.194s) [        CAFFD640]loaded_executable_insta:95       1| LoadedExecutableInstance::Execute
2025-08-12 15:39:45.752 (  46.194s) [        CAFFD640]loaded_executable_insta:125      1| [DEVICE] Runtime device already opened, reusing existing device
2025-08-12 15:39:45.752 (  46.194s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565445888ef0 (arg 0)
2025-08-12 15:39:45.752 (  46.194s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56541956fb70 (arg 1)
2025-08-12 15:39:45.752 (  46.194s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae08a20 (arg 2)
2025-08-12 15:39:45.753 (  46.196s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442df5210 (arg 3)
2025-08-12 15:39:45.753 (  46.196s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ff22d90 (arg 4)
2025-08-12 15:39:45.753 (  46.196s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae22a60 (arg 5)
2025-08-12 15:39:45.836 (  46.279s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565433e38450 (arg 6)
2025-08-12 15:39:45.837 (  46.280s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ff46020 (arg 7)
2025-08-12 15:39:45.837 (  46.280s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c34ea40 (arg 8)
2025-08-12 15:39:45.838 (  46.280s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae1a5b0 (arg 9)
2025-08-12 15:39:45.846 (  46.289s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c203830 (arg 10)
2025-08-12 15:39:45.847 (  46.289s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544d19c270 (arg 11)
2025-08-12 15:39:45.848 (  46.290s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565419544220 (arg 12)
2025-08-12 15:39:45.854 (  46.296s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ff47ba0 (arg 13)
2025-08-12 15:39:45.862 (  46.305s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x5654070aebb0 (arg 14)
2025-08-12 15:39:45.866 (  46.309s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442d65330 (arg 15)
2025-08-12 15:39:45.866 (  46.309s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544d2685d0 (arg 16)
2025-08-12 15:39:45.866 (  46.309s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae3cb00 (arg 17)
2025-08-12 15:39:45.869 (  46.312s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544438c610 (arg 18)
2025-08-12 15:39:45.870 (  46.313s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544d22fa40 (arg 19)
2025-08-12 15:39:45.878 (  46.321s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c218220 (arg 20)
2025-08-12 15:39:45.879 (  46.322s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c1ea690 (arg 21)
2025-08-12 15:39:45.880 (  46.322s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae84780 (arg 22)
2025-08-12 15:39:45.881 (  46.323s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c348aa0 (arg 23)
2025-08-12 15:39:45.881 (  46.324s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x5654447af160 (arg 24)
2025-08-12 15:39:45.968 (  46.411s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544584e8c0 (arg 25)
2025-08-12 15:39:45.982 (  46.425s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442b58f60 (arg 26)
2025-08-12 15:39:45.990 (  46.433s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae7e8c0 (arg 27)
2025-08-12 15:39:45.994 (  46.437s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544572c160 (arg 28)
2025-08-12 15:39:45.997 (  46.440s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c2222e0 (arg 29)
2025-08-12 15:39:45.998 (  46.441s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x5654447c4260 (arg 30)
2025-08-12 15:39:46.006 (  46.449s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56541967fae0 (arg 31)
2025-08-12 15:39:46.132 (  46.574s) [        CAFFD640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:46.132 (  46.574s) [        CAFFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:46.132 (  46.574s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.132 (  46.574s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:46.132 (  46.574s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.132 (  46.574s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:46.132 (  46.574s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.132 (  46.574s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:46.132 (  46.574s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.132 (  46.574s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:46.132 (  46.574s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.132 (  46.575s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:46.132 (  46.575s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.132 (  46.575s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:46.132 (  46.575s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.132 (  46.575s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:46.132 (  46.575s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.132 (  46.575s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:46.132 (  46.575s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.132 (  46.575s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:46.132 (  46.575s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.132 (  46.575s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:46.132 (  46.575s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.132 (  46.575s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:46.132 (  46.575s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.132 (  46.575s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:46.132 (  46.575s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:46.132 (  46.575s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:46.132 (  46.575s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:46.132 (  46.575s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:46.132 (  46.575s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.132 (  46.575s) [        C545D1C0]     buffer_instance.cc:454      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-08-12 15:39:46.132 (  46.575s) [        C545D1C0]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:46.132 (  46.575s) [        C545D1C0]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.132 (  46.575s) [        C545D1C0]     buffer_instance.cc:476      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-08-12 15:39:46.132 (  46.575s) [        C545D1C0]     buffer_instance.cc:497      1| [BUFFER_TO_HOST] Source buffer shape: [1, 1, 128256], data_type: 13, required_size: 256512 bytes
2025-08-12 15:39:46.132 (  46.575s) [        C545D1C0]     buffer_instance.cc:508      1| [BUFFER_TO_HOST] Destination buffer: dst_size=256512 bytes, dst_ptr=0x565442b9ad00
2025-08-12 15:39:46.132 (  46.575s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:46.132 (  46.575s) [        121D0640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:46.137 (  46.580s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:46.137 (  46.580s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
alink2025-08-12 15:39:46.138 (  46.581s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:46.138 (  46.581s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:46.138 (  46.581s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:46.138 (  46.581s) [        C545D1C0]     buffer_instance.cc:213      1| [BUFFER] Creating OWNED host tensor with shape [1, 1] (semantics: other, supported_dtype: false)
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-08-12 15:39:46.138 (  46.581s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:46.138 (  46.581s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:46.139 (  46.581s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:46.139 (  46.581s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:46.139 (  46.581s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:46.139 (  46.581s) [        C545D1C0]     buffer_instance.cc:213      1| [BUFFER] Creating OWNED host tensor with shape [1] (semantics: other, supported_dtype: false)
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-08-12 15:39:46.139 (  46.581s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:46.139 (  46.582s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:46.144 (  46.586s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:46.144 (  46.586s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:46.144 (  46.586s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:46.144 (  46.586s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1, 1, 1, 128] (semantics: ZeroCopy/other)
2025-08-12 15:39:46.144 (  46.586s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:46.144 (  46.586s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:46.144 (  46.586s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:46.144 (  46.586s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:46.144 (  46.586s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.149 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
Tensor id via xlac: 423 with shape torch.Size([3072])
Tensor id via xlac: 424 with shape torch.Size([3072])
Tensor id via xlac: 425 with shape torch.Size([3072])
Tensor id via xlac: 426 with shape torch.Size([3072])
Tensor id via xlac: 427 with shape torch.Size([3072])
Tensor id via xlac: 428 with shape torch.Size([128256, 3072])
Tensor id via xlac: 429 with shape torch.Size([1, 64, 1])
Tensor id via xlac: 430 with shape torch.Size([3072, 3072])
Tensor id via xlac: 431 with shape torch.Size([3072, 1024])
Tensor id via xlac: 432 with shape torch.Size([3072, 1024])
Tensor id via xlac: 433 with shape torch.Size([3072, 3072])
Tensor id via xlac: 434 with shape torch.Size([3072, 8192])
Tensor id via xlac: 435 with shape torch.Size([3072, 8192])
Tensor id via xlac: 436 with shape torch.Size([8192, 3072])
Tensor id via xlac: 437 with shape torch.Size([3072, 3072])
Tensor id via xlac: 438 with shape torch.Size([3072, 1024])
Tensor id via xlac: 439 with shape torch.Size([3072, 1024])
Tensor id via xlac: 440 with shape torch.Size([3072, 3072])
Tensor id via xlac: 441 with shape torch.Size([3072, 8192])
Tensor id via xlac: 442 with shape torch.Size([3072, 8192])
Tensor id via xlac: 443 with shape torch.Size([8192, 3072])
Tensor id via xlac: 444 with shape torch.Size([3072, 128256])
Tensor id via xlac: 445 with shape torch.Size([3072, 3072])
Tensor id via xlac: 446 with shape torch.Size([1024, 3072])
Tensor id via xlac: 447 with shape torch.Size([1024, 3072])
Tensor id via xlac: 448 with shape torch.Size([3072, 3072])
Tensor id via xlac: 449 with shape torch.Size([8192, 3072])
Tensor id via xlac: 450 with shape torch.Size([8192, 3072])
Tensor id via xlac: 451 with shape torch.Size([3072, 8192])
Tensor id via xlac: 452 with shape torch.Size([3072, 3072])
Tensor id via xlac: 453 with shape torch.Size([1024, 3072])
Tensor id via xlac: 454 with shape torch.Size([1024, 3072])
Tensor id via xlac: 455 with shape torch.Size([3072, 3072])
Tensor id via xlac: 456 with shape torch.Size([8192, 3072])
Tensor id via xlac: 457 with shape torch.Size([8192, 3072])
Tensor id via xlac: 458 with shape torch.Size([3072, 8192])
Tensor id via xlac: 459 with shape torch.Size([128256, 3072])
Tensor id via xlac: 460 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 461 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 462 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 463 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 464 with shape torch.Size([64])
Tensor id via xlac: 1977 with shape torch.Size([1, 1])
Tensor id via xlac: 1978 with shape torch.Size([1])
Tensor id via xlac: 1979 with shape torch.Size([1, 1, 1, 128])
[XLA Cache] Previously seen inputs: ['input_0_shape_[3072]_dtype_torch.bfloat16 (idx=0)', 'input_1_shape_[3072]_dtype_torch.bfloat16 (idx=1)', 'input_2_shape_[3072]_dtype_torch.bfloat16 (idx=2)', 'input_3_shape_[3072]_dtype_torch.bfloat16 (idx=3)', 'input_4_shape_[3072]_dtype_torch.bfloat16 (idx=4)', 'input_5_shape_[128256, 3072]_dtype_torch.bfloat16 (idx=5)', 'input_6_shape_[1, 64, 1]_dtype_torch.float32 (idx=6)', 'input_7_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=7)', 'input_8_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=8)', 'input_9_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=9)', 'input_10_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=10)', 'input_11_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=11)', 'input_12_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=12)', 'input_13_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=13)', 'input_14_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=14)', 'input_15_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=15)', 'input_16_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=16)', 'input_17_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=17)', 'input_18_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=18)', 'input_19_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=19)', 'input_20_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=20)', 'input_21_shape_[3072, 128256]_dtype_torch.bfloat16 (idx=21)', 'input_22_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=22)', 'input_23_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=23)', 'input_24_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=24)', 'input_25_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=25)', 'input_26_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=26)', 'input_27_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=27)', 'input_28_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=28)', 'input_29_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=29)', 'input_30_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=30)', 'input_31_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=31)', 'input_32_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=32)', 'input_33_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=33)', 'input_34_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=34)', 'input_35_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=35)', 'input_36_shape_[128256, 3072]_dtype_torch.bfloat16 (idx=36)', 'input_37_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=37)', 'input_38_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=38)', 'input_39_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=39)', 'input_40_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=40)', 'input_41_shape_[64]_dtype_torch.float32 (idx=41)']
[XLA Cache] New inputs: ['input_42_shape_[1, 1]_dtype_torch.int64 (idx=42)', 'input_43_shape_[1]_dtype_torch.int64 (idx=43)', 'input_44_shape_[1, 1, 1, 128]_dtype_torch.bfloat16 (idx=44)']
[XLA Cache] Total cache size: 57 unique tensors
Hlo input positions pre normalization [444, 2307, 443, 442, 440, 439, 436, 435, 433, 432, 1977, 459, 423, 1978, -1, 461, 1979, 2245, 429, 431, 460, 430, 424, 434, 425, 463, 438, 462, 437, 426, 441, 427]
Hlo input positions post normalization [445, 2308, 444, 443, 441, 440, 437, 436, 434, 433, 1978, 460, 424, 1979, 0, 462, 1980, 2246, 430, 432, 461, 431, 425, 435, 426, 464, 439, 463, 438, 427, 442, 428]
match key in_spec.target L__self___model_layers__modules__0___input_layernorm_weight with ID 139658584088448 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_layers__modules__0___post_attention_layernorm_weight with ID 139658589162128 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_layers__modules__1___input_layernorm_weight with ID 139658589165728 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_layers__modules__1___post_attention_layernorm_weight with ID 139658581142368 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_norm_weight with ID 139658581024640 and kind InputKind.PARAMETER
match key in_spec.target L__self___lm_head.weight with ID 139658584091808 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.0 with ID 139654952040096 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.1 with ID 139654952040496 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.2 with ID 139654952042416 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.3 with ID 139654952041616 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.4 with ID 139654952040336 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.5 with ID 139654952041056 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.6 with ID 139654952040816 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.7 with ID 139654952040576 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.8 with ID 139654952038416 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.9 with ID 139654952041216 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.10 with ID 139654952030496 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.11 with ID 139654952040656 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.12 with ID 139654952039776 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.13 with ID 139654952041696 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.14 with ID 139654952040976 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.15 with ID 139654952041856 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_q_proj.weight with ID 139658589160768 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_k_proj.weight with ID 139658589162688 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_v_proj.weight with ID 139658589173008 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_o_proj.weight with ID 139658589164768 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___mlp_gate_proj.weight with ID 139658584087008 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___mlp_up_proj.weight with ID 139658589172608 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___mlp_down_proj.weight with ID 139658584095888 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_q_proj.weight with ID 139658581139488 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_k_proj.weight with ID 139658581145168 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_v_proj.weight with ID 139658581134368 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_o_proj.weight with ID 139658581144048 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___mlp_gate_proj.weight with ID 139658589162288 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___mlp_up_proj.weight with ID 139658589171488 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___mlp_down_proj.weight with ID 139658589163008 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___lm_head.weight with ID 139658584091808 and kind InputKind.PARAMETER
match key in_spec.target kwargs____past_key_values___key_cache_0 with ID 139659373973792 and kind InputKind.BUFFER
match key in_spec.target kwargs____past_key_values___value_cache_0 with ID 139659373983872 and kind InputKind.BUFFER
match key in_spec.target kwargs____past_key_values___key_cache_1 with ID 139659373976192 and kind InputKind.BUFFER
match key in_spec.target kwargs____past_key_values___value_cache_1 with ID 139659373976432 and kind InputKind.BUFFER
match key in_spec.target const_subgraph_module.L__self___model_rotary_emb_inv_freq with ID 139658581032480 and kind InputKind.BUFFER
[JAMES] setting arg ref map to  refs=constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,139658584088448,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.592s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.593s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.593s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.593s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.593s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.593s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.593s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.593s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.593s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.593s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.593s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.593s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.593s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.593s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.593s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.593s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.593s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.593s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.593s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.593s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.593s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.593s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.593s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.593s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.593s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.593s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.593s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.593s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.593s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.593s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.593s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640] executable_instance.cc:138      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]loaded_executable_insta:571      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]loaded_executable_insta:95       1| LoadedExecutableInstance::Execute
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]loaded_executable_insta:125      1| [DEVICE] Runtime device already opened, reusing existing device
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442e56f80 (arg 0)
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56541956fb70 (arg 1)
2025-08-12 15:39:46.150 (  46.593s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae08a20 (arg 2)
2025-08-12 15:39:46.151 (  46.594s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442df5210 (arg 3)
2025-08-12 15:39:46.152 (  46.594s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565445829370 (arg 4)
2025-08-12 15:39:46.152 (  46.594s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae22a60 (arg 5)
2025-08-12 15:39:46.235 (  46.678s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565433e38450 (arg 6)
2025-08-12 15:39:46.236 (  46.678s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ff46020 (arg 7)
2025-08-12 15:39:46.236 (  46.679s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c35a520 (arg 8)
2025-08-12 15:39:46.236 (  46.679s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae1a5b0 (arg 9)
2025-08-12 15:39:46.245 (  46.688s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c46ef50 (arg 10)
2025-08-12 15:39:46.245 (  46.688s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544d19c270 (arg 11)
2025-08-12 15:39:46.246 (  46.689s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565419544220 (arg 12)
2025-08-12 15:39:46.252 (  46.695s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ff47ba0 (arg 13)
2025-08-12 15:39:46.261 (  46.704s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x5654070aebb0 (arg 14)
2025-08-12 15:39:46.264 (  46.707s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c46d430 (arg 15)
2025-08-12 15:39:46.265 (  46.708s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544d2685d0 (arg 16)
2025-08-12 15:39:46.265 (  46.708s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae3cb00 (arg 17)
2025-08-12 15:39:46.268 (  46.711s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544438c610 (arg 18)
2025-08-12 15:39:46.269 (  46.712s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544d22fa40 (arg 19)
2025-08-12 15:39:46.277 (  46.720s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c218220 (arg 20)
2025-08-12 15:39:46.278 (  46.721s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c234f10 (arg 21)
2025-08-12 15:39:46.279 (  46.721s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae84780 (arg 22)
2025-08-12 15:39:46.280 (  46.723s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c360b00 (arg 23)
2025-08-12 15:39:46.280 (  46.723s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x5654447af160 (arg 24)
2025-08-12 15:39:46.367 (  46.810s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544584e8c0 (arg 25)
2025-08-12 15:39:46.381 (  46.824s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442b58f60 (arg 26)
2025-08-12 15:39:46.390 (  46.832s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae7e8c0 (arg 27)
2025-08-12 15:39:46.393 (  46.836s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544572c160 (arg 28)
2025-08-12 15:39:46.397 (  46.839s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c2222e0 (arg 29)
2025-08-12 15:39:46.397 (  46.840s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x5654447c4260 (arg 30)
2025-08-12 15:39:46.406 (  46.848s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56541967fae0 (arg 31)
2025-08-12 15:39:46.526 (  46.969s) [        CAFFD640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:46.526 (  46.969s) [        CAFFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:46.526 (  46.969s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.526 (  46.969s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:46.526 (  46.969s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.526 (  46.969s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:46.526 (  46.969s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.526 (  46.969s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:46.526 (  46.969s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.526 (  46.969s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:46.526 (  46.969s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.526 (  46.969s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:46.526 (  46.969s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.526 (  46.969s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:46.526 (  46.969s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.526 (  46.969s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:46.526 (  46.969s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.526 (  46.969s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:46.526 (  46.969s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.526 (  46.969s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:46.526 (  46.969s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.526 (  46.969s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:46.526 (  46.969s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.526 (  46.969s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:46.526 (  46.969s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.526 (  46.969s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:46.526 (  46.969s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:46.526 (  46.969s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:46.526 (  46.969s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:46.526 (  46.969s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:46.526 (  46.969s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.526 (  46.969s) [        C545D1C0]     buffer_instance.cc:454      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-08-12 15:39:46.526 (  46.969s) [        C545D1C0]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:46.526 (  46.969s) [        C545D1C0]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.527 (  46.969s) [        C545D1C0]     buffer_instance.cc:476      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-08-12 15:39:46.527 (  46.969s) [        C545D1C0]     buffer_instance.cc:497      1| [BUFFER_TO_HOST] Source buffer shape: [1, 1, 128256], data_type: 13, required_size: 256512 bytes
2025-08-12 15:39:46.527 (  46.969s) [        C545D1C0]     buffer_instance.cc:508      1| [BUFFER_TO_HOST] Destination buffer: dst_size=256512 bytes, dst_ptr=0x56544cf25bc0
2025-08-12 15:39:46.527 (  46.969s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:46.527 (  46.970s) [        111CE640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:46.527 (  46.970s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:46.527 (  46.970s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
alink2025-08-12 15:39:46.528 (  46.971s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:46.528 (  46.971s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:46.528 (  46.971s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:46.528 (  46.971s) [        C545D1C0]     buffer_instance.cc:213      1| [BUFFER] Creating OWNED host tensor with shape [1, 1] (semantics: other, supported_dtype: false)
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-08-12 15:39:46.528 (  46.971s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:46.528 (  46.971s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:46.528 (  46.971s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:46.528 (  46.971s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:46.528 (  46.971s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:46.529 (  46.971s) [        C545D1C0]     buffer_instance.cc:213      1| [BUFFER] Creating OWNED host tensor with shape [1] (semantics: other, supported_dtype: false)
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-08-12 15:39:46.529 (  46.971s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:46.529 (  46.971s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:46.529 (  46.971s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:46.529 (  46.971s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:46.529 (  46.971s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:46.529 (  46.972s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1, 1, 1, 128] (semantics: ZeroCopy/other)
2025-08-12 15:39:46.529 (  46.972s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:46.529 (  46.972s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:46.529 (  46.972s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:46.529 (  46.972s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:46.529 (  46.972s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:46.539 (  46.981s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.981s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.981s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.981s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.981s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.981s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.981s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.981s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.981s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.981s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.981s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.981s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.981s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.981s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.981s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.981s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.981s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.981s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.981s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
Tensor id via xlac: 423 with shape torch.Size([3072])
Tensor id via xlac: 424 with shape torch.Size([3072])
Tensor id via xlac: 425 with shape torch.Size([3072])
Tensor id via xlac: 426 with shape torch.Size([3072])
Tensor id via xlac: 427 with shape torch.Size([3072])
Tensor id via xlac: 428 with shape torch.Size([128256, 3072])
Tensor id via xlac: 429 with shape torch.Size([1, 64, 1])
Tensor id via xlac: 430 with shape torch.Size([3072, 3072])
Tensor id via xlac: 431 with shape torch.Size([3072, 1024])
Tensor id via xlac: 432 with shape torch.Size([3072, 1024])
Tensor id via xlac: 433 with shape torch.Size([3072, 3072])
Tensor id via xlac: 434 with shape torch.Size([3072, 8192])
Tensor id via xlac: 435 with shape torch.Size([3072, 8192])
Tensor id via xlac: 436 with shape torch.Size([8192, 3072])
Tensor id via xlac: 437 with shape torch.Size([3072, 3072])
Tensor id via xlac: 438 with shape torch.Size([3072, 1024])
Tensor id via xlac: 439 with shape torch.Size([3072, 1024])
Tensor id via xlac: 440 with shape torch.Size([3072, 3072])
Tensor id via xlac: 441 with shape torch.Size([3072, 8192])
Tensor id via xlac: 442 with shape torch.Size([3072, 8192])
Tensor id via xlac: 443 with shape torch.Size([8192, 3072])
Tensor id via xlac: 444 with shape torch.Size([3072, 128256])
Tensor id via xlac: 445 with shape torch.Size([3072, 3072])
Tensor id via xlac: 446 with shape torch.Size([1024, 3072])
Tensor id via xlac: 447 with shape torch.Size([1024, 3072])
Tensor id via xlac: 448 with shape torch.Size([3072, 3072])
Tensor id via xlac: 449 with shape torch.Size([8192, 3072])
Tensor id via xlac: 450 with shape torch.Size([8192, 3072])
Tensor id via xlac: 451 with shape torch.Size([3072, 8192])
Tensor id via xlac: 452 with shape torch.Size([3072, 3072])
Tensor id via xlac: 453 with shape torch.Size([1024, 3072])
Tensor id via xlac: 454 with shape torch.Size([1024, 3072])
Tensor id via xlac: 455 with shape torch.Size([3072, 3072])
Tensor id via xlac: 456 with shape torch.Size([8192, 3072])
Tensor id via xlac: 457 with shape torch.Size([8192, 3072])
Tensor id via xlac: 458 with shape torch.Size([3072, 8192])
Tensor id via xlac: 459 with shape torch.Size([128256, 3072])
Tensor id via xlac: 460 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 461 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 462 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 463 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 464 with shape torch.Size([64])
Tensor id via xlac: 2355 with shape torch.Size([1, 1])
Tensor id via xlac: 2356 with shape torch.Size([1])
Tensor id via xlac: 2357 with shape torch.Size([1, 1, 1, 128])
[XLA Cache] Previously seen inputs: ['input_0_shape_[3072]_dtype_torch.bfloat16 (idx=0)', 'input_1_shape_[3072]_dtype_torch.bfloat16 (idx=1)', 'input_2_shape_[3072]_dtype_torch.bfloat16 (idx=2)', 'input_3_shape_[3072]_dtype_torch.bfloat16 (idx=3)', 'input_4_shape_[3072]_dtype_torch.bfloat16 (idx=4)', 'input_5_shape_[128256, 3072]_dtype_torch.bfloat16 (idx=5)', 'input_6_shape_[1, 64, 1]_dtype_torch.float32 (idx=6)', 'input_7_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=7)', 'input_8_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=8)', 'input_9_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=9)', 'input_10_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=10)', 'input_11_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=11)', 'input_12_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=12)', 'input_13_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=13)', 'input_14_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=14)', 'input_15_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=15)', 'input_16_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=16)', 'input_17_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=17)', 'input_18_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=18)', 'input_19_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=19)', 'input_20_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=20)', 'input_21_shape_[3072, 128256]_dtype_torch.bfloat16 (idx=21)', 'input_22_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=22)', 'input_23_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=23)', 'input_24_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=24)', 'input_25_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=25)', 'input_26_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=26)', 'input_27_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=27)', 'input_28_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=28)', 'input_29_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=29)', 'input_30_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=30)', 'input_31_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=31)', 'input_32_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=32)', 'input_33_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=33)', 'input_34_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=34)', 'input_35_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=35)', 'input_36_shape_[128256, 3072]_dtype_torch.bfloat16 (idx=36)', 'input_37_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=37)', 'input_38_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=38)', 'input_39_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=39)', 'input_40_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=40)', 'input_41_shape_[64]_dtype_torch.float32 (idx=41)', 'input_44_shape_[1, 1, 1, 128]_dtype_torch.bfloat16 (idx=44)']
[XLA Cache] New inputs: ['input_42_shape_[1, 1]_dtype_torch.int64 (idx=42)', 'input_43_shape_[1]_dtype_torch.int64 (idx=43)']
[XLA Cache] Total cache size: 59 unique tensors
Hlo input positions pre normalization [444, 2685, 443, 442, 440, 439, 436, 435, 433, 432, 2355, 459, 423, 2356, -1, 461, 2357, 2623, 429, 431, 460, 430, 424, 434, 425, 463, 438, 462, 437, 426, 441, 427]
Hlo input positions post normalization [445, 2686, 444, 443, 441, 440, 437, 436, 434, 433, 2356, 460, 424, 2357, 0, 462, 2358, 2624, 430, 432, 461, 431, 425, 435, 426, 464, 439, 463, 438, 427, 442, 428]
match key in_spec.target L__self___model_layers__modules__0___input_layernorm_weight with ID 139658584088448 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_layers__modules__0___post_attention_layernorm_weight with ID 139658589162128 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_layers__modules__1___input_layernorm_weight with ID 139658589165728 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_layers__modules__1___post_attention_layernorm_weight with ID 139658581142368 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_norm_weight with ID 139658581024640 and kind InputKind.PARAMETER
match key in_spec.target L__self___lm_head.weight with ID 139658584091808 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.0 with ID 139654952040096 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.1 with ID 139654952040496 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.2 with ID 139654952042416 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.3 with ID 139654952041616 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.4 with ID 139654952040336 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.5 with ID 139654952041056 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.6 with ID 139654952040816 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.7 with ID 139654952040576 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.8 with ID 139654952038416 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.9 with ID 139654952041216 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.10 with ID 139654952030496 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.11 with ID 139654952040656 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.12 with ID 139654952039776 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.13 with ID 139654952041696 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.14 with ID 139654952040976 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.15 with ID 139654952041856 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_q_proj.weight with ID 139658589160768 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_k_proj.weight with ID 139658589162688 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_v_proj.weight with ID 139658589173008 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_o_proj.weight with ID 139658589164768 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___mlp_gate_proj.weight with ID 139658584087008 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___mlp_up_proj.weight with ID 139658589172608 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___mlp_down_proj.weight with ID 139658584095888 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_q_proj.weight with ID 139658581139488 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_k_proj.weight with ID 139658581145168 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_v_proj.weight with ID 139658581134368 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_o_proj.weight with ID 139658581144048 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___mlp_gate_proj.weight with ID 139658589162288 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___mlp_up_proj.weight with ID 139658589171488 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___mlp_down_proj.weight with ID 139658589163008 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___lm_head.weight with ID 139658584091808 and kind InputKind.PARAMETER
match key in_spec.target kwargs____past_key_values___key_cache_0 with ID 139659373973792 and kind InputKind.BUFFER
match key in_spec.target kwargs____past_key_values___value_cache_0 with ID 139659373983872 and kind InputKind.BUFFER
match key in_spec.target kwargs____past_key_values___key_cache_1 with ID 139659373976192 and kind InputKind.BUFFER
match key in_spec.target kwargs____past_key_values___value_cache_1 with ID 139659373976432 and kind InputKind.BUFFER
match key in_spec.target const_subgraph_module.L__self___model_rotary_emb_inv_freq with ID 139658581032480 and kind InputKind.BUFFER
[JAMES] setting arg ref map to  refs=constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,139658584088448,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.539 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.540 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.540 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.540 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.540 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.540 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.540 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.540 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.540 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.540 (  46.982s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.540 (  46.982s) [        CAFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-12 15:39:46.540 (  46.982s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.982s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.982s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.982s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.982s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.982s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.982s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.982s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.982s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.982s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.982s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.982s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.982s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.982s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.982s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.982s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.982s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.982s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.982s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.982s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.982s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.983s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.983s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.983s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.983s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.983s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.983s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.983s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.983s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.983s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.983s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.983s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.540 (  46.983s) [        CAFFD640] executable_instance.cc:138      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-08-12 15:39:46.540 (  46.983s) [        CAFFD640]loaded_executable_insta:571      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-08-12 15:39:46.540 (  46.983s) [        CAFFD640]loaded_executable_insta:95       1| LoadedExecutableInstance::Execute
2025-08-12 15:39:46.540 (  46.983s) [        CAFFD640]loaded_executable_insta:125      1| [DEVICE] Runtime device already opened, reusing existing device
2025-08-12 15:39:46.540 (  46.983s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565445888ef0 (arg 0)
2025-08-12 15:39:46.540 (  46.983s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56541956fb70 (arg 1)
2025-08-12 15:39:46.540 (  46.983s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae08a20 (arg 2)
2025-08-12 15:39:46.541 (  46.984s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442df5210 (arg 3)
2025-08-12 15:39:46.541 (  46.984s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442d65330 (arg 4)
2025-08-12 15:39:46.541 (  46.984s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae22a60 (arg 5)
2025-08-12 15:39:46.624 (  47.067s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565433e38450 (arg 6)
2025-08-12 15:39:46.625 (  47.068s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ff46020 (arg 7)
2025-08-12 15:39:46.625 (  47.068s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c4e5af0 (arg 8)
2025-08-12 15:39:46.626 (  47.068s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae1a5b0 (arg 9)
2025-08-12 15:39:46.634 (  47.077s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c3539d0 (arg 10)
2025-08-12 15:39:46.635 (  47.077s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544d19c270 (arg 11)
2025-08-12 15:39:46.636 (  47.078s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565419544220 (arg 12)
2025-08-12 15:39:46.642 (  47.084s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ff47ba0 (arg 13)
2025-08-12 15:39:46.650 (  47.093s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x5654070aebb0 (arg 14)
2025-08-12 15:39:46.654 (  47.096s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ff22d90 (arg 15)
2025-08-12 15:39:46.654 (  47.097s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544d2685d0 (arg 16)
2025-08-12 15:39:46.654 (  47.097s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae3cb00 (arg 17)
2025-08-12 15:39:46.657 (  47.100s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544438c610 (arg 18)
2025-08-12 15:39:46.658 (  47.101s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544d22fa40 (arg 19)
2025-08-12 15:39:46.666 (  47.109s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c218220 (arg 20)
2025-08-12 15:39:46.667 (  47.109s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c367830 (arg 21)
2025-08-12 15:39:46.667 (  47.110s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae84780 (arg 22)
2025-08-12 15:39:46.668 (  47.111s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c34ea40 (arg 23)
2025-08-12 15:39:46.669 (  47.111s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x5654447af160 (arg 24)
2025-08-12 15:39:46.756 (  47.198s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544584e8c0 (arg 25)
2025-08-12 15:39:46.769 (  47.212s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442b58f60 (arg 26)
2025-08-12 15:39:46.778 (  47.221s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae7e8c0 (arg 27)
2025-08-12 15:39:46.782 (  47.224s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544572c160 (arg 28)
2025-08-12 15:39:46.785 (  47.228s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c2222e0 (arg 29)
2025-08-12 15:39:46.786 (  47.228s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x5654447c4260 (arg 30)
2025-08-12 15:39:46.794 (  47.237s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56541967fae0 (arg 31)
2025-08-12 15:39:46.912 (  47.355s) [        CAFFD640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:46.912 (  47.355s) [        CAFFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:46.912 (  47.355s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.912 (  47.355s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:46.912 (  47.355s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.912 (  47.355s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:46.912 (  47.355s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.912 (  47.355s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:46.912 (  47.355s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.912 (  47.355s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:46.912 (  47.355s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.912 (  47.355s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:46.912 (  47.355s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.912 (  47.355s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:46.912 (  47.355s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.912 (  47.355s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:46.912 (  47.355s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.912 (  47.355s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:46.912 (  47.355s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.912 (  47.355s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:46.912 (  47.355s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.912 (  47.355s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:46.912 (  47.355s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.912 (  47.355s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:46.912 (  47.355s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.912 (  47.355s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:46.912 (  47.355s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:46.912 (  47.355s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:46.912 (  47.355s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:46.912 (  47.355s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:46.912 (  47.355s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.912 (  47.355s) [        C545D1C0]     buffer_instance.cc:454      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-08-12 15:39:46.912 (  47.355s) [        C545D1C0]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:46.912 (  47.355s) [        C545D1C0]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:46.912 (  47.355s) [        C545D1C0]     buffer_instance.cc:476      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-08-12 15:39:46.912 (  47.355s) [        C545D1C0]     buffer_instance.cc:497      1| [BUFFER_TO_HOST] Source buffer shape: [1, 1, 128256], data_type: 13, required_size: 256512 bytes
2025-08-12 15:39:46.912 (  47.355s) [        C545D1C0]     buffer_instance.cc:508      1| [BUFFER_TO_HOST] Destination buffer: dst_size=256512 bytes, dst_ptr=0x565442b9ad00
2025-08-12 15:39:46.912 (  47.355s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:46.912 (  47.355s) [        129D1640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:46.913 (  47.355s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:46.913 (  47.356s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
alink2025-08-12 15:39:46.914 (  47.356s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:46.914 (  47.356s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:46.914 (  47.356s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:46.914 (  47.357s) [        C545D1C0]     buffer_instance.cc:213      1| [BUFFER] Creating OWNED host tensor with shape [1, 1] (semantics: other, supported_dtype: false)
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-08-12 15:39:46.914 (  47.357s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:46.914 (  47.357s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:46.914 (  47.357s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:46.914 (  47.357s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:46.914 (  47.357s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:46.914 (  47.357s) [        C545D1C0]     buffer_instance.cc:213      1| [BUFFER] Creating OWNED host tensor with shape [1] (semantics: other, supported_dtype: false)
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-08-12 15:39:46.914 (  47.357s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:46.914 (  47.357s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:46.914 (  47.357s) [        C545D1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-12 15:39:46.914 (  47.357s) [        C545D1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-12 15:39:46.914 (  47.357s) [        C545D1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-12 15:39:46.914 (  47.357s) [        C545D1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1, 1, 1, 128] (semantics: ZeroCopy/other)
2025-08-12 15:39:46.914 (  47.357s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:46.914 (  47.357s) [        C545D1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:46.914 (  47.357s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:46.914 (  47.357s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:46.914 (  47.357s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
Tensor id via xlac: 423 with shape torch.Size([3072])
Tensor id via xlac: 424 with shape torch.Size([3072])
Tensor id via xlac: 425 with shape torch.Size([3072])
Tensor id via xlac: 426 with shape torch.Size([3072])
Tensor id via xlac: 427 with shape torch.Size([3072])
Tensor id via xlac: 428 with shape torch.Size([128256, 3072])
Tensor id via xlac: 429 with shape torch.Size([1, 64, 1])
Tensor id via xlac: 430 with shape torch.Size([3072, 3072])
Tensor id via xlac: 431 with shape torch.Size([3072, 1024])
Tensor id via xlac: 432 with shape torch.Size([3072, 1024])
Tensor id via xlac: 433 with shape torch.Size([3072, 3072])
Tensor id via xlac: 434 with shape torch.Size([3072, 8192])
Tensor id via xlac: 435 with shape torch.Size([3072, 8192])
Tensor id via xlac: 436 with shape torch.Size([8192, 3072])
Tensor id via xlac: 437 with shape torch.Size([3072, 3072])
Tensor id via xlac: 438 with shape torch.Size([3072, 1024])
Tensor id via xlac: 439 with shape torch.Size([3072, 1024])
Tensor id via xlac: 440 with shape torch.Size([3072, 3072])
Tensor id via xlac: 441 with shape torch.Size([3072, 8192])
Tensor id via xlac: 442 with shape torch.Size([3072, 8192])
Tensor id via xlac: 443 with shape torch.Size([8192, 3072])
Tensor id via xlac: 444 with shape torch.Size([3072, 128256])
Tensor id via xlac: 445 with shape torch.Size([3072, 3072])
Tensor id via xlac: 446 with shape torch.Size([1024, 3072])
Tensor id via xlac: 447 with shape torch.Size([1024, 3072])
Tensor id via xlac: 448 with shape torch.Size([3072, 3072])
Tensor id via xlac: 449 with shape torch.Size([8192, 3072])
Tensor id via xlac: 450 with shape torch.Size([8192, 3072])
Tensor id via xlac: 451 with shape torch.Size([3072, 8192])
Tensor id via xlac: 452 with shape torch.Size([3072, 3072])
Tensor id via xlac: 453 with shape torch.Size([1024, 3072])
Tensor id via xlac: 454 with shape torch.Size([1024, 3072])
Tensor id via xlac: 455 with shape torch.Size([3072, 3072])
Tensor id via xlac: 456 with shape torch.Size([8192, 3072])
Tensor id via xlac: 457 with shape torch.Size([8192, 3072])
Tensor id via xlac: 458 with shape torch.Size([3072, 8192])
Tensor id via xlac: 459 with shape torch.Size([128256, 3072])
Tensor id via xlac: 460 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 461 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 462 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 463 with shape torch.Size([1, 8, 128, 128])
Tensor id via xlac: 464 with shape torch.Size([64])
Tensor id via xlac: 2733 with shape torch.Size([1, 1])
Tensor id via xlac: 2734 with shape torch.Size([1])
Tensor id via xlac: 2735 with shape torch.Size([1, 1, 1, 128])
[XLA Cache] Previously seen inputs: ['input_0_shape_[3072]_dtype_torch.bfloat16 (idx=0)', 'input_1_shape_[3072]_dtype_torch.bfloat16 (idx=1)', 'input_2_shape_[3072]_dtype_torch.bfloat16 (idx=2)', 'input_3_shape_[3072]_dtype_torch.bfloat16 (idx=3)', 'input_4_shape_[3072]_dtype_torch.bfloat16 (idx=4)', 'input_5_shape_[128256, 3072]_dtype_torch.bfloat16 (idx=5)', 'input_6_shape_[1, 64, 1]_dtype_torch.float32 (idx=6)', 'input_7_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=7)', 'input_8_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=8)', 'input_9_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=9)', 'input_10_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=10)', 'input_11_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=11)', 'input_12_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=12)', 'input_13_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=13)', 'input_14_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=14)', 'input_15_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=15)', 'input_16_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=16)', 'input_17_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=17)', 'input_18_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=18)', 'input_19_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=19)', 'input_20_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=20)', 'input_21_shape_[3072, 128256]_dtype_torch.bfloat16 (idx=21)', 'input_22_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=22)', 'input_23_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=23)', 'input_24_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=24)', 'input_25_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=25)', 'input_26_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=26)', 'input_27_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=27)', 'input_28_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=28)', 'input_29_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=29)', 'input_30_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=30)', 'input_31_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=31)', 'input_32_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=32)', 'input_33_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=33)', 'input_34_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=34)', 'input_35_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=35)', 'input_36_shape_[128256, 3072]_dtype_torch.bfloat16 (idx=36)', 'input_37_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=37)', 'input_38_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=38)', 'input_39_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=39)', 'input_40_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=40)', 'input_41_shape_[64]_dtype_torch.float32 (idx=41)']
[XLA Cache] New inputs: ['input_42_shape_[1, 1]_dtype_torch.int64 (idx=42)', 'input_43_shape_[1]_dtype_torch.int64 (idx=43)', 'input_44_shape_[1, 1, 1, 128]_dtype_torch.bfloat16 (idx=44)']
[XLA Cache] Total cache size: 62 unique tensors
Hlo input positions pre normalization [444, 3063, 443, 442, 440, 439, 436, 435, 433, 432, 2733, 459, 423, 2734, -1, 461, 2735, 3001, 429, 431, 460, 430, 424, 434, 425, 463, 438, 462, 437, 426, 441, 427]
Hlo input positions post normalization [445, 3064, 444, 443, 441, 440, 437, 436, 434, 433, 2734, 460, 424, 2735, 0, 462, 2736, 3002, 430, 432, 461, 431, 425, 435, 426, 464, 439, 463, 438, 427, 442, 428]
match key in_spec.target L__self___model_layers__modules__0___input_layernorm_weight with ID 139658584088448 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_layers__modules__0___post_attention_layernorm_weight with ID 139658589162128 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_layers__modules__1___input_layernorm_weight with ID 139658589165728 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_layers__modules__1___post_attention_layernorm_weight with ID 139658581142368 and kind InputKind.PARAMETER
match key in_spec.target L__self___model_norm_weight with ID 139658581024640 and kind InputKind.PARAMETER
match key in_spec.target L__self___lm_head.weight with ID 139658584091808 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.0 with ID 139654952040096 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.1 with ID 139654952040496 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.2 with ID 139654952042416 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.3 with ID 139654952041616 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.4 with ID 139654952040336 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.5 with ID 139654952041056 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.6 with ID 139654952040816 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.7 with ID 139654952040576 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.8 with ID 139654952038416 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.9 with ID 139654952041216 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.10 with ID 139654952030496 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.11 with ID 139654952040656 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.12 with ID 139654952039776 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.13 with ID 139654952041696 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.14 with ID 139654952040976 and kind InputKind.PARAMETER
match key in_spec.target _FX_CONST_FOLDED_ATTRS.15 with ID 139654952041856 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_q_proj.weight with ID 139658589160768 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_k_proj.weight with ID 139658589162688 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_v_proj.weight with ID 139658589173008 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___self_attn_o_proj.weight with ID 139658589164768 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___mlp_gate_proj.weight with ID 139658584087008 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___mlp_up_proj.weight with ID 139658589172608 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__0___mlp_down_proj.weight with ID 139658584095888 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_q_proj.weight with ID 139658581139488 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_k_proj.weight with ID 139658581145168 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_v_proj.weight with ID 139658581134368 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___self_attn_o_proj.weight with ID 139658581144048 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___mlp_gate_proj.weight with ID 139658589162288 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___mlp_up_proj.weight with ID 139658589171488 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___model_layers__modules__1___mlp_down_proj.weight with ID 139658589163008 and kind InputKind.PARAMETER
match key in_spec.target const_subgraph_module.L__self___lm_head.weight with ID 139658584091808 and kind InputKind.PARAMETER
match key in_spec.target kwargs____past_key_values___key_cache_0 with ID 139659373973792 and kind InputKind.BUFFER
match key in_spec.target kwargs____past_key_values___value_cache_0 with ID 139659373983872 and kind InputKind.BUFFER
match key in_spec.target kwargs____past_key_values___key_cache_1 with ID 139659373976192 and kind InputKind.BUFFER
match key in_spec.target kwargs____past_key_values___value_cache_1 with ID 139659373976432 and kind InputKind.BUFFER
match key in_spec.target const_subgraph_module.L__self___model_rotary_emb_inv_freq with ID 139658581032480 and kind InputKind.BUFFER
[JAMES] setting arg ref map to  refs=constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,139658584088448,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.920 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.363s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.364s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.364s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.364s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.364s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.364s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.364s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.364s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.364s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.364s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.364s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.364s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.364s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.364s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.364s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.364s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.364s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.364s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.364s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.364s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.364s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.364s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.364s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.364s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.364s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640] executable_instance.cc:138      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]loaded_executable_insta:571      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]loaded_executable_insta:95       1| LoadedExecutableInstance::Execute
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]loaded_executable_insta:125      1| [DEVICE] Runtime device already opened, reusing existing device
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442e56f80 (arg 0)
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56541956fb70 (arg 1)
2025-08-12 15:39:46.921 (  47.364s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae08a20 (arg 2)
2025-08-12 15:39:46.923 (  47.365s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442df5210 (arg 3)
2025-08-12 15:39:46.923 (  47.365s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c46d430 (arg 4)
2025-08-12 15:39:46.923 (  47.365s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae22a60 (arg 5)
2025-08-12 15:39:47.006 (  47.448s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565433e38450 (arg 6)
2025-08-12 15:39:47.006 (  47.449s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ff46020 (arg 7)
2025-08-12 15:39:47.007 (  47.449s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c4e0610 (arg 8)
2025-08-12 15:39:47.007 (  47.450s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae1a5b0 (arg 9)
2025-08-12 15:39:47.016 (  47.458s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c46ef50 (arg 10)
2025-08-12 15:39:47.016 (  47.459s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544d19c270 (arg 11)
2025-08-12 15:39:47.017 (  47.460s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565419544220 (arg 12)
2025-08-12 15:39:47.023 (  47.466s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ff47ba0 (arg 13)
2025-08-12 15:39:47.032 (  47.475s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x5654070aebb0 (arg 14)
2025-08-12 15:39:47.036 (  47.478s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565445829370 (arg 15)
2025-08-12 15:39:47.036 (  47.479s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544d2685d0 (arg 16)
2025-08-12 15:39:47.036 (  47.479s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae3cb00 (arg 17)
2025-08-12 15:39:47.039 (  47.482s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544438c610 (arg 18)
2025-08-12 15:39:47.040 (  47.483s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544d22fa40 (arg 19)
2025-08-12 15:39:47.048 (  47.491s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c218220 (arg 20)
2025-08-12 15:39:47.049 (  47.492s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c35b3f0 (arg 21)
2025-08-12 15:39:47.049 (  47.492s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae84780 (arg 22)
2025-08-12 15:39:47.051 (  47.493s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c479f40 (arg 23)
2025-08-12 15:39:47.051 (  47.494s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x5654447af160 (arg 24)
2025-08-12 15:39:47.138 (  47.581s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544584e8c0 (arg 25)
2025-08-12 15:39:47.152 (  47.595s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x565442b58f60 (arg 26)
2025-08-12 15:39:47.161 (  47.604s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56543ae7e8c0 (arg 27)
2025-08-12 15:39:47.164 (  47.607s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56544572c160 (arg 28)
2025-08-12 15:39:47.168 (  47.610s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7efa8c2222e0 (arg 29)
2025-08-12 15:39:47.168 (  47.611s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x5654447c4260 (arg 30)
2025-08-12 15:39:47.176 (  47.619s) [        CAFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x56541967fae0 (arg 31)
2025-08-12 15:39:47.296 (  47.739s) [        CAFFD640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:47.296 (  47.739s) [        CAFFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:47.296 (  47.739s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:47.296 (  47.739s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:47.296 (  47.739s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:47.296 (  47.739s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:47.296 (  47.739s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:47.296 (  47.739s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:47.296 (  47.739s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:47.296 (  47.739s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:47.296 (  47.739s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:47.296 (  47.739s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:47.296 (  47.739s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:47.296 (  47.739s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:47.296 (  47.739s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:47.296 (  47.739s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:47.296 (  47.739s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:47.296 (  47.739s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:47.296 (  47.739s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:47.296 (  47.739s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:47.296 (  47.739s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:47.296 (  47.739s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:47.296 (  47.739s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:47.296 (  47.739s) [        CAFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-12 15:39:47.296 (  47.739s) [        CAFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:47.296 (  47.739s) [        CAFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:47.296 (  47.739s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.296 (  47.739s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.296 (  47.739s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.296 (  47.739s) [        CAFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.296 (  47.739s) [        C545D1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-12 15:39:47.297 (  47.739s) [        C545D1C0]     buffer_instance.cc:454      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-08-12 15:39:47.297 (  47.739s) [        C545D1C0]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-12 15:39:47.297 (  47.739s) [        C545D1C0]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-12 15:39:47.297 (  47.739s) [        C545D1C0]     buffer_instance.cc:476      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-08-12 15:39:47.297 (  47.739s) [        C545D1C0]     buffer_instance.cc:497      1| [BUFFER_TO_HOST] Source buffer shape: [1, 1, 128256], data_type: 13, required_size: 256512 bytes
2025-08-12 15:39:47.297 (  47.739s) [        C545D1C0]     buffer_instance.cc:508      1| [BUFFER_TO_HOST] Destination buffer: dst_size=256512 bytes, dst_ptr=0x5654446f37c0
2025-08-12 15:39:47.297 (  47.740s) [        C545D1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-12 15:39:47.297 (  47.740s) [        121D0640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-12 15:39:47.303 (  47.746s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.303 (  47.746s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
alink2025-08-12 15:39:47.304 (  47.747s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.305 (  47.748s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.305 (  47.748s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.305 (  47.748s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.305 (  47.748s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.305 (  47.748s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.305 (  47.748s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.305 (  47.748s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.305 (  47.748s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.340 (  47.783s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.343 (  47.786s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.345 (  47.788s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.348 (  47.790s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.348 (  47.791s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.348 (  47.791s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.348 (  47.791s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.348 (  47.791s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.351 (  47.793s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.353 (  47.796s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.356 (  47.798s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.356 (  47.799s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.359 (  47.802s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.360 (  47.802s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.360 (  47.803s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.392 (  47.835s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.394 (  47.837s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.396 (  47.839s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.398 (  47.841s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.399 (  47.841s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.399 (  47.841s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.399 (  47.842s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.399 (  47.842s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.401 (  47.844s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.403 (  47.846s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.405 (  47.848s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.408 (  47.851s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.408 (  47.851s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.408 (  47.851s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.408 (  47.851s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.408 (  47.851s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.443 (  47.886s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.443 (  47.886s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.443 (  47.886s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.443 (  47.886s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.444 (  47.886s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.524 (  47.967s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.524 (  47.967s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.524 (  47.967s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.524 (  47.967s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.524 (  47.967s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.524 (  47.967s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.524 (  47.967s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.524 (  47.967s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.524 (  47.967s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.566 (  48.009s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.569 (  48.012s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.572 (  48.014s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.575 (  48.017s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.575 (  48.018s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.575 (  48.018s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.575 (  48.018s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.575 (  48.018s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.578 (  48.021s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.581 (  48.024s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.584 (  48.027s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.584 (  48.027s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.585 (  48.027s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.585 (  48.028s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.585 (  48.028s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.622 (  48.065s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.625 (  48.067s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.627 (  48.070s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.630 (  48.073s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.630 (  48.073s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.630 (  48.073s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.630 (  48.073s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.631 (  48.073s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.633 (  48.076s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.636 (  48.078s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.638 (  48.081s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.638 (  48.081s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.638 (  48.081s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.639 (  48.081s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.639 (  48.081s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.639 (  48.082s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.681 (  48.124s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.681 (  48.124s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.682 (  48.124s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.682 (  48.124s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-12 15:39:47.682 (  48.125s) [        C545D1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy

================================================================================
GENERATION SUMMARY
================================================================================
Initial prompt: '<|begin_of_text|>I like taking walks in the'
Generated text: '<|begin_of_text|>I like taking walks in thealinkalinkalinkalinkalinkalinkalinkalink'

Model loading time: 0.822s
Total generation time: 49.668s
Tokens generated: 8
Average tokens/second: 0.16

PREFILL (first token):
  Time: 39.609s

DECODE (subsequent tokens):
  Count: 7
  Average time: 1.437s
  Min time: 0.385s
  Max time: 7.711s
  Average tokens/second: 0.70

DETAILED TIMING (all iterations):
Iter | Phase   | Total   | Token
--------------------------------------------------
   0 | prefill | 39.609s | 'alink'
   1 | decode  |  7.711s | 'alink'
   2 | decode  |  0.397s | 'alink'
   3 | decode  |  0.390s | 'alink'
   4 | decode  |  0.394s | 'alink'
   5 | decode  |  0.390s | 'alink'
   6 | decode  |  0.385s | 'alink'
   7 | decode  |  0.391s | 'alink'
PASSED

=============================== warnings summary ===============================
tests/models/llama/test_llama3_generative.py::test_llama3_generate
tests/models/llama/test_llama3_generative.py::test_llama3_generate
  /localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/torch/fx/experimental/const_fold.py:264: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer
    new_node = root_const_gm.graph.get_attr(in_node.target)

tests/models/llama/test_llama3_generative.py::test_llama3_generate
tests/models/llama/test_llama3_generative.py::test_llama3_generate
  /localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/torch/fx/graph.py:1316: UserWarning: Failed to fetch module L__self___model_layers__modules__0___self_attn_q_proj!
    warnings.warn(f"Failed to fetch module {module_path}!")

tests/models/llama/test_llama3_generative.py::test_llama3_generate
tests/models/llama/test_llama3_generative.py::test_llama3_generate
  /localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/torch/fx/graph.py:1316: UserWarning: Failed to fetch module L__self___model_layers__modules__0___self_attn_k_proj!
    warnings.warn(f"Failed to fetch module {module_path}!")

tests/models/llama/test_llama3_generative.py::test_llama3_generate
tests/models/llama/test_llama3_generative.py::test_llama3_generate
  /localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/torch/fx/graph.py:1316: UserWarning: Failed to fetch module L__self___model_layers__modules__0___self_attn_v_proj!
    warnings.warn(f"Failed to fetch module {module_path}!")

tests/models/llama/test_llama3_generative.py::test_llama3_generate
tests/models/llama/test_llama3_generative.py::test_llama3_generate
  /localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/torch/fx/graph.py:1316: UserWarning: Failed to fetch module L__self___model_layers__modules__0___self_attn_o_proj!
    warnings.warn(f"Failed to fetch module {module_path}!")

tests/models/llama/test_llama3_generative.py::test_llama3_generate
tests/models/llama/test_llama3_generative.py::test_llama3_generate
  /localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/torch/fx/graph.py:1316: UserWarning: Failed to fetch module L__self___model_layers__modules__0___mlp_gate_proj!
    warnings.warn(f"Failed to fetch module {module_path}!")

tests/models/llama/test_llama3_generative.py::test_llama3_generate
tests/models/llama/test_llama3_generative.py::test_llama3_generate
  /localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/torch/fx/graph.py:1316: UserWarning: Failed to fetch module L__self___model_layers__modules__0___mlp_up_proj!
    warnings.warn(f"Failed to fetch module {module_path}!")

tests/models/llama/test_llama3_generative.py::test_llama3_generate
tests/models/llama/test_llama3_generative.py::test_llama3_generate
  /localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/torch/fx/graph.py:1316: UserWarning: Failed to fetch module L__self___model_layers__modules__0___mlp_down_proj!
    warnings.warn(f"Failed to fetch module {module_path}!")

tests/models/llama/test_llama3_generative.py::test_llama3_generate
tests/models/llama/test_llama3_generative.py::test_llama3_generate
  /localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/torch/fx/graph.py:1316: UserWarning: Failed to fetch module L__self___model_layers__modules__1___self_attn_q_proj!
    warnings.warn(f"Failed to fetch module {module_path}!")

tests/models/llama/test_llama3_generative.py::test_llama3_generate
tests/models/llama/test_llama3_generative.py::test_llama3_generate
  /localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/torch/fx/graph.py:1316: UserWarning: Failed to fetch module L__self___model_layers__modules__1___self_attn_k_proj!
    warnings.warn(f"Failed to fetch module {module_path}!")

tests/models/llama/test_llama3_generative.py::test_llama3_generate
tests/models/llama/test_llama3_generative.py::test_llama3_generate
  /localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/torch/fx/graph.py:1316: UserWarning: Failed to fetch module L__self___model_layers__modules__1___self_attn_v_proj!
    warnings.warn(f"Failed to fetch module {module_path}!")

tests/models/llama/test_llama3_generative.py::test_llama3_generate
tests/models/llama/test_llama3_generative.py::test_llama3_generate
  /localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/torch/fx/graph.py:1316: UserWarning: Failed to fetch module L__self___model_layers__modules__1___self_attn_o_proj!
    warnings.warn(f"Failed to fetch module {module_path}!")

tests/models/llama/test_llama3_generative.py::test_llama3_generate
tests/models/llama/test_llama3_generative.py::test_llama3_generate
  /localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/torch/fx/graph.py:1316: UserWarning: Failed to fetch module L__self___model_layers__modules__1___mlp_gate_proj!
    warnings.warn(f"Failed to fetch module {module_path}!")

tests/models/llama/test_llama3_generative.py::test_llama3_generate
tests/models/llama/test_llama3_generative.py::test_llama3_generate
  /localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/torch/fx/graph.py:1316: UserWarning: Failed to fetch module L__self___model_layers__modules__1___mlp_up_proj!
    warnings.warn(f"Failed to fetch module {module_path}!")

tests/models/llama/test_llama3_generative.py::test_llama3_generate
tests/models/llama/test_llama3_generative.py::test_llama3_generate
  /localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/torch/fx/graph.py:1316: UserWarning: Failed to fetch module L__self___model_layers__modules__1___mlp_down_proj!
    warnings.warn(f"Failed to fetch module {module_path}!")

tests/models/llama/test_llama3_generative.py::test_llama3_generate
tests/models/llama/test_llama3_generative.py::test_llama3_generate
  /localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/torch/fx/graph.py:1316: UserWarning: Failed to fetch module L__self___lm_head!
    warnings.warn(f"Failed to fetch module {module_path}!")

tests/models/llama/test_llama3_generative.py::test_llama3_generate
tests/models/llama/test_llama3_generative.py::test_llama3_generate
tests/models/llama/test_llama3_generative.py::test_llama3_generate
tests/models/llama/test_llama3_generative.py::test_llama3_generate
tests/models/llama/test_llama3_generative.py::test_llama3_generate
tests/models/llama/test_llama3_generative.py::test_llama3_generate
tests/models/llama/test_llama3_generative.py::test_llama3_generate
tests/models/llama/test_llama3_generative.py::test_llama3_generate
  /localdev/jameszianxu/gen_mc/tt-torch/tt_torch/dynamo/experimental/xla_backend.py:810: DeprecationWarning: Use torch_xla.sync instead
    xm.mark_step() # explicit compile step

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
======================= 1 passed, 40 warnings in 53.73s ========================
2025-08-12 15:39:50.353 (  50.795s) [        C545D1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-12 15:39:50.353 (  50.795s) [        C545D1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-12 15:39:50.353 (  50.795s) [        C545D1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-12 15:39:50.353 (  50.795s) [        C545D1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
