WARNING:root:Defaulting to PJRT_DEVICE=CPU
Read unexpected run_mailbox value: 0x40 (expected 0x80 or 0x0)
2025-09-17 20:01:46.716 | critical |          Always | Read unexpected run_mailbox value from core (x=25,y=17) (assert.hpp:111)
Read unexpected run_mailbox value: 0x40 (expected 0x80 or 0x0)
2025-09-17 20:01:46.777 | critical |          Always | Read unexpected run_mailbox value from core (x=25,y=16) (assert.hpp:111)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Device grid size = { 8, 8 }
2025-09-17 20:01:50.200619: W torch_xla/csrc/runtime/profiler.cpp:88] Profiler API not found for PJRT plugin
/localdev/jameszianxu/tt-xla/examples/pytorch/llama.py:69: DeprecationWarning: Use torch_xla.device instead
  device = xm.xla_device()
Using TT-Metal from the source tree: /localdev/jameszianxu/tt-xla/third_party/tt-mlir/src/tt-mlir/third_party/tt-metal/src/tt-metal
WARNING: TT plugin is setting XLA_STABLEHLO_COMPILE to 1. This is required for TT PJRT plugin to work correctly.
Setting up XLA environment...
XLA environment configured.
Created device mesh: (1, 2) with 2 devices
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 52.17it/s]
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
#loc1 = loc("p0.3")
#loc2 = loc("p1.13")
#loc3 = loc("p2.31")
#loc4 = loc("p3.37")
#loc5 = loc("p4.39")
#loc6 = loc("p5.44")
#loc7 = loc("p6.81")
#loc8 = loc("p7.118")
#loc9 = loc("p8.130")
#loc10 = loc("p9.139")
#loc11 = loc("p10.160")
#loc12 = loc("p11.169")
#loc13 = loc("p12.177")
#loc14 = loc("p13.182")
#loc15 = loc("p14.190")
#loc16 = loc("p15.220")
#loc17 = loc("p16.254")
#loc18 = loc("p17.269")
#loc19 = loc("p18.371")
#loc20 = loc("p19.383")
#loc21 = loc("p20.435")
#loc47 = loc("reduce.60")
#loc99 = loc("scatter.136")
#loc109 = loc("scatter.166")
#loc163 = loc("reduce.316")
#loc168 = loc("reduce.325")
#loc194 = loc("reduce.350")
#loc239 = loc("reduce.414")
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<7x!vhlo.i64_v1> loc("p0.3"), %arg1: !vhlo.tensor_v1<64x!vhlo.f32_v1> loc("p1.13"), %arg2: !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1> loc("p2.31"), %arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("p3.37"), %arg4: !vhlo.tensor_v1<1x7x!vhlo.i64_v1> loc("p4.39"), %arg5: !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1> loc("p5.44"), %arg6: !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc("p6.81"), %arg7: !vhlo.tensor_v1<!vhlo.i64_v1> loc("p7.118"), %arg8: !vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1> loc("p8.130"), %arg9: !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1> loc("p9.139"), %arg10: !vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1> loc("p10.160"), %arg11: !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1> loc("p11.169"), %arg12: !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1> loc("p12.177"), %arg13: !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1> loc("p13.182"), %arg14: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1> loc("p14.190"), %arg15: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("p15.220"), %arg16: !vhlo.tensor_v1<!vhlo.f32_v1> loc("p16.254"), %arg17: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1> loc("p17.269"), %arg18: !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc("p18.371"), %arg19: !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1> loc("p19.383"), %arg20: !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc("p20.435")) -> (!vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>>}> : () -> !vhlo.tensor_v1<16x!vhlo.i64_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>>}> : () -> !vhlo.tensor_v1<7x!vhlo.i64_v1> loc(#loc)
    %3 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF800000> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %4 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0> : tensor<7xi64>>}> : () -> !vhlo.tensor_v1<7x!vhlo.i64_v1> loc(#loc)
    %5 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %6 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<3.25520843E-4> : tensor<1x7xf32>>}> : () -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1> loc(#loc)
    %7 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1> : tensor<i64>>}> : () -> !vhlo.tensor_v1<!vhlo.i64_v1> loc(#loc)
    %8 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc)
    %9 = "vhlo.broadcast_in_dim_v1"(%8) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.bf16_v1> loc(#loc)
    %10 = "vhlo.broadcast_in_dim_v1"(%7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.i64_v1> loc(#loc)
    %11 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1> loc(#loc)
    %12 = "vhlo.custom_call_v1"(%arg8) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_2">}>} : (!vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1> loc(#loc22)
    %13 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1> loc(#loc23)
    %14 = "vhlo.custom_call_v1"(%13) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_1">}>} : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1> loc(#loc24)
    %15 = "vhlo.reshape_v1"(%14) : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1> loc(#loc25)
    %16 = "vhlo.compare_v1"(%15, %4) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 LT>}> : (!vhlo.tensor_v1<7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.bool_v1> loc(#loc26)
    %17 = "vhlo.broadcast_in_dim_v1"(%arg7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1> loc(#loc27)
    %18 = "vhlo.add_v1"(%15, %17) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1> loc(#loc28)
    %19 = "vhlo.select_v1"(%16, %18, %15) : (!vhlo.tensor_v1<7x!vhlo.bool_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1> loc(#loc29)
    %20 = "vhlo.reshape_v1"(%19) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x1x!vhlo.i64_v1> loc(#loc30)
    %21 = "vhlo.reshape_v1"(%arg6) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc31)
    %22 = "vhlo.custom_call_v1"(%21) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___input_layernorm_weight">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc32)
    %23 = "vhlo.reshape_v1"(%22) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc(#loc33)
    %24 = "vhlo.convert_v1"(%23) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1> loc(#loc34)
    %25 = "vhlo.broadcast_in_dim_v1"(%24) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1> loc(#loc35)
    %26 = "vhlo.reshape_v1"(%arg5) : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1> loc(#loc36)
    %27 = "vhlo.custom_call_v1"(%26) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_embed_tokens_weight">}>} : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1> loc(#loc37)
    %28 = "vhlo.reshape_v1"(%27) : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1> loc(#loc38)
    %29 = "vhlo.reshape_v1"(%arg4) : (!vhlo.tensor_v1<1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1> loc(#loc39)
    %30 = "vhlo.custom_call_v1"(%29) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_0">}>} : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1> loc(#loc40)
    %31 = "vhlo.reshape_v1"(%30) : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1> loc(#loc41)
    %32 = "vhlo.convert_v1"(%31) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.ui32_v1> loc(#loc42)
    %33 = "vhlo.gather_v2"(%28, %32) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 3072]> : tensor<2xi64>>, start_index_map = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x!vhlo.ui32_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1> loc(#loc43)
    %34 = "vhlo.reshape_v1"(%33) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1> loc(#loc44)
    %35 = "vhlo.convert_v1"(%34) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1> loc(#loc45)
    %36 = "vhlo.power_v1"(%35, %11) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1> loc(#loc46)
    %37 = "vhlo.reduce_v1"(%36, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.60"), %arg22: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.60")):
      %244 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc48)
      "vhlo.return_v1"(%244) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1> loc(#loc47)
    %38 = "vhlo.multiply_v1"(%37, %6) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1> loc(#loc49)
    %39 = "vhlo.reshape_v1"(%38) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1> loc(#loc50)
    %40 = "vhlo.broadcast_in_dim_v1"(%arg3) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1> loc(#loc51)
    %41 = "vhlo.add_v1"(%39, %40) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1> loc(#loc52)
    %42 = "vhlo.rsqrt_v2"(%41) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1> loc(#loc53)
    %43 = "vhlo.reshape_v1"(%42) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1> loc(#loc54)
    %44 = "vhlo.broadcast_in_dim_v1"(%43) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1> loc(#loc55)
    %45 = "vhlo.multiply_v1"(%35, %44) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1> loc(#loc56)
    %46 = "vhlo.convert_v1"(%45) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1> loc(#loc57)
    %47 = "vhlo.convert_v1"(%46) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1> loc(#loc58)
    %48 = "vhlo.multiply_v1"(%25, %47) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1> loc(#loc59)
    %49 = "vhlo.convert_v1"(%48) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1> loc(#loc60)
    %50 = "vhlo.reshape_v1"(%49) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1> loc(#loc61)
    %51 = "vhlo.reshape_v1"(%arg2) : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1> loc(#loc62)
    %52 = "vhlo.custom_call_v1"(%51) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1> loc(#loc63)
    %53 = "vhlo.reshape_v1"(%52) : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1> loc(#loc64)
    %54 = "vhlo.transpose_v1"(%53) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,1024]{0,1}">} : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1> loc(#loc65)
    %55 = "vhlo.dot_general_v2"(%50, %54) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1> loc(#loc66)
    %56 = "vhlo.reshape_v1"(%55) : (!vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1> loc(#loc67)
    %57 = "vhlo.transpose_v1"(%56) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1> loc(#loc68)
    %58 = "vhlo.convert_v1"(%57) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1> loc(#loc69)
    %59 = "vhlo.reshape_v1"(%arg1) : (!vhlo.tensor_v1<64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1> loc(#loc70)
    %60 = "vhlo.custom_call_v1"(%59) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_rotary_emb_inv_freq">}>} : (!vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1> loc(#loc71)
    %61 = "vhlo.reshape_v1"(%60) : (!vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x1x!vhlo.f32_v1> loc(#loc72)
    %62 = "vhlo.convert_v1"(%14) : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.f32_v1> loc(#loc73)
    %63 = "vhlo.dot_general_v2"(%61, %62) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x!vhlo.f32_v1> loc(#loc74)
    %64 = "vhlo.transpose_v1"(%63) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1]> : tensor<3xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[1, 2, 0]> : tensor<3xindex>>, xla_shape = #vhlo.string_v1<"f32[1,7,64]{1,2,0}">} : (!vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x64x!vhlo.f32_v1> loc(#loc75)
    %65 = "vhlo.concatenate_v1"(%64, %64) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1> loc(#loc76)
    %66 = "vhlo.cosine_v2"(%65) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1> loc(#loc77)
    %67 = "vhlo.convert_v1"(%66) : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1> loc(#loc78)
    %68 = "vhlo.reshape_v1"(%67) : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1> loc(#loc79)
    %69 = "vhlo.convert_v1"(%68) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1> loc(#loc80)
    %70 = "vhlo.reshape_v1"(%69) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1> loc(#loc81)
    %71 = "vhlo.broadcast_in_dim_v1"(%70) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1> loc(#loc82)
    %72 = "vhlo.multiply_v1"(%58, %71) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1> loc(#loc83)
    %73 = "vhlo.convert_v1"(%72) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1> loc(#loc84)
    %74 = "vhlo.slice_v1"(%57) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 7, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1> loc(#loc85)
    %75 = "vhlo.negate_v1"(%74) : (!vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1> loc(#loc86)
    %76 = "vhlo.slice_v1"(%57) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 7, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1> loc(#loc87)
    %77 = "vhlo.concatenate_v1"(%75, %76) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1> loc(#loc88)
    %78 = "vhlo.convert_v1"(%77) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1> loc(#loc89)
    %79 = "vhlo.sine_v2"(%65) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1> loc(#loc90)
    %80 = "vhlo.convert_v1"(%79) : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1> loc(#loc91)
    %81 = "vhlo.reshape_v1"(%80) : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1> loc(#loc92)
    %82 = "vhlo.convert_v1"(%81) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1> loc(#loc93)
    %83 = "vhlo.reshape_v1"(%82) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1> loc(#loc94)
    %84 = "vhlo.broadcast_in_dim_v1"(%83) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1> loc(#loc95)
    %85 = "vhlo.multiply_v1"(%78, %84) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1> loc(#loc96)
    %86 = "vhlo.convert_v1"(%85) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1> loc(#loc97)
    %87 = "vhlo.add_v1"(%73, %86) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1> loc(#loc98)
    %88 = "vhlo.scatter_v2"(%12, %20, %87) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("scatter.136"), %arg22: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("scatter.136")):
      "vhlo.return_v1"(%arg22) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1> loc(#loc99)
    %89 = "vhlo.custom_call_v1"(%88) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"Sharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>">}>, mhlo.sharding = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">} : (!vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1> loc(#loc100)
    %90 = "vhlo.custom_call_v1"(%arg10) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_3">}>} : (!vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1> loc(#loc101)
    %91 = "vhlo.reshape_v1"(%arg9) : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1> loc(#loc102)
    %92 = "vhlo.custom_call_v1"(%91) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1> loc(#loc103)
    %93 = "vhlo.reshape_v1"(%92) : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1> loc(#loc104)
    %94 = "vhlo.transpose_v1"(%93) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,1024]{0,1}">} : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1> loc(#loc105)
    %95 = "vhlo.dot_general_v2"(%50, %94) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1> loc(#loc106)
    %96 = "vhlo.reshape_v1"(%95) : (!vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1> loc(#loc107)
    %97 = "vhlo.transpose_v1"(%96) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1> loc(#loc108)
    %98 = "vhlo.scatter_v2"(%90, %20, %97) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("scatter.166"), %arg22: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("scatter.166")):
      "vhlo.return_v1"(%arg22) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1> loc(#loc109)
    %99 = "vhlo.custom_call_v1"(%98) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"Sharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>">}>, mhlo.sharding = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">} : (!vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1> loc(#loc110)
    %100 = "vhlo.reshape_v1"(%arg20) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc111)
    %101 = "vhlo.custom_call_v1"(%100) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_norm_weight">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc112)
    %102 = "vhlo.reshape_v1"(%101) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc(#loc113)
    %103 = "vhlo.convert_v1"(%102) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1> loc(#loc114)
    %104 = "vhlo.broadcast_in_dim_v1"(%103) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1> loc(#loc115)
    %105 = "vhlo.reshape_v1"(%arg17) : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1> loc(#loc116)
    %106 = "vhlo.custom_call_v1"(%105) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1> loc(#loc117)
    %107 = "vhlo.reshape_v1"(%106) : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1> loc(#loc118)
    %108 = "vhlo.transpose_v1"(%107) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,3072]{0,1}">} : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1> loc(#loc119)
    %109 = "vhlo.dot_general_v2"(%50, %108) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1> loc(#loc120)
    %110 = "vhlo.reshape_v1"(%109) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1> loc(#loc121)
    %111 = "vhlo.transpose_v1"(%110) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,24,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1> loc(#loc122)
    %112 = "vhlo.convert_v1"(%111) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,24,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1> loc(#loc123)
    %113 = "vhlo.broadcast_in_dim_v1"(%70) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1> loc(#loc124)
    %114 = "vhlo.multiply_v1"(%112, %113) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1> loc(#loc125)
    %115 = "vhlo.convert_v1"(%114) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1> loc(#loc126)
    %116 = "vhlo.slice_v1"(%111) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 7, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1> loc(#loc127)
    %117 = "vhlo.negate_v1"(%116) : (!vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1> loc(#loc128)
    %118 = "vhlo.slice_v1"(%111) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 7, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1> loc(#loc129)
    %119 = "vhlo.concatenate_v1"(%117, %118) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1> loc(#loc130)
    %120 = "vhlo.convert_v1"(%119) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1> loc(#loc131)
    %121 = "vhlo.broadcast_in_dim_v1"(%83) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1> loc(#loc132)
    %122 = "vhlo.multiply_v1"(%120, %121) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1> loc(#loc133)
    %123 = "vhlo.convert_v1"(%122) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1> loc(#loc134)
    %124 = "vhlo.add_v1"(%115, %123) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1> loc(#loc135)
    %125 = "vhlo.reshape_v1"(%124) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1> loc(#loc136)
    %126 = "vhlo.broadcast_in_dim_v1"(%88) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x16x128x!vhlo.bf16_v1> loc(#loc137)
    %127 = "vhlo.reshape_v1"(%126) : (!vhlo.tensor_v1<1x8x3x16x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x16x128x!vhlo.bf16_v1> loc(#loc138)
    %128 = "vhlo.transpose_v1"(%127) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 3, 1, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,24,128,16]{2,3,1,0}">} : (!vhlo.tensor_v1<1x24x16x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x128x16x!vhlo.bf16_v1> loc(#loc139)
    %129 = "vhlo.reshape_v1"(%128) : (!vhlo.tensor_v1<1x24x128x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x128x16x!vhlo.bf16_v1> loc(#loc140)
    %130 = "vhlo.dot_general_v2"(%125, %129) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x128x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x16x!vhlo.bf16_v1> loc(#loc141)
    %131 = "vhlo.reshape_v1"(%130) : (!vhlo.tensor_v1<24x7x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.bf16_v1> loc(#loc142)
    %132 = "vhlo.convert_v1"(%131) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1> loc(#loc143)
    %133 = "vhlo.broadcast_in_dim_v1"(%arg16) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1> loc(#loc144)
    %134 = "vhlo.multiply_v1"(%132, %133) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1> loc(#loc145)
    %135 = "vhlo.convert_v1"(%134) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.bf16_v1> loc(#loc146)
    %136 = "vhlo.broadcast_in_dim_v1"(%1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<16x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.i64_v1> loc(#loc147)
    %137 = "vhlo.broadcast_in_dim_v1"(%2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.i64_v1> loc(#loc148)
    %138 = "vhlo.subtract_v1"(%136, %137) : (!vhlo.tensor_v1<7x16x!vhlo.i64_v1>, !vhlo.tensor_v1<7x16x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.i64_v1> loc(#loc149)
    %139 = "vhlo.compare_v1"(%138, %10) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GE>}> : (!vhlo.tensor_v1<7x16x!vhlo.i64_v1>, !vhlo.tensor_v1<7x16x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.bool_v1> loc(#loc150)
    %140 = "vhlo.broadcast_in_dim_v1"(%arg15) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.bf16_v1> loc(#loc151)
    %141 = "vhlo.select_v1"(%139, %140, %9) : (!vhlo.tensor_v1<7x16x!vhlo.bool_v1>, !vhlo.tensor_v1<7x16x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.bf16_v1> loc(#loc152)
    %142 = "vhlo.convert_v1"(%141) : (!vhlo.tensor_v1<7x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.f32_v1> loc(#loc153)
    %143 = "vhlo.broadcast_in_dim_v1"(%15) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.i64_v1> loc(#loc154)
    %144 = "vhlo.compare_v1"(%136, %143) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<7x16x!vhlo.i64_v1>, !vhlo.tensor_v1<7x16x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.bool_v1> loc(#loc155)
    %145 = "vhlo.convert_v1"(%144) : (!vhlo.tensor_v1<7x16x!vhlo.bool_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.f32_v1> loc(#loc156)
    %146 = "vhlo.multiply_v1"(%142, %145) : (!vhlo.tensor_v1<7x16x!vhlo.f32_v1>, !vhlo.tensor_v1<7x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.f32_v1> loc(#loc157)
    %147 = "vhlo.convert_v1"(%146) : (!vhlo.tensor_v1<7x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.bf16_v1> loc(#loc158)
    %148 = "vhlo.reshape_v1"(%147) : (!vhlo.tensor_v1<7x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x16x!vhlo.bf16_v1> loc(#loc159)
    %149 = "vhlo.broadcast_in_dim_v1"(%148) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.bf16_v1> loc(#loc160)
    %150 = "vhlo.add_v1"(%135, %149) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x7x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.bf16_v1> loc(#loc161)
    %151 = "vhlo.convert_v1"(%150) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1> loc(#loc162)
    %152 = "vhlo.reduce_v1"(%151, %3) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.316"), %arg22: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.316")):
      %244 = "vhlo.maximum_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc164)
      "vhlo.return_v1"(%244) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x!vhlo.f32_v1> loc(#loc163)
    %153 = "vhlo.broadcast_in_dim_v1"(%152) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1> loc(#loc165)
    %154 = "vhlo.subtract_v1"(%151, %153) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1> loc(#loc166)
    %155 = "vhlo.exponential_v2"(%154) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1> loc(#loc167)
    %156 = "vhlo.reduce_v1"(%155, %0) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.325"), %arg22: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.325")):
      %244 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc169)
      "vhlo.return_v1"(%244) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x!vhlo.f32_v1> loc(#loc168)
    %157 = "vhlo.broadcast_in_dim_v1"(%156) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1> loc(#loc170)
    %158 = "vhlo.divide_v1"(%155, %157) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1> loc(#loc171)
    %159 = "vhlo.convert_v1"(%158) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.bf16_v1> loc(#loc172)
    %160 = "vhlo.reshape_v1"(%159) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x16x!vhlo.bf16_v1> loc(#loc173)
    %161 = "vhlo.broadcast_in_dim_v1"(%98) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x16x128x!vhlo.bf16_v1> loc(#loc174)
    %162 = "vhlo.reshape_v1"(%161) : (!vhlo.tensor_v1<1x8x3x16x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x16x128x!vhlo.bf16_v1> loc(#loc175)
    %163 = "vhlo.dot_general_v2"(%160, %162) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x7x16x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x16x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1> loc(#loc176)
    %164 = "vhlo.reshape_v1"(%163) : (!vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1> loc(#loc177)
    %165 = "vhlo.transpose_v1"(%164) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,7,24,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1> loc(#loc178)
    %166 = "vhlo.reshape_v1"(%165) : (!vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1> loc(#loc179)
    %167 = "vhlo.reshape_v1"(%arg14) : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1> loc(#loc180)
    %168 = "vhlo.custom_call_v1"(%167) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_o_proj_weight">}>} : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1> loc(#loc181)
    %169 = "vhlo.reshape_v1"(%168) : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1> loc(#loc182)
    %170 = "vhlo.transpose_v1"(%169) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,3072]{0,1}">} : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1> loc(#loc183)
    %171 = "vhlo.dot_general_v2"(%166, %170) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1> loc(#loc184)
    %172 = "vhlo.reshape_v1"(%171) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1> loc(#loc185)
    %173 = "vhlo.add_v1"(%34, %172) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1> loc(#loc186)
    %174 = "vhlo.reshape_v1"(%arg18) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc187)
    %175 = "vhlo.custom_call_v1"(%174) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___post_attention_layernorm_weight">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc188)
    %176 = "vhlo.reshape_v1"(%175) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc(#loc189)
    %177 = "vhlo.convert_v1"(%176) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1> loc(#loc190)
    %178 = "vhlo.broadcast_in_dim_v1"(%177) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1> loc(#loc191)
    %179 = "vhlo.convert_v1"(%173) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1> loc(#loc192)
    %180 = "vhlo.power_v1"(%179, %11) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1> loc(#loc193)
    %181 = "vhlo.reduce_v1"(%180, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.350"), %arg22: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.350")):
      %244 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc195)
      "vhlo.return_v1"(%244) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1> loc(#loc194)
    %182 = "vhlo.multiply_v1"(%181, %6) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1> loc(#loc196)
    %183 = "vhlo.reshape_v1"(%182) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1> loc(#loc197)
    %184 = "vhlo.add_v1"(%183, %40) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1> loc(#loc198)
    %185 = "vhlo.rsqrt_v2"(%184) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1> loc(#loc199)
    %186 = "vhlo.reshape_v1"(%185) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1> loc(#loc200)
    %187 = "vhlo.broadcast_in_dim_v1"(%186) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1> loc(#loc201)
    %188 = "vhlo.multiply_v1"(%179, %187) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1> loc(#loc202)
    %189 = "vhlo.convert_v1"(%188) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1> loc(#loc203)
    %190 = "vhlo.convert_v1"(%189) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1> loc(#loc204)
    %191 = "vhlo.multiply_v1"(%178, %190) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1> loc(#loc205)
    %192 = "vhlo.convert_v1"(%191) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1> loc(#loc206)
    %193 = "vhlo.reshape_v1"(%192) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1> loc(#loc207)
    %194 = "vhlo.reshape_v1"(%arg19) : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1> loc(#loc208)
    %195 = "vhlo.custom_call_v1"(%194) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_gate_proj_weight">}>} : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1> loc(#loc209)
    %196 = "vhlo.reshape_v1"(%195) : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1> loc(#loc210)
    %197 = "vhlo.transpose_v1"(%196) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,8192]{0,1}">} : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1> loc(#loc211)
    %198 = "vhlo.dot_general_v2"(%193, %197) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1> loc(#loc212)
    %199 = "vhlo.reshape_v1"(%198) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1> loc(#loc213)
    %200 = "vhlo.convert_v1"(%199) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1> loc(#loc214)
    %201 = "vhlo.logistic_v2"(%199) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1> loc(#loc215)
    %202 = "vhlo.convert_v1"(%201) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1> loc(#loc216)
    %203 = "vhlo.multiply_v1"(%200, %202) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1> loc(#loc217)
    %204 = "vhlo.convert_v1"(%203) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1> loc(#loc218)
    %205 = "vhlo.convert_v1"(%204) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1> loc(#loc219)
    %206 = "vhlo.reshape_v1"(%arg13) : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1> loc(#loc220)
    %207 = "vhlo.custom_call_v1"(%206) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_up_proj_weight">}>} : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1> loc(#loc221)
    %208 = "vhlo.reshape_v1"(%207) : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1> loc(#loc222)
    %209 = "vhlo.transpose_v1"(%208) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,8192]{0,1}">} : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1> loc(#loc223)
    %210 = "vhlo.dot_general_v2"(%193, %209) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1> loc(#loc224)
    %211 = "vhlo.reshape_v1"(%210) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1> loc(#loc225)
    %212 = "vhlo.convert_v1"(%211) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1> loc(#loc226)
    %213 = "vhlo.multiply_v1"(%205, %212) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1> loc(#loc227)
    %214 = "vhlo.convert_v1"(%213) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1> loc(#loc228)
    %215 = "vhlo.reshape_v1"(%214) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1> loc(#loc229)
    %216 = "vhlo.reshape_v1"(%arg12) : (!vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1> loc(#loc230)
    %217 = "vhlo.custom_call_v1"(%216) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_down_proj_weight">}>} : (!vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1> loc(#loc231)
    %218 = "vhlo.reshape_v1"(%217) : (!vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1> loc(#loc232)
    %219 = "vhlo.transpose_v1"(%218) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,3072]{0,1}">} : (!vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1> loc(#loc233)
    %220 = "vhlo.dot_general_v2"(%215, %219) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1> loc(#loc234)
    %221 = "vhlo.reshape_v1"(%220) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1> loc(#loc235)
    %222 = "vhlo.add_v1"(%173, %221) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1> loc(#loc236)
    %223 = "vhlo.convert_v1"(%222) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1> loc(#loc237)
    %224 = "vhlo.power_v1"(%223, %11) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1> loc(#loc238)
    %225 = "vhlo.reduce_v1"(%224, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.414"), %arg22: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.414")):
      %244 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc240)
      "vhlo.return_v1"(%244) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1> loc(#loc239)
    %226 = "vhlo.multiply_v1"(%225, %6) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1> loc(#loc241)
    %227 = "vhlo.reshape_v1"(%226) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1> loc(#loc242)
    %228 = "vhlo.add_v1"(%227, %40) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1> loc(#loc243)
    %229 = "vhlo.rsqrt_v2"(%228) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1> loc(#loc244)
    %230 = "vhlo.reshape_v1"(%229) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1> loc(#loc245)
    %231 = "vhlo.broadcast_in_dim_v1"(%230) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1> loc(#loc246)
    %232 = "vhlo.multiply_v1"(%223, %231) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1> loc(#loc247)
    %233 = "vhlo.convert_v1"(%232) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1> loc(#loc248)
    %234 = "vhlo.convert_v1"(%233) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1> loc(#loc249)
    %235 = "vhlo.multiply_v1"(%104, %234) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1> loc(#loc250)
    %236 = "vhlo.convert_v1"(%235) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1> loc(#loc251)
    %237 = "vhlo.reshape_v1"(%236) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1> loc(#loc252)
    %238 = "vhlo.reshape_v1"(%arg11) : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1> loc(#loc253)
    %239 = "vhlo.custom_call_v1"(%238) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___lm_head_weight">}>} : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1> loc(#loc254)
    %240 = "vhlo.reshape_v1"(%239) : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1> loc(#loc255)
    %241 = "vhlo.transpose_v1"(%240) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,128256]{0,1}">} : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x128256x!vhlo.bf16_v1> loc(#loc256)
    %242 = "vhlo.dot_general_v2"(%237, %241) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x128256x!vhlo.bf16_v1> loc(#loc257)
    %243 = "vhlo.reshape_v1"(%242) : (!vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1> loc(#loc258)
    "vhlo.return_v1"(%89, %99, %242, %243) : (!vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">} loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc22 = loc("custom-call.131")
#loc23 = loc("reshape.4")
#loc24 = loc("custom-call.5")
#loc25 = loc("reshape.6")
#loc26 = loc("compare.126")
#loc27 = loc("broadcast.122")
#loc28 = loc("add.123")
#loc29 = loc("select.127")
#loc30 = loc("reshape.128")
#loc31 = loc("reshape.82")
#loc32 = loc("custom-call.83")
#loc33 = loc("reshape.84")
#loc34 = loc("convert.85")
#loc35 = loc("broadcast.86")
#loc36 = loc("reshape.45")
#loc37 = loc("custom-call.46")
#loc38 = loc("reshape.47")
#loc39 = loc("reshape.40")
#loc40 = loc("custom-call.41")
#loc41 = loc("reshape.43")
#loc42 = loc("convert.48")
#loc43 = loc("gather.49")
#loc44 = loc("reshape.50")
#loc45 = loc("convert.51")
#loc46 = loc("power.53")
#loc48 = loc("add.59")
#loc49 = loc("multiply.69")
#loc50 = loc("reshape.70")
#loc51 = loc("broadcast.73")
#loc52 = loc("add.74")
#loc53 = loc("rsqrt.75")
#loc54 = loc("reshape.76")
#loc55 = loc("broadcast.77")
#loc56 = loc("multiply.78")
#loc57 = loc("convert.79")
#loc58 = loc("convert.80")
#loc59 = loc("multiply.87")
#loc60 = loc("convert.88")
#loc61 = loc("reshape.89")
#loc62 = loc("reshape.32")
#loc63 = loc("custom-call.33")
#loc64 = loc("reshape.34")
#loc65 = loc("transpose.35")
#loc66 = loc("dot.90")
#loc67 = loc("reshape.92")
#loc68 = loc("transpose.93")
#loc69 = loc("convert.110")
#loc70 = loc("reshape.14")
#loc71 = loc("custom-call.15")
#loc72 = loc("reshape.19")
#loc73 = loc("convert.11")
#loc74 = loc("dot.22")
#loc75 = loc("transpose.23")
#loc76 = loc("concatenate.24")
#loc77 = loc("cosine.104")
#loc78 = loc("convert.107")
#loc79 = loc("reshape.108")
#loc80 = loc("convert.109")
#loc81 = loc("reshape.111")
#loc82 = loc("broadcast.112")
#loc83 = loc("multiply.113")
#loc84 = loc("convert.114")
#loc85 = loc("slice.95")
#loc86 = loc("negate.96")
#loc87 = loc("slice.94")
#loc88 = loc("concatenate.97")
#loc89 = loc("convert.98")
#loc90 = loc("sine.25")
#loc91 = loc("convert.28")
#loc92 = loc("reshape.29")
#loc93 = loc("convert.30")
#loc94 = loc("reshape.99")
#loc95 = loc("broadcast.100")
#loc96 = loc("multiply.101")
#loc97 = loc("convert.102")
#loc98 = loc("add.117")
#loc100 = loc("custom-call.138")
#loc101 = loc("custom-call.161")
#loc102 = loc("reshape.140")
#loc103 = loc("custom-call.141")
#loc104 = loc("reshape.142")
#loc105 = loc("transpose.143")
#loc106 = loc("dot.145")
#loc107 = loc("reshape.147")
#loc108 = loc("transpose.148")
#loc110 = loc("custom-call.168")
#loc111 = loc("reshape.436")
#loc112 = loc("custom-call.437")
#loc113 = loc("reshape.438")
#loc114 = loc("convert.439")
#loc115 = loc("broadcast.440")
#loc116 = loc("reshape.270")
#loc117 = loc("custom-call.271")
#loc118 = loc("reshape.272")
#loc119 = loc("transpose.273")
#loc120 = loc("dot.275")
#loc121 = loc("reshape.277")
#loc122 = loc("transpose.278")
#loc123 = loc("convert.289")
#loc124 = loc("broadcast.291")
#loc125 = loc("multiply.292")
#loc126 = loc("convert.293")
#loc127 = loc("slice.280")
#loc128 = loc("negate.281")
#loc129 = loc("slice.279")
#loc130 = loc("concatenate.282")
#loc131 = loc("convert.283")
#loc132 = loc("broadcast.285")
#loc133 = loc("multiply.286")
#loc134 = loc("convert.287")
#loc135 = loc("add.296")
#loc136 = loc("reshape.298")
#loc137 = loc("broadcast.262")
#loc138 = loc("reshape.263")
#loc139 = loc("transpose.264")
#loc140 = loc("reshape.266")
#loc141 = loc("dot.299")
#loc142 = loc("reshape.300")
#loc143 = loc("convert.301")
#loc144 = loc("broadcast.302")
#loc145 = loc("multiply.303")
#loc146 = loc("convert.304")
#loc147 = loc("broadcast.235")
#loc148 = loc("broadcast.237")
#loc149 = loc("subtract.238")
#loc150 = loc("compare.240")
#loc151 = loc("broadcast.224")
#loc152 = loc("select.242")
#loc153 = loc("convert.243")
#loc154 = loc("broadcast.211")
#loc155 = loc("compare.212")
#loc156 = loc("convert.213")
#loc157 = loc("multiply.244")
#loc158 = loc("convert.245")
#loc159 = loc("reshape.246")
#loc160 = loc("broadcast.308")
#loc161 = loc("add.309")
#loc162 = loc("convert.310")
#loc164 = loc("maximum.315")
#loc165 = loc("broadcast.317")
#loc166 = loc("subtract.318")
#loc167 = loc("exponential.319")
#loc169 = loc("add.324")
#loc170 = loc("broadcast.326")
#loc171 = loc("divide.327")
#loc172 = loc("convert.328")
#loc173 = loc("reshape.330")
#loc174 = loc("broadcast.202")
#loc175 = loc("reshape.205")
#loc176 = loc("dot.331")
#loc177 = loc("reshape.332")
#loc178 = loc("transpose.333")
#loc179 = loc("reshape.335")
#loc180 = loc("reshape.191")
#loc181 = loc("custom-call.192")
#loc182 = loc("reshape.193")
#loc183 = loc("transpose.194")
#loc184 = loc("dot.336")
#loc185 = loc("reshape.337")
#loc186 = loc("add.340")
#loc187 = loc("reshape.372")
#loc188 = loc("custom-call.373")
#loc189 = loc("reshape.374")
#loc190 = loc("convert.375")
#loc191 = loc("broadcast.376")
#loc192 = loc("convert.341")
#loc193 = loc("power.343")
#loc195 = loc("add.349")
#loc196 = loc("multiply.359")
#loc197 = loc("reshape.360")
#loc198 = loc("add.364")
#loc199 = loc("rsqrt.365")
#loc200 = loc("reshape.366")
#loc201 = loc("broadcast.367")
#loc202 = loc("multiply.368")
#loc203 = loc("convert.369")
#loc204 = loc("convert.370")
#loc205 = loc("multiply.377")
#loc206 = loc("convert.378")
#loc207 = loc("reshape.388")
#loc208 = loc("reshape.384")
#loc209 = loc("custom-call.385")
#loc210 = loc("reshape.386")
#loc211 = loc("transpose.387")
#loc212 = loc("dot.389")
#loc213 = loc("reshape.390")
#loc214 = loc("convert.393")
#loc215 = loc("logistic.391")
#loc216 = loc("convert.392")
#loc217 = loc("multiply.394")
#loc218 = loc("convert.395")
#loc219 = loc("convert.396")
#loc220 = loc("reshape.183")
#loc221 = loc("custom-call.184")
#loc222 = loc("reshape.185")
#loc223 = loc("transpose.186")
#loc224 = loc("dot.380")
#loc225 = loc("reshape.381")
#loc226 = loc("convert.382")
#loc227 = loc("multiply.397")
#loc228 = loc("convert.398")
#loc229 = loc("reshape.399")
#loc230 = loc("reshape.178")
#loc231 = loc("custom-call.179")
#loc232 = loc("reshape.180")
#loc233 = loc("transpose.181")
#loc234 = loc("dot.400")
#loc235 = loc("reshape.401")
#loc236 = loc("add.404")
#loc237 = loc("convert.405")
#loc238 = loc("power.407")
#loc240 = loc("add.413")
#loc241 = loc("multiply.423")
#loc242 = loc("reshape.424")
#loc243 = loc("add.428")
#loc244 = loc("rsqrt.429")
#loc245 = loc("reshape.430")
#loc246 = loc("broadcast.431")
#loc247 = loc("multiply.432")
#loc248 = loc("convert.433")
#loc249 = loc("convert.434")
#loc250 = loc("multiply.441")
#loc251 = loc("convert.442")
#loc252 = loc("reshape.446")
#loc253 = loc("reshape.170")
#loc254 = loc("custom-call.171")
#loc255 = loc("reshape.172")
#loc256 = loc("transpose.173")
#loc257 = loc("dot.447")
#loc258 = loc("reshape.448")
// -----// IR Dump Before VhloToVersionPass (vhlo-to-version) ('builtin.module' operation: @SyncTensorsGraph.450) //----- //
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<7x!vhlo.i64_v1>, %arg1: !vhlo.tensor_v1<64x!vhlo.f32_v1>, %arg2: !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg4: !vhlo.tensor_v1<1x7x!vhlo.i64_v1>, %arg5: !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, %arg6: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg7: !vhlo.tensor_v1<!vhlo.i64_v1>, %arg8: !vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>, %arg9: !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>, %arg10: !vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>, %arg11: !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>, %arg13: !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>, %arg14: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg15: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg16: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg17: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg18: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg19: !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>, %arg20: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>>}> : () -> !vhlo.tensor_v1<16x!vhlo.i64_v1>
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>>}> : () -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %3 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF800000> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %4 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0> : tensor<7xi64>>}> : () -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %5 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %6 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<3.25520843E-4> : tensor<1x7xf32>>}> : () -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %7 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1> : tensor<i64>>}> : () -> !vhlo.tensor_v1<!vhlo.i64_v1>
    %8 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %9 = "vhlo.broadcast_in_dim_v1"(%8) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.bf16_v1>
    %10 = "vhlo.broadcast_in_dim_v1"(%7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.i64_v1>
    %11 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %12 = "vhlo.custom_call_v1"(%arg8) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_2">}>} : (!vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>
    %13 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>
    %14 = "vhlo.custom_call_v1"(%13) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_1">}>} : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>
    %15 = "vhlo.reshape_v1"(%14) : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %16 = "vhlo.compare_v1"(%15, %4) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 LT>}> : (!vhlo.tensor_v1<7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.bool_v1>
    %17 = "vhlo.broadcast_in_dim_v1"(%arg7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %18 = "vhlo.add_v1"(%15, %17) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %19 = "vhlo.select_v1"(%16, %18, %15) : (!vhlo.tensor_v1<7x!vhlo.bool_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %20 = "vhlo.reshape_v1"(%19) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x1x!vhlo.i64_v1>
    %21 = "vhlo.reshape_v1"(%arg6) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %22 = "vhlo.custom_call_v1"(%21) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___input_layernorm_weight">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %23 = "vhlo.reshape_v1"(%22) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1>
    %24 = "vhlo.convert_v1"(%23) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %25 = "vhlo.broadcast_in_dim_v1"(%24) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %26 = "vhlo.reshape_v1"(%arg5) : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>
    %27 = "vhlo.custom_call_v1"(%26) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_embed_tokens_weight">}>} : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>
    %28 = "vhlo.reshape_v1"(%27) : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>
    %29 = "vhlo.reshape_v1"(%arg4) : (!vhlo.tensor_v1<1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>
    %30 = "vhlo.custom_call_v1"(%29) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_0">}>} : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>
    %31 = "vhlo.reshape_v1"(%30) : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %32 = "vhlo.convert_v1"(%31) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.ui32_v1>
    %33 = "vhlo.gather_v2"(%28, %32) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 3072]> : tensor<2xi64>>, start_index_map = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x!vhlo.ui32_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %34 = "vhlo.reshape_v1"(%33) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %35 = "vhlo.convert_v1"(%34) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %36 = "vhlo.power_v1"(%35, %11) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %37 = "vhlo.reduce_v1"(%36, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %244 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%244) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %38 = "vhlo.multiply_v1"(%37, %6) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %39 = "vhlo.reshape_v1"(%38) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %40 = "vhlo.broadcast_in_dim_v1"(%arg3) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %41 = "vhlo.add_v1"(%39, %40) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %42 = "vhlo.rsqrt_v2"(%41) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %43 = "vhlo.reshape_v1"(%42) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %44 = "vhlo.broadcast_in_dim_v1"(%43) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %45 = "vhlo.multiply_v1"(%35, %44) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %46 = "vhlo.convert_v1"(%45) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %47 = "vhlo.convert_v1"(%46) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %48 = "vhlo.multiply_v1"(%25, %47) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %49 = "vhlo.convert_v1"(%48) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %50 = "vhlo.reshape_v1"(%49) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %51 = "vhlo.reshape_v1"(%arg2) : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>
    %52 = "vhlo.custom_call_v1"(%51) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>
    %53 = "vhlo.reshape_v1"(%52) : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>
    %54 = "vhlo.transpose_v1"(%53) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,1024]{0,1}">} : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>
    %55 = "vhlo.dot_general_v2"(%50, %54) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>
    %56 = "vhlo.reshape_v1"(%55) : (!vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>
    %57 = "vhlo.transpose_v1"(%56) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %58 = "vhlo.convert_v1"(%57) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %59 = "vhlo.reshape_v1"(%arg1) : (!vhlo.tensor_v1<64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>
    %60 = "vhlo.custom_call_v1"(%59) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_rotary_emb_inv_freq">}>} : (!vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>
    %61 = "vhlo.reshape_v1"(%60) : (!vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>
    %62 = "vhlo.convert_v1"(%14) : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.f32_v1>
    %63 = "vhlo.dot_general_v2"(%61, %62) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>
    %64 = "vhlo.transpose_v1"(%63) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1]> : tensor<3xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[1, 2, 0]> : tensor<3xindex>>, xla_shape = #vhlo.string_v1<"f32[1,7,64]{1,2,0}">} : (!vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>
    %65 = "vhlo.concatenate_v1"(%64, %64) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %66 = "vhlo.cosine_v2"(%65) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %67 = "vhlo.convert_v1"(%66) : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>
    %68 = "vhlo.reshape_v1"(%67) : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>
    %69 = "vhlo.convert_v1"(%68) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>
    %70 = "vhlo.reshape_v1"(%69) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %71 = "vhlo.broadcast_in_dim_v1"(%70) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %72 = "vhlo.multiply_v1"(%58, %71) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %73 = "vhlo.convert_v1"(%72) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %74 = "vhlo.slice_v1"(%57) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 7, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %75 = "vhlo.negate_v1"(%74) : (!vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %76 = "vhlo.slice_v1"(%57) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 7, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %77 = "vhlo.concatenate_v1"(%75, %76) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %78 = "vhlo.convert_v1"(%77) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %79 = "vhlo.sine_v2"(%65) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %80 = "vhlo.convert_v1"(%79) : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>
    %81 = "vhlo.reshape_v1"(%80) : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>
    %82 = "vhlo.convert_v1"(%81) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>
    %83 = "vhlo.reshape_v1"(%82) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %84 = "vhlo.broadcast_in_dim_v1"(%83) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %85 = "vhlo.multiply_v1"(%78, %84) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %86 = "vhlo.convert_v1"(%85) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %87 = "vhlo.add_v1"(%73, %86) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %88 = "vhlo.scatter_v2"(%12, %20, %87) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg22: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg22) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>
    %89 = "vhlo.custom_call_v1"(%88) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"Sharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>">}>, mhlo.sharding = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">} : (!vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>
    %90 = "vhlo.custom_call_v1"(%arg10) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_3">}>} : (!vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>
    %91 = "vhlo.reshape_v1"(%arg9) : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>
    %92 = "vhlo.custom_call_v1"(%91) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>
    %93 = "vhlo.reshape_v1"(%92) : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>
    %94 = "vhlo.transpose_v1"(%93) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,1024]{0,1}">} : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>
    %95 = "vhlo.dot_general_v2"(%50, %94) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>
    %96 = "vhlo.reshape_v1"(%95) : (!vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>
    %97 = "vhlo.transpose_v1"(%96) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %98 = "vhlo.scatter_v2"(%90, %20, %97) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg22: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg22) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>
    %99 = "vhlo.custom_call_v1"(%98) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"Sharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>">}>, mhlo.sharding = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">} : (!vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>
    %100 = "vhlo.reshape_v1"(%arg20) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %101 = "vhlo.custom_call_v1"(%100) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_norm_weight">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %102 = "vhlo.reshape_v1"(%101) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1>
    %103 = "vhlo.convert_v1"(%102) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %104 = "vhlo.broadcast_in_dim_v1"(%103) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %105 = "vhlo.reshape_v1"(%arg17) : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>
    %106 = "vhlo.custom_call_v1"(%105) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>
    %107 = "vhlo.reshape_v1"(%106) : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %108 = "vhlo.transpose_v1"(%107) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,3072]{0,1}">} : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %109 = "vhlo.dot_general_v2"(%50, %108) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %110 = "vhlo.reshape_v1"(%109) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>
    %111 = "vhlo.transpose_v1"(%110) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,24,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %112 = "vhlo.convert_v1"(%111) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,24,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %113 = "vhlo.broadcast_in_dim_v1"(%70) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %114 = "vhlo.multiply_v1"(%112, %113) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %115 = "vhlo.convert_v1"(%114) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %116 = "vhlo.slice_v1"(%111) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 7, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>
    %117 = "vhlo.negate_v1"(%116) : (!vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>
    %118 = "vhlo.slice_v1"(%111) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 7, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>
    %119 = "vhlo.concatenate_v1"(%117, %118) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %120 = "vhlo.convert_v1"(%119) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %121 = "vhlo.broadcast_in_dim_v1"(%83) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %122 = "vhlo.multiply_v1"(%120, %121) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %123 = "vhlo.convert_v1"(%122) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %124 = "vhlo.add_v1"(%115, %123) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %125 = "vhlo.reshape_v1"(%124) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>
    %126 = "vhlo.broadcast_in_dim_v1"(%88) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x16x128x!vhlo.bf16_v1>
    %127 = "vhlo.reshape_v1"(%126) : (!vhlo.tensor_v1<1x8x3x16x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x16x128x!vhlo.bf16_v1>
    %128 = "vhlo.transpose_v1"(%127) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 3, 1, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,24,128,16]{2,3,1,0}">} : (!vhlo.tensor_v1<1x24x16x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x128x16x!vhlo.bf16_v1>
    %129 = "vhlo.reshape_v1"(%128) : (!vhlo.tensor_v1<1x24x128x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x128x16x!vhlo.bf16_v1>
    %130 = "vhlo.dot_general_v2"(%125, %129) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x128x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x16x!vhlo.bf16_v1>
    %131 = "vhlo.reshape_v1"(%130) : (!vhlo.tensor_v1<24x7x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.bf16_v1>
    %132 = "vhlo.convert_v1"(%131) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>
    %133 = "vhlo.broadcast_in_dim_v1"(%arg16) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>
    %134 = "vhlo.multiply_v1"(%132, %133) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>
    %135 = "vhlo.convert_v1"(%134) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.bf16_v1>
    %136 = "vhlo.broadcast_in_dim_v1"(%1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<16x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.i64_v1>
    %137 = "vhlo.broadcast_in_dim_v1"(%2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.i64_v1>
    %138 = "vhlo.subtract_v1"(%136, %137) : (!vhlo.tensor_v1<7x16x!vhlo.i64_v1>, !vhlo.tensor_v1<7x16x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.i64_v1>
    %139 = "vhlo.compare_v1"(%138, %10) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GE>}> : (!vhlo.tensor_v1<7x16x!vhlo.i64_v1>, !vhlo.tensor_v1<7x16x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.bool_v1>
    %140 = "vhlo.broadcast_in_dim_v1"(%arg15) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.bf16_v1>
    %141 = "vhlo.select_v1"(%139, %140, %9) : (!vhlo.tensor_v1<7x16x!vhlo.bool_v1>, !vhlo.tensor_v1<7x16x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.bf16_v1>
    %142 = "vhlo.convert_v1"(%141) : (!vhlo.tensor_v1<7x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.f32_v1>
    %143 = "vhlo.broadcast_in_dim_v1"(%15) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.i64_v1>
    %144 = "vhlo.compare_v1"(%136, %143) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<7x16x!vhlo.i64_v1>, !vhlo.tensor_v1<7x16x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.bool_v1>
    %145 = "vhlo.convert_v1"(%144) : (!vhlo.tensor_v1<7x16x!vhlo.bool_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.f32_v1>
    %146 = "vhlo.multiply_v1"(%142, %145) : (!vhlo.tensor_v1<7x16x!vhlo.f32_v1>, !vhlo.tensor_v1<7x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.f32_v1>
    %147 = "vhlo.convert_v1"(%146) : (!vhlo.tensor_v1<7x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.bf16_v1>
    %148 = "vhlo.reshape_v1"(%147) : (!vhlo.tensor_v1<7x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x16x!vhlo.bf16_v1>
    %149 = "vhlo.broadcast_in_dim_v1"(%148) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.bf16_v1>
    %150 = "vhlo.add_v1"(%135, %149) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x7x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.bf16_v1>
    %151 = "vhlo.convert_v1"(%150) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>
    %152 = "vhlo.reduce_v1"(%151, %3) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %244 = "vhlo.maximum_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%244) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>
    %153 = "vhlo.broadcast_in_dim_v1"(%152) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>
    %154 = "vhlo.subtract_v1"(%151, %153) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>
    %155 = "vhlo.exponential_v2"(%154) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>
    %156 = "vhlo.reduce_v1"(%155, %0) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %244 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%244) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>
    %157 = "vhlo.broadcast_in_dim_v1"(%156) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>
    %158 = "vhlo.divide_v1"(%155, %157) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>
    %159 = "vhlo.convert_v1"(%158) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.bf16_v1>
    %160 = "vhlo.reshape_v1"(%159) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x16x!vhlo.bf16_v1>
    %161 = "vhlo.broadcast_in_dim_v1"(%98) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x16x128x!vhlo.bf16_v1>
    %162 = "vhlo.reshape_v1"(%161) : (!vhlo.tensor_v1<1x8x3x16x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x16x128x!vhlo.bf16_v1>
    %163 = "vhlo.dot_general_v2"(%160, %162) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x7x16x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x16x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>
    %164 = "vhlo.reshape_v1"(%163) : (!vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %165 = "vhlo.transpose_v1"(%164) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,7,24,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>
    %166 = "vhlo.reshape_v1"(%165) : (!vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %167 = "vhlo.reshape_v1"(%arg14) : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>
    %168 = "vhlo.custom_call_v1"(%167) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_o_proj_weight">}>} : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>
    %169 = "vhlo.reshape_v1"(%168) : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %170 = "vhlo.transpose_v1"(%169) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,3072]{0,1}">} : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %171 = "vhlo.dot_general_v2"(%166, %170) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %172 = "vhlo.reshape_v1"(%171) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %173 = "vhlo.add_v1"(%34, %172) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %174 = "vhlo.reshape_v1"(%arg18) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %175 = "vhlo.custom_call_v1"(%174) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___post_attention_layernorm_weight">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %176 = "vhlo.reshape_v1"(%175) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1>
    %177 = "vhlo.convert_v1"(%176) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %178 = "vhlo.broadcast_in_dim_v1"(%177) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %179 = "vhlo.convert_v1"(%173) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %180 = "vhlo.power_v1"(%179, %11) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %181 = "vhlo.reduce_v1"(%180, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %244 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%244) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %182 = "vhlo.multiply_v1"(%181, %6) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %183 = "vhlo.reshape_v1"(%182) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %184 = "vhlo.add_v1"(%183, %40) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %185 = "vhlo.rsqrt_v2"(%184) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %186 = "vhlo.reshape_v1"(%185) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %187 = "vhlo.broadcast_in_dim_v1"(%186) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %188 = "vhlo.multiply_v1"(%179, %187) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %189 = "vhlo.convert_v1"(%188) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %190 = "vhlo.convert_v1"(%189) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %191 = "vhlo.multiply_v1"(%178, %190) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %192 = "vhlo.convert_v1"(%191) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %193 = "vhlo.reshape_v1"(%192) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %194 = "vhlo.reshape_v1"(%arg19) : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>
    %195 = "vhlo.custom_call_v1"(%194) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_gate_proj_weight">}>} : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>
    %196 = "vhlo.reshape_v1"(%195) : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>
    %197 = "vhlo.transpose_v1"(%196) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,8192]{0,1}">} : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>
    %198 = "vhlo.dot_general_v2"(%193, %197) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %199 = "vhlo.reshape_v1"(%198) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %200 = "vhlo.convert_v1"(%199) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %201 = "vhlo.logistic_v2"(%199) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %202 = "vhlo.convert_v1"(%201) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %203 = "vhlo.multiply_v1"(%200, %202) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %204 = "vhlo.convert_v1"(%203) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %205 = "vhlo.convert_v1"(%204) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %206 = "vhlo.reshape_v1"(%arg13) : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>
    %207 = "vhlo.custom_call_v1"(%206) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_up_proj_weight">}>} : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>
    %208 = "vhlo.reshape_v1"(%207) : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>
    %209 = "vhlo.transpose_v1"(%208) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,8192]{0,1}">} : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>
    %210 = "vhlo.dot_general_v2"(%193, %209) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %211 = "vhlo.reshape_v1"(%210) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %212 = "vhlo.convert_v1"(%211) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %213 = "vhlo.multiply_v1"(%205, %212) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %214 = "vhlo.convert_v1"(%213) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %215 = "vhlo.reshape_v1"(%214) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %216 = "vhlo.reshape_v1"(%arg12) : (!vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1>
    %217 = "vhlo.custom_call_v1"(%216) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_down_proj_weight">}>} : (!vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1>
    %218 = "vhlo.reshape_v1"(%217) : (!vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>
    %219 = "vhlo.transpose_v1"(%218) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,3072]{0,1}">} : (!vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>
    %220 = "vhlo.dot_general_v2"(%215, %219) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %221 = "vhlo.reshape_v1"(%220) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %222 = "vhlo.add_v1"(%173, %221) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %223 = "vhlo.convert_v1"(%222) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %224 = "vhlo.power_v1"(%223, %11) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %225 = "vhlo.reduce_v1"(%224, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %244 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%244) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %226 = "vhlo.multiply_v1"(%225, %6) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %227 = "vhlo.reshape_v1"(%226) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %228 = "vhlo.add_v1"(%227, %40) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %229 = "vhlo.rsqrt_v2"(%228) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %230 = "vhlo.reshape_v1"(%229) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %231 = "vhlo.broadcast_in_dim_v1"(%230) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %232 = "vhlo.multiply_v1"(%223, %231) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %233 = "vhlo.convert_v1"(%232) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %234 = "vhlo.convert_v1"(%233) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %235 = "vhlo.multiply_v1"(%104, %234) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %236 = "vhlo.convert_v1"(%235) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %237 = "vhlo.reshape_v1"(%236) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %238 = "vhlo.reshape_v1"(%arg11) : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>
    %239 = "vhlo.custom_call_v1"(%238) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___lm_head_weight">}>} : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>
    %240 = "vhlo.reshape_v1"(%239) : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>
    %241 = "vhlo.transpose_v1"(%240) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,128256]{0,1}">} : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x128256x!vhlo.bf16_v1>
    %242 = "vhlo.dot_general_v2"(%237, %241) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>
    %243 = "vhlo.reshape_v1"(%242) : (!vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>
    "vhlo.return_v1"(%89, %99, %242, %243) : (!vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}


// -----// IR Dump Before VhloLegalizeToStablehloPass (vhlo-legalize-to-stablehlo) ('builtin.module' operation: @SyncTensorsGraph.450) //----- //
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<7x!vhlo.i64_v1>, %arg1: !vhlo.tensor_v1<64x!vhlo.f32_v1>, %arg2: !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg4: !vhlo.tensor_v1<1x7x!vhlo.i64_v1>, %arg5: !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, %arg6: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg7: !vhlo.tensor_v1<!vhlo.i64_v1>, %arg8: !vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>, %arg9: !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>, %arg10: !vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>, %arg11: !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>, %arg13: !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>, %arg14: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg15: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg16: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg17: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg18: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg19: !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>, %arg20: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>>}> : () -> !vhlo.tensor_v1<16x!vhlo.i64_v1>
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>>}> : () -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %3 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF800000> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %4 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0> : tensor<7xi64>>}> : () -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %5 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %6 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<3.25520843E-4> : tensor<1x7xf32>>}> : () -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %7 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1> : tensor<i64>>}> : () -> !vhlo.tensor_v1<!vhlo.i64_v1>
    %8 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %9 = "vhlo.broadcast_in_dim_v1"(%8) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.bf16_v1>
    %10 = "vhlo.broadcast_in_dim_v1"(%7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.i64_v1>
    %11 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %12 = "vhlo.custom_call_v1"(%arg8) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_2">}>} : (!vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>
    %13 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>
    %14 = "vhlo.custom_call_v1"(%13) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_1">}>} : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>
    %15 = "vhlo.reshape_v1"(%14) : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %16 = "vhlo.compare_v1"(%15, %4) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 LT>}> : (!vhlo.tensor_v1<7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.bool_v1>
    %17 = "vhlo.broadcast_in_dim_v1"(%arg7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %18 = "vhlo.add_v1"(%15, %17) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %19 = "vhlo.select_v1"(%16, %18, %15) : (!vhlo.tensor_v1<7x!vhlo.bool_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %20 = "vhlo.reshape_v1"(%19) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x1x!vhlo.i64_v1>
    %21 = "vhlo.reshape_v1"(%arg6) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %22 = "vhlo.custom_call_v1"(%21) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___input_layernorm_weight">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %23 = "vhlo.reshape_v1"(%22) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1>
    %24 = "vhlo.convert_v1"(%23) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %25 = "vhlo.broadcast_in_dim_v1"(%24) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %26 = "vhlo.reshape_v1"(%arg5) : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>
    %27 = "vhlo.custom_call_v1"(%26) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_embed_tokens_weight">}>} : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>
    %28 = "vhlo.reshape_v1"(%27) : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>
    %29 = "vhlo.reshape_v1"(%arg4) : (!vhlo.tensor_v1<1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>
    %30 = "vhlo.custom_call_v1"(%29) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_0">}>} : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>
    %31 = "vhlo.reshape_v1"(%30) : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %32 = "vhlo.convert_v1"(%31) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.ui32_v1>
    %33 = "vhlo.gather_v2"(%28, %32) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 3072]> : tensor<2xi64>>, start_index_map = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x!vhlo.ui32_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %34 = "vhlo.reshape_v1"(%33) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %35 = "vhlo.convert_v1"(%34) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %36 = "vhlo.power_v1"(%35, %11) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %37 = "vhlo.reduce_v1"(%36, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %244 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%244) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %38 = "vhlo.multiply_v1"(%37, %6) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %39 = "vhlo.reshape_v1"(%38) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %40 = "vhlo.broadcast_in_dim_v1"(%arg3) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %41 = "vhlo.add_v1"(%39, %40) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %42 = "vhlo.rsqrt_v2"(%41) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %43 = "vhlo.reshape_v1"(%42) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %44 = "vhlo.broadcast_in_dim_v1"(%43) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %45 = "vhlo.multiply_v1"(%35, %44) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %46 = "vhlo.convert_v1"(%45) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %47 = "vhlo.convert_v1"(%46) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %48 = "vhlo.multiply_v1"(%25, %47) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %49 = "vhlo.convert_v1"(%48) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %50 = "vhlo.reshape_v1"(%49) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %51 = "vhlo.reshape_v1"(%arg2) : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>
    %52 = "vhlo.custom_call_v1"(%51) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>
    %53 = "vhlo.reshape_v1"(%52) : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>
    %54 = "vhlo.transpose_v1"(%53) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,1024]{0,1}">} : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>
    %55 = "vhlo.dot_general_v2"(%50, %54) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>
    %56 = "vhlo.reshape_v1"(%55) : (!vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>
    %57 = "vhlo.transpose_v1"(%56) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %58 = "vhlo.convert_v1"(%57) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %59 = "vhlo.reshape_v1"(%arg1) : (!vhlo.tensor_v1<64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>
    %60 = "vhlo.custom_call_v1"(%59) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_rotary_emb_inv_freq">}>} : (!vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>
    %61 = "vhlo.reshape_v1"(%60) : (!vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>
    %62 = "vhlo.convert_v1"(%14) : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.f32_v1>
    %63 = "vhlo.dot_general_v2"(%61, %62) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>
    %64 = "vhlo.transpose_v1"(%63) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1]> : tensor<3xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[1, 2, 0]> : tensor<3xindex>>, xla_shape = #vhlo.string_v1<"f32[1,7,64]{1,2,0}">} : (!vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>
    %65 = "vhlo.concatenate_v1"(%64, %64) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %66 = "vhlo.cosine_v2"(%65) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %67 = "vhlo.convert_v1"(%66) : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>
    %68 = "vhlo.reshape_v1"(%67) : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>
    %69 = "vhlo.convert_v1"(%68) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>
    %70 = "vhlo.reshape_v1"(%69) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %71 = "vhlo.broadcast_in_dim_v1"(%70) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %72 = "vhlo.multiply_v1"(%58, %71) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %73 = "vhlo.convert_v1"(%72) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %74 = "vhlo.slice_v1"(%57) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 7, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %75 = "vhlo.negate_v1"(%74) : (!vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %76 = "vhlo.slice_v1"(%57) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 7, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %77 = "vhlo.concatenate_v1"(%75, %76) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %78 = "vhlo.convert_v1"(%77) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %79 = "vhlo.sine_v2"(%65) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %80 = "vhlo.convert_v1"(%79) : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>
    %81 = "vhlo.reshape_v1"(%80) : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>
    %82 = "vhlo.convert_v1"(%81) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>
    %83 = "vhlo.reshape_v1"(%82) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %84 = "vhlo.broadcast_in_dim_v1"(%83) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %85 = "vhlo.multiply_v1"(%78, %84) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %86 = "vhlo.convert_v1"(%85) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %87 = "vhlo.add_v1"(%73, %86) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %88 = "vhlo.scatter_v2"(%12, %20, %87) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg22: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg22) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>
    %89 = "vhlo.custom_call_v1"(%88) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"Sharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>">}>, mhlo.sharding = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">} : (!vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>
    %90 = "vhlo.custom_call_v1"(%arg10) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_3">}>} : (!vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>
    %91 = "vhlo.reshape_v1"(%arg9) : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>
    %92 = "vhlo.custom_call_v1"(%91) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>
    %93 = "vhlo.reshape_v1"(%92) : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>
    %94 = "vhlo.transpose_v1"(%93) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,1024]{0,1}">} : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>
    %95 = "vhlo.dot_general_v2"(%50, %94) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>
    %96 = "vhlo.reshape_v1"(%95) : (!vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>
    %97 = "vhlo.transpose_v1"(%96) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %98 = "vhlo.scatter_v2"(%90, %20, %97) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg22: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg22) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>
    %99 = "vhlo.custom_call_v1"(%98) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"Sharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>">}>, mhlo.sharding = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">} : (!vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>
    %100 = "vhlo.reshape_v1"(%arg20) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %101 = "vhlo.custom_call_v1"(%100) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_norm_weight">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %102 = "vhlo.reshape_v1"(%101) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1>
    %103 = "vhlo.convert_v1"(%102) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %104 = "vhlo.broadcast_in_dim_v1"(%103) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %105 = "vhlo.reshape_v1"(%arg17) : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>
    %106 = "vhlo.custom_call_v1"(%105) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>
    %107 = "vhlo.reshape_v1"(%106) : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %108 = "vhlo.transpose_v1"(%107) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,3072]{0,1}">} : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %109 = "vhlo.dot_general_v2"(%50, %108) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %110 = "vhlo.reshape_v1"(%109) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>
    %111 = "vhlo.transpose_v1"(%110) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,24,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %112 = "vhlo.convert_v1"(%111) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,24,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %113 = "vhlo.broadcast_in_dim_v1"(%70) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %114 = "vhlo.multiply_v1"(%112, %113) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %115 = "vhlo.convert_v1"(%114) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %116 = "vhlo.slice_v1"(%111) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 7, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>
    %117 = "vhlo.negate_v1"(%116) : (!vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>
    %118 = "vhlo.slice_v1"(%111) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 7, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>
    %119 = "vhlo.concatenate_v1"(%117, %118) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %120 = "vhlo.convert_v1"(%119) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %121 = "vhlo.broadcast_in_dim_v1"(%83) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %122 = "vhlo.multiply_v1"(%120, %121) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %123 = "vhlo.convert_v1"(%122) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %124 = "vhlo.add_v1"(%115, %123) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %125 = "vhlo.reshape_v1"(%124) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>
    %126 = "vhlo.broadcast_in_dim_v1"(%88) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x16x128x!vhlo.bf16_v1>
    %127 = "vhlo.reshape_v1"(%126) : (!vhlo.tensor_v1<1x8x3x16x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x16x128x!vhlo.bf16_v1>
    %128 = "vhlo.transpose_v1"(%127) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 3, 1, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,24,128,16]{2,3,1,0}">} : (!vhlo.tensor_v1<1x24x16x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x128x16x!vhlo.bf16_v1>
    %129 = "vhlo.reshape_v1"(%128) : (!vhlo.tensor_v1<1x24x128x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x128x16x!vhlo.bf16_v1>
    %130 = "vhlo.dot_general_v2"(%125, %129) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x128x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x16x!vhlo.bf16_v1>
    %131 = "vhlo.reshape_v1"(%130) : (!vhlo.tensor_v1<24x7x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.bf16_v1>
    %132 = "vhlo.convert_v1"(%131) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>
    %133 = "vhlo.broadcast_in_dim_v1"(%arg16) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>
    %134 = "vhlo.multiply_v1"(%132, %133) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>
    %135 = "vhlo.convert_v1"(%134) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.bf16_v1>
    %136 = "vhlo.broadcast_in_dim_v1"(%1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<16x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.i64_v1>
    %137 = "vhlo.broadcast_in_dim_v1"(%2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.i64_v1>
    %138 = "vhlo.subtract_v1"(%136, %137) : (!vhlo.tensor_v1<7x16x!vhlo.i64_v1>, !vhlo.tensor_v1<7x16x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.i64_v1>
    %139 = "vhlo.compare_v1"(%138, %10) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GE>}> : (!vhlo.tensor_v1<7x16x!vhlo.i64_v1>, !vhlo.tensor_v1<7x16x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.bool_v1>
    %140 = "vhlo.broadcast_in_dim_v1"(%arg15) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.bf16_v1>
    %141 = "vhlo.select_v1"(%139, %140, %9) : (!vhlo.tensor_v1<7x16x!vhlo.bool_v1>, !vhlo.tensor_v1<7x16x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.bf16_v1>
    %142 = "vhlo.convert_v1"(%141) : (!vhlo.tensor_v1<7x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.f32_v1>
    %143 = "vhlo.broadcast_in_dim_v1"(%15) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.i64_v1>
    %144 = "vhlo.compare_v1"(%136, %143) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<7x16x!vhlo.i64_v1>, !vhlo.tensor_v1<7x16x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.bool_v1>
    %145 = "vhlo.convert_v1"(%144) : (!vhlo.tensor_v1<7x16x!vhlo.bool_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.f32_v1>
    %146 = "vhlo.multiply_v1"(%142, %145) : (!vhlo.tensor_v1<7x16x!vhlo.f32_v1>, !vhlo.tensor_v1<7x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.f32_v1>
    %147 = "vhlo.convert_v1"(%146) : (!vhlo.tensor_v1<7x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<7x16x!vhlo.bf16_v1>
    %148 = "vhlo.reshape_v1"(%147) : (!vhlo.tensor_v1<7x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x16x!vhlo.bf16_v1>
    %149 = "vhlo.broadcast_in_dim_v1"(%148) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.bf16_v1>
    %150 = "vhlo.add_v1"(%135, %149) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x7x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.bf16_v1>
    %151 = "vhlo.convert_v1"(%150) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>
    %152 = "vhlo.reduce_v1"(%151, %3) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %244 = "vhlo.maximum_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%244) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>
    %153 = "vhlo.broadcast_in_dim_v1"(%152) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>
    %154 = "vhlo.subtract_v1"(%151, %153) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>
    %155 = "vhlo.exponential_v2"(%154) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>
    %156 = "vhlo.reduce_v1"(%155, %0) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %244 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%244) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>
    %157 = "vhlo.broadcast_in_dim_v1"(%156) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>
    %158 = "vhlo.divide_v1"(%155, %157) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>
    %159 = "vhlo.convert_v1"(%158) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x16x!vhlo.bf16_v1>
    %160 = "vhlo.reshape_v1"(%159) : (!vhlo.tensor_v1<1x24x7x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x16x!vhlo.bf16_v1>
    %161 = "vhlo.broadcast_in_dim_v1"(%98) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x16x128x!vhlo.bf16_v1>
    %162 = "vhlo.reshape_v1"(%161) : (!vhlo.tensor_v1<1x8x3x16x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x16x128x!vhlo.bf16_v1>
    %163 = "vhlo.dot_general_v2"(%160, %162) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x7x16x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x16x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>
    %164 = "vhlo.reshape_v1"(%163) : (!vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %165 = "vhlo.transpose_v1"(%164) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,7,24,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>
    %166 = "vhlo.reshape_v1"(%165) : (!vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %167 = "vhlo.reshape_v1"(%arg14) : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>
    %168 = "vhlo.custom_call_v1"(%167) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_o_proj_weight">}>} : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>
    %169 = "vhlo.reshape_v1"(%168) : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %170 = "vhlo.transpose_v1"(%169) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,3072]{0,1}">} : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %171 = "vhlo.dot_general_v2"(%166, %170) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %172 = "vhlo.reshape_v1"(%171) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %173 = "vhlo.add_v1"(%34, %172) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %174 = "vhlo.reshape_v1"(%arg18) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %175 = "vhlo.custom_call_v1"(%174) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___post_attention_layernorm_weight">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %176 = "vhlo.reshape_v1"(%175) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1>
    %177 = "vhlo.convert_v1"(%176) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %178 = "vhlo.broadcast_in_dim_v1"(%177) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %179 = "vhlo.convert_v1"(%173) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %180 = "vhlo.power_v1"(%179, %11) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %181 = "vhlo.reduce_v1"(%180, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %244 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%244) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %182 = "vhlo.multiply_v1"(%181, %6) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %183 = "vhlo.reshape_v1"(%182) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %184 = "vhlo.add_v1"(%183, %40) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %185 = "vhlo.rsqrt_v2"(%184) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %186 = "vhlo.reshape_v1"(%185) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %187 = "vhlo.broadcast_in_dim_v1"(%186) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %188 = "vhlo.multiply_v1"(%179, %187) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %189 = "vhlo.convert_v1"(%188) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %190 = "vhlo.convert_v1"(%189) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %191 = "vhlo.multiply_v1"(%178, %190) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %192 = "vhlo.convert_v1"(%191) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %193 = "vhlo.reshape_v1"(%192) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %194 = "vhlo.reshape_v1"(%arg19) : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>
    %195 = "vhlo.custom_call_v1"(%194) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_gate_proj_weight">}>} : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>
    %196 = "vhlo.reshape_v1"(%195) : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>
    %197 = "vhlo.transpose_v1"(%196) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,8192]{0,1}">} : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>
    %198 = "vhlo.dot_general_v2"(%193, %197) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %199 = "vhlo.reshape_v1"(%198) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %200 = "vhlo.convert_v1"(%199) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %201 = "vhlo.logistic_v2"(%199) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %202 = "vhlo.convert_v1"(%201) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %203 = "vhlo.multiply_v1"(%200, %202) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %204 = "vhlo.convert_v1"(%203) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %205 = "vhlo.convert_v1"(%204) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %206 = "vhlo.reshape_v1"(%arg13) : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>
    %207 = "vhlo.custom_call_v1"(%206) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_up_proj_weight">}>} : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>
    %208 = "vhlo.reshape_v1"(%207) : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>
    %209 = "vhlo.transpose_v1"(%208) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,8192]{0,1}">} : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>
    %210 = "vhlo.dot_general_v2"(%193, %209) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %211 = "vhlo.reshape_v1"(%210) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %212 = "vhlo.convert_v1"(%211) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %213 = "vhlo.multiply_v1"(%205, %212) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %214 = "vhlo.convert_v1"(%213) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %215 = "vhlo.reshape_v1"(%214) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %216 = "vhlo.reshape_v1"(%arg12) : (!vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1>
    %217 = "vhlo.custom_call_v1"(%216) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_down_proj_weight">}>} : (!vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1>
    %218 = "vhlo.reshape_v1"(%217) : (!vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>
    %219 = "vhlo.transpose_v1"(%218) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,3072]{0,1}">} : (!vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>
    %220 = "vhlo.dot_general_v2"(%215, %219) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %221 = "vhlo.reshape_v1"(%220) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %222 = "vhlo.add_v1"(%173, %221) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %223 = "vhlo.convert_v1"(%222) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %224 = "vhlo.power_v1"(%223, %11) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %225 = "vhlo.reduce_v1"(%224, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %244 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%244) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %226 = "vhlo.multiply_v1"(%225, %6) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %227 = "vhlo.reshape_v1"(%226) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %228 = "vhlo.add_v1"(%227, %40) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %229 = "vhlo.rsqrt_v2"(%228) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %230 = "vhlo.reshape_v1"(%229) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %231 = "vhlo.broadcast_in_dim_v1"(%230) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %232 = "vhlo.multiply_v1"(%223, %231) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %233 = "vhlo.convert_v1"(%232) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %234 = "vhlo.convert_v1"(%233) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %235 = "vhlo.multiply_v1"(%104, %234) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %236 = "vhlo.convert_v1"(%235) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %237 = "vhlo.reshape_v1"(%236) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %238 = "vhlo.reshape_v1"(%arg11) : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>
    %239 = "vhlo.custom_call_v1"(%238) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___lm_head_weight">}>} : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>
    %240 = "vhlo.reshape_v1"(%239) : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>
    %241 = "vhlo.transpose_v1"(%240) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,128256]{0,1}">} : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x128256x!vhlo.bf16_v1>
    %242 = "vhlo.dot_general_v2"(%237, %241) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>
    %243 = "vhlo.reshape_v1"(%242) : (!vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>
    "vhlo.return_v1"(%89, %99, %242, %243) : (!vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x16x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}


// -----// IR Dump After VhloLegalizeToStablehloPass (vhlo-legalize-to-stablehlo) ('builtin.module' operation: @SyncTensorsGraph.450) //----- //
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg1: tensor<64xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg2: tensor<1024x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}"}, %arg3: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg4: tensor<1x7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg5: tensor<128256x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<1x8x16x128xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"}, %arg9: tensor<1024x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}"}, %arg10: tensor<1x8x16x128xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"}, %arg11: tensor<128256x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg12: tensor<3072x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}"}, %arg13: tensor<8192x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}"}, %arg14: tensor<3072x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}"}, %arg15: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg16: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg17: tensor<3072x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}"}, %arg18: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg19: tensor<8192x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}"}, %arg20: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}) -> (tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64>
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.custom_call @tt.mark_argument(%arg8) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_2"}} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %4 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %5 = stablehlo.custom_call @tt.mark_argument(%4) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_1"}} : (tensor<1x1x7xi64>) -> tensor<1x1x7xi64>
    %6 = stablehlo.reshape %5 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %7 = stablehlo.compare  LT, %6, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %8 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %9 = stablehlo.add %6, %8 : tensor<7xi64>
    %10 = stablehlo.select %7, %9, %6 : tensor<7xi1>, tensor<7xi64>
    %11 = stablehlo.reshape %10 : (tensor<7xi64>) -> tensor<7x1xi64>
    %12 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %13 = stablehlo.custom_call @tt.mark_argument(%12) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %14 = stablehlo.reshape %13 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %15 = stablehlo.convert %14 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %16 = stablehlo.broadcast_in_dim %15, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %17 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %18 = stablehlo.custom_call @tt.mark_argument(%17) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_embed_tokens_weight"}} : (tensor<1x128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %19 = stablehlo.reshape %18 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %20 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %21 = stablehlo.custom_call @tt.mark_argument(%20) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_0"}} : (tensor<1x1x7xi64>) -> tensor<1x1x7xi64>
    %22 = stablehlo.reshape %21 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %23 = stablehlo.convert %22 : (tensor<7xi64>) -> tensor<7xui32>
    %24 = "stablehlo.gather"(%19, %23) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %25 = stablehlo.reshape %24 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %26 = stablehlo.convert %25 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %27 = stablehlo.power %26, %2 : tensor<1x7x3072xf32>
    %28 = stablehlo.reduce(%27 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %29 = stablehlo.multiply %28, %cst_4 : tensor<1x7xf32>
    %30 = stablehlo.reshape %29 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %31 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %32 = stablehlo.add %30, %31 : tensor<1x7x1xf32>
    %33 = stablehlo.rsqrt %32 : tensor<1x7x1xf32>
    %34 = stablehlo.reshape %33 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %35 = stablehlo.broadcast_in_dim %34, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %36 = stablehlo.multiply %26, %35 : tensor<1x7x3072xf32>
    %37 = stablehlo.convert %36 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %38 = stablehlo.convert %37 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %39 = stablehlo.multiply %16, %38 : tensor<1x7x3072xf32>
    %40 = stablehlo.convert %39 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %41 = stablehlo.reshape %40 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %42 = stablehlo.reshape %arg2 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %43 = stablehlo.custom_call @tt.mark_argument(%42) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}} : (tensor<1x1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %44 = stablehlo.reshape %43 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %45 = stablehlo.transpose %44, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %46 = stablehlo.dot_general %41, %45, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %47 = stablehlo.reshape %46 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %49 = stablehlo.convert %48 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %50 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %51 = stablehlo.custom_call @tt.mark_argument(%50) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "l__self___model_rotary_emb_inv_freq"}} : (tensor<1x1x64xf32>) -> tensor<1x1x64xf32>
    %52 = stablehlo.reshape %51 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %53 = stablehlo.convert %5 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %54 = stablehlo.dot_general %52, %53, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %55 = stablehlo.transpose %54, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %56 = stablehlo.concatenate %55, %55, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %57 = stablehlo.cosine %56 : tensor<1x7x128xf32>
    %58 = stablehlo.convert %57 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %59 = stablehlo.reshape %58 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %60 = stablehlo.convert %59 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %61 = stablehlo.reshape %60 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %62 = stablehlo.broadcast_in_dim %61, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %63 = stablehlo.multiply %49, %62 : tensor<1x8x7x128xf32>
    %64 = stablehlo.convert %63 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %65 = stablehlo.slice %48 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %66 = stablehlo.negate %65 : tensor<1x8x7x64xbf16>
    %67 = stablehlo.slice %48 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %68 = stablehlo.concatenate %66, %67, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %69 = stablehlo.convert %68 : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %70 = stablehlo.sine %56 : tensor<1x7x128xf32>
    %71 = stablehlo.convert %70 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %72 = stablehlo.reshape %71 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %73 = stablehlo.convert %72 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %74 = stablehlo.reshape %73 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %75 = stablehlo.broadcast_in_dim %74, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %76 = stablehlo.multiply %69, %75 : tensor<1x8x7x128xf32>
    %77 = stablehlo.convert %76 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %78 = stablehlo.add %64, %77 : tensor<1x8x7x128xbf16>
    %79 = "stablehlo.scatter"(%3, %11, %78) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %80 = stablehlo.custom_call @Sharding(%79) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %81 = stablehlo.custom_call @tt.mark_argument(%arg10) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_3"}} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %82 = stablehlo.reshape %arg9 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %83 = stablehlo.custom_call @tt.mark_argument(%82) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}} : (tensor<1x1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %84 = stablehlo.reshape %83 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %85 = stablehlo.transpose %84, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %86 = stablehlo.dot_general %41, %85, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %87 = stablehlo.reshape %86 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %88 = stablehlo.transpose %87, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %89 = "stablehlo.scatter"(%81, %11, %88) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %90 = stablehlo.custom_call @Sharding(%89) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %91 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %92 = stablehlo.custom_call @tt.mark_argument(%91) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_norm_weight"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %93 = stablehlo.reshape %92 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %94 = stablehlo.convert %93 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %95 = stablehlo.broadcast_in_dim %94, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %96 = stablehlo.reshape %arg17 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %97 = stablehlo.custom_call @tt.mark_argument(%96) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}} : (tensor<1x3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %98 = stablehlo.reshape %97 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %99 = stablehlo.transpose %98, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %100 = stablehlo.dot_general %41, %99, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %101 = stablehlo.reshape %100 : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %102 = stablehlo.transpose %101, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %103 = stablehlo.convert %102 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %104 = stablehlo.broadcast_in_dim %61, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %105 = stablehlo.multiply %103, %104 : tensor<1x24x7x128xf32>
    %106 = stablehlo.convert %105 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %107 = stablehlo.slice %102 [0:1, 0:24, 0:7, 64:128] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %108 = stablehlo.negate %107 : tensor<1x24x7x64xbf16>
    %109 = stablehlo.slice %102 [0:1, 0:24, 0:7, 0:64] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %110 = stablehlo.concatenate %108, %109, dim = 3 : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %111 = stablehlo.convert %110 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %112 = stablehlo.broadcast_in_dim %74, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %113 = stablehlo.multiply %111, %112 : tensor<1x24x7x128xf32>
    %114 = stablehlo.convert %113 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %115 = stablehlo.add %106, %114 : tensor<1x24x7x128xbf16>
    %116 = stablehlo.reshape %115 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %117 = stablehlo.broadcast_in_dim %79, dims = [0, 1, 3, 4] : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %118 = stablehlo.reshape %117 : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16>
    %119 = stablehlo.transpose %118, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16>
    %120 = stablehlo.reshape %119 : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16>
    %121 = stablehlo.dot_general %116, %120, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16>
    %122 = stablehlo.reshape %121 : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %123 = stablehlo.convert %122 : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %124 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<1x24x7x16xf32>
    %125 = stablehlo.multiply %123, %124 : tensor<1x24x7x16xf32>
    %126 = stablehlo.convert %125 : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %127 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64>
    %128 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %129 = stablehlo.subtract %127, %128 : tensor<7x16xi64>
    %130 = stablehlo.compare  GE, %129, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %131 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %132 = stablehlo.select %130, %131, %0 : tensor<7x16xi1>, tensor<7x16xbf16>
    %133 = stablehlo.convert %132 : (tensor<7x16xbf16>) -> tensor<7x16xf32>
    %134 = stablehlo.broadcast_in_dim %6, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %135 = stablehlo.compare  GT, %127, %134 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %136 = stablehlo.convert %135 : (tensor<7x16xi1>) -> tensor<7x16xf32>
    %137 = stablehlo.multiply %133, %136 : tensor<7x16xf32>
    %138 = stablehlo.convert %137 : (tensor<7x16xf32>) -> tensor<7x16xbf16>
    %139 = stablehlo.reshape %138 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16>
    %140 = stablehlo.broadcast_in_dim %139, dims = [0, 2, 3] : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %141 = stablehlo.add %126, %140 : tensor<1x24x7x16xbf16>
    %142 = stablehlo.convert %141 : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %143 = stablehlo.reduce(%142 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %144 = stablehlo.broadcast_in_dim %143, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %145 = stablehlo.subtract %142, %144 : tensor<1x24x7x16xf32>
    %146 = stablehlo.exponential %145 : tensor<1x24x7x16xf32>
    %147 = stablehlo.reduce(%146 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %148 = stablehlo.broadcast_in_dim %147, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %149 = stablehlo.divide %146, %148 : tensor<1x24x7x16xf32>
    %150 = stablehlo.convert %149 : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16>
    %152 = stablehlo.broadcast_in_dim %89, dims = [0, 1, 3, 4] : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %153 = stablehlo.reshape %152 : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16>
    %154 = stablehlo.dot_general %151, %153, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16>
    %155 = stablehlo.reshape %154 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %156 = stablehlo.transpose %155, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %157 = stablehlo.reshape %156 : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %158 = stablehlo.reshape %arg14 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %159 = stablehlo.custom_call @tt.mark_argument(%158) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}} : (tensor<1x3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %160 = stablehlo.reshape %159 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %161 = stablehlo.transpose %160, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %162 = stablehlo.dot_general %157, %161, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %163 = stablehlo.reshape %162 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %164 = stablehlo.add %25, %163 : tensor<1x7x3072xbf16>
    %165 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %166 = stablehlo.custom_call @tt.mark_argument(%165) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %167 = stablehlo.reshape %166 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %168 = stablehlo.convert %167 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %169 = stablehlo.broadcast_in_dim %168, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %170 = stablehlo.convert %164 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %171 = stablehlo.power %170, %2 : tensor<1x7x3072xf32>
    %172 = stablehlo.reduce(%171 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %173 = stablehlo.multiply %172, %cst_4 : tensor<1x7xf32>
    %174 = stablehlo.reshape %173 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %175 = stablehlo.add %174, %31 : tensor<1x7x1xf32>
    %176 = stablehlo.rsqrt %175 : tensor<1x7x1xf32>
    %177 = stablehlo.reshape %176 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %178 = stablehlo.broadcast_in_dim %177, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %179 = stablehlo.multiply %170, %178 : tensor<1x7x3072xf32>
    %180 = stablehlo.convert %179 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %181 = stablehlo.convert %180 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %182 = stablehlo.multiply %169, %181 : tensor<1x7x3072xf32>
    %183 = stablehlo.convert %182 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %184 = stablehlo.reshape %183 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %185 = stablehlo.reshape %arg19 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %186 = stablehlo.custom_call @tt.mark_argument(%185) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}} : (tensor<1x8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %187 = stablehlo.reshape %186 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %188 = stablehlo.transpose %187, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %189 = stablehlo.dot_general %184, %188, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %190 = stablehlo.reshape %189 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %191 = stablehlo.convert %190 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %192 = stablehlo.logistic %190 : tensor<1x7x8192xbf16>
    %193 = stablehlo.convert %192 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %194 = stablehlo.multiply %191, %193 : tensor<1x7x8192xf32>
    %195 = stablehlo.convert %194 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %196 = stablehlo.convert %195 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %197 = stablehlo.reshape %arg13 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %198 = stablehlo.custom_call @tt.mark_argument(%197) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}} : (tensor<1x8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %199 = stablehlo.reshape %198 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %200 = stablehlo.transpose %199, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %201 = stablehlo.dot_general %184, %200, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %202 = stablehlo.reshape %201 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %203 = stablehlo.convert %202 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %204 = stablehlo.multiply %196, %203 : tensor<1x7x8192xf32>
    %205 = stablehlo.convert %204 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %206 = stablehlo.reshape %205 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %207 = stablehlo.reshape %arg12 : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %208 = stablehlo.custom_call @tt.mark_argument(%207) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}} : (tensor<1x3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %209 = stablehlo.reshape %208 : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %210 = stablehlo.transpose %209, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %211 = stablehlo.dot_general %206, %210, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %212 = stablehlo.reshape %211 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %213 = stablehlo.add %164, %212 : tensor<1x7x3072xbf16>
    %214 = stablehlo.convert %213 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %215 = stablehlo.power %214, %2 : tensor<1x7x3072xf32>
    %216 = stablehlo.reduce(%215 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %217 = stablehlo.multiply %216, %cst_4 : tensor<1x7xf32>
    %218 = stablehlo.reshape %217 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %219 = stablehlo.add %218, %31 : tensor<1x7x1xf32>
    %220 = stablehlo.rsqrt %219 : tensor<1x7x1xf32>
    %221 = stablehlo.reshape %220 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %222 = stablehlo.broadcast_in_dim %221, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %223 = stablehlo.multiply %214, %222 : tensor<1x7x3072xf32>
    %224 = stablehlo.convert %223 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %225 = stablehlo.convert %224 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %226 = stablehlo.multiply %95, %225 : tensor<1x7x3072xf32>
    %227 = stablehlo.convert %226 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %228 = stablehlo.reshape %227 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %229 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %230 = stablehlo.custom_call @tt.mark_argument(%229) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___lm_head_weight"}} : (tensor<1x128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %231 = stablehlo.reshape %230 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %232 = stablehlo.transpose %231, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %233 = stablehlo.dot_general %228, %232, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %234 = stablehlo.reshape %233 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %80, %90, %233, %234 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}


#loc1 = loc("p0.3")
#loc2 = loc("p1.13")
#loc3 = loc("p2.31")
#loc4 = loc("p3.37")
#loc5 = loc("p4.39")
#loc6 = loc("p5.44")
#loc7 = loc("p6.81")
#loc8 = loc("p7.118")
#loc9 = loc("p8.130")
#loc10 = loc("p9.139")
#loc11 = loc("p10.160")
#loc12 = loc("p11.169")
#loc13 = loc("p12.177")
#loc14 = loc("p13.182")
#loc15 = loc("p14.190")
#loc16 = loc("p15.220")
#loc17 = loc("p16.254")
#loc18 = loc("p17.269")
#loc19 = loc("p18.371")
#loc20 = loc("p19.383")
#loc21 = loc("p20.435")
#loc98 = loc("scatter.136")
#loc108 = loc("scatter.166")
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"} loc("p0.3"), %arg1: tensor<64xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"} loc("p1.13"), %arg2: tensor<1024x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}"} loc("p2.31"), %arg3: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"} loc("p3.37"), %arg4: tensor<1x7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"} loc("p4.39"), %arg5: tensor<128256x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"} loc("p5.44"), %arg6: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"} loc("p6.81"), %arg7: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"} loc("p7.118"), %arg8: tensor<1x8x16x128xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} loc("p8.130"), %arg9: tensor<1024x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}"} loc("p9.139"), %arg10: tensor<1x8x16x128xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} loc("p10.160"), %arg11: tensor<128256x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"} loc("p11.169"), %arg12: tensor<3072x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}"} loc("p12.177"), %arg13: tensor<8192x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}"} loc("p13.182"), %arg14: tensor<3072x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}"} loc("p14.190"), %arg15: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"} loc("p15.220"), %arg16: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"} loc("p16.254"), %arg17: tensor<3072x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}"} loc("p17.269"), %arg18: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"} loc("p18.371"), %arg19: tensor<8192x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}"} loc("p19.383"), %arg20: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"} loc("p20.435")) -> (tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64> loc(#loc)
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64> loc(#loc)
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64> loc(#loc)
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32> loc(#loc)
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32> loc(#loc)
    %c_5 = stablehlo.constant dense<1> : tensor<i64> loc(#loc)
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16> loc(#loc)
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64> loc(#loc)
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32> loc(#loc)
    %3 = stablehlo.custom_call @tt.mark_argument(%arg8) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_2"}} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16> loc(#loc22)
    %4 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64> loc(#loc23)
    %5 = stablehlo.custom_call @tt.mark_argument(%4) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_1"}} : (tensor<1x1x7xi64>) -> tensor<1x1x7xi64> loc(#loc24)
    %6 = stablehlo.reshape %5 : (tensor<1x1x7xi64>) -> tensor<7xi64> loc(#loc25)
    %7 = stablehlo.compare  LT, %6, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1> loc(#loc26)
    %8 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64> loc(#loc27)
    %9 = stablehlo.add %6, %8 : tensor<7xi64> loc(#loc28)
    %10 = stablehlo.select %7, %9, %6 : tensor<7xi1>, tensor<7xi64> loc(#loc29)
    %11 = stablehlo.reshape %10 : (tensor<7xi64>) -> tensor<7x1xi64> loc(#loc30)
    %12 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc31)
    %13 = stablehlo.custom_call @tt.mark_argument(%12) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc32)
    %14 = stablehlo.reshape %13 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc33)
    %15 = stablehlo.convert %14 : (tensor<3072xbf16>) -> tensor<3072xf32> loc(#loc34)
    %16 = stablehlo.broadcast_in_dim %15, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32> loc(#loc35)
    %17 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16> loc(#loc36)
    %18 = stablehlo.custom_call @tt.mark_argument(%17) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_embed_tokens_weight"}} : (tensor<1x128256x3072xbf16>) -> tensor<1x128256x3072xbf16> loc(#loc37)
    %19 = stablehlo.reshape %18 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16> loc(#loc38)
    %20 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64> loc(#loc39)
    %21 = stablehlo.custom_call @tt.mark_argument(%20) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_0"}} : (tensor<1x1x7xi64>) -> tensor<1x1x7xi64> loc(#loc40)
    %22 = stablehlo.reshape %21 : (tensor<1x1x7xi64>) -> tensor<7xi64> loc(#loc41)
    %23 = stablehlo.convert %22 : (tensor<7xi64>) -> tensor<7xui32> loc(#loc42)
    %24 = "stablehlo.gather"(%19, %23) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16> loc(#loc43)
    %25 = stablehlo.reshape %24 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16> loc(#loc44)
    %26 = stablehlo.convert %25 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32> loc(#loc45)
    %27 = stablehlo.power %26, %2 : tensor<1x7x3072xf32> loc(#loc46)
    %28 = stablehlo.reduce(%27 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32> loc(#loc47)
    %29 = stablehlo.multiply %28, %cst_4 : tensor<1x7xf32> loc(#loc48)
    %30 = stablehlo.reshape %29 : (tensor<1x7xf32>) -> tensor<1x7x1xf32> loc(#loc49)
    %31 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32> loc(#loc50)
    %32 = stablehlo.add %30, %31 : tensor<1x7x1xf32> loc(#loc51)
    %33 = stablehlo.rsqrt %32 : tensor<1x7x1xf32> loc(#loc52)
    %34 = stablehlo.reshape %33 : (tensor<1x7x1xf32>) -> tensor<1x7xf32> loc(#loc53)
    %35 = stablehlo.broadcast_in_dim %34, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32> loc(#loc54)
    %36 = stablehlo.multiply %26, %35 : tensor<1x7x3072xf32> loc(#loc55)
    %37 = stablehlo.convert %36 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16> loc(#loc56)
    %38 = stablehlo.convert %37 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32> loc(#loc57)
    %39 = stablehlo.multiply %16, %38 : tensor<1x7x3072xf32> loc(#loc58)
    %40 = stablehlo.convert %39 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16> loc(#loc59)
    %41 = stablehlo.reshape %40 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16> loc(#loc60)
    %42 = stablehlo.reshape %arg2 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16> loc(#loc61)
    %43 = stablehlo.custom_call @tt.mark_argument(%42) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}} : (tensor<1x1024x3072xbf16>) -> tensor<1x1024x3072xbf16> loc(#loc62)
    %44 = stablehlo.reshape %43 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16> loc(#loc63)
    %45 = stablehlo.transpose %44, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16> loc(#loc64)
    %46 = stablehlo.dot_general %41, %45, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16> loc(#loc65)
    %47 = stablehlo.reshape %46 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16> loc(#loc66)
    %48 = stablehlo.transpose %47, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16> loc(#loc67)
    %49 = stablehlo.convert %48 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32> loc(#loc68)
    %50 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32> loc(#loc69)
    %51 = stablehlo.custom_call @tt.mark_argument(%50) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "l__self___model_rotary_emb_inv_freq"}} : (tensor<1x1x64xf32>) -> tensor<1x1x64xf32> loc(#loc70)
    %52 = stablehlo.reshape %51 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32> loc(#loc71)
    %53 = stablehlo.convert %5 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32> loc(#loc72)
    %54 = stablehlo.dot_general %52, %53, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32> loc(#loc73)
    %55 = stablehlo.transpose %54, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32> loc(#loc74)
    %56 = stablehlo.concatenate %55, %55, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32> loc(#loc75)
    %57 = stablehlo.cosine %56 : tensor<1x7x128xf32> loc(#loc76)
    %58 = stablehlo.convert %57 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16> loc(#loc77)
    %59 = stablehlo.reshape %58 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16> loc(#loc78)
    %60 = stablehlo.convert %59 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32> loc(#loc79)
    %61 = stablehlo.reshape %60 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32> loc(#loc80)
    %62 = stablehlo.broadcast_in_dim %61, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32> loc(#loc81)
    %63 = stablehlo.multiply %49, %62 : tensor<1x8x7x128xf32> loc(#loc82)
    %64 = stablehlo.convert %63 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16> loc(#loc83)
    %65 = stablehlo.slice %48 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16> loc(#loc84)
    %66 = stablehlo.negate %65 : tensor<1x8x7x64xbf16> loc(#loc85)
    %67 = stablehlo.slice %48 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16> loc(#loc86)
    %68 = stablehlo.concatenate %66, %67, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16> loc(#loc87)
    %69 = stablehlo.convert %68 : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32> loc(#loc88)
    %70 = stablehlo.sine %56 : tensor<1x7x128xf32> loc(#loc89)
    %71 = stablehlo.convert %70 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16> loc(#loc90)
    %72 = stablehlo.reshape %71 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16> loc(#loc91)
    %73 = stablehlo.convert %72 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32> loc(#loc92)
    %74 = stablehlo.reshape %73 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32> loc(#loc93)
    %75 = stablehlo.broadcast_in_dim %74, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32> loc(#loc94)
    %76 = stablehlo.multiply %69, %75 : tensor<1x8x7x128xf32> loc(#loc95)
    %77 = stablehlo.convert %76 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16> loc(#loc96)
    %78 = stablehlo.add %64, %77 : tensor<1x8x7x128xbf16> loc(#loc97)
    %79 = "stablehlo.scatter"(%3, %11, %78) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16> loc("scatter.136"), %arg22: tensor<bf16> loc("scatter.136")):
      stablehlo.return %arg22 : tensor<bf16> loc(#loc)
    }) : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16> loc(#loc98)
    %80 = stablehlo.custom_call @Sharding(%79) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16> loc(#loc99)
    %81 = stablehlo.custom_call @tt.mark_argument(%arg10) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_3"}} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16> loc(#loc100)
    %82 = stablehlo.reshape %arg9 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16> loc(#loc101)
    %83 = stablehlo.custom_call @tt.mark_argument(%82) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}} : (tensor<1x1024x3072xbf16>) -> tensor<1x1024x3072xbf16> loc(#loc102)
    %84 = stablehlo.reshape %83 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16> loc(#loc103)
    %85 = stablehlo.transpose %84, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16> loc(#loc104)
    %86 = stablehlo.dot_general %41, %85, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16> loc(#loc105)
    %87 = stablehlo.reshape %86 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16> loc(#loc106)
    %88 = stablehlo.transpose %87, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16> loc(#loc107)
    %89 = "stablehlo.scatter"(%81, %11, %88) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16> loc("scatter.166"), %arg22: tensor<bf16> loc("scatter.166")):
      stablehlo.return %arg22 : tensor<bf16> loc(#loc)
    }) : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16> loc(#loc108)
    %90 = stablehlo.custom_call @Sharding(%89) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16> loc(#loc109)
    %91 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc110)
    %92 = stablehlo.custom_call @tt.mark_argument(%91) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_norm_weight"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc111)
    %93 = stablehlo.reshape %92 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc112)
    %94 = stablehlo.convert %93 : (tensor<3072xbf16>) -> tensor<3072xf32> loc(#loc113)
    %95 = stablehlo.broadcast_in_dim %94, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32> loc(#loc114)
    %96 = stablehlo.reshape %arg17 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16> loc(#loc115)
    %97 = stablehlo.custom_call @tt.mark_argument(%96) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}} : (tensor<1x3072x3072xbf16>) -> tensor<1x3072x3072xbf16> loc(#loc116)
    %98 = stablehlo.reshape %97 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16> loc(#loc117)
    %99 = stablehlo.transpose %98, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16> loc(#loc118)
    %100 = stablehlo.dot_general %41, %99, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16> loc(#loc119)
    %101 = stablehlo.reshape %100 : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16> loc(#loc120)
    %102 = stablehlo.transpose %101, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16> loc(#loc121)
    %103 = stablehlo.convert %102 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32> loc(#loc122)
    %104 = stablehlo.broadcast_in_dim %61, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32> loc(#loc123)
    %105 = stablehlo.multiply %103, %104 : tensor<1x24x7x128xf32> loc(#loc124)
    %106 = stablehlo.convert %105 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16> loc(#loc125)
    %107 = stablehlo.slice %102 [0:1, 0:24, 0:7, 64:128] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16> loc(#loc126)
    %108 = stablehlo.negate %107 : tensor<1x24x7x64xbf16> loc(#loc127)
    %109 = stablehlo.slice %102 [0:1, 0:24, 0:7, 0:64] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16> loc(#loc128)
    %110 = stablehlo.concatenate %108, %109, dim = 3 : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16> loc(#loc129)
    %111 = stablehlo.convert %110 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32> loc(#loc130)
    %112 = stablehlo.broadcast_in_dim %74, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32> loc(#loc131)
    %113 = stablehlo.multiply %111, %112 : tensor<1x24x7x128xf32> loc(#loc132)
    %114 = stablehlo.convert %113 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16> loc(#loc133)
    %115 = stablehlo.add %106, %114 : tensor<1x24x7x128xbf16> loc(#loc134)
    %116 = stablehlo.reshape %115 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16> loc(#loc135)
    %117 = stablehlo.broadcast_in_dim %79, dims = [0, 1, 3, 4] : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16> loc(#loc136)
    %118 = stablehlo.reshape %117 : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16> loc(#loc137)
    %119 = stablehlo.transpose %118, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16> loc(#loc138)
    %120 = stablehlo.reshape %119 : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16> loc(#loc139)
    %121 = stablehlo.dot_general %116, %120, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16> loc(#loc140)
    %122 = stablehlo.reshape %121 : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16> loc(#loc141)
    %123 = stablehlo.convert %122 : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32> loc(#loc142)
    %124 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<1x24x7x16xf32> loc(#loc143)
    %125 = stablehlo.multiply %123, %124 : tensor<1x24x7x16xf32> loc(#loc144)
    %126 = stablehlo.convert %125 : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16> loc(#loc145)
    %127 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64> loc(#loc146)
    %128 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64> loc(#loc147)
    %129 = stablehlo.subtract %127, %128 : tensor<7x16xi64> loc(#loc148)
    %130 = stablehlo.compare  GE, %129, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1> loc(#loc149)
    %131 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16> loc(#loc150)
    %132 = stablehlo.select %130, %131, %0 : tensor<7x16xi1>, tensor<7x16xbf16> loc(#loc151)
    %133 = stablehlo.convert %132 : (tensor<7x16xbf16>) -> tensor<7x16xf32> loc(#loc152)
    %134 = stablehlo.broadcast_in_dim %6, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64> loc(#loc153)
    %135 = stablehlo.compare  GT, %127, %134 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1> loc(#loc154)
    %136 = stablehlo.convert %135 : (tensor<7x16xi1>) -> tensor<7x16xf32> loc(#loc155)
    %137 = stablehlo.multiply %133, %136 : tensor<7x16xf32> loc(#loc156)
    %138 = stablehlo.convert %137 : (tensor<7x16xf32>) -> tensor<7x16xbf16> loc(#loc157)
    %139 = stablehlo.reshape %138 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16> loc(#loc158)
    %140 = stablehlo.broadcast_in_dim %139, dims = [0, 2, 3] : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16> loc(#loc159)
    %141 = stablehlo.add %126, %140 : tensor<1x24x7x16xbf16> loc(#loc160)
    %142 = stablehlo.convert %141 : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32> loc(#loc161)
    %143 = stablehlo.reduce(%142 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32> loc(#loc162)
    %144 = stablehlo.broadcast_in_dim %143, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32> loc(#loc163)
    %145 = stablehlo.subtract %142, %144 : tensor<1x24x7x16xf32> loc(#loc164)
    %146 = stablehlo.exponential %145 : tensor<1x24x7x16xf32> loc(#loc165)
    %147 = stablehlo.reduce(%146 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32> loc(#loc166)
    %148 = stablehlo.broadcast_in_dim %147, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32> loc(#loc167)
    %149 = stablehlo.divide %146, %148 : tensor<1x24x7x16xf32> loc(#loc168)
    %150 = stablehlo.convert %149 : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16> loc(#loc169)
    %151 = stablehlo.reshape %150 : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16> loc(#loc170)
    %152 = stablehlo.broadcast_in_dim %89, dims = [0, 1, 3, 4] : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16> loc(#loc171)
    %153 = stablehlo.reshape %152 : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16> loc(#loc172)
    %154 = stablehlo.dot_general %151, %153, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16> loc(#loc173)
    %155 = stablehlo.reshape %154 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16> loc(#loc174)
    %156 = stablehlo.transpose %155, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16> loc(#loc175)
    %157 = stablehlo.reshape %156 : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16> loc(#loc176)
    %158 = stablehlo.reshape %arg14 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16> loc(#loc177)
    %159 = stablehlo.custom_call @tt.mark_argument(%158) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}} : (tensor<1x3072x3072xbf16>) -> tensor<1x3072x3072xbf16> loc(#loc178)
    %160 = stablehlo.reshape %159 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16> loc(#loc179)
    %161 = stablehlo.transpose %160, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16> loc(#loc180)
    %162 = stablehlo.dot_general %157, %161, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16> loc(#loc181)
    %163 = stablehlo.reshape %162 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16> loc(#loc182)
    %164 = stablehlo.add %25, %163 : tensor<1x7x3072xbf16> loc(#loc183)
    %165 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc184)
    %166 = stablehlo.custom_call @tt.mark_argument(%165) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc185)
    %167 = stablehlo.reshape %166 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc186)
    %168 = stablehlo.convert %167 : (tensor<3072xbf16>) -> tensor<3072xf32> loc(#loc187)
    %169 = stablehlo.broadcast_in_dim %168, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32> loc(#loc188)
    %170 = stablehlo.convert %164 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32> loc(#loc189)
    %171 = stablehlo.power %170, %2 : tensor<1x7x3072xf32> loc(#loc190)
    %172 = stablehlo.reduce(%171 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32> loc(#loc191)
    %173 = stablehlo.multiply %172, %cst_4 : tensor<1x7xf32> loc(#loc192)
    %174 = stablehlo.reshape %173 : (tensor<1x7xf32>) -> tensor<1x7x1xf32> loc(#loc193)
    %175 = stablehlo.add %174, %31 : tensor<1x7x1xf32> loc(#loc194)
    %176 = stablehlo.rsqrt %175 : tensor<1x7x1xf32> loc(#loc195)
    %177 = stablehlo.reshape %176 : (tensor<1x7x1xf32>) -> tensor<1x7xf32> loc(#loc196)
    %178 = stablehlo.broadcast_in_dim %177, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32> loc(#loc197)
    %179 = stablehlo.multiply %170, %178 : tensor<1x7x3072xf32> loc(#loc198)
    %180 = stablehlo.convert %179 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16> loc(#loc199)
    %181 = stablehlo.convert %180 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32> loc(#loc200)
    %182 = stablehlo.multiply %169, %181 : tensor<1x7x3072xf32> loc(#loc201)
    %183 = stablehlo.convert %182 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16> loc(#loc202)
    %184 = stablehlo.reshape %183 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16> loc(#loc203)
    %185 = stablehlo.reshape %arg19 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16> loc(#loc204)
    %186 = stablehlo.custom_call @tt.mark_argument(%185) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}} : (tensor<1x8192x3072xbf16>) -> tensor<1x8192x3072xbf16> loc(#loc205)
    %187 = stablehlo.reshape %186 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16> loc(#loc206)
    %188 = stablehlo.transpose %187, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16> loc(#loc207)
    %189 = stablehlo.dot_general %184, %188, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16> loc(#loc208)
    %190 = stablehlo.reshape %189 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16> loc(#loc209)
    %191 = stablehlo.convert %190 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32> loc(#loc210)
    %192 = stablehlo.logistic %190 : tensor<1x7x8192xbf16> loc(#loc211)
    %193 = stablehlo.convert %192 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32> loc(#loc212)
    %194 = stablehlo.multiply %191, %193 : tensor<1x7x8192xf32> loc(#loc213)
    %195 = stablehlo.convert %194 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16> loc(#loc214)
    %196 = stablehlo.convert %195 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32> loc(#loc215)
    %197 = stablehlo.reshape %arg13 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16> loc(#loc216)
    %198 = stablehlo.custom_call @tt.mark_argument(%197) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}} : (tensor<1x8192x3072xbf16>) -> tensor<1x8192x3072xbf16> loc(#loc217)
    %199 = stablehlo.reshape %198 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16> loc(#loc218)
    %200 = stablehlo.transpose %199, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16> loc(#loc219)
    %201 = stablehlo.dot_general %184, %200, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16> loc(#loc220)
    %202 = stablehlo.reshape %201 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16> loc(#loc221)
    %203 = stablehlo.convert %202 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32> loc(#loc222)
    %204 = stablehlo.multiply %196, %203 : tensor<1x7x8192xf32> loc(#loc223)
    %205 = stablehlo.convert %204 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16> loc(#loc224)
    %206 = stablehlo.reshape %205 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16> loc(#loc225)
    %207 = stablehlo.reshape %arg12 : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16> loc(#loc226)
    %208 = stablehlo.custom_call @tt.mark_argument(%207) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}} : (tensor<1x3072x8192xbf16>) -> tensor<1x3072x8192xbf16> loc(#loc227)
    %209 = stablehlo.reshape %208 : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16> loc(#loc228)
    %210 = stablehlo.transpose %209, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16> loc(#loc229)
    %211 = stablehlo.dot_general %206, %210, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16> loc(#loc230)
    %212 = stablehlo.reshape %211 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16> loc(#loc231)
    %213 = stablehlo.add %164, %212 : tensor<1x7x3072xbf16> loc(#loc232)
    %214 = stablehlo.convert %213 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32> loc(#loc233)
    %215 = stablehlo.power %214, %2 : tensor<1x7x3072xf32> loc(#loc234)
    %216 = stablehlo.reduce(%215 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32> loc(#loc235)
    %217 = stablehlo.multiply %216, %cst_4 : tensor<1x7xf32> loc(#loc236)
    %218 = stablehlo.reshape %217 : (tensor<1x7xf32>) -> tensor<1x7x1xf32> loc(#loc237)
    %219 = stablehlo.add %218, %31 : tensor<1x7x1xf32> loc(#loc238)
    %220 = stablehlo.rsqrt %219 : tensor<1x7x1xf32> loc(#loc239)
    %221 = stablehlo.reshape %220 : (tensor<1x7x1xf32>) -> tensor<1x7xf32> loc(#loc240)
    %222 = stablehlo.broadcast_in_dim %221, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32> loc(#loc241)
    %223 = stablehlo.multiply %214, %222 : tensor<1x7x3072xf32> loc(#loc242)
    %224 = stablehlo.convert %223 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16> loc(#loc243)
    %225 = stablehlo.convert %224 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32> loc(#loc244)
    %226 = stablehlo.multiply %95, %225 : tensor<1x7x3072xf32> loc(#loc245)
    %227 = stablehlo.convert %226 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16> loc(#loc246)
    %228 = stablehlo.reshape %227 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16> loc(#loc247)
    %229 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16> loc(#loc248)
    %230 = stablehlo.custom_call @tt.mark_argument(%229) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___lm_head_weight"}} : (tensor<1x128256x3072xbf16>) -> tensor<1x128256x3072xbf16> loc(#loc249)
    %231 = stablehlo.reshape %230 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16> loc(#loc250)
    %232 = stablehlo.transpose %231, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16> loc(#loc251)
    %233 = stablehlo.dot_general %228, %232, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16> loc(#loc252)
    %234 = stablehlo.reshape %233 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16> loc(#loc253)
    return %80, %90, %233, %234 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc22 = loc("custom-call.131")
#loc23 = loc("reshape.4")
#loc24 = loc("custom-call.5")
#loc25 = loc("reshape.6")
#loc26 = loc("compare.126")
#loc27 = loc("broadcast.122")
#loc28 = loc("add.123")
#loc29 = loc("select.127")
#loc30 = loc("reshape.128")
#loc31 = loc("reshape.82")
#loc32 = loc("custom-call.83")
#loc33 = loc("reshape.84")
#loc34 = loc("convert.85")
#loc35 = loc("broadcast.86")
#loc36 = loc("reshape.45")
#loc37 = loc("custom-call.46")
#loc38 = loc("reshape.47")
#loc39 = loc("reshape.40")
#loc40 = loc("custom-call.41")
#loc41 = loc("reshape.43")
#loc42 = loc("convert.48")
#loc43 = loc("gather.49")
#loc44 = loc("reshape.50")
#loc45 = loc("convert.51")
#loc46 = loc("power.53")
#loc47 = loc("reduce.60")
#loc48 = loc("multiply.69")
#loc49 = loc("reshape.70")
#loc50 = loc("broadcast.73")
#loc51 = loc("add.74")
#loc52 = loc("rsqrt.75")
#loc53 = loc("reshape.76")
#loc54 = loc("broadcast.77")
#loc55 = loc("multiply.78")
#loc56 = loc("convert.79")
#loc57 = loc("convert.80")
#loc58 = loc("multiply.87")
#loc59 = loc("convert.88")
#loc60 = loc("reshape.89")
#loc61 = loc("reshape.32")
#loc62 = loc("custom-call.33")
#loc63 = loc("reshape.34")
#loc64 = loc("transpose.35")
#loc65 = loc("dot.90")
#loc66 = loc("reshape.92")
#loc67 = loc("transpose.93")
#loc68 = loc("convert.110")
#loc69 = loc("reshape.14")
#loc70 = loc("custom-call.15")
#loc71 = loc("reshape.19")
#loc72 = loc("convert.11")
#loc73 = loc("dot.22")
#loc74 = loc("transpose.23")
#loc75 = loc("concatenate.24")
#loc76 = loc("cosine.104")
#loc77 = loc("convert.107")
#loc78 = loc("reshape.108")
#loc79 = loc("convert.109")
#loc80 = loc("reshape.111")
#loc81 = loc("broadcast.112")
#loc82 = loc("multiply.113")
#loc83 = loc("convert.114")
#loc84 = loc("slice.95")
#loc85 = loc("negate.96")
#loc86 = loc("slice.94")
#loc87 = loc("concatenate.97")
#loc88 = loc("convert.98")
#loc89 = loc("sine.25")
#loc90 = loc("convert.28")
#loc91 = loc("reshape.29")
#loc92 = loc("convert.30")
#loc93 = loc("reshape.99")
#loc94 = loc("broadcast.100")
#loc95 = loc("multiply.101")
#loc96 = loc("convert.102")
#loc97 = loc("add.117")
#loc99 = loc("custom-call.138")
#loc100 = loc("custom-call.161")
#loc101 = loc("reshape.140")
#loc102 = loc("custom-call.141")
#loc103 = loc("reshape.142")
#loc104 = loc("transpose.143")
#loc105 = loc("dot.145")
#loc106 = loc("reshape.147")
#loc107 = loc("transpose.148")
#loc109 = loc("custom-call.168")
#loc110 = loc("reshape.436")
#loc111 = loc("custom-call.437")
#loc112 = loc("reshape.438")
#loc113 = loc("convert.439")
#loc114 = loc("broadcast.440")
#loc115 = loc("reshape.270")
#loc116 = loc("custom-call.271")
#loc117 = loc("reshape.272")
#loc118 = loc("transpose.273")
#loc119 = loc("dot.275")
#loc120 = loc("reshape.277")
#loc121 = loc("transpose.278")
#loc122 = loc("convert.289")
#loc123 = loc("broadcast.291")
#loc124 = loc("multiply.292")
#loc125 = loc("convert.293")
#loc126 = loc("slice.280")
#loc127 = loc("negate.281")
#loc128 = loc("slice.279")
#loc129 = loc("concatenate.282")
#loc130 = loc("convert.283")
#loc131 = loc("broadcast.285")
#loc132 = loc("multiply.286")
#loc133 = loc("convert.287")
#loc134 = loc("add.296")
#loc135 = loc("reshape.298")
#loc136 = loc("broadcast.262")
#loc137 = loc("reshape.263")
#loc138 = loc("transpose.264")
#loc139 = loc("reshape.266")
#loc140 = loc("dot.299")
#loc141 = loc("reshape.300")
#loc142 = loc("convert.301")
#loc143 = loc("broadcast.302")
#loc144 = loc("multiply.303")
#loc145 = loc("convert.304")
#loc146 = loc("broadcast.235")
#loc147 = loc("broadcast.237")
#loc148 = loc("subtract.238")
#loc149 = loc("compare.240")
#loc150 = loc("broadcast.224")
#loc151 = loc("select.242")
#loc152 = loc("convert.243")
#loc153 = loc("broadcast.211")
#loc154 = loc("compare.212")
#loc155 = loc("convert.213")
#loc156 = loc("multiply.244")
#loc157 = loc("convert.245")
#loc158 = loc("reshape.246")
#loc159 = loc("broadcast.308")
#loc160 = loc("add.309")
#loc161 = loc("convert.310")
#loc162 = loc("reduce.316")
#loc163 = loc("broadcast.317")
#loc164 = loc("subtract.318")
#loc165 = loc("exponential.319")
#loc166 = loc("reduce.325")
#loc167 = loc("broadcast.326")
#loc168 = loc("divide.327")
#loc169 = loc("convert.328")
#loc170 = loc("reshape.330")
#loc171 = loc("broadcast.202")
#loc172 = loc("reshape.205")
#loc173 = loc("dot.331")
#loc174 = loc("reshape.332")
#loc175 = loc("transpose.333")
#loc176 = loc("reshape.335")
#loc177 = loc("reshape.191")
#loc178 = loc("custom-call.192")
#loc179 = loc("reshape.193")
#loc180 = loc("transpose.194")
#loc181 = loc("dot.336")
#loc182 = loc("reshape.337")
#loc183 = loc("add.340")
#loc184 = loc("reshape.372")
#loc185 = loc("custom-call.373")
#loc186 = loc("reshape.374")
#loc187 = loc("convert.375")
#loc188 = loc("broadcast.376")
#loc189 = loc("convert.341")
#loc190 = loc("power.343")
#loc191 = loc("reduce.350")
#loc192 = loc("multiply.359")
#loc193 = loc("reshape.360")
#loc194 = loc("add.364")
#loc195 = loc("rsqrt.365")
#loc196 = loc("reshape.366")
#loc197 = loc("broadcast.367")
#loc198 = loc("multiply.368")
#loc199 = loc("convert.369")
#loc200 = loc("convert.370")
#loc201 = loc("multiply.377")
#loc202 = loc("convert.378")
#loc203 = loc("reshape.388")
#loc204 = loc("reshape.384")
#loc205 = loc("custom-call.385")
#loc206 = loc("reshape.386")
#loc207 = loc("transpose.387")
#loc208 = loc("dot.389")
#loc209 = loc("reshape.390")
#loc210 = loc("convert.393")
#loc211 = loc("logistic.391")
#loc212 = loc("convert.392")
#loc213 = loc("multiply.394")
#loc214 = loc("convert.395")
#loc215 = loc("convert.396")
#loc216 = loc("reshape.183")
#loc217 = loc("custom-call.184")
#loc218 = loc("reshape.185")
#loc219 = loc("transpose.186")
#loc220 = loc("dot.380")
#loc221 = loc("reshape.381")
#loc222 = loc("convert.382")
#loc223 = loc("multiply.397")
#loc224 = loc("convert.398")
#loc225 = loc("reshape.399")
#loc226 = loc("reshape.178")
#loc227 = loc("custom-call.179")
#loc228 = loc("reshape.180")
#loc229 = loc("transpose.181")
#loc230 = loc("dot.400")
#loc231 = loc("reshape.401")
#loc232 = loc("add.404")
#loc233 = loc("convert.405")
#loc234 = loc("power.407")
#loc235 = loc("reduce.414")
#loc236 = loc("multiply.423")
#loc237 = loc("reshape.424")
#loc238 = loc("add.428")
#loc239 = loc("rsqrt.429")
#loc240 = loc("reshape.430")
#loc241 = loc("broadcast.431")
#loc242 = loc("multiply.432")
#loc243 = loc("convert.433")
#loc244 = loc("convert.434")
#loc245 = loc("multiply.441")
#loc246 = loc("convert.442")
#loc247 = loc("reshape.446")
#loc248 = loc("reshape.170")
#loc249 = loc("custom-call.171")
#loc250 = loc("reshape.172")
#loc251 = loc("transpose.173")
#loc252 = loc("dot.447")
#loc253 = loc("reshape.448")
#loc1 = loc("p0.3")
#loc2 = loc("p1.13")
#loc3 = loc("p2.31")
#loc4 = loc("p3.37")
#loc5 = loc("p4.39")
#loc6 = loc("p5.44")
#loc7 = loc("p6.81")
#loc8 = loc("p7.118")
#loc9 = loc("p8.130")
#loc10 = loc("p9.139")
#loc11 = loc("p10.160")
#loc12 = loc("p11.169")
#loc13 = loc("p12.177")
#loc14 = loc("p13.182")
#loc15 = loc("p14.190")
#loc16 = loc("p15.220")
#loc17 = loc("p16.254")
#loc18 = loc("p17.269")
#loc19 = loc("p18.371")
#loc20 = loc("p19.383")
#loc21 = loc("p20.435")
#loc91 = loc("scatter.136")
#loc99 = loc("scatter.166")
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_1"} loc("p0.3"), %arg1: tensor<64xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "l__self___model_rotary_emb_inv_freq"} loc("p1.13"), %arg2: tensor<1024x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"} loc("p2.31"), %arg3: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_0"} loc("p3.37"), %arg4: tensor<1x7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"} loc("p4.39"), %arg5: tensor<128256x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_embed_tokens_weight"} loc("p5.44"), %arg6: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"} loc("p6.81"), %arg7: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_1"} loc("p7.118"), %arg8: tensor<1x8x16x128xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_2"} loc("p8.130"), %arg9: tensor<1024x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"} loc("p9.139"), %arg10: tensor<1x8x16x128xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_3"} loc("p10.160"), %arg11: tensor<128256x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___lm_head_weight"} loc("p11.169"), %arg12: tensor<3072x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"} loc("p12.177"), %arg13: tensor<8192x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"} loc("p13.182"), %arg14: tensor<3072x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"} loc("p14.190"), %arg15: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_2"} loc("p15.220"), %arg16: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_3"} loc("p16.254"), %arg17: tensor<3072x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"} loc("p17.269"), %arg18: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"} loc("p18.371"), %arg19: tensor<8192x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"} loc("p19.383"), %arg20: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_norm_weight"} loc("p20.435")) -> (tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64> loc(#loc)
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64> loc(#loc)
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64> loc(#loc)
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32> loc(#loc)
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32> loc(#loc)
    %c_5 = stablehlo.constant dense<1> : tensor<i64> loc(#loc)
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16> loc(#loc)
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64> loc(#loc)
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32> loc(#loc)
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64> loc(#loc22)
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64> loc(#loc23)
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1> loc(#loc24)
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64> loc(#loc25)
    %7 = stablehlo.add %4, %6 : tensor<7xi64> loc(#loc26)
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64> loc(#loc27)
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64> loc(#loc28)
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc29)
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc30)
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32> loc(#loc31)
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32> loc(#loc32)
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16> loc(#loc33)
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16> loc(#loc34)
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64> loc(#loc35)
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64> loc(#loc36)
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32> loc(#loc37)
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16> loc(#loc38)
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16> loc(#loc39)
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32> loc(#loc40)
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32> loc(#loc41)
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32> loc(#loc42)
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32> loc(#loc43)
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32> loc(#loc44)
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32> loc(#loc45)
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32> loc(#loc46)
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32> loc(#loc47)
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32> loc(#loc48)
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32> loc(#loc49)
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32> loc(#loc50)
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16> loc(#loc51)
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32> loc(#loc52)
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32> loc(#loc53)
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16> loc(#loc54)
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16> loc(#loc55)
    %37 = stablehlo.reshape %arg2 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16> loc(#loc56)
    %38 = stablehlo.reshape %37 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16> loc(#loc57)
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16> loc(#loc58)
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16> loc(#loc59)
    %41 = stablehlo.reshape %40 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16> loc(#loc60)
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16> loc(#loc61)
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32> loc(#loc62)
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32> loc(#loc63)
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32> loc(#loc64)
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32> loc(#loc65)
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32> loc(#loc66)
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32> loc(#loc67)
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32> loc(#loc68)
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32> loc(#loc69)
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16> loc(#loc70)
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16> loc(#loc71)
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32> loc(#loc72)
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32> loc(#loc73)
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32> loc(#loc74)
    %56 = stablehlo.multiply %43, %55 : tensor<1x8x7x128xf32> loc(#loc75)
    %57 = stablehlo.convert %56 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16> loc(#loc76)
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16> loc(#loc77)
    %59 = stablehlo.negate %58 : tensor<1x8x7x64xbf16> loc(#loc78)
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16> loc(#loc79)
    %61 = stablehlo.concatenate %59, %60, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16> loc(#loc80)
    %62 = stablehlo.convert %61 : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32> loc(#loc81)
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32> loc(#loc82)
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16> loc(#loc83)
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16> loc(#loc84)
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32> loc(#loc85)
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32> loc(#loc86)
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32> loc(#loc87)
    %69 = stablehlo.multiply %62, %68 : tensor<1x8x7x128xf32> loc(#loc88)
    %70 = stablehlo.convert %69 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16> loc(#loc89)
    %71 = stablehlo.add %57, %70 : tensor<1x8x7x128xbf16> loc(#loc90)
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16> loc("scatter.136"), %arg22: tensor<bf16> loc("scatter.136")):
      stablehlo.return %arg22 : tensor<bf16> loc(#loc)
    }) : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16> loc(#loc91)
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16> loc(#loc92)
    %74 = stablehlo.reshape %arg9 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16> loc(#loc93)
    %75 = stablehlo.reshape %74 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16> loc(#loc94)
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16> loc(#loc95)
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16> loc(#loc96)
    %78 = stablehlo.reshape %77 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16> loc(#loc97)
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16> loc(#loc98)
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16> loc("scatter.166"), %arg22: tensor<bf16> loc("scatter.166")):
      stablehlo.return %arg22 : tensor<bf16> loc(#loc)
    }) : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16> loc(#loc99)
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16> loc(#loc100)
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc101)
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc102)
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32> loc(#loc103)
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32> loc(#loc104)
    %86 = stablehlo.reshape %arg17 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16> loc(#loc105)
    %87 = stablehlo.reshape %86 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16> loc(#loc106)
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16> loc(#loc107)
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16> loc(#loc108)
    %90 = stablehlo.reshape %89 : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16> loc(#loc109)
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16> loc(#loc110)
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32> loc(#loc111)
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32> loc(#loc112)
    %94 = stablehlo.multiply %92, %93 : tensor<1x24x7x128xf32> loc(#loc113)
    %95 = stablehlo.convert %94 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16> loc(#loc114)
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16> loc(#loc115)
    %97 = stablehlo.negate %96 : tensor<1x24x7x64xbf16> loc(#loc116)
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16> loc(#loc117)
    %99 = stablehlo.concatenate %97, %98, dim = 3 : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16> loc(#loc118)
    %100 = stablehlo.convert %99 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32> loc(#loc119)
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32> loc(#loc120)
    %102 = stablehlo.multiply %100, %101 : tensor<1x24x7x128xf32> loc(#loc121)
    %103 = stablehlo.convert %102 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16> loc(#loc122)
    %104 = stablehlo.add %95, %103 : tensor<1x24x7x128xbf16> loc(#loc123)
    %105 = stablehlo.reshape %104 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16> loc(#loc124)
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16> loc(#loc125)
    %107 = stablehlo.reshape %106 : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16> loc(#loc126)
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16> loc(#loc127)
    %109 = stablehlo.reshape %108 : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16> loc(#loc128)
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16> loc(#loc129)
    %111 = stablehlo.reshape %110 : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16> loc(#loc130)
    %112 = stablehlo.convert %111 : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32> loc(#loc131)
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<1x24x7x16xf32> loc(#loc132)
    %114 = stablehlo.multiply %112, %113 : tensor<1x24x7x16xf32> loc(#loc133)
    %115 = stablehlo.convert %114 : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16> loc(#loc134)
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64> loc(#loc135)
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64> loc(#loc136)
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64> loc(#loc137)
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1> loc(#loc138)
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16> loc(#loc139)
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16> loc(#loc140)
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32> loc(#loc141)
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64> loc(#loc142)
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1> loc(#loc143)
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32> loc(#loc144)
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32> loc(#loc145)
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16> loc(#loc146)
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16> loc(#loc147)
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16> loc(#loc148)
    %130 = stablehlo.add %115, %129 : tensor<1x24x7x16xbf16> loc(#loc149)
    %131 = stablehlo.convert %130 : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32> loc(#loc150)
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32> loc(#loc151)
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32> loc(#loc152)
    %134 = stablehlo.subtract %131, %133 : tensor<1x24x7x16xf32> loc(#loc153)
    %135 = stablehlo.exponential %134 : tensor<1x24x7x16xf32> loc(#loc154)
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32> loc(#loc155)
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32> loc(#loc156)
    %138 = stablehlo.divide %135, %137 : tensor<1x24x7x16xf32> loc(#loc157)
    %139 = stablehlo.convert %138 : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16> loc(#loc158)
    %140 = stablehlo.reshape %139 : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16> loc(#loc159)
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16> loc(#loc160)
    %142 = stablehlo.reshape %141 : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16> loc(#loc161)
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16> loc(#loc162)
    %144 = stablehlo.reshape %143 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16> loc(#loc163)
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16> loc(#loc164)
    %146 = stablehlo.reshape %145 : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16> loc(#loc165)
    %147 = stablehlo.reshape %arg14 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16> loc(#loc166)
    %148 = stablehlo.reshape %147 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16> loc(#loc167)
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16> loc(#loc168)
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16> loc(#loc169)
    %151 = stablehlo.reshape %150 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16> loc(#loc170)
    %152 = stablehlo.add %20, %151 : tensor<1x7x3072xbf16> loc(#loc171)
    %153 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc172)
    %154 = stablehlo.reshape %153 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc173)
    %155 = stablehlo.convert %154 : (tensor<3072xbf16>) -> tensor<3072xf32> loc(#loc174)
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32> loc(#loc175)
    %157 = stablehlo.convert %152 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32> loc(#loc176)
    %158 = stablehlo.power %157, %2 : tensor<1x7x3072xf32> loc(#loc177)
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32> loc(#loc178)
    %160 = stablehlo.multiply %159, %cst_4 : tensor<1x7xf32> loc(#loc179)
    %161 = stablehlo.reshape %160 : (tensor<1x7xf32>) -> tensor<1x7x1xf32> loc(#loc180)
    %162 = stablehlo.add %161, %26 : tensor<1x7x1xf32> loc(#loc181)
    %163 = stablehlo.rsqrt %162 : tensor<1x7x1xf32> loc(#loc182)
    %164 = stablehlo.reshape %163 : (tensor<1x7x1xf32>) -> tensor<1x7xf32> loc(#loc183)
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32> loc(#loc184)
    %166 = stablehlo.multiply %157, %165 : tensor<1x7x3072xf32> loc(#loc185)
    %167 = stablehlo.convert %166 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16> loc(#loc186)
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32> loc(#loc187)
    %169 = stablehlo.multiply %156, %168 : tensor<1x7x3072xf32> loc(#loc188)
    %170 = stablehlo.convert %169 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16> loc(#loc189)
    %171 = stablehlo.reshape %170 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16> loc(#loc190)
    %172 = stablehlo.reshape %arg19 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16> loc(#loc191)
    %173 = stablehlo.reshape %172 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16> loc(#loc192)
    %174 = stablehlo.transpose %173, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16> loc(#loc193)
    %175 = stablehlo.dot_general %171, %174, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16> loc(#loc194)
    %176 = stablehlo.reshape %175 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16> loc(#loc195)
    %177 = stablehlo.convert %176 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32> loc(#loc196)
    %178 = stablehlo.logistic %176 : tensor<1x7x8192xbf16> loc(#loc197)
    %179 = stablehlo.convert %178 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32> loc(#loc198)
    %180 = stablehlo.multiply %177, %179 : tensor<1x7x8192xf32> loc(#loc199)
    %181 = stablehlo.convert %180 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16> loc(#loc200)
    %182 = stablehlo.convert %181 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32> loc(#loc201)
    %183 = stablehlo.reshape %arg13 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16> loc(#loc202)
    %184 = stablehlo.reshape %183 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16> loc(#loc203)
    %185 = stablehlo.transpose %184, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16> loc(#loc204)
    %186 = stablehlo.dot_general %171, %185, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16> loc(#loc205)
    %187 = stablehlo.reshape %186 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16> loc(#loc206)
    %188 = stablehlo.convert %187 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32> loc(#loc207)
    %189 = stablehlo.multiply %182, %188 : tensor<1x7x8192xf32> loc(#loc208)
    %190 = stablehlo.convert %189 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16> loc(#loc209)
    %191 = stablehlo.reshape %190 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16> loc(#loc210)
    %192 = stablehlo.reshape %arg12 : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16> loc(#loc211)
    %193 = stablehlo.reshape %192 : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16> loc(#loc212)
    %194 = stablehlo.transpose %193, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16> loc(#loc213)
    %195 = stablehlo.dot_general %191, %194, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16> loc(#loc214)
    %196 = stablehlo.reshape %195 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16> loc(#loc215)
    %197 = stablehlo.add %152, %196 : tensor<1x7x3072xbf16> loc(#loc216)
    %198 = stablehlo.convert %197 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32> loc(#loc217)
    %199 = stablehlo.power %198, %2 : tensor<1x7x3072xf32> loc(#loc218)
    %200 = stablehlo.reduce(%199 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32> loc(#loc219)
    %201 = stablehlo.multiply %200, %cst_4 : tensor<1x7xf32> loc(#loc220)
    %202 = stablehlo.reshape %201 : (tensor<1x7xf32>) -> tensor<1x7x1xf32> loc(#loc221)
    %203 = stablehlo.add %202, %26 : tensor<1x7x1xf32> loc(#loc222)
    %204 = stablehlo.rsqrt %203 : tensor<1x7x1xf32> loc(#loc223)
    %205 = stablehlo.reshape %204 : (tensor<1x7x1xf32>) -> tensor<1x7xf32> loc(#loc224)
    %206 = stablehlo.broadcast_in_dim %205, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32> loc(#loc225)
    %207 = stablehlo.multiply %198, %206 : tensor<1x7x3072xf32> loc(#loc226)
    %208 = stablehlo.convert %207 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16> loc(#loc227)
    %209 = stablehlo.convert %208 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32> loc(#loc228)
    %210 = stablehlo.multiply %85, %209 : tensor<1x7x3072xf32> loc(#loc229)
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16> loc(#loc230)
    %212 = stablehlo.reshape %211 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16> loc(#loc231)
    %213 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16> loc(#loc232)
    %214 = stablehlo.reshape %213 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16> loc(#loc233)
    %215 = stablehlo.transpose %214, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16> loc(#loc234)
    %216 = stablehlo.dot_general %212, %215, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16> loc(#loc235)
    %217 = stablehlo.reshape %216 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16> loc(#loc236)
    return %73, %81, %216, %217 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc22 = loc("reshape.4")
#loc23 = loc("reshape.6")
#loc24 = loc("compare.126")
#loc25 = loc("broadcast.122")
#loc26 = loc("add.123")
#loc27 = loc("select.127")
#loc28 = loc("reshape.128")
#loc29 = loc("reshape.82")
#loc30 = loc("reshape.84")
#loc31 = loc("convert.85")
#loc32 = loc("broadcast.86")
#loc33 = loc("reshape.45")
#loc34 = loc("reshape.47")
#loc35 = loc("reshape.40")
#loc36 = loc("reshape.43")
#loc37 = loc("convert.48")
#loc38 = loc("gather.49")
#loc39 = loc("reshape.50")
#loc40 = loc("convert.51")
#loc41 = loc("power.53")
#loc42 = loc("reduce.60")
#loc43 = loc("multiply.69")
#loc44 = loc("reshape.70")
#loc45 = loc("broadcast.73")
#loc46 = loc("add.74")
#loc47 = loc("rsqrt.75")
#loc48 = loc("reshape.76")
#loc49 = loc("broadcast.77")
#loc50 = loc("multiply.78")
#loc51 = loc("convert.79")
#loc52 = loc("convert.80")
#loc53 = loc("multiply.87")
#loc54 = loc("convert.88")
#loc55 = loc("reshape.89")
#loc56 = loc("reshape.32")
#loc57 = loc("reshape.34")
#loc58 = loc("transpose.35")
#loc59 = loc("dot.90")
#loc60 = loc("reshape.92")
#loc61 = loc("transpose.93")
#loc62 = loc("convert.110")
#loc63 = loc("reshape.14")
#loc64 = loc("reshape.19")
#loc65 = loc("convert.11")
#loc66 = loc("dot.22")
#loc67 = loc("transpose.23")
#loc68 = loc("concatenate.24")
#loc69 = loc("cosine.104")
#loc70 = loc("convert.107")
#loc71 = loc("reshape.108")
#loc72 = loc("convert.109")
#loc73 = loc("reshape.111")
#loc74 = loc("broadcast.112")
#loc75 = loc("multiply.113")
#loc76 = loc("convert.114")
#loc77 = loc("slice.95")
#loc78 = loc("negate.96")
#loc79 = loc("slice.94")
#loc80 = loc("concatenate.97")
#loc81 = loc("convert.98")
#loc82 = loc("sine.25")
#loc83 = loc("convert.28")
#loc84 = loc("reshape.29")
#loc85 = loc("convert.30")
#loc86 = loc("reshape.99")
#loc87 = loc("broadcast.100")
#loc88 = loc("multiply.101")
#loc89 = loc("convert.102")
#loc90 = loc("add.117")
#loc92 = loc("custom-call.138")
#loc93 = loc("reshape.140")
#loc94 = loc("reshape.142")
#loc95 = loc("transpose.143")
#loc96 = loc("dot.145")
#loc97 = loc("reshape.147")
#loc98 = loc("transpose.148")
#loc100 = loc("custom-call.168")
#loc101 = loc("reshape.436")
#loc102 = loc("reshape.438")
#loc103 = loc("convert.439")
#loc104 = loc("broadcast.440")
#loc105 = loc("reshape.270")
#loc106 = loc("reshape.272")
#loc107 = loc("transpose.273")
#loc108 = loc("dot.275")
#loc109 = loc("reshape.277")
#loc110 = loc("transpose.278")
#loc111 = loc("convert.289")
#loc112 = loc("broadcast.291")
#loc113 = loc("multiply.292")
#loc114 = loc("convert.293")
#loc115 = loc("slice.280")
#loc116 = loc("negate.281")
#loc117 = loc("slice.279")
#loc118 = loc("concatenate.282")
#loc119 = loc("convert.283")
#loc120 = loc("broadcast.285")
#loc121 = loc("multiply.286")
#loc122 = loc("convert.287")
#loc123 = loc("add.296")
#loc124 = loc("reshape.298")
#loc125 = loc("broadcast.262")
#loc126 = loc("reshape.263")
#loc127 = loc("transpose.264")
#loc128 = loc("reshape.266")
#loc129 = loc("dot.299")
#loc130 = loc("reshape.300")
#loc131 = loc("convert.301")
#loc132 = loc("broadcast.302")
#loc133 = loc("multiply.303")
#loc134 = loc("convert.304")
#loc135 = loc("broadcast.235")
#loc136 = loc("broadcast.237")
#loc137 = loc("subtract.238")
#loc138 = loc("compare.240")
#loc139 = loc("broadcast.224")
#loc140 = loc("select.242")
#loc141 = loc("convert.243")
#loc142 = loc("broadcast.211")
#loc143 = loc("compare.212")
#loc144 = loc("convert.213")
#loc145 = loc("multiply.244")
#loc146 = loc("convert.245")
#loc147 = loc("reshape.246")
#loc148 = loc("broadcast.308")
#loc149 = loc("add.309")
#loc150 = loc("convert.310")
#loc151 = loc("reduce.316")
#loc152 = loc("broadcast.317")
#loc153 = loc("subtract.318")
#loc154 = loc("exponential.319")
#loc155 = loc("reduce.325")
#loc156 = loc("broadcast.326")
#loc157 = loc("divide.327")
#loc158 = loc("convert.328")
#loc159 = loc("reshape.330")
#loc160 = loc("broadcast.202")
#loc161 = loc("reshape.205")
#loc162 = loc("dot.331")
#loc163 = loc("reshape.332")
#loc164 = loc("transpose.333")
#loc165 = loc("reshape.335")
#loc166 = loc("reshape.191")
#loc167 = loc("reshape.193")
#loc168 = loc("transpose.194")
#loc169 = loc("dot.336")
#loc170 = loc("reshape.337")
#loc171 = loc("add.340")
#loc172 = loc("reshape.372")
#loc173 = loc("reshape.374")
#loc174 = loc("convert.375")
#loc175 = loc("broadcast.376")
#loc176 = loc("convert.341")
#loc177 = loc("power.343")
#loc178 = loc("reduce.350")
#loc179 = loc("multiply.359")
#loc180 = loc("reshape.360")
#loc181 = loc("add.364")
#loc182 = loc("rsqrt.365")
#loc183 = loc("reshape.366")
#loc184 = loc("broadcast.367")
#loc185 = loc("multiply.368")
#loc186 = loc("convert.369")
#loc187 = loc("convert.370")
#loc188 = loc("multiply.377")
#loc189 = loc("convert.378")
#loc190 = loc("reshape.388")
#loc191 = loc("reshape.384")
#loc192 = loc("reshape.386")
#loc193 = loc("transpose.387")
#loc194 = loc("dot.389")
#loc195 = loc("reshape.390")
#loc196 = loc("convert.393")
#loc197 = loc("logistic.391")
#loc198 = loc("convert.392")
#loc199 = loc("multiply.394")
#loc200 = loc("convert.395")
#loc201 = loc("convert.396")
#loc202 = loc("reshape.183")
#loc203 = loc("reshape.185")
#loc204 = loc("transpose.186")
#loc205 = loc("dot.380")
#loc206 = loc("reshape.381")
#loc207 = loc("convert.382")
#loc208 = loc("multiply.397")
#loc209 = loc("convert.398")
#loc210 = loc("reshape.399")
#loc211 = loc("reshape.178")
#loc212 = loc("reshape.180")
#loc213 = loc("transpose.181")
#loc214 = loc("dot.400")
#loc215 = loc("reshape.401")
#loc216 = loc("add.404")
#loc217 = loc("convert.405")
#loc218 = loc("power.407")
#loc219 = loc("reduce.414")
#loc220 = loc("multiply.423")
#loc221 = loc("reshape.424")
#loc222 = loc("add.428")
#loc223 = loc("rsqrt.429")
#loc224 = loc("reshape.430")
#loc225 = loc("broadcast.431")
#loc226 = loc("multiply.432")
#loc227 = loc("convert.433")
#loc228 = loc("convert.434")
#loc229 = loc("multiply.441")
#loc230 = loc("convert.442")
#loc231 = loc("reshape.446")
#loc232 = loc("reshape.170")
#loc233 = loc("reshape.172")
#loc234 = loc("transpose.173")
#loc235 = loc("dot.447")
#loc236 = loc("reshape.448")
// -----// IR Dump Before Inliner (inline) ('builtin.module' operation: @SyncTensorsGraph.450) //----- //
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_1"}, %arg1: tensor<64xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg2: tensor<1024x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg3: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_0"}, %arg4: tensor<1x7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"}, %arg5: tensor<128256x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg6: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg7: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_1"}, %arg8: tensor<1x8x16x128xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_2"}, %arg9: tensor<1024x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg10: tensor<1x8x16x128xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_3"}, %arg11: tensor<128256x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___lm_head_weight"}, %arg12: tensor<3072x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg13: tensor<8192x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg14: tensor<3072x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg15: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_2"}, %arg16: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_3"}, %arg17: tensor<3072x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64>
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %7 = stablehlo.add %4, %6 : tensor<7xi64>
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64>
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64>
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32>
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32>
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32>
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32>
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32>
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %37 = stablehlo.reshape %arg2 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %38 = stablehlo.reshape %37 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %56 = stablehlo.multiply %43, %55 : tensor<1x8x7x128xf32>
    %57 = stablehlo.convert %56 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.convert %61 : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32>
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %69 = stablehlo.multiply %62, %68 : tensor<1x8x7x128xf32>
    %70 = stablehlo.convert %69 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %71 = stablehlo.add %57, %70 : tensor<1x8x7x128xbf16>
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %74 = stablehlo.reshape %arg9 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %86 = stablehlo.reshape %arg17 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %87 = stablehlo.reshape %86 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %90 = stablehlo.reshape %89 : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %94 = stablehlo.multiply %92, %93 : tensor<1x24x7x128xf32>
    %95 = stablehlo.convert %94 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %97 = stablehlo.negate %96 : tensor<1x24x7x64xbf16>
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %99 = stablehlo.concatenate %97, %98, dim = 3 : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %100 = stablehlo.convert %99 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.multiply %100, %101 : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %95, %103 : tensor<1x24x7x128xbf16>
    %105 = stablehlo.reshape %104 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %107 = stablehlo.reshape %106 : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16>
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16>
    %109 = stablehlo.reshape %108 : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16>
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16>
    %111 = stablehlo.reshape %110 : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %112 = stablehlo.convert %111 : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<1x24x7x16xf32>
    %114 = stablehlo.multiply %112, %113 : tensor<1x24x7x16xf32>
    %115 = stablehlo.convert %114 : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64>
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64>
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16>
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32>
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32>
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32>
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16>
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %130 = stablehlo.add %115, %129 : tensor<1x24x7x16xbf16>
    %131 = stablehlo.convert %130 : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %134 = stablehlo.subtract %131, %133 : tensor<1x24x7x16xf32>
    %135 = stablehlo.exponential %134 : tensor<1x24x7x16xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %138 = stablehlo.divide %135, %137 : tensor<1x24x7x16xf32>
    %139 = stablehlo.convert %138 : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %140 = stablehlo.reshape %139 : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16>
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %142 = stablehlo.reshape %141 : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16>
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16>
    %144 = stablehlo.reshape %143 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %146 = stablehlo.reshape %145 : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %147 = stablehlo.reshape %arg14 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %148 = stablehlo.reshape %147 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %151 = stablehlo.reshape %150 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %152 = stablehlo.add %20, %151 : tensor<1x7x3072xbf16>
    %153 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %155 = stablehlo.convert %154 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %157 = stablehlo.convert %152 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %158 = stablehlo.power %157, %2 : tensor<1x7x3072xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %160 = stablehlo.multiply %159, %cst_4 : tensor<1x7xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %162 = stablehlo.add %161, %26 : tensor<1x7x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x7x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x7x3072xf32>
    %167 = stablehlo.convert %166 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %169 = stablehlo.multiply %156, %168 : tensor<1x7x3072xf32>
    %170 = stablehlo.convert %169 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %172 = stablehlo.reshape %arg19 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %174 = stablehlo.transpose %173, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %175 = stablehlo.dot_general %171, %174, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %176 = stablehlo.reshape %175 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %177 = stablehlo.convert %176 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %178 = stablehlo.logistic %176 : tensor<1x7x8192xbf16>
    %179 = stablehlo.convert %178 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %180 = stablehlo.multiply %177, %179 : tensor<1x7x8192xf32>
    %181 = stablehlo.convert %180 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %182 = stablehlo.convert %181 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %183 = stablehlo.reshape %arg13 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %184 = stablehlo.reshape %183 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %185 = stablehlo.transpose %184, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %186 = stablehlo.dot_general %171, %185, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %187 = stablehlo.reshape %186 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %188 = stablehlo.convert %187 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %189 = stablehlo.multiply %182, %188 : tensor<1x7x8192xf32>
    %190 = stablehlo.convert %189 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %191 = stablehlo.reshape %190 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %192 = stablehlo.reshape %arg12 : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %193 = stablehlo.reshape %192 : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %194 = stablehlo.transpose %193, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %195 = stablehlo.dot_general %191, %194, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %196 = stablehlo.reshape %195 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %197 = stablehlo.add %152, %196 : tensor<1x7x3072xbf16>
    %198 = stablehlo.convert %197 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %199 = stablehlo.power %198, %2 : tensor<1x7x3072xf32>
    %200 = stablehlo.reduce(%199 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %201 = stablehlo.multiply %200, %cst_4 : tensor<1x7xf32>
    %202 = stablehlo.reshape %201 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %203 = stablehlo.add %202, %26 : tensor<1x7x1xf32>
    %204 = stablehlo.rsqrt %203 : tensor<1x7x1xf32>
    %205 = stablehlo.reshape %204 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %207 = stablehlo.multiply %198, %206 : tensor<1x7x3072xf32>
    %208 = stablehlo.convert %207 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %209 = stablehlo.convert %208 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %210 = stablehlo.multiply %85, %209 : tensor<1x7x3072xf32>
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %212 = stablehlo.reshape %211 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %213 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %214 = stablehlo.reshape %213 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %215 = stablehlo.transpose %214, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %216 = stablehlo.dot_general %212, %215, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %217 = stablehlo.reshape %216 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %73, %81, %216, %217 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_1"}, %arg1: tensor<64xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg2: tensor<1024x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg3: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_0"}, %arg4: tensor<1x7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"}, %arg5: tensor<128256x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg6: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg7: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_1"}, %arg8: tensor<1x8x16x128xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_2"}, %arg9: tensor<1024x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg10: tensor<1x8x16x128xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_3"}, %arg11: tensor<128256x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___lm_head_weight"}, %arg12: tensor<3072x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg13: tensor<8192x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg14: tensor<3072x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg15: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_2"}, %arg16: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_3"}, %arg17: tensor<3072x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64>
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %7 = stablehlo.add %4, %6 : tensor<7xi64>
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64>
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64>
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32>
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32>
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32>
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32>
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32>
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %37 = stablehlo.reshape %arg2 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %38 = stablehlo.reshape %37 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %56 = stablehlo.multiply %43, %55 : tensor<1x8x7x128xf32>
    %57 = stablehlo.convert %56 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.convert %61 : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32>
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %69 = stablehlo.multiply %62, %68 : tensor<1x8x7x128xf32>
    %70 = stablehlo.convert %69 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %71 = stablehlo.add %57, %70 : tensor<1x8x7x128xbf16>
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %74 = stablehlo.reshape %arg9 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %86 = stablehlo.reshape %arg17 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %87 = stablehlo.reshape %86 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %90 = stablehlo.reshape %89 : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %94 = stablehlo.multiply %92, %93 : tensor<1x24x7x128xf32>
    %95 = stablehlo.convert %94 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %97 = stablehlo.negate %96 : tensor<1x24x7x64xbf16>
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %99 = stablehlo.concatenate %97, %98, dim = 3 : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %100 = stablehlo.convert %99 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.multiply %100, %101 : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %95, %103 : tensor<1x24x7x128xbf16>
    %105 = stablehlo.reshape %104 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %107 = stablehlo.reshape %106 : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16>
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16>
    %109 = stablehlo.reshape %108 : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16>
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16>
    %111 = stablehlo.reshape %110 : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %112 = stablehlo.convert %111 : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<1x24x7x16xf32>
    %114 = stablehlo.multiply %112, %113 : tensor<1x24x7x16xf32>
    %115 = stablehlo.convert %114 : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64>
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64>
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16>
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32>
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32>
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32>
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16>
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %130 = stablehlo.add %115, %129 : tensor<1x24x7x16xbf16>
    %131 = stablehlo.convert %130 : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %134 = stablehlo.subtract %131, %133 : tensor<1x24x7x16xf32>
    %135 = stablehlo.exponential %134 : tensor<1x24x7x16xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %138 = stablehlo.divide %135, %137 : tensor<1x24x7x16xf32>
    %139 = stablehlo.convert %138 : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %140 = stablehlo.reshape %139 : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16>
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %142 = stablehlo.reshape %141 : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16>
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16>
    %144 = stablehlo.reshape %143 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %146 = stablehlo.reshape %145 : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %147 = stablehlo.reshape %arg14 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %148 = stablehlo.reshape %147 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %151 = stablehlo.reshape %150 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %152 = stablehlo.add %20, %151 : tensor<1x7x3072xbf16>
    %153 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %155 = stablehlo.convert %154 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %157 = stablehlo.convert %152 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %158 = stablehlo.power %157, %2 : tensor<1x7x3072xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %160 = stablehlo.multiply %159, %cst_4 : tensor<1x7xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %162 = stablehlo.add %161, %26 : tensor<1x7x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x7x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x7x3072xf32>
    %167 = stablehlo.convert %166 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %169 = stablehlo.multiply %156, %168 : tensor<1x7x3072xf32>
    %170 = stablehlo.convert %169 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %172 = stablehlo.reshape %arg19 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %174 = stablehlo.transpose %173, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %175 = stablehlo.dot_general %171, %174, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %176 = stablehlo.reshape %175 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %177 = stablehlo.convert %176 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %178 = stablehlo.logistic %176 : tensor<1x7x8192xbf16>
    %179 = stablehlo.convert %178 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %180 = stablehlo.multiply %177, %179 : tensor<1x7x8192xf32>
    %181 = stablehlo.convert %180 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %182 = stablehlo.convert %181 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %183 = stablehlo.reshape %arg13 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %184 = stablehlo.reshape %183 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %185 = stablehlo.transpose %184, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %186 = stablehlo.dot_general %171, %185, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %187 = stablehlo.reshape %186 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %188 = stablehlo.convert %187 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %189 = stablehlo.multiply %182, %188 : tensor<1x7x8192xf32>
    %190 = stablehlo.convert %189 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %191 = stablehlo.reshape %190 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %192 = stablehlo.reshape %arg12 : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %193 = stablehlo.reshape %192 : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %194 = stablehlo.transpose %193, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %195 = stablehlo.dot_general %191, %194, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %196 = stablehlo.reshape %195 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %197 = stablehlo.add %152, %196 : tensor<1x7x3072xbf16>
    %198 = stablehlo.convert %197 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %199 = stablehlo.power %198, %2 : tensor<1x7x3072xf32>
    %200 = stablehlo.reduce(%199 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %201 = stablehlo.multiply %200, %cst_4 : tensor<1x7xf32>
    %202 = stablehlo.reshape %201 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %203 = stablehlo.add %202, %26 : tensor<1x7x1xf32>
    %204 = stablehlo.rsqrt %203 : tensor<1x7x1xf32>
    %205 = stablehlo.reshape %204 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %207 = stablehlo.multiply %198, %206 : tensor<1x7x3072xf32>
    %208 = stablehlo.convert %207 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %209 = stablehlo.convert %208 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %210 = stablehlo.multiply %85, %209 : tensor<1x7x3072xf32>
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %212 = stablehlo.reshape %211 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %213 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %214 = stablehlo.reshape %213 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %215 = stablehlo.transpose %214, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %216 = stablehlo.dot_general %212, %215, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %217 = stablehlo.reshape %216 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %73, %81, %216, %217 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump Before TTPopulateArgumentTypes (tt-populate-argument-types) ('builtin.module' operation: @SyncTensorsGraph.450) //----- //
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_1"}, %arg1: tensor<64xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg2: tensor<1024x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg3: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_0"}, %arg4: tensor<1x7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"}, %arg5: tensor<128256x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg6: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg7: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_1"}, %arg8: tensor<1x8x16x128xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_2"}, %arg9: tensor<1024x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg10: tensor<1x8x16x128xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_3"}, %arg11: tensor<128256x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___lm_head_weight"}, %arg12: tensor<3072x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg13: tensor<8192x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg14: tensor<3072x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg15: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_2"}, %arg16: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_3"}, %arg17: tensor<3072x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64>
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %7 = stablehlo.add %4, %6 : tensor<7xi64>
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64>
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64>
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32>
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32>
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32>
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32>
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32>
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %37 = stablehlo.reshape %arg2 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %38 = stablehlo.reshape %37 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %56 = stablehlo.multiply %43, %55 : tensor<1x8x7x128xf32>
    %57 = stablehlo.convert %56 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.convert %61 : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32>
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %69 = stablehlo.multiply %62, %68 : tensor<1x8x7x128xf32>
    %70 = stablehlo.convert %69 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %71 = stablehlo.add %57, %70 : tensor<1x8x7x128xbf16>
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %74 = stablehlo.reshape %arg9 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %86 = stablehlo.reshape %arg17 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %87 = stablehlo.reshape %86 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %90 = stablehlo.reshape %89 : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %94 = stablehlo.multiply %92, %93 : tensor<1x24x7x128xf32>
    %95 = stablehlo.convert %94 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %97 = stablehlo.negate %96 : tensor<1x24x7x64xbf16>
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %99 = stablehlo.concatenate %97, %98, dim = 3 : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %100 = stablehlo.convert %99 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.multiply %100, %101 : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %95, %103 : tensor<1x24x7x128xbf16>
    %105 = stablehlo.reshape %104 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %107 = stablehlo.reshape %106 : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16>
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16>
    %109 = stablehlo.reshape %108 : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16>
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16>
    %111 = stablehlo.reshape %110 : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %112 = stablehlo.convert %111 : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<1x24x7x16xf32>
    %114 = stablehlo.multiply %112, %113 : tensor<1x24x7x16xf32>
    %115 = stablehlo.convert %114 : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64>
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64>
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16>
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32>
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32>
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32>
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16>
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %130 = stablehlo.add %115, %129 : tensor<1x24x7x16xbf16>
    %131 = stablehlo.convert %130 : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %134 = stablehlo.subtract %131, %133 : tensor<1x24x7x16xf32>
    %135 = stablehlo.exponential %134 : tensor<1x24x7x16xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %138 = stablehlo.divide %135, %137 : tensor<1x24x7x16xf32>
    %139 = stablehlo.convert %138 : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %140 = stablehlo.reshape %139 : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16>
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %142 = stablehlo.reshape %141 : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16>
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16>
    %144 = stablehlo.reshape %143 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %146 = stablehlo.reshape %145 : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %147 = stablehlo.reshape %arg14 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %148 = stablehlo.reshape %147 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %151 = stablehlo.reshape %150 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %152 = stablehlo.add %20, %151 : tensor<1x7x3072xbf16>
    %153 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %155 = stablehlo.convert %154 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %157 = stablehlo.convert %152 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %158 = stablehlo.power %157, %2 : tensor<1x7x3072xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %160 = stablehlo.multiply %159, %cst_4 : tensor<1x7xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %162 = stablehlo.add %161, %26 : tensor<1x7x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x7x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x7x3072xf32>
    %167 = stablehlo.convert %166 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %169 = stablehlo.multiply %156, %168 : tensor<1x7x3072xf32>
    %170 = stablehlo.convert %169 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %172 = stablehlo.reshape %arg19 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %174 = stablehlo.transpose %173, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %175 = stablehlo.dot_general %171, %174, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %176 = stablehlo.reshape %175 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %177 = stablehlo.convert %176 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %178 = stablehlo.logistic %176 : tensor<1x7x8192xbf16>
    %179 = stablehlo.convert %178 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %180 = stablehlo.multiply %177, %179 : tensor<1x7x8192xf32>
    %181 = stablehlo.convert %180 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %182 = stablehlo.convert %181 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %183 = stablehlo.reshape %arg13 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %184 = stablehlo.reshape %183 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %185 = stablehlo.transpose %184, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %186 = stablehlo.dot_general %171, %185, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %187 = stablehlo.reshape %186 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %188 = stablehlo.convert %187 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %189 = stablehlo.multiply %182, %188 : tensor<1x7x8192xf32>
    %190 = stablehlo.convert %189 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %191 = stablehlo.reshape %190 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %192 = stablehlo.reshape %arg12 : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %193 = stablehlo.reshape %192 : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %194 = stablehlo.transpose %193, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %195 = stablehlo.dot_general %191, %194, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %196 = stablehlo.reshape %195 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %197 = stablehlo.add %152, %196 : tensor<1x7x3072xbf16>
    %198 = stablehlo.convert %197 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %199 = stablehlo.power %198, %2 : tensor<1x7x3072xf32>
    %200 = stablehlo.reduce(%199 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %201 = stablehlo.multiply %200, %cst_4 : tensor<1x7xf32>
    %202 = stablehlo.reshape %201 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %203 = stablehlo.add %202, %26 : tensor<1x7x1xf32>
    %204 = stablehlo.rsqrt %203 : tensor<1x7x1xf32>
    %205 = stablehlo.reshape %204 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %207 = stablehlo.multiply %198, %206 : tensor<1x7x3072xf32>
    %208 = stablehlo.convert %207 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %209 = stablehlo.convert %208 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %210 = stablehlo.multiply %85, %209 : tensor<1x7x3072xf32>
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %212 = stablehlo.reshape %211 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %213 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %214 = stablehlo.reshape %213 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %215 = stablehlo.transpose %214, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %216 = stablehlo.dot_general %212, %215, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %217 = stablehlo.reshape %216 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %73, %81, %216, %217 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump Before ApplyArgumentShardStatusPass (apply-argument-shard-status) ('builtin.module' operation: @SyncTensorsGraph.450) //----- //
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_1"}, %arg1: tensor<64xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg2: tensor<1024x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg3: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_0"}, %arg4: tensor<1x7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"}, %arg5: tensor<128256x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg6: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg7: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_1"}, %arg8: tensor<1x8x16x128xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_2"}, %arg9: tensor<1024x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg10: tensor<1x8x16x128xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_3"}, %arg11: tensor<128256x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___lm_head_weight"}, %arg12: tensor<3072x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg13: tensor<8192x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg14: tensor<3072x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg15: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_2"}, %arg16: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_3"}, %arg17: tensor<3072x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64>
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %7 = stablehlo.add %4, %6 : tensor<7xi64>
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64>
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64>
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32>
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32>
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32>
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32>
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32>
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %37 = stablehlo.reshape %arg2 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %38 = stablehlo.reshape %37 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %56 = stablehlo.multiply %43, %55 : tensor<1x8x7x128xf32>
    %57 = stablehlo.convert %56 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.convert %61 : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32>
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %69 = stablehlo.multiply %62, %68 : tensor<1x8x7x128xf32>
    %70 = stablehlo.convert %69 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %71 = stablehlo.add %57, %70 : tensor<1x8x7x128xbf16>
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %74 = stablehlo.reshape %arg9 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %86 = stablehlo.reshape %arg17 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %87 = stablehlo.reshape %86 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %90 = stablehlo.reshape %89 : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %94 = stablehlo.multiply %92, %93 : tensor<1x24x7x128xf32>
    %95 = stablehlo.convert %94 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %97 = stablehlo.negate %96 : tensor<1x24x7x64xbf16>
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %99 = stablehlo.concatenate %97, %98, dim = 3 : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %100 = stablehlo.convert %99 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.multiply %100, %101 : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %95, %103 : tensor<1x24x7x128xbf16>
    %105 = stablehlo.reshape %104 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %107 = stablehlo.reshape %106 : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16>
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16>
    %109 = stablehlo.reshape %108 : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16>
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16>
    %111 = stablehlo.reshape %110 : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %112 = stablehlo.convert %111 : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<1x24x7x16xf32>
    %114 = stablehlo.multiply %112, %113 : tensor<1x24x7x16xf32>
    %115 = stablehlo.convert %114 : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64>
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64>
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16>
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32>
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32>
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32>
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16>
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %130 = stablehlo.add %115, %129 : tensor<1x24x7x16xbf16>
    %131 = stablehlo.convert %130 : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %134 = stablehlo.subtract %131, %133 : tensor<1x24x7x16xf32>
    %135 = stablehlo.exponential %134 : tensor<1x24x7x16xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %138 = stablehlo.divide %135, %137 : tensor<1x24x7x16xf32>
    %139 = stablehlo.convert %138 : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %140 = stablehlo.reshape %139 : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16>
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %142 = stablehlo.reshape %141 : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16>
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16>
    %144 = stablehlo.reshape %143 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %146 = stablehlo.reshape %145 : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %147 = stablehlo.reshape %arg14 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %148 = stablehlo.reshape %147 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %151 = stablehlo.reshape %150 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %152 = stablehlo.add %20, %151 : tensor<1x7x3072xbf16>
    %153 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %155 = stablehlo.convert %154 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %157 = stablehlo.convert %152 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %158 = stablehlo.power %157, %2 : tensor<1x7x3072xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %160 = stablehlo.multiply %159, %cst_4 : tensor<1x7xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %162 = stablehlo.add %161, %26 : tensor<1x7x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x7x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x7x3072xf32>
    %167 = stablehlo.convert %166 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %169 = stablehlo.multiply %156, %168 : tensor<1x7x3072xf32>
    %170 = stablehlo.convert %169 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %172 = stablehlo.reshape %arg19 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %174 = stablehlo.transpose %173, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %175 = stablehlo.dot_general %171, %174, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %176 = stablehlo.reshape %175 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %177 = stablehlo.convert %176 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %178 = stablehlo.logistic %176 : tensor<1x7x8192xbf16>
    %179 = stablehlo.convert %178 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %180 = stablehlo.multiply %177, %179 : tensor<1x7x8192xf32>
    %181 = stablehlo.convert %180 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %182 = stablehlo.convert %181 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %183 = stablehlo.reshape %arg13 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %184 = stablehlo.reshape %183 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %185 = stablehlo.transpose %184, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %186 = stablehlo.dot_general %171, %185, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %187 = stablehlo.reshape %186 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %188 = stablehlo.convert %187 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %189 = stablehlo.multiply %182, %188 : tensor<1x7x8192xf32>
    %190 = stablehlo.convert %189 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %191 = stablehlo.reshape %190 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %192 = stablehlo.reshape %arg12 : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %193 = stablehlo.reshape %192 : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %194 = stablehlo.transpose %193, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %195 = stablehlo.dot_general %191, %194, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %196 = stablehlo.reshape %195 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %197 = stablehlo.add %152, %196 : tensor<1x7x3072xbf16>
    %198 = stablehlo.convert %197 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %199 = stablehlo.power %198, %2 : tensor<1x7x3072xf32>
    %200 = stablehlo.reduce(%199 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %201 = stablehlo.multiply %200, %cst_4 : tensor<1x7xf32>
    %202 = stablehlo.reshape %201 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %203 = stablehlo.add %202, %26 : tensor<1x7x1xf32>
    %204 = stablehlo.rsqrt %203 : tensor<1x7x1xf32>
    %205 = stablehlo.reshape %204 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %207 = stablehlo.multiply %198, %206 : tensor<1x7x3072xf32>
    %208 = stablehlo.convert %207 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %209 = stablehlo.convert %208 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %210 = stablehlo.multiply %85, %209 : tensor<1x7x3072xf32>
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %212 = stablehlo.reshape %211 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %213 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %214 = stablehlo.reshape %213 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %215 = stablehlo.transpose %214, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %216 = stablehlo.dot_general %212, %215, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %217 = stablehlo.reshape %216 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %73, %81, %216, %217 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}


DEBUG: Checking for GSPMD annotations in module
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_1"}, %arg1: tensor<64xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg2: tensor<1024x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg3: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_0"}, %arg4: tensor<1x7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"}, %arg5: tensor<128256x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg6: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg7: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_1"}, %arg8: tensor<1x8x16x128xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_2"}, %arg9: tensor<1024x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg10: tensor<1x8x16x128xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_3"}, %arg11: tensor<128256x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___lm_head_weight"}, %arg12: tensor<3072x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg13: tensor<8192x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg14: tensor<3072x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg15: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_2"}, %arg16: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_3"}, %arg17: tensor<3072x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64>
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %7 = stablehlo.add %4, %6 : tensor<7xi64>
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64>
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64>
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32>
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32>
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32>
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32>
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32>
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %37 = stablehlo.reshape %arg2 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %38 = stablehlo.reshape %37 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %56 = stablehlo.multiply %43, %55 : tensor<1x8x7x128xf32>
    %57 = stablehlo.convert %56 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.convert %61 : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32>
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %69 = stablehlo.multiply %62, %68 : tensor<1x8x7x128xf32>
    %70 = stablehlo.convert %69 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %71 = stablehlo.add %57, %70 : tensor<1x8x7x128xbf16>
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %74 = stablehlo.reshape %arg9 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %86 = stablehlo.reshape %arg17 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %87 = stablehlo.reshape %86 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %90 = stablehlo.reshape %89 : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %94 = stablehlo.multiply %92, %93 : tensor<1x24x7x128xf32>
    %95 = stablehlo.convert %94 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %97 = stablehlo.negate %96 : tensor<1x24x7x64xbf16>
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %99 = stablehlo.concatenate %97, %98, dim = 3 : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %100 = stablehlo.convert %99 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.multiply %100, %101 : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %95, %103 : tensor<1x24x7x128xbf16>
    %105 = stablehlo.reshape %104 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %107 = stablehlo.reshape %106 : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16>
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16>
    %109 = stablehlo.reshape %108 : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16>
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16>
    %111 = stablehlo.reshape %110 : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %112 = stablehlo.convert %111 : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<1x24x7x16xf32>
    %114 = stablehlo.multiply %112, %113 : tensor<1x24x7x16xf32>
    %115 = stablehlo.convert %114 : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64>
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64>
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16>
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32>
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32>
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32>
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16>
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %130 = stablehlo.add %115, %129 : tensor<1x24x7x16xbf16>
    %131 = stablehlo.convert %130 : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %134 = stablehlo.subtract %131, %133 : tensor<1x24x7x16xf32>
    %135 = stablehlo.exponential %134 : tensor<1x24x7x16xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %138 = stablehlo.divide %135, %137 : tensor<1x24x7x16xf32>
    %139 = stablehlo.convert %138 : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %140 = stablehlo.reshape %139 : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16>
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %142 = stablehlo.reshape %141 : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16>
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16>
    %144 = stablehlo.reshape %143 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %146 = stablehlo.reshape %145 : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %147 = stablehlo.reshape %arg14 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %148 = stablehlo.reshape %147 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %151 = stablehlo.reshape %150 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %152 = stablehlo.add %20, %151 : tensor<1x7x3072xbf16>
    %153 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %155 = stablehlo.convert %154 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %157 = stablehlo.convert %152 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %158 = stablehlo.power %157, %2 : tensor<1x7x3072xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %160 = stablehlo.multiply %159, %cst_4 : tensor<1x7xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %162 = stablehlo.add %161, %26 : tensor<1x7x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x7x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x7x3072xf32>
    %167 = stablehlo.convert %166 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %169 = stablehlo.multiply %156, %168 : tensor<1x7x3072xf32>
    %170 = stablehlo.convert %169 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %172 = stablehlo.reshape %arg19 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %174 = stablehlo.transpose %173, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %175 = stablehlo.dot_general %171, %174, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %176 = stablehlo.reshape %175 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %177 = stablehlo.convert %176 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %178 = stablehlo.logistic %176 : tensor<1x7x8192xbf16>
    %179 = stablehlo.convert %178 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %180 = stablehlo.multiply %177, %179 : tensor<1x7x8192xf32>
    %181 = stablehlo.convert %180 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %182 = stablehlo.convert %181 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %183 = stablehlo.reshape %arg13 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %184 = stablehlo.reshape %183 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %185 = stablehlo.transpose %184, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %186 = stablehlo.dot_general %171, %185, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %187 = stablehlo.reshape %186 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %188 = stablehlo.convert %187 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %189 = stablehlo.multiply %182, %188 : tensor<1x7x8192xf32>
    %190 = stablehlo.convert %189 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %191 = stablehlo.reshape %190 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %192 = stablehlo.reshape %arg12 : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %193 = stablehlo.reshape %192 : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %194 = stablehlo.transpose %193, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %195 = stablehlo.dot_general %191, %194, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %196 = stablehlo.reshape %195 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %197 = stablehlo.add %152, %196 : tensor<1x7x3072xbf16>
    %198 = stablehlo.convert %197 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %199 = stablehlo.power %198, %2 : tensor<1x7x3072xf32>
    %200 = stablehlo.reduce(%199 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %201 = stablehlo.multiply %200, %cst_4 : tensor<1x7xf32>
    %202 = stablehlo.reshape %201 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %203 = stablehlo.add %202, %26 : tensor<1x7x1xf32>
    %204 = stablehlo.rsqrt %203 : tensor<1x7x1xf32>
    %205 = stablehlo.reshape %204 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %207 = stablehlo.multiply %198, %206 : tensor<1x7x3072xf32>
    %208 = stablehlo.convert %207 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %209 = stablehlo.convert %208 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %210 = stablehlo.multiply %85, %209 : tensor<1x7x3072xf32>
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %212 = stablehlo.reshape %211 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %213 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %214 = stablehlo.reshape %213 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %215 = stablehlo.transpose %214, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %216 = stablehlo.dot_general %212, %215, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %217 = stablehlo.reshape %216 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %73, %81, %216, %217 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}
// -----// IR Dump After ApplyArgumentShardStatusPass (apply-argument-shard-status) ('builtin.module' operation: @SyncTensorsGraph.450) //----- //
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg1: tensor<64xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg2: tensor<1024x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg3: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg4: tensor<1x7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg5: tensor<128256x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg6: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg7: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg8: tensor<1x8x16x128xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg9: tensor<1024x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg10: tensor<1x8x16x128xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg11: tensor<128256x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg12: tensor<3072x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg13: tensor<8192x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg14: tensor<3072x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg15: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg16: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg17: tensor<3072x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64>
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %7 = stablehlo.add %4, %6 : tensor<7xi64>
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64>
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64>
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32>
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32>
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32>
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32>
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32>
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %37 = stablehlo.reshape %arg2 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %38 = stablehlo.reshape %37 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %56 = stablehlo.multiply %43, %55 : tensor<1x8x7x128xf32>
    %57 = stablehlo.convert %56 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.convert %61 : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32>
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %69 = stablehlo.multiply %62, %68 : tensor<1x8x7x128xf32>
    %70 = stablehlo.convert %69 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %71 = stablehlo.add %57, %70 : tensor<1x8x7x128xbf16>
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %74 = stablehlo.reshape %arg9 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %86 = stablehlo.reshape %arg17 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %87 = stablehlo.reshape %86 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %90 = stablehlo.reshape %89 : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %94 = stablehlo.multiply %92, %93 : tensor<1x24x7x128xf32>
    %95 = stablehlo.convert %94 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %97 = stablehlo.negate %96 : tensor<1x24x7x64xbf16>
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %99 = stablehlo.concatenate %97, %98, dim = 3 : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %100 = stablehlo.convert %99 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.multiply %100, %101 : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %95, %103 : tensor<1x24x7x128xbf16>
    %105 = stablehlo.reshape %104 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %107 = stablehlo.reshape %106 : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16>
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16>
    %109 = stablehlo.reshape %108 : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16>
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16>
    %111 = stablehlo.reshape %110 : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %112 = stablehlo.convert %111 : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<1x24x7x16xf32>
    %114 = stablehlo.multiply %112, %113 : tensor<1x24x7x16xf32>
    %115 = stablehlo.convert %114 : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64>
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64>
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16>
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32>
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32>
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32>
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16>
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %130 = stablehlo.add %115, %129 : tensor<1x24x7x16xbf16>
    %131 = stablehlo.convert %130 : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %134 = stablehlo.subtract %131, %133 : tensor<1x24x7x16xf32>
    %135 = stablehlo.exponential %134 : tensor<1x24x7x16xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %138 = stablehlo.divide %135, %137 : tensor<1x24x7x16xf32>
    %139 = stablehlo.convert %138 : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %140 = stablehlo.reshape %139 : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16>
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %142 = stablehlo.reshape %141 : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16>
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16>
    %144 = stablehlo.reshape %143 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %146 = stablehlo.reshape %145 : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %147 = stablehlo.reshape %arg14 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %148 = stablehlo.reshape %147 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %151 = stablehlo.reshape %150 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %152 = stablehlo.add %20, %151 : tensor<1x7x3072xbf16>
    %153 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %155 = stablehlo.convert %154 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %157 = stablehlo.convert %152 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %158 = stablehlo.power %157, %2 : tensor<1x7x3072xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %160 = stablehlo.multiply %159, %cst_4 : tensor<1x7xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %162 = stablehlo.add %161, %26 : tensor<1x7x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x7x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x7x3072xf32>
    %167 = stablehlo.convert %166 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %169 = stablehlo.multiply %156, %168 : tensor<1x7x3072xf32>
    %170 = stablehlo.convert %169 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %172 = stablehlo.reshape %arg19 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %174 = stablehlo.transpose %173, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %175 = stablehlo.dot_general %171, %174, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %176 = stablehlo.reshape %175 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %177 = stablehlo.convert %176 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %178 = stablehlo.logistic %176 : tensor<1x7x8192xbf16>
    %179 = stablehlo.convert %178 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %180 = stablehlo.multiply %177, %179 : tensor<1x7x8192xf32>
    %181 = stablehlo.convert %180 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %182 = stablehlo.convert %181 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %183 = stablehlo.reshape %arg13 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %184 = stablehlo.reshape %183 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %185 = stablehlo.transpose %184, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %186 = stablehlo.dot_general %171, %185, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %187 = stablehlo.reshape %186 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %188 = stablehlo.convert %187 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %189 = stablehlo.multiply %182, %188 : tensor<1x7x8192xf32>
    %190 = stablehlo.convert %189 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %191 = stablehlo.reshape %190 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %192 = stablehlo.reshape %arg12 : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %193 = stablehlo.reshape %192 : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %194 = stablehlo.transpose %193, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %195 = stablehlo.dot_general %191, %194, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %196 = stablehlo.reshape %195 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %197 = stablehlo.add %152, %196 : tensor<1x7x3072xbf16>
    %198 = stablehlo.convert %197 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %199 = stablehlo.power %198, %2 : tensor<1x7x3072xf32>
    %200 = stablehlo.reduce(%199 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %201 = stablehlo.multiply %200, %cst_4 : tensor<1x7xf32>
    %202 = stablehlo.reshape %201 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %203 = stablehlo.add %202, %26 : tensor<1x7x1xf32>
    %204 = stablehlo.rsqrt %203 : tensor<1x7x1xf32>
    %205 = stablehlo.reshape %204 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %207 = stablehlo.multiply %198, %206 : tensor<1x7x3072xf32>
    %208 = stablehlo.convert %207 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %209 = stablehlo.convert %208 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %210 = stablehlo.multiply %85, %209 : tensor<1x7x3072xf32>
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %212 = stablehlo.reshape %211 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %213 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %214 = stablehlo.reshape %213 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %215 = stablehlo.transpose %214, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %216 = stablehlo.dot_general %212, %215, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %217 = stablehlo.reshape %216 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %73, %81, %216, %217 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump Before AnalyzeMeshPass (analyze-mesh) ('builtin.module' operation: @SyncTensorsGraph.450) //----- //
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg1: tensor<64xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg2: tensor<1024x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg3: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg4: tensor<1x7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg5: tensor<128256x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg6: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg7: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg8: tensor<1x8x16x128xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg9: tensor<1024x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg10: tensor<1x8x16x128xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg11: tensor<128256x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg12: tensor<3072x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg13: tensor<8192x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg14: tensor<3072x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg15: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg16: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg17: tensor<3072x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64>
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %7 = stablehlo.add %4, %6 : tensor<7xi64>
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64>
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64>
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32>
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32>
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32>
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32>
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32>
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %37 = stablehlo.reshape %arg2 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %38 = stablehlo.reshape %37 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %56 = stablehlo.multiply %43, %55 : tensor<1x8x7x128xf32>
    %57 = stablehlo.convert %56 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.convert %61 : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32>
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %69 = stablehlo.multiply %62, %68 : tensor<1x8x7x128xf32>
    %70 = stablehlo.convert %69 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %71 = stablehlo.add %57, %70 : tensor<1x8x7x128xbf16>
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %74 = stablehlo.reshape %arg9 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %86 = stablehlo.reshape %arg17 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %87 = stablehlo.reshape %86 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %90 = stablehlo.reshape %89 : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %94 = stablehlo.multiply %92, %93 : tensor<1x24x7x128xf32>
    %95 = stablehlo.convert %94 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %97 = stablehlo.negate %96 : tensor<1x24x7x64xbf16>
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %99 = stablehlo.concatenate %97, %98, dim = 3 : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %100 = stablehlo.convert %99 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.multiply %100, %101 : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %95, %103 : tensor<1x24x7x128xbf16>
    %105 = stablehlo.reshape %104 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %107 = stablehlo.reshape %106 : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16>
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16>
    %109 = stablehlo.reshape %108 : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16>
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16>
    %111 = stablehlo.reshape %110 : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %112 = stablehlo.convert %111 : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<1x24x7x16xf32>
    %114 = stablehlo.multiply %112, %113 : tensor<1x24x7x16xf32>
    %115 = stablehlo.convert %114 : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64>
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64>
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16>
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32>
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32>
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32>
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16>
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %130 = stablehlo.add %115, %129 : tensor<1x24x7x16xbf16>
    %131 = stablehlo.convert %130 : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %134 = stablehlo.subtract %131, %133 : tensor<1x24x7x16xf32>
    %135 = stablehlo.exponential %134 : tensor<1x24x7x16xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %138 = stablehlo.divide %135, %137 : tensor<1x24x7x16xf32>
    %139 = stablehlo.convert %138 : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %140 = stablehlo.reshape %139 : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16>
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %142 = stablehlo.reshape %141 : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16>
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16>
    %144 = stablehlo.reshape %143 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %146 = stablehlo.reshape %145 : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %147 = stablehlo.reshape %arg14 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %148 = stablehlo.reshape %147 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %151 = stablehlo.reshape %150 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %152 = stablehlo.add %20, %151 : tensor<1x7x3072xbf16>
    %153 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %155 = stablehlo.convert %154 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %157 = stablehlo.convert %152 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %158 = stablehlo.power %157, %2 : tensor<1x7x3072xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %160 = stablehlo.multiply %159, %cst_4 : tensor<1x7xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %162 = stablehlo.add %161, %26 : tensor<1x7x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x7x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x7x3072xf32>
    %167 = stablehlo.convert %166 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %169 = stablehlo.multiply %156, %168 : tensor<1x7x3072xf32>
    %170 = stablehlo.convert %169 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %172 = stablehlo.reshape %arg19 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %174 = stablehlo.transpose %173, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %175 = stablehlo.dot_general %171, %174, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %176 = stablehlo.reshape %175 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %177 = stablehlo.convert %176 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %178 = stablehlo.logistic %176 : tensor<1x7x8192xbf16>
    %179 = stablehlo.convert %178 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %180 = stablehlo.multiply %177, %179 : tensor<1x7x8192xf32>
    %181 = stablehlo.convert %180 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %182 = stablehlo.convert %181 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %183 = stablehlo.reshape %arg13 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %184 = stablehlo.reshape %183 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %185 = stablehlo.transpose %184, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %186 = stablehlo.dot_general %171, %185, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %187 = stablehlo.reshape %186 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %188 = stablehlo.convert %187 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %189 = stablehlo.multiply %182, %188 : tensor<1x7x8192xf32>
    %190 = stablehlo.convert %189 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %191 = stablehlo.reshape %190 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %192 = stablehlo.reshape %arg12 : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %193 = stablehlo.reshape %192 : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %194 = stablehlo.transpose %193, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %195 = stablehlo.dot_general %191, %194, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %196 = stablehlo.reshape %195 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %197 = stablehlo.add %152, %196 : tensor<1x7x3072xbf16>
    %198 = stablehlo.convert %197 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %199 = stablehlo.power %198, %2 : tensor<1x7x3072xf32>
    %200 = stablehlo.reduce(%199 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %201 = stablehlo.multiply %200, %cst_4 : tensor<1x7xf32>
    %202 = stablehlo.reshape %201 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %203 = stablehlo.add %202, %26 : tensor<1x7x1xf32>
    %204 = stablehlo.rsqrt %203 : tensor<1x7x1xf32>
    %205 = stablehlo.reshape %204 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %207 = stablehlo.multiply %198, %206 : tensor<1x7x3072xf32>
    %208 = stablehlo.convert %207 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %209 = stablehlo.convert %208 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %210 = stablehlo.multiply %85, %209 : tensor<1x7x3072xf32>
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %212 = stablehlo.reshape %211 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %213 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %214 = stablehlo.reshape %213 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %215 = stablehlo.transpose %214, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %216 = stablehlo.dot_general %212, %215, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %217 = stablehlo.reshape %216 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %73, %81, %216, %217 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}


DEBUG: Checking for GSPMD annotations in module
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg1: tensor<64xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg2: tensor<1024x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg3: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg4: tensor<1x7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg5: tensor<128256x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg6: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg7: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg8: tensor<1x8x16x128xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg9: tensor<1024x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg10: tensor<1x8x16x128xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg11: tensor<128256x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg12: tensor<3072x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg13: tensor<8192x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg14: tensor<3072x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg15: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg16: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg17: tensor<3072x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64>
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %7 = stablehlo.add %4, %6 : tensor<7xi64>
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64>
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64>
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32>
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32>
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32>
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32>
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32>
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %37 = stablehlo.reshape %arg2 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %38 = stablehlo.reshape %37 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %56 = stablehlo.multiply %43, %55 : tensor<1x8x7x128xf32>
    %57 = stablehlo.convert %56 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.convert %61 : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32>
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %69 = stablehlo.multiply %62, %68 : tensor<1x8x7x128xf32>
    %70 = stablehlo.convert %69 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %71 = stablehlo.add %57, %70 : tensor<1x8x7x128xbf16>
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %74 = stablehlo.reshape %arg9 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %86 = stablehlo.reshape %arg17 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %87 = stablehlo.reshape %86 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %90 = stablehlo.reshape %89 : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %94 = stablehlo.multiply %92, %93 : tensor<1x24x7x128xf32>
    %95 = stablehlo.convert %94 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %97 = stablehlo.negate %96 : tensor<1x24x7x64xbf16>
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %99 = stablehlo.concatenate %97, %98, dim = 3 : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %100 = stablehlo.convert %99 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.multiply %100, %101 : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %95, %103 : tensor<1x24x7x128xbf16>
    %105 = stablehlo.reshape %104 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %107 = stablehlo.reshape %106 : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16>
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16>
    %109 = stablehlo.reshape %108 : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16>
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16>
    %111 = stablehlo.reshape %110 : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %112 = stablehlo.convert %111 : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<1x24x7x16xf32>
    %114 = stablehlo.multiply %112, %113 : tensor<1x24x7x16xf32>
    %115 = stablehlo.convert %114 : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64>
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64>
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16>
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32>
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32>
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32>
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16>
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %130 = stablehlo.add %115, %129 : tensor<1x24x7x16xbf16>
    %131 = stablehlo.convert %130 : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %134 = stablehlo.subtract %131, %133 : tensor<1x24x7x16xf32>
    %135 = stablehlo.exponential %134 : tensor<1x24x7x16xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %138 = stablehlo.divide %135, %137 : tensor<1x24x7x16xf32>
    %139 = stablehlo.convert %138 : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %140 = stablehlo.reshape %139 : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16>
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %142 = stablehlo.reshape %141 : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16>
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16>
    %144 = stablehlo.reshape %143 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %146 = stablehlo.reshape %145 : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %147 = stablehlo.reshape %arg14 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %148 = stablehlo.reshape %147 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %151 = stablehlo.reshape %150 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %152 = stablehlo.add %20, %151 : tensor<1x7x3072xbf16>
    %153 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %155 = stablehlo.convert %154 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %157 = stablehlo.convert %152 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %158 = stablehlo.power %157, %2 : tensor<1x7x3072xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %160 = stablehlo.multiply %159, %cst_4 : tensor<1x7xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %162 = stablehlo.add %161, %26 : tensor<1x7x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x7x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x7x3072xf32>
    %167 = stablehlo.convert %166 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %169 = stablehlo.multiply %156, %168 : tensor<1x7x3072xf32>
    %170 = stablehlo.convert %169 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %172 = stablehlo.reshape %arg19 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %174 = stablehlo.transpose %173, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %175 = stablehlo.dot_general %171, %174, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %176 = stablehlo.reshape %175 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %177 = stablehlo.convert %176 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %178 = stablehlo.logistic %176 : tensor<1x7x8192xbf16>
    %179 = stablehlo.convert %178 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %180 = stablehlo.multiply %177, %179 : tensor<1x7x8192xf32>
    %181 = stablehlo.convert %180 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %182 = stablehlo.convert %181 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %183 = stablehlo.reshape %arg13 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %184 = stablehlo.reshape %183 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %185 = stablehlo.transpose %184, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %186 = stablehlo.dot_general %171, %185, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %187 = stablehlo.reshape %186 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %188 = stablehlo.convert %187 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %189 = stablehlo.multiply %182, %188 : tensor<1x7x8192xf32>
    %190 = stablehlo.convert %189 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %191 = stablehlo.reshape %190 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %192 = stablehlo.reshape %arg12 : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %193 = stablehlo.reshape %192 : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %194 = stablehlo.transpose %193, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %195 = stablehlo.dot_general %191, %194, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %196 = stablehlo.reshape %195 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %197 = stablehlo.add %152, %196 : tensor<1x7x3072xbf16>
    %198 = stablehlo.convert %197 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %199 = stablehlo.power %198, %2 : tensor<1x7x3072xf32>
    %200 = stablehlo.reduce(%199 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %201 = stablehlo.multiply %200, %cst_4 : tensor<1x7xf32>
    %202 = stablehlo.reshape %201 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %203 = stablehlo.add %202, %26 : tensor<1x7x1xf32>
    %204 = stablehlo.rsqrt %203 : tensor<1x7x1xf32>
    %205 = stablehlo.reshape %204 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %207 = stablehlo.multiply %198, %206 : tensor<1x7x3072xf32>
    %208 = stablehlo.convert %207 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %209 = stablehlo.convert %208 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %210 = stablehlo.multiply %85, %209 : tensor<1x7x3072xf32>
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %212 = stablehlo.reshape %211 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %213 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %214 = stablehlo.reshape %213 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %215 = stablehlo.transpose %214, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %216 = stablehlo.dot_general %212, %215, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %217 = stablehlo.reshape %216 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %73, %81, %216, %217 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}
// -----// IR Dump After AnalyzeMeshPass (analyze-mesh) ('builtin.module' operation: @SyncTensorsGraph.450) //----- //
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg1: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg2: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg3: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg4: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg5: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg7: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg8: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg9: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg10: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg11: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg12: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg13: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg14: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg15: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg16: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64>
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %7 = stablehlo.add %4, %6 : tensor<7xi64>
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64>
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64>
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32>
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32>
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32>
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32>
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32>
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %37 = stablehlo.reshape %arg2 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %38 = stablehlo.reshape %37 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %56 = stablehlo.multiply %43, %55 : tensor<1x8x7x128xf32>
    %57 = stablehlo.convert %56 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.convert %61 : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32>
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %69 = stablehlo.multiply %62, %68 : tensor<1x8x7x128xf32>
    %70 = stablehlo.convert %69 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %71 = stablehlo.add %57, %70 : tensor<1x8x7x128xbf16>
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %74 = stablehlo.reshape %arg9 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %86 = stablehlo.reshape %arg17 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %87 = stablehlo.reshape %86 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %90 = stablehlo.reshape %89 : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %94 = stablehlo.multiply %92, %93 : tensor<1x24x7x128xf32>
    %95 = stablehlo.convert %94 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %97 = stablehlo.negate %96 : tensor<1x24x7x64xbf16>
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %99 = stablehlo.concatenate %97, %98, dim = 3 : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %100 = stablehlo.convert %99 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.multiply %100, %101 : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %95, %103 : tensor<1x24x7x128xbf16>
    %105 = stablehlo.reshape %104 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %107 = stablehlo.reshape %106 : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16>
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16>
    %109 = stablehlo.reshape %108 : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16>
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16>
    %111 = stablehlo.reshape %110 : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %112 = stablehlo.convert %111 : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<1x24x7x16xf32>
    %114 = stablehlo.multiply %112, %113 : tensor<1x24x7x16xf32>
    %115 = stablehlo.convert %114 : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64>
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64>
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16>
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32>
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32>
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32>
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16>
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %130 = stablehlo.add %115, %129 : tensor<1x24x7x16xbf16>
    %131 = stablehlo.convert %130 : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %134 = stablehlo.subtract %131, %133 : tensor<1x24x7x16xf32>
    %135 = stablehlo.exponential %134 : tensor<1x24x7x16xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %138 = stablehlo.divide %135, %137 : tensor<1x24x7x16xf32>
    %139 = stablehlo.convert %138 : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %140 = stablehlo.reshape %139 : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16>
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %142 = stablehlo.reshape %141 : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16>
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16>
    %144 = stablehlo.reshape %143 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %146 = stablehlo.reshape %145 : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %147 = stablehlo.reshape %arg14 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %148 = stablehlo.reshape %147 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %151 = stablehlo.reshape %150 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %152 = stablehlo.add %20, %151 : tensor<1x7x3072xbf16>
    %153 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %155 = stablehlo.convert %154 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %157 = stablehlo.convert %152 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %158 = stablehlo.power %157, %2 : tensor<1x7x3072xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %160 = stablehlo.multiply %159, %cst_4 : tensor<1x7xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %162 = stablehlo.add %161, %26 : tensor<1x7x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x7x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x7x3072xf32>
    %167 = stablehlo.convert %166 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %169 = stablehlo.multiply %156, %168 : tensor<1x7x3072xf32>
    %170 = stablehlo.convert %169 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %172 = stablehlo.reshape %arg19 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %174 = stablehlo.transpose %173, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %175 = stablehlo.dot_general %171, %174, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %176 = stablehlo.reshape %175 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %177 = stablehlo.convert %176 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %178 = stablehlo.logistic %176 : tensor<1x7x8192xbf16>
    %179 = stablehlo.convert %178 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %180 = stablehlo.multiply %177, %179 : tensor<1x7x8192xf32>
    %181 = stablehlo.convert %180 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %182 = stablehlo.convert %181 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %183 = stablehlo.reshape %arg13 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %184 = stablehlo.reshape %183 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %185 = stablehlo.transpose %184, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %186 = stablehlo.dot_general %171, %185, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %187 = stablehlo.reshape %186 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %188 = stablehlo.convert %187 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %189 = stablehlo.multiply %182, %188 : tensor<1x7x8192xf32>
    %190 = stablehlo.convert %189 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %191 = stablehlo.reshape %190 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %192 = stablehlo.reshape %arg12 : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %193 = stablehlo.reshape %192 : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %194 = stablehlo.transpose %193, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %195 = stablehlo.dot_general %191, %194, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %196 = stablehlo.reshape %195 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %197 = stablehlo.add %152, %196 : tensor<1x7x3072xbf16>
    %198 = stablehlo.convert %197 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %199 = stablehlo.power %198, %2 : tensor<1x7x3072xf32>
    %200 = stablehlo.reduce(%199 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %201 = stablehlo.multiply %200, %cst_4 : tensor<1x7xf32>
    %202 = stablehlo.reshape %201 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %203 = stablehlo.add %202, %26 : tensor<1x7x1xf32>
    %204 = stablehlo.rsqrt %203 : tensor<1x7x1xf32>
    %205 = stablehlo.reshape %204 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %207 = stablehlo.multiply %198, %206 : tensor<1x7x3072xf32>
    %208 = stablehlo.convert %207 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %209 = stablehlo.convert %208 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %210 = stablehlo.multiply %85, %209 : tensor<1x7x3072xf32>
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %212 = stablehlo.reshape %211 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %213 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %214 = stablehlo.reshape %213 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %215 = stablehlo.transpose %214, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %216 = stablehlo.dot_general %212, %215, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %217 = stablehlo.reshape %216 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %73, %81, %216, %217 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump Before ApplyShardingConstraintsPass (sdy-apply-sharding-constraints) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg1: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg2: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg3: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg4: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg5: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg7: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg8: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg9: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg10: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg11: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg12: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg13: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg14: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg15: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg16: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64>
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %7 = stablehlo.add %4, %6 : tensor<7xi64>
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64>
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64>
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32>
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32>
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32>
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32>
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32>
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %37 = stablehlo.reshape %arg2 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %38 = stablehlo.reshape %37 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %56 = stablehlo.multiply %43, %55 : tensor<1x8x7x128xf32>
    %57 = stablehlo.convert %56 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.convert %61 : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32>
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %69 = stablehlo.multiply %62, %68 : tensor<1x8x7x128xf32>
    %70 = stablehlo.convert %69 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %71 = stablehlo.add %57, %70 : tensor<1x8x7x128xbf16>
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %74 = stablehlo.reshape %arg9 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %86 = stablehlo.reshape %arg17 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %87 = stablehlo.reshape %86 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %90 = stablehlo.reshape %89 : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %94 = stablehlo.multiply %92, %93 : tensor<1x24x7x128xf32>
    %95 = stablehlo.convert %94 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %97 = stablehlo.negate %96 : tensor<1x24x7x64xbf16>
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %99 = stablehlo.concatenate %97, %98, dim = 3 : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %100 = stablehlo.convert %99 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.multiply %100, %101 : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %95, %103 : tensor<1x24x7x128xbf16>
    %105 = stablehlo.reshape %104 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %107 = stablehlo.reshape %106 : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16>
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16>
    %109 = stablehlo.reshape %108 : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16>
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16>
    %111 = stablehlo.reshape %110 : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %112 = stablehlo.convert %111 : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<1x24x7x16xf32>
    %114 = stablehlo.multiply %112, %113 : tensor<1x24x7x16xf32>
    %115 = stablehlo.convert %114 : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64>
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64>
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16>
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32>
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32>
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32>
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16>
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %130 = stablehlo.add %115, %129 : tensor<1x24x7x16xbf16>
    %131 = stablehlo.convert %130 : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %134 = stablehlo.subtract %131, %133 : tensor<1x24x7x16xf32>
    %135 = stablehlo.exponential %134 : tensor<1x24x7x16xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %138 = stablehlo.divide %135, %137 : tensor<1x24x7x16xf32>
    %139 = stablehlo.convert %138 : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %140 = stablehlo.reshape %139 : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16>
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %142 = stablehlo.reshape %141 : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16>
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16>
    %144 = stablehlo.reshape %143 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %146 = stablehlo.reshape %145 : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %147 = stablehlo.reshape %arg14 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %148 = stablehlo.reshape %147 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %151 = stablehlo.reshape %150 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %152 = stablehlo.add %20, %151 : tensor<1x7x3072xbf16>
    %153 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %155 = stablehlo.convert %154 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %157 = stablehlo.convert %152 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %158 = stablehlo.power %157, %2 : tensor<1x7x3072xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %160 = stablehlo.multiply %159, %cst_4 : tensor<1x7xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %162 = stablehlo.add %161, %26 : tensor<1x7x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x7x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x7x3072xf32>
    %167 = stablehlo.convert %166 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %169 = stablehlo.multiply %156, %168 : tensor<1x7x3072xf32>
    %170 = stablehlo.convert %169 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %172 = stablehlo.reshape %arg19 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %174 = stablehlo.transpose %173, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %175 = stablehlo.dot_general %171, %174, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %176 = stablehlo.reshape %175 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %177 = stablehlo.convert %176 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %178 = stablehlo.logistic %176 : tensor<1x7x8192xbf16>
    %179 = stablehlo.convert %178 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %180 = stablehlo.multiply %177, %179 : tensor<1x7x8192xf32>
    %181 = stablehlo.convert %180 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %182 = stablehlo.convert %181 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %183 = stablehlo.reshape %arg13 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %184 = stablehlo.reshape %183 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %185 = stablehlo.transpose %184, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %186 = stablehlo.dot_general %171, %185, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %187 = stablehlo.reshape %186 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %188 = stablehlo.convert %187 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %189 = stablehlo.multiply %182, %188 : tensor<1x7x8192xf32>
    %190 = stablehlo.convert %189 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %191 = stablehlo.reshape %190 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %192 = stablehlo.reshape %arg12 : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %193 = stablehlo.reshape %192 : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %194 = stablehlo.transpose %193, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %195 = stablehlo.dot_general %191, %194, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %196 = stablehlo.reshape %195 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %197 = stablehlo.add %152, %196 : tensor<1x7x3072xbf16>
    %198 = stablehlo.convert %197 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %199 = stablehlo.power %198, %2 : tensor<1x7x3072xf32>
    %200 = stablehlo.reduce(%199 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %201 = stablehlo.multiply %200, %cst_4 : tensor<1x7xf32>
    %202 = stablehlo.reshape %201 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %203 = stablehlo.add %202, %26 : tensor<1x7x1xf32>
    %204 = stablehlo.rsqrt %203 : tensor<1x7x1xf32>
    %205 = stablehlo.reshape %204 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %207 = stablehlo.multiply %198, %206 : tensor<1x7x3072xf32>
    %208 = stablehlo.convert %207 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %209 = stablehlo.convert %208 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %210 = stablehlo.multiply %85, %209 : tensor<1x7x3072xf32>
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %212 = stablehlo.reshape %211 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %213 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %214 = stablehlo.reshape %213 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %215 = stablehlo.transpose %214, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %216 = stablehlo.dot_general %212, %215, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %217 = stablehlo.reshape %216 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %73, %81, %216, %217 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump Before AggressivePropagationPass (sdy-aggressive-propagate) ('builtin.module' operation: @SyncTensorsGraph.450) //----- //
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg1: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg2: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg3: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg4: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg5: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg7: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg8: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg9: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg10: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg11: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg12: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg13: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg14: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg15: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg16: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64>
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %7 = stablehlo.add %4, %6 : tensor<7xi64>
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64>
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64>
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32>
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32>
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32>
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32>
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32>
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %37 = stablehlo.reshape %arg2 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %38 = stablehlo.reshape %37 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %56 = stablehlo.multiply %43, %55 : tensor<1x8x7x128xf32>
    %57 = stablehlo.convert %56 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.convert %61 : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32>
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %69 = stablehlo.multiply %62, %68 : tensor<1x8x7x128xf32>
    %70 = stablehlo.convert %69 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %71 = stablehlo.add %57, %70 : tensor<1x8x7x128xbf16>
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %74 = stablehlo.reshape %arg9 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %86 = stablehlo.reshape %arg17 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %87 = stablehlo.reshape %86 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %90 = stablehlo.reshape %89 : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %94 = stablehlo.multiply %92, %93 : tensor<1x24x7x128xf32>
    %95 = stablehlo.convert %94 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %97 = stablehlo.negate %96 : tensor<1x24x7x64xbf16>
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %99 = stablehlo.concatenate %97, %98, dim = 3 : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %100 = stablehlo.convert %99 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.multiply %100, %101 : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %95, %103 : tensor<1x24x7x128xbf16>
    %105 = stablehlo.reshape %104 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %107 = stablehlo.reshape %106 : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16>
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16>
    %109 = stablehlo.reshape %108 : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16>
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16>
    %111 = stablehlo.reshape %110 : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %112 = stablehlo.convert %111 : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<1x24x7x16xf32>
    %114 = stablehlo.multiply %112, %113 : tensor<1x24x7x16xf32>
    %115 = stablehlo.convert %114 : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64>
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64>
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16>
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32>
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32>
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32>
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16>
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %130 = stablehlo.add %115, %129 : tensor<1x24x7x16xbf16>
    %131 = stablehlo.convert %130 : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %134 = stablehlo.subtract %131, %133 : tensor<1x24x7x16xf32>
    %135 = stablehlo.exponential %134 : tensor<1x24x7x16xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %138 = stablehlo.divide %135, %137 : tensor<1x24x7x16xf32>
    %139 = stablehlo.convert %138 : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %140 = stablehlo.reshape %139 : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16>
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %142 = stablehlo.reshape %141 : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16>
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16>
    %144 = stablehlo.reshape %143 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %146 = stablehlo.reshape %145 : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %147 = stablehlo.reshape %arg14 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %148 = stablehlo.reshape %147 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %151 = stablehlo.reshape %150 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %152 = stablehlo.add %20, %151 : tensor<1x7x3072xbf16>
    %153 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %155 = stablehlo.convert %154 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %157 = stablehlo.convert %152 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %158 = stablehlo.power %157, %2 : tensor<1x7x3072xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %160 = stablehlo.multiply %159, %cst_4 : tensor<1x7xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %162 = stablehlo.add %161, %26 : tensor<1x7x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x7x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x7x3072xf32>
    %167 = stablehlo.convert %166 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %169 = stablehlo.multiply %156, %168 : tensor<1x7x3072xf32>
    %170 = stablehlo.convert %169 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %172 = stablehlo.reshape %arg19 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %174 = stablehlo.transpose %173, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %175 = stablehlo.dot_general %171, %174, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %176 = stablehlo.reshape %175 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %177 = stablehlo.convert %176 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %178 = stablehlo.logistic %176 : tensor<1x7x8192xbf16>
    %179 = stablehlo.convert %178 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %180 = stablehlo.multiply %177, %179 : tensor<1x7x8192xf32>
    %181 = stablehlo.convert %180 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %182 = stablehlo.convert %181 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %183 = stablehlo.reshape %arg13 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %184 = stablehlo.reshape %183 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %185 = stablehlo.transpose %184, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %186 = stablehlo.dot_general %171, %185, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %187 = stablehlo.reshape %186 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %188 = stablehlo.convert %187 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %189 = stablehlo.multiply %182, %188 : tensor<1x7x8192xf32>
    %190 = stablehlo.convert %189 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %191 = stablehlo.reshape %190 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %192 = stablehlo.reshape %arg12 : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %193 = stablehlo.reshape %192 : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %194 = stablehlo.transpose %193, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %195 = stablehlo.dot_general %191, %194, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %196 = stablehlo.reshape %195 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %197 = stablehlo.add %152, %196 : tensor<1x7x3072xbf16>
    %198 = stablehlo.convert %197 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %199 = stablehlo.power %198, %2 : tensor<1x7x3072xf32>
    %200 = stablehlo.reduce(%199 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %201 = stablehlo.multiply %200, %cst_4 : tensor<1x7xf32>
    %202 = stablehlo.reshape %201 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %203 = stablehlo.add %202, %26 : tensor<1x7x1xf32>
    %204 = stablehlo.rsqrt %203 : tensor<1x7x1xf32>
    %205 = stablehlo.reshape %204 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %207 = stablehlo.multiply %198, %206 : tensor<1x7x3072xf32>
    %208 = stablehlo.convert %207 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %209 = stablehlo.convert %208 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %210 = stablehlo.multiply %85, %209 : tensor<1x7x3072xf32>
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %212 = stablehlo.reshape %211 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %213 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %214 = stablehlo.reshape %213 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %215 = stablehlo.transpose %214, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %216 = stablehlo.dot_general %212, %215, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %217 = stablehlo.reshape %216 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %73, %81, %216, %217 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump After AggressivePropagationPass (sdy-aggressive-propagate) ('builtin.module' operation: @SyncTensorsGraph.450) //----- //
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg1: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg2: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg3: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg4: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg5: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg7: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg8: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg9: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg10: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg11: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg12: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg13: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg14: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg15: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg16: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64>
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %7 = stablehlo.add %4, %6 : tensor<7xi64>
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64>
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64>
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32>
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32>
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32>
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32>
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32>
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %37 = stablehlo.reshape %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %38 = stablehlo.reshape %37 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %56 = stablehlo.multiply %43, %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xf32>
    %57 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.convert %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32>
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %69 = stablehlo.multiply %62, %68 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xf32>
    %70 = stablehlo.convert %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %71 = stablehlo.add %57, %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %74 = stablehlo.reshape %arg9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %86 = stablehlo.reshape %arg17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %87 = stablehlo.reshape %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %90 = stablehlo.reshape %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %94 = stablehlo.multiply %92, %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x128xf32>
    %95 = stablehlo.convert %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %97 = stablehlo.negate %96 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x64xbf16>
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %99 = stablehlo.concatenate %97, %98, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %100 = stablehlo.convert %99 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.multiply %100, %101 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %95, %103 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x128xbf16>
    %105 = stablehlo.reshape %104 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %107 = stablehlo.reshape %106 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16>
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16>
    %109 = stablehlo.reshape %108 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16>
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16>
    %111 = stablehlo.reshape %110 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %112 = stablehlo.convert %111 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<1x24x7x16xf32>
    %114 = stablehlo.multiply %112, %113 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %115 = stablehlo.convert %114 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64>
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64>
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16>
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32>
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32>
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32>
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16>
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %130 = stablehlo.add %115, %129 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xbf16>
    %131 = stablehlo.convert %130 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %134 = stablehlo.subtract %131, %133 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %135 = stablehlo.exponential %134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %138 = stablehlo.divide %135, %137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %139 = stablehlo.convert %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %140 = stablehlo.reshape %139 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16>
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %142 = stablehlo.reshape %141 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16>
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16>
    %144 = stablehlo.reshape %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %146 = stablehlo.reshape %145 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %147 = stablehlo.reshape %arg14 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %148 = stablehlo.reshape %147 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %151 = stablehlo.reshape %150 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %152 = stablehlo.add %20, %151 : tensor<1x7x3072xbf16>
    %153 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %155 = stablehlo.convert %154 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %157 = stablehlo.convert %152 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %158 = stablehlo.power %157, %2 : tensor<1x7x3072xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %160 = stablehlo.multiply %159, %cst_4 : tensor<1x7xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %162 = stablehlo.add %161, %26 : tensor<1x7x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x7x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x7x3072xf32>
    %167 = stablehlo.convert %166 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %169 = stablehlo.multiply %156, %168 : tensor<1x7x3072xf32>
    %170 = stablehlo.convert %169 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %172 = stablehlo.reshape %arg19 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %173 = stablehlo.reshape %172 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %174 = stablehlo.transpose %173, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %175 = stablehlo.dot_general %171, %174, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %176 = stablehlo.reshape %175 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %177 = stablehlo.convert %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %178 = stablehlo.logistic %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xbf16>
    %179 = stablehlo.convert %178 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %180 = stablehlo.multiply %177, %179 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
    %181 = stablehlo.convert %180 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %182 = stablehlo.convert %181 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %183 = stablehlo.reshape %arg13 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %184 = stablehlo.reshape %183 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %185 = stablehlo.transpose %184, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %186 = stablehlo.dot_general %171, %185, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %187 = stablehlo.reshape %186 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %188 = stablehlo.convert %187 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %189 = stablehlo.multiply %182, %188 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
    %190 = stablehlo.convert %189 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %191 = stablehlo.reshape %190 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %192 = stablehlo.reshape %arg12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %193 = stablehlo.reshape %192 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %194 = stablehlo.transpose %193, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %195 = stablehlo.dot_general %191, %194, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %196 = stablehlo.reshape %195 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %197 = stablehlo.add %152, %196 : tensor<1x7x3072xbf16>
    %198 = stablehlo.convert %197 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %199 = stablehlo.power %198, %2 : tensor<1x7x3072xf32>
    %200 = stablehlo.reduce(%199 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %201 = stablehlo.multiply %200, %cst_4 : tensor<1x7xf32>
    %202 = stablehlo.reshape %201 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %203 = stablehlo.add %202, %26 : tensor<1x7x1xf32>
    %204 = stablehlo.rsqrt %203 : tensor<1x7x1xf32>
    %205 = stablehlo.reshape %204 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %207 = stablehlo.multiply %198, %206 : tensor<1x7x3072xf32>
    %208 = stablehlo.convert %207 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %209 = stablehlo.convert %208 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %210 = stablehlo.multiply %85, %209 : tensor<1x7x3072xf32>
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %212 = stablehlo.reshape %211 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %213 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %214 = stablehlo.reshape %213 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %215 = stablehlo.transpose %214, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %216 = stablehlo.dot_general %212, %215, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %217 = stablehlo.reshape %216 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %73, %81, %216, %217 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump Before ShardingConstraintToReshardPass (sdy-sharding-constraint-to-reshard) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg1: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg2: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg3: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg4: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg5: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg7: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg8: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg9: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg10: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg11: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg12: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg13: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg14: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg15: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg16: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64>
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %7 = stablehlo.add %4, %6 : tensor<7xi64>
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64>
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64>
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32>
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32>
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32>
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32>
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32>
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %37 = stablehlo.reshape %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %38 = stablehlo.reshape %37 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %56 = stablehlo.multiply %43, %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xf32>
    %57 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.convert %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32>
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %69 = stablehlo.multiply %62, %68 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xf32>
    %70 = stablehlo.convert %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %71 = stablehlo.add %57, %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %74 = stablehlo.reshape %arg9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %86 = stablehlo.reshape %arg17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %87 = stablehlo.reshape %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %90 = stablehlo.reshape %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %94 = stablehlo.multiply %92, %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x128xf32>
    %95 = stablehlo.convert %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %97 = stablehlo.negate %96 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x64xbf16>
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %99 = stablehlo.concatenate %97, %98, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %100 = stablehlo.convert %99 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.multiply %100, %101 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %95, %103 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x128xbf16>
    %105 = stablehlo.reshape %104 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %107 = stablehlo.reshape %106 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16>
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16>
    %109 = stablehlo.reshape %108 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16>
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16>
    %111 = stablehlo.reshape %110 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %112 = stablehlo.convert %111 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<1x24x7x16xf32>
    %114 = stablehlo.multiply %112, %113 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %115 = stablehlo.convert %114 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64>
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64>
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16>
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32>
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32>
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32>
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16>
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %130 = stablehlo.add %115, %129 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xbf16>
    %131 = stablehlo.convert %130 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %134 = stablehlo.subtract %131, %133 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %135 = stablehlo.exponential %134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %138 = stablehlo.divide %135, %137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %139 = stablehlo.convert %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %140 = stablehlo.reshape %139 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16>
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %142 = stablehlo.reshape %141 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16>
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16>
    %144 = stablehlo.reshape %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %146 = stablehlo.reshape %145 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %147 = stablehlo.reshape %arg14 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %148 = stablehlo.reshape %147 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %151 = stablehlo.reshape %150 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %152 = stablehlo.add %20, %151 : tensor<1x7x3072xbf16>
    %153 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %155 = stablehlo.convert %154 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %157 = stablehlo.convert %152 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %158 = stablehlo.power %157, %2 : tensor<1x7x3072xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %160 = stablehlo.multiply %159, %cst_4 : tensor<1x7xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %162 = stablehlo.add %161, %26 : tensor<1x7x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x7x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x7x3072xf32>
    %167 = stablehlo.convert %166 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %169 = stablehlo.multiply %156, %168 : tensor<1x7x3072xf32>
    %170 = stablehlo.convert %169 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %172 = stablehlo.reshape %arg19 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %173 = stablehlo.reshape %172 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %174 = stablehlo.transpose %173, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %175 = stablehlo.dot_general %171, %174, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %176 = stablehlo.reshape %175 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %177 = stablehlo.convert %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %178 = stablehlo.logistic %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xbf16>
    %179 = stablehlo.convert %178 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %180 = stablehlo.multiply %177, %179 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
    %181 = stablehlo.convert %180 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %182 = stablehlo.convert %181 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %183 = stablehlo.reshape %arg13 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %184 = stablehlo.reshape %183 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %185 = stablehlo.transpose %184, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %186 = stablehlo.dot_general %171, %185, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %187 = stablehlo.reshape %186 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %188 = stablehlo.convert %187 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %189 = stablehlo.multiply %182, %188 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
    %190 = stablehlo.convert %189 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %191 = stablehlo.reshape %190 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %192 = stablehlo.reshape %arg12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %193 = stablehlo.reshape %192 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %194 = stablehlo.transpose %193, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %195 = stablehlo.dot_general %191, %194, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %196 = stablehlo.reshape %195 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %197 = stablehlo.add %152, %196 : tensor<1x7x3072xbf16>
    %198 = stablehlo.convert %197 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %199 = stablehlo.power %198, %2 : tensor<1x7x3072xf32>
    %200 = stablehlo.reduce(%199 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %201 = stablehlo.multiply %200, %cst_4 : tensor<1x7xf32>
    %202 = stablehlo.reshape %201 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %203 = stablehlo.add %202, %26 : tensor<1x7x1xf32>
    %204 = stablehlo.rsqrt %203 : tensor<1x7x1xf32>
    %205 = stablehlo.reshape %204 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %207 = stablehlo.multiply %198, %206 : tensor<1x7x3072xf32>
    %208 = stablehlo.convert %207 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %209 = stablehlo.convert %208 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %210 = stablehlo.multiply %85, %209 : tensor<1x7x3072xf32>
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %212 = stablehlo.reshape %211 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %213 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %214 = stablehlo.reshape %213 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %215 = stablehlo.transpose %214, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %216 = stablehlo.dot_general %212, %215, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %217 = stablehlo.reshape %216 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %73, %81, %216, %217 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump Before InsertExplicitReshardsPass (sdy-insert-explicit-reshards) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg1: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg2: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg3: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg4: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg5: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg7: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg8: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg9: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg10: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg11: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg12: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg13: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg14: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg15: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg16: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64>
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %7 = stablehlo.add %4, %6 : tensor<7xi64>
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64>
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64>
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32>
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32>
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32>
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32>
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32>
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %37 = stablehlo.reshape %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %38 = stablehlo.reshape %37 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %56 = stablehlo.multiply %43, %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xf32>
    %57 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.convert %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32>
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %69 = stablehlo.multiply %62, %68 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xf32>
    %70 = stablehlo.convert %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %71 = stablehlo.add %57, %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %74 = stablehlo.reshape %arg9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %86 = stablehlo.reshape %arg17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %87 = stablehlo.reshape %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %90 = stablehlo.reshape %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %94 = stablehlo.multiply %92, %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x128xf32>
    %95 = stablehlo.convert %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %97 = stablehlo.negate %96 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x64xbf16>
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %99 = stablehlo.concatenate %97, %98, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %100 = stablehlo.convert %99 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.multiply %100, %101 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %95, %103 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x128xbf16>
    %105 = stablehlo.reshape %104 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %107 = stablehlo.reshape %106 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16>
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16>
    %109 = stablehlo.reshape %108 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16>
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16>
    %111 = stablehlo.reshape %110 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %112 = stablehlo.convert %111 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<1x24x7x16xf32>
    %114 = stablehlo.multiply %112, %113 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %115 = stablehlo.convert %114 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64>
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64>
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16>
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32>
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32>
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32>
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16>
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %130 = stablehlo.add %115, %129 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xbf16>
    %131 = stablehlo.convert %130 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %134 = stablehlo.subtract %131, %133 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %135 = stablehlo.exponential %134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %138 = stablehlo.divide %135, %137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %139 = stablehlo.convert %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %140 = stablehlo.reshape %139 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16>
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %142 = stablehlo.reshape %141 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16>
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16>
    %144 = stablehlo.reshape %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %146 = stablehlo.reshape %145 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %147 = stablehlo.reshape %arg14 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %148 = stablehlo.reshape %147 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %151 = stablehlo.reshape %150 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %152 = stablehlo.add %20, %151 : tensor<1x7x3072xbf16>
    %153 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %155 = stablehlo.convert %154 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %157 = stablehlo.convert %152 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %158 = stablehlo.power %157, %2 : tensor<1x7x3072xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %160 = stablehlo.multiply %159, %cst_4 : tensor<1x7xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %162 = stablehlo.add %161, %26 : tensor<1x7x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x7x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x7x3072xf32>
    %167 = stablehlo.convert %166 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %169 = stablehlo.multiply %156, %168 : tensor<1x7x3072xf32>
    %170 = stablehlo.convert %169 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %172 = stablehlo.reshape %arg19 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %173 = stablehlo.reshape %172 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %174 = stablehlo.transpose %173, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %175 = stablehlo.dot_general %171, %174, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %176 = stablehlo.reshape %175 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %177 = stablehlo.convert %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %178 = stablehlo.logistic %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xbf16>
    %179 = stablehlo.convert %178 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %180 = stablehlo.multiply %177, %179 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
    %181 = stablehlo.convert %180 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %182 = stablehlo.convert %181 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %183 = stablehlo.reshape %arg13 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %184 = stablehlo.reshape %183 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %185 = stablehlo.transpose %184, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %186 = stablehlo.dot_general %171, %185, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %187 = stablehlo.reshape %186 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %188 = stablehlo.convert %187 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %189 = stablehlo.multiply %182, %188 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
    %190 = stablehlo.convert %189 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %191 = stablehlo.reshape %190 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %192 = stablehlo.reshape %arg12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %193 = stablehlo.reshape %192 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %194 = stablehlo.transpose %193, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %195 = stablehlo.dot_general %191, %194, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %196 = stablehlo.reshape %195 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %197 = stablehlo.add %152, %196 : tensor<1x7x3072xbf16>
    %198 = stablehlo.convert %197 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %199 = stablehlo.power %198, %2 : tensor<1x7x3072xf32>
    %200 = stablehlo.reduce(%199 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %201 = stablehlo.multiply %200, %cst_4 : tensor<1x7xf32>
    %202 = stablehlo.reshape %201 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %203 = stablehlo.add %202, %26 : tensor<1x7x1xf32>
    %204 = stablehlo.rsqrt %203 : tensor<1x7x1xf32>
    %205 = stablehlo.reshape %204 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %207 = stablehlo.multiply %198, %206 : tensor<1x7x3072xf32>
    %208 = stablehlo.convert %207 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %209 = stablehlo.convert %208 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %210 = stablehlo.multiply %85, %209 : tensor<1x7x3072xf32>
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %212 = stablehlo.reshape %211 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %213 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %214 = stablehlo.reshape %213 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %215 = stablehlo.transpose %214, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %216 = stablehlo.dot_general %212, %215, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %217 = stablehlo.reshape %216 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %73, %81, %216, %217 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump After InsertExplicitReshardsPass (sdy-insert-explicit-reshards) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg1: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg2: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg3: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg4: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg5: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg7: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg8: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg9: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg10: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg11: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg12: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg13: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg14: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg15: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg16: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64>
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %7 = stablehlo.add %4, %6 : tensor<7xi64>
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64>
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64>
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32>
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32>
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32>
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32>
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32>
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %37 = stablehlo.reshape %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %38 = stablehlo.reshape %37 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %56 = stablehlo.multiply %43, %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xf32>
    %57 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.convert %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32>
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %69 = stablehlo.multiply %62, %68 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xf32>
    %70 = stablehlo.convert %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %71 = stablehlo.add %57, %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %74 = stablehlo.reshape %arg9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %86 = stablehlo.reshape %arg17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %87 = stablehlo.reshape %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %90 = stablehlo.reshape %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %94 = stablehlo.multiply %92, %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x128xf32>
    %95 = stablehlo.convert %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %97 = stablehlo.negate %96 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x64xbf16>
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %99 = stablehlo.concatenate %97, %98, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %100 = stablehlo.convert %99 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.multiply %100, %101 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %95, %103 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x128xbf16>
    %105 = stablehlo.reshape %104 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %107 = stablehlo.reshape %106 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16>
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16>
    %109 = stablehlo.reshape %108 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16>
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16>
    %111 = stablehlo.reshape %110 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %112 = stablehlo.convert %111 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<1x24x7x16xf32>
    %114 = stablehlo.multiply %112, %113 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %115 = stablehlo.convert %114 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64>
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64>
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16>
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32>
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32>
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32>
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16>
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %130 = stablehlo.add %115, %129 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xbf16>
    %131 = stablehlo.convert %130 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %134 = stablehlo.subtract %131, %133 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %135 = stablehlo.exponential %134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %138 = stablehlo.divide %135, %137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %139 = stablehlo.convert %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %140 = stablehlo.reshape %139 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16>
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %142 = stablehlo.reshape %141 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16>
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16>
    %144 = stablehlo.reshape %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %146 = stablehlo.reshape %145 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %147 = stablehlo.reshape %arg14 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %148 = stablehlo.reshape %147 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %151 = sdy.all_reduce {"_axis_0"} %150 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %152 = stablehlo.reshape %151 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %153 = stablehlo.add %20, %152 : tensor<1x7x3072xbf16>
    %154 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %155 = stablehlo.reshape %154 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %156 = stablehlo.convert %155 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %157 = stablehlo.broadcast_in_dim %156, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %158 = stablehlo.convert %153 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %159 = stablehlo.power %158, %2 : tensor<1x7x3072xf32>
    %160 = stablehlo.reduce(%159 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %161 = stablehlo.multiply %160, %cst_4 : tensor<1x7xf32>
    %162 = stablehlo.reshape %161 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %163 = stablehlo.add %162, %26 : tensor<1x7x1xf32>
    %164 = stablehlo.rsqrt %163 : tensor<1x7x1xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %166 = stablehlo.broadcast_in_dim %165, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %167 = stablehlo.multiply %158, %166 : tensor<1x7x3072xf32>
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %169 = stablehlo.convert %168 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %170 = stablehlo.multiply %157, %169 : tensor<1x7x3072xf32>
    %171 = stablehlo.convert %170 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %172 = stablehlo.reshape %171 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %173 = stablehlo.reshape %arg19 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %174 = stablehlo.reshape %173 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %175 = stablehlo.transpose %174, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %176 = stablehlo.dot_general %172, %175, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %177 = stablehlo.reshape %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %178 = stablehlo.convert %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %179 = stablehlo.logistic %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xbf16>
    %180 = stablehlo.convert %179 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %181 = stablehlo.multiply %178, %180 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
    %182 = stablehlo.convert %181 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %183 = stablehlo.convert %182 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %184 = stablehlo.reshape %arg13 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %185 = stablehlo.reshape %184 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %186 = stablehlo.transpose %185, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %187 = stablehlo.dot_general %172, %186, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %188 = stablehlo.reshape %187 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %189 = stablehlo.convert %188 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %190 = stablehlo.multiply %183, %189 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
    %191 = stablehlo.convert %190 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %192 = stablehlo.reshape %191 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %193 = stablehlo.reshape %arg12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %194 = stablehlo.reshape %193 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %195 = stablehlo.transpose %194, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %196 = stablehlo.dot_general %192, %195, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %197 = sdy.all_reduce {"_axis_0"} %196 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %198 = stablehlo.reshape %197 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %199 = stablehlo.add %153, %198 : tensor<1x7x3072xbf16>
    %200 = stablehlo.convert %199 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %201 = stablehlo.power %200, %2 : tensor<1x7x3072xf32>
    %202 = stablehlo.reduce(%201 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %203 = stablehlo.multiply %202, %cst_4 : tensor<1x7xf32>
    %204 = stablehlo.reshape %203 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %205 = stablehlo.add %204, %26 : tensor<1x7x1xf32>
    %206 = stablehlo.rsqrt %205 : tensor<1x7x1xf32>
    %207 = stablehlo.reshape %206 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %208 = stablehlo.broadcast_in_dim %207, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %209 = stablehlo.multiply %200, %208 : tensor<1x7x3072xf32>
    %210 = stablehlo.convert %209 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %212 = stablehlo.multiply %85, %211 : tensor<1x7x3072xf32>
    %213 = stablehlo.convert %212 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %214 = stablehlo.reshape %213 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %215 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %216 = stablehlo.reshape %215 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %217 = stablehlo.transpose %216, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %218 = stablehlo.dot_general %214, %217, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %219 = stablehlo.reshape %218 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %73, %81, %218, %219 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump Before WrapUnderManualComputationPass (wrap-under-manual-computation) ('builtin.module' operation: @SyncTensorsGraph.450) //----- //
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg1: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg2: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg3: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg4: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg5: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg7: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg8: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg9: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg10: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg11: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg12: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg13: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg14: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg15: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg16: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64>
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %7 = stablehlo.add %4, %6 : tensor<7xi64>
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64>
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64>
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32>
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32>
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32>
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32>
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32>
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %37 = stablehlo.reshape %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %38 = stablehlo.reshape %37 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %56 = stablehlo.multiply %43, %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xf32>
    %57 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.convert %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32>
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %69 = stablehlo.multiply %62, %68 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xf32>
    %70 = stablehlo.convert %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %71 = stablehlo.add %57, %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %74 = stablehlo.reshape %arg9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %86 = stablehlo.reshape %arg17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %87 = stablehlo.reshape %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %90 = stablehlo.reshape %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %94 = stablehlo.multiply %92, %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x128xf32>
    %95 = stablehlo.convert %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %97 = stablehlo.negate %96 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x64xbf16>
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %99 = stablehlo.concatenate %97, %98, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %100 = stablehlo.convert %99 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.multiply %100, %101 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %95, %103 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x128xbf16>
    %105 = stablehlo.reshape %104 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %107 = stablehlo.reshape %106 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16>
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16>
    %109 = stablehlo.reshape %108 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16>
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16>
    %111 = stablehlo.reshape %110 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %112 = stablehlo.convert %111 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<1x24x7x16xf32>
    %114 = stablehlo.multiply %112, %113 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %115 = stablehlo.convert %114 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64>
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64>
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16>
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32>
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32>
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32>
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16>
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %130 = stablehlo.add %115, %129 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xbf16>
    %131 = stablehlo.convert %130 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %134 = stablehlo.subtract %131, %133 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %135 = stablehlo.exponential %134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %138 = stablehlo.divide %135, %137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %139 = stablehlo.convert %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %140 = stablehlo.reshape %139 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16>
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %142 = stablehlo.reshape %141 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16>
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16>
    %144 = stablehlo.reshape %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %146 = stablehlo.reshape %145 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %147 = stablehlo.reshape %arg14 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %148 = stablehlo.reshape %147 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %151 = sdy.all_reduce {"_axis_0"} %150 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %152 = stablehlo.reshape %151 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %153 = stablehlo.add %20, %152 : tensor<1x7x3072xbf16>
    %154 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %155 = stablehlo.reshape %154 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %156 = stablehlo.convert %155 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %157 = stablehlo.broadcast_in_dim %156, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %158 = stablehlo.convert %153 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %159 = stablehlo.power %158, %2 : tensor<1x7x3072xf32>
    %160 = stablehlo.reduce(%159 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %161 = stablehlo.multiply %160, %cst_4 : tensor<1x7xf32>
    %162 = stablehlo.reshape %161 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %163 = stablehlo.add %162, %26 : tensor<1x7x1xf32>
    %164 = stablehlo.rsqrt %163 : tensor<1x7x1xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %166 = stablehlo.broadcast_in_dim %165, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %167 = stablehlo.multiply %158, %166 : tensor<1x7x3072xf32>
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %169 = stablehlo.convert %168 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %170 = stablehlo.multiply %157, %169 : tensor<1x7x3072xf32>
    %171 = stablehlo.convert %170 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %172 = stablehlo.reshape %171 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %173 = stablehlo.reshape %arg19 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %174 = stablehlo.reshape %173 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %175 = stablehlo.transpose %174, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %176 = stablehlo.dot_general %172, %175, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %177 = stablehlo.reshape %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %178 = stablehlo.convert %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %179 = stablehlo.logistic %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xbf16>
    %180 = stablehlo.convert %179 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %181 = stablehlo.multiply %178, %180 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
    %182 = stablehlo.convert %181 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %183 = stablehlo.convert %182 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %184 = stablehlo.reshape %arg13 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %185 = stablehlo.reshape %184 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %186 = stablehlo.transpose %185, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %187 = stablehlo.dot_general %172, %186, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %188 = stablehlo.reshape %187 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %189 = stablehlo.convert %188 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %190 = stablehlo.multiply %183, %189 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
    %191 = stablehlo.convert %190 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %192 = stablehlo.reshape %191 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %193 = stablehlo.reshape %arg12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %194 = stablehlo.reshape %193 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %195 = stablehlo.transpose %194, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %196 = stablehlo.dot_general %192, %195, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %197 = sdy.all_reduce {"_axis_0"} %196 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %198 = stablehlo.reshape %197 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %199 = stablehlo.add %153, %198 : tensor<1x7x3072xbf16>
    %200 = stablehlo.convert %199 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %201 = stablehlo.power %200, %2 : tensor<1x7x3072xf32>
    %202 = stablehlo.reduce(%201 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %203 = stablehlo.multiply %202, %cst_4 : tensor<1x7xf32>
    %204 = stablehlo.reshape %203 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %205 = stablehlo.add %204, %26 : tensor<1x7x1xf32>
    %206 = stablehlo.rsqrt %205 : tensor<1x7x1xf32>
    %207 = stablehlo.reshape %206 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %208 = stablehlo.broadcast_in_dim %207, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %209 = stablehlo.multiply %200, %208 : tensor<1x7x3072xf32>
    %210 = stablehlo.convert %209 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %212 = stablehlo.multiply %85, %211 : tensor<1x7x3072xf32>
    %213 = stablehlo.convert %212 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %214 = stablehlo.reshape %213 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %215 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %216 = stablehlo.reshape %215 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %217 = stablehlo.transpose %216, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %218 = stablehlo.dot_general %214, %217, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %219 = stablehlo.reshape %218 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %73, %81, %218, %219 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}


DEBUG: Checking for GSPMD annotations in module
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg1: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg2: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg3: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg4: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg5: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg7: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg8: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg9: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg10: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg11: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg12: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg13: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg14: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg15: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg16: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64>
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %7 = stablehlo.add %4, %6 : tensor<7xi64>
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64>
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64>
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32>
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32>
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32>
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32>
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32>
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %37 = stablehlo.reshape %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %38 = stablehlo.reshape %37 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %56 = stablehlo.multiply %43, %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xf32>
    %57 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.convert %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32>
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %69 = stablehlo.multiply %62, %68 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xf32>
    %70 = stablehlo.convert %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %71 = stablehlo.add %57, %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %74 = stablehlo.reshape %arg9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %86 = stablehlo.reshape %arg17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %87 = stablehlo.reshape %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %90 = stablehlo.reshape %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %94 = stablehlo.multiply %92, %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x128xf32>
    %95 = stablehlo.convert %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %97 = stablehlo.negate %96 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x64xbf16>
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %99 = stablehlo.concatenate %97, %98, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %100 = stablehlo.convert %99 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.multiply %100, %101 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %95, %103 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x128xbf16>
    %105 = stablehlo.reshape %104 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %107 = stablehlo.reshape %106 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16>
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16>
    %109 = stablehlo.reshape %108 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16>
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16>
    %111 = stablehlo.reshape %110 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %112 = stablehlo.convert %111 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<1x24x7x16xf32>
    %114 = stablehlo.multiply %112, %113 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %115 = stablehlo.convert %114 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64>
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64>
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16>
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32>
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32>
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32>
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16>
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %130 = stablehlo.add %115, %129 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xbf16>
    %131 = stablehlo.convert %130 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %134 = stablehlo.subtract %131, %133 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %135 = stablehlo.exponential %134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %138 = stablehlo.divide %135, %137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %139 = stablehlo.convert %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %140 = stablehlo.reshape %139 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16>
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %142 = stablehlo.reshape %141 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16>
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16>
    %144 = stablehlo.reshape %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %146 = stablehlo.reshape %145 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %147 = stablehlo.reshape %arg14 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %148 = stablehlo.reshape %147 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %151 = sdy.all_reduce {"_axis_0"} %150 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %152 = stablehlo.reshape %151 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %153 = stablehlo.add %20, %152 : tensor<1x7x3072xbf16>
    %154 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %155 = stablehlo.reshape %154 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %156 = stablehlo.convert %155 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %157 = stablehlo.broadcast_in_dim %156, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %158 = stablehlo.convert %153 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %159 = stablehlo.power %158, %2 : tensor<1x7x3072xf32>
    %160 = stablehlo.reduce(%159 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %161 = stablehlo.multiply %160, %cst_4 : tensor<1x7xf32>
    %162 = stablehlo.reshape %161 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %163 = stablehlo.add %162, %26 : tensor<1x7x1xf32>
    %164 = stablehlo.rsqrt %163 : tensor<1x7x1xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %166 = stablehlo.broadcast_in_dim %165, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %167 = stablehlo.multiply %158, %166 : tensor<1x7x3072xf32>
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %169 = stablehlo.convert %168 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %170 = stablehlo.multiply %157, %169 : tensor<1x7x3072xf32>
    %171 = stablehlo.convert %170 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %172 = stablehlo.reshape %171 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %173 = stablehlo.reshape %arg19 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %174 = stablehlo.reshape %173 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %175 = stablehlo.transpose %174, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %176 = stablehlo.dot_general %172, %175, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %177 = stablehlo.reshape %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %178 = stablehlo.convert %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %179 = stablehlo.logistic %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xbf16>
    %180 = stablehlo.convert %179 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %181 = stablehlo.multiply %178, %180 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
    %182 = stablehlo.convert %181 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %183 = stablehlo.convert %182 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %184 = stablehlo.reshape %arg13 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %185 = stablehlo.reshape %184 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %186 = stablehlo.transpose %185, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %187 = stablehlo.dot_general %172, %186, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %188 = stablehlo.reshape %187 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %189 = stablehlo.convert %188 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %190 = stablehlo.multiply %183, %189 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
    %191 = stablehlo.convert %190 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %192 = stablehlo.reshape %191 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %193 = stablehlo.reshape %arg12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %194 = stablehlo.reshape %193 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %195 = stablehlo.transpose %194, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %196 = stablehlo.dot_general %192, %195, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %197 = sdy.all_reduce {"_axis_0"} %196 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %198 = stablehlo.reshape %197 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %199 = stablehlo.add %153, %198 : tensor<1x7x3072xbf16>
    %200 = stablehlo.convert %199 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %201 = stablehlo.power %200, %2 : tensor<1x7x3072xf32>
    %202 = stablehlo.reduce(%201 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %203 = stablehlo.multiply %202, %cst_4 : tensor<1x7xf32>
    %204 = stablehlo.reshape %203 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %205 = stablehlo.add %204, %26 : tensor<1x7x1xf32>
    %206 = stablehlo.rsqrt %205 : tensor<1x7x1xf32>
    %207 = stablehlo.reshape %206 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %208 = stablehlo.broadcast_in_dim %207, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %209 = stablehlo.multiply %200, %208 : tensor<1x7x3072xf32>
    %210 = stablehlo.convert %209 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %212 = stablehlo.multiply %85, %211 : tensor<1x7x3072xf32>
    %213 = stablehlo.convert %212 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %214 = stablehlo.reshape %213 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %215 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %216 = stablehlo.reshape %215 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %217 = stablehlo.transpose %216, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %218 = stablehlo.dot_general %214, %217, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %219 = stablehlo.reshape %218 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %73, %81, %218, %219 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}
DEBUG: Found CustomCallOp with target: Sharding
DEBUG: Interrupting walk due to matching custom call: Sharding
// -----// IR Dump Before ReshardToCollectivesPass (sdy-reshard-to-collectives) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg1: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg2: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg3: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg4: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg5: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg7: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg8: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg9: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg10: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg11: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg12: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg13: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg14: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg15: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg16: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64>
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %7 = stablehlo.add %4, %6 : tensor<7xi64>
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64>
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64>
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32>
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32>
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32>
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32>
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32>
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %37 = stablehlo.reshape %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %38 = stablehlo.reshape %37 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %56 = stablehlo.multiply %43, %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xf32>
    %57 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.convert %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32>
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %69 = stablehlo.multiply %62, %68 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xf32>
    %70 = stablehlo.convert %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %71 = stablehlo.add %57, %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %74 = stablehlo.reshape %arg9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %86 = stablehlo.reshape %arg17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %87 = stablehlo.reshape %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %90 = stablehlo.reshape %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %94 = stablehlo.multiply %92, %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x128xf32>
    %95 = stablehlo.convert %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %97 = stablehlo.negate %96 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x64xbf16>
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %99 = stablehlo.concatenate %97, %98, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %100 = stablehlo.convert %99 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.multiply %100, %101 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %95, %103 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x128xbf16>
    %105 = stablehlo.reshape %104 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %107 = stablehlo.reshape %106 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16>
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16>
    %109 = stablehlo.reshape %108 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16>
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16>
    %111 = stablehlo.reshape %110 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %112 = stablehlo.convert %111 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<1x24x7x16xf32>
    %114 = stablehlo.multiply %112, %113 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %115 = stablehlo.convert %114 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64>
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64>
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16>
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32>
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32>
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32>
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16>
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %130 = stablehlo.add %115, %129 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xbf16>
    %131 = stablehlo.convert %130 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %134 = stablehlo.subtract %131, %133 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %135 = stablehlo.exponential %134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %138 = stablehlo.divide %135, %137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %139 = stablehlo.convert %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %140 = stablehlo.reshape %139 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16>
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %142 = stablehlo.reshape %141 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16>
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16>
    %144 = stablehlo.reshape %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %146 = stablehlo.reshape %145 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %147 = stablehlo.reshape %arg14 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %148 = stablehlo.reshape %147 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %151 = sdy.all_reduce {"_axis_0"} %150 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %152 = stablehlo.reshape %151 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %153 = stablehlo.add %20, %152 : tensor<1x7x3072xbf16>
    %154 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %155 = stablehlo.reshape %154 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %156 = stablehlo.convert %155 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %157 = stablehlo.broadcast_in_dim %156, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %158 = stablehlo.convert %153 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %159 = stablehlo.power %158, %2 : tensor<1x7x3072xf32>
    %160 = stablehlo.reduce(%159 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %161 = stablehlo.multiply %160, %cst_4 : tensor<1x7xf32>
    %162 = stablehlo.reshape %161 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %163 = stablehlo.add %162, %26 : tensor<1x7x1xf32>
    %164 = stablehlo.rsqrt %163 : tensor<1x7x1xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %166 = stablehlo.broadcast_in_dim %165, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %167 = stablehlo.multiply %158, %166 : tensor<1x7x3072xf32>
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %169 = stablehlo.convert %168 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %170 = stablehlo.multiply %157, %169 : tensor<1x7x3072xf32>
    %171 = stablehlo.convert %170 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %172 = stablehlo.reshape %171 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %173 = stablehlo.reshape %arg19 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %174 = stablehlo.reshape %173 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %175 = stablehlo.transpose %174, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %176 = stablehlo.dot_general %172, %175, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %177 = stablehlo.reshape %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %178 = stablehlo.convert %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %179 = stablehlo.logistic %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xbf16>
    %180 = stablehlo.convert %179 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %181 = stablehlo.multiply %178, %180 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
    %182 = stablehlo.convert %181 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %183 = stablehlo.convert %182 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %184 = stablehlo.reshape %arg13 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %185 = stablehlo.reshape %184 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %186 = stablehlo.transpose %185, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %187 = stablehlo.dot_general %172, %186, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %188 = stablehlo.reshape %187 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %189 = stablehlo.convert %188 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %190 = stablehlo.multiply %183, %189 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
    %191 = stablehlo.convert %190 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %192 = stablehlo.reshape %191 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %193 = stablehlo.reshape %arg12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %194 = stablehlo.reshape %193 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %195 = stablehlo.transpose %194, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %196 = stablehlo.dot_general %192, %195, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %197 = sdy.all_reduce {"_axis_0"} %196 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %198 = stablehlo.reshape %197 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %199 = stablehlo.add %153, %198 : tensor<1x7x3072xbf16>
    %200 = stablehlo.convert %199 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %201 = stablehlo.power %200, %2 : tensor<1x7x3072xf32>
    %202 = stablehlo.reduce(%201 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %203 = stablehlo.multiply %202, %cst_4 : tensor<1x7xf32>
    %204 = stablehlo.reshape %203 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %205 = stablehlo.add %204, %26 : tensor<1x7x1xf32>
    %206 = stablehlo.rsqrt %205 : tensor<1x7x1xf32>
    %207 = stablehlo.reshape %206 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %208 = stablehlo.broadcast_in_dim %207, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %209 = stablehlo.multiply %200, %208 : tensor<1x7x3072xf32>
    %210 = stablehlo.convert %209 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %212 = stablehlo.multiply %85, %211 : tensor<1x7x3072xf32>
    %213 = stablehlo.convert %212 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %214 = stablehlo.reshape %213 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %215 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %216 = stablehlo.reshape %215 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %217 = stablehlo.transpose %216, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %218 = stablehlo.dot_general %214, %217, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %219 = stablehlo.reshape %218 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %73, %81, %218, %219 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump Before UpdateGlobalToLocalShapesPass (update-global-to-local-shapes) ('builtin.module' operation: @SyncTensorsGraph.450) //----- //
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg1: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg2: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg3: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg4: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg5: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg7: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg8: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg9: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg10: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg11: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg12: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg13: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg14: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg15: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg16: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64>
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %7 = stablehlo.add %4, %6 : tensor<7xi64>
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64>
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64>
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32>
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32>
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32>
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32>
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32>
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %37 = stablehlo.reshape %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %38 = stablehlo.reshape %37 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %56 = stablehlo.multiply %43, %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xf32>
    %57 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.convert %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32>
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %69 = stablehlo.multiply %62, %68 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xf32>
    %70 = stablehlo.convert %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %71 = stablehlo.add %57, %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %74 = stablehlo.reshape %arg9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %86 = stablehlo.reshape %arg17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %87 = stablehlo.reshape %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %90 = stablehlo.reshape %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %94 = stablehlo.multiply %92, %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x128xf32>
    %95 = stablehlo.convert %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %97 = stablehlo.negate %96 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x64xbf16>
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %99 = stablehlo.concatenate %97, %98, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %100 = stablehlo.convert %99 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.multiply %100, %101 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %95, %103 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x128xbf16>
    %105 = stablehlo.reshape %104 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %107 = stablehlo.reshape %106 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16>
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16>
    %109 = stablehlo.reshape %108 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16>
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16>
    %111 = stablehlo.reshape %110 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %112 = stablehlo.convert %111 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<1x24x7x16xf32>
    %114 = stablehlo.multiply %112, %113 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %115 = stablehlo.convert %114 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64>
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64>
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16>
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32>
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32>
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32>
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16>
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %130 = stablehlo.add %115, %129 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xbf16>
    %131 = stablehlo.convert %130 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %134 = stablehlo.subtract %131, %133 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %135 = stablehlo.exponential %134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %138 = stablehlo.divide %135, %137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %139 = stablehlo.convert %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %140 = stablehlo.reshape %139 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16>
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %142 = stablehlo.reshape %141 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16>
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16>
    %144 = stablehlo.reshape %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %146 = stablehlo.reshape %145 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %147 = stablehlo.reshape %arg14 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %148 = stablehlo.reshape %147 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %151 = sdy.all_reduce {"_axis_0"} %150 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %152 = stablehlo.reshape %151 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %153 = stablehlo.add %20, %152 : tensor<1x7x3072xbf16>
    %154 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %155 = stablehlo.reshape %154 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %156 = stablehlo.convert %155 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %157 = stablehlo.broadcast_in_dim %156, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %158 = stablehlo.convert %153 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %159 = stablehlo.power %158, %2 : tensor<1x7x3072xf32>
    %160 = stablehlo.reduce(%159 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %161 = stablehlo.multiply %160, %cst_4 : tensor<1x7xf32>
    %162 = stablehlo.reshape %161 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %163 = stablehlo.add %162, %26 : tensor<1x7x1xf32>
    %164 = stablehlo.rsqrt %163 : tensor<1x7x1xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %166 = stablehlo.broadcast_in_dim %165, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %167 = stablehlo.multiply %158, %166 : tensor<1x7x3072xf32>
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %169 = stablehlo.convert %168 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %170 = stablehlo.multiply %157, %169 : tensor<1x7x3072xf32>
    %171 = stablehlo.convert %170 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %172 = stablehlo.reshape %171 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %173 = stablehlo.reshape %arg19 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %174 = stablehlo.reshape %173 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %175 = stablehlo.transpose %174, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %176 = stablehlo.dot_general %172, %175, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %177 = stablehlo.reshape %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %178 = stablehlo.convert %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %179 = stablehlo.logistic %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xbf16>
    %180 = stablehlo.convert %179 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %181 = stablehlo.multiply %178, %180 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
    %182 = stablehlo.convert %181 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %183 = stablehlo.convert %182 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %184 = stablehlo.reshape %arg13 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %185 = stablehlo.reshape %184 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %186 = stablehlo.transpose %185, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %187 = stablehlo.dot_general %172, %186, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %188 = stablehlo.reshape %187 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %189 = stablehlo.convert %188 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %190 = stablehlo.multiply %183, %189 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
    %191 = stablehlo.convert %190 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %192 = stablehlo.reshape %191 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %193 = stablehlo.reshape %arg12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %194 = stablehlo.reshape %193 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %195 = stablehlo.transpose %194, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %196 = stablehlo.dot_general %192, %195, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %197 = sdy.all_reduce {"_axis_0"} %196 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %198 = stablehlo.reshape %197 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %199 = stablehlo.add %153, %198 : tensor<1x7x3072xbf16>
    %200 = stablehlo.convert %199 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %201 = stablehlo.power %200, %2 : tensor<1x7x3072xf32>
    %202 = stablehlo.reduce(%201 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %203 = stablehlo.multiply %202, %cst_4 : tensor<1x7xf32>
    %204 = stablehlo.reshape %203 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %205 = stablehlo.add %204, %26 : tensor<1x7x1xf32>
    %206 = stablehlo.rsqrt %205 : tensor<1x7x1xf32>
    %207 = stablehlo.reshape %206 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %208 = stablehlo.broadcast_in_dim %207, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %209 = stablehlo.multiply %200, %208 : tensor<1x7x3072xf32>
    %210 = stablehlo.convert %209 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %212 = stablehlo.multiply %85, %211 : tensor<1x7x3072xf32>
    %213 = stablehlo.convert %212 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %214 = stablehlo.reshape %213 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %215 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %216 = stablehlo.reshape %215 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %217 = stablehlo.transpose %216, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %218 = stablehlo.dot_general %214, %217, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %219 = stablehlo.reshape %218 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %73, %81, %218, %219 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}


DEBUG: Checking for GSPMD annotations in module
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg1: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg2: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg3: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg4: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg5: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg7: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg8: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg9: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg10: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg11: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg12: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg13: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg14: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg15: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg16: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64>
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %7 = stablehlo.add %4, %6 : tensor<7xi64>
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64>
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64>
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32>
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32>
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32>
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32>
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32>
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %37 = stablehlo.reshape %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %38 = stablehlo.reshape %37 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %56 = stablehlo.multiply %43, %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xf32>
    %57 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.convert %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32>
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %69 = stablehlo.multiply %62, %68 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xf32>
    %70 = stablehlo.convert %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %71 = stablehlo.add %57, %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %74 = stablehlo.reshape %arg9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %86 = stablehlo.reshape %arg17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %87 = stablehlo.reshape %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %90 = stablehlo.reshape %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %94 = stablehlo.multiply %92, %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x128xf32>
    %95 = stablehlo.convert %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %97 = stablehlo.negate %96 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x64xbf16>
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %99 = stablehlo.concatenate %97, %98, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %100 = stablehlo.convert %99 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.multiply %100, %101 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %95, %103 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x128xbf16>
    %105 = stablehlo.reshape %104 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %107 = stablehlo.reshape %106 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16>
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16>
    %109 = stablehlo.reshape %108 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16>
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16>
    %111 = stablehlo.reshape %110 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %112 = stablehlo.convert %111 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<1x24x7x16xf32>
    %114 = stablehlo.multiply %112, %113 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %115 = stablehlo.convert %114 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64>
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64>
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16>
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32>
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32>
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32>
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16>
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %130 = stablehlo.add %115, %129 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xbf16>
    %131 = stablehlo.convert %130 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %134 = stablehlo.subtract %131, %133 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %135 = stablehlo.exponential %134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %138 = stablehlo.divide %135, %137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %139 = stablehlo.convert %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %140 = stablehlo.reshape %139 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16>
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %142 = stablehlo.reshape %141 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16>
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16>
    %144 = stablehlo.reshape %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %146 = stablehlo.reshape %145 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %147 = stablehlo.reshape %arg14 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %148 = stablehlo.reshape %147 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %151 = sdy.all_reduce {"_axis_0"} %150 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %152 = stablehlo.reshape %151 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %153 = stablehlo.add %20, %152 : tensor<1x7x3072xbf16>
    %154 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %155 = stablehlo.reshape %154 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %156 = stablehlo.convert %155 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %157 = stablehlo.broadcast_in_dim %156, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %158 = stablehlo.convert %153 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %159 = stablehlo.power %158, %2 : tensor<1x7x3072xf32>
    %160 = stablehlo.reduce(%159 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %161 = stablehlo.multiply %160, %cst_4 : tensor<1x7xf32>
    %162 = stablehlo.reshape %161 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %163 = stablehlo.add %162, %26 : tensor<1x7x1xf32>
    %164 = stablehlo.rsqrt %163 : tensor<1x7x1xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %166 = stablehlo.broadcast_in_dim %165, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %167 = stablehlo.multiply %158, %166 : tensor<1x7x3072xf32>
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %169 = stablehlo.convert %168 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %170 = stablehlo.multiply %157, %169 : tensor<1x7x3072xf32>
    %171 = stablehlo.convert %170 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %172 = stablehlo.reshape %171 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %173 = stablehlo.reshape %arg19 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %174 = stablehlo.reshape %173 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %175 = stablehlo.transpose %174, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %176 = stablehlo.dot_general %172, %175, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %177 = stablehlo.reshape %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %178 = stablehlo.convert %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %179 = stablehlo.logistic %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xbf16>
    %180 = stablehlo.convert %179 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %181 = stablehlo.multiply %178, %180 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
    %182 = stablehlo.convert %181 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %183 = stablehlo.convert %182 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %184 = stablehlo.reshape %arg13 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %185 = stablehlo.reshape %184 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %186 = stablehlo.transpose %185, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %187 = stablehlo.dot_general %172, %186, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %188 = stablehlo.reshape %187 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %189 = stablehlo.convert %188 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %190 = stablehlo.multiply %183, %189 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
    %191 = stablehlo.convert %190 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %192 = stablehlo.reshape %191 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %193 = stablehlo.reshape %arg12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %194 = stablehlo.reshape %193 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %195 = stablehlo.transpose %194, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %196 = stablehlo.dot_general %192, %195, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %197 = sdy.all_reduce {"_axis_0"} %196 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %198 = stablehlo.reshape %197 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %199 = stablehlo.add %153, %198 : tensor<1x7x3072xbf16>
    %200 = stablehlo.convert %199 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %201 = stablehlo.power %200, %2 : tensor<1x7x3072xf32>
    %202 = stablehlo.reduce(%201 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %203 = stablehlo.multiply %202, %cst_4 : tensor<1x7xf32>
    %204 = stablehlo.reshape %203 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %205 = stablehlo.add %204, %26 : tensor<1x7x1xf32>
    %206 = stablehlo.rsqrt %205 : tensor<1x7x1xf32>
    %207 = stablehlo.reshape %206 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %208 = stablehlo.broadcast_in_dim %207, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %209 = stablehlo.multiply %200, %208 : tensor<1x7x3072xf32>
    %210 = stablehlo.convert %209 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %212 = stablehlo.multiply %85, %211 : tensor<1x7x3072xf32>
    %213 = stablehlo.convert %212 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %214 = stablehlo.reshape %213 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %215 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %216 = stablehlo.reshape %215 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %217 = stablehlo.transpose %216, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %218 = stablehlo.dot_general %214, %217, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %219 = stablehlo.reshape %218 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %73, %81, %218, %219 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}
DEBUG: Found CustomCallOp with target: Sharding
DEBUG: Interrupting walk due to matching custom call: Sharding
// -----// IR Dump Before CloseShardingsPass (sdy-close-shardings) ('builtin.module' operation: @SyncTensorsGraph.450) //----- //
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg1: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg2: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg3: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg4: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg5: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg7: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg8: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg9: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg10: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg11: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg12: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg13: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg14: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg15: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg16: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64>
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %7 = stablehlo.add %4, %6 : tensor<7xi64>
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64>
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64>
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32>
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32>
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32>
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32>
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32>
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %37 = stablehlo.reshape %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %38 = stablehlo.reshape %37 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %56 = stablehlo.multiply %43, %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xf32>
    %57 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.convert %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32>
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %69 = stablehlo.multiply %62, %68 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xf32>
    %70 = stablehlo.convert %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %71 = stablehlo.add %57, %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %74 = stablehlo.reshape %arg9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %86 = stablehlo.reshape %arg17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %87 = stablehlo.reshape %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %90 = stablehlo.reshape %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %94 = stablehlo.multiply %92, %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x128xf32>
    %95 = stablehlo.convert %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %97 = stablehlo.negate %96 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x64xbf16>
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %99 = stablehlo.concatenate %97, %98, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %100 = stablehlo.convert %99 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.multiply %100, %101 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %95, %103 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x128xbf16>
    %105 = stablehlo.reshape %104 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %107 = stablehlo.reshape %106 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16>
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16>
    %109 = stablehlo.reshape %108 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16>
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16>
    %111 = stablehlo.reshape %110 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %112 = stablehlo.convert %111 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<1x24x7x16xf32>
    %114 = stablehlo.multiply %112, %113 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %115 = stablehlo.convert %114 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64>
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64>
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16>
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32>
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32>
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32>
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16>
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %130 = stablehlo.add %115, %129 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xbf16>
    %131 = stablehlo.convert %130 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %134 = stablehlo.subtract %131, %133 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %135 = stablehlo.exponential %134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %138 = stablehlo.divide %135, %137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x24x7x16xf32>
    %139 = stablehlo.convert %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %140 = stablehlo.reshape %139 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16>
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %142 = stablehlo.reshape %141 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16>
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16>
    %144 = stablehlo.reshape %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %146 = stablehlo.reshape %145 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %147 = stablehlo.reshape %arg14 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %148 = stablehlo.reshape %147 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %151 = sdy.all_reduce {"_axis_0"} %150 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %152 = stablehlo.reshape %151 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %153 = stablehlo.add %20, %152 : tensor<1x7x3072xbf16>
    %154 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %155 = stablehlo.reshape %154 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %156 = stablehlo.convert %155 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %157 = stablehlo.broadcast_in_dim %156, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %158 = stablehlo.convert %153 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %159 = stablehlo.power %158, %2 : tensor<1x7x3072xf32>
    %160 = stablehlo.reduce(%159 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %161 = stablehlo.multiply %160, %cst_4 : tensor<1x7xf32>
    %162 = stablehlo.reshape %161 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %163 = stablehlo.add %162, %26 : tensor<1x7x1xf32>
    %164 = stablehlo.rsqrt %163 : tensor<1x7x1xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %166 = stablehlo.broadcast_in_dim %165, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %167 = stablehlo.multiply %158, %166 : tensor<1x7x3072xf32>
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %169 = stablehlo.convert %168 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %170 = stablehlo.multiply %157, %169 : tensor<1x7x3072xf32>
    %171 = stablehlo.convert %170 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %172 = stablehlo.reshape %171 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %173 = stablehlo.reshape %arg19 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %174 = stablehlo.reshape %173 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %175 = stablehlo.transpose %174, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %176 = stablehlo.dot_general %172, %175, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %177 = stablehlo.reshape %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %178 = stablehlo.convert %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %179 = stablehlo.logistic %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xbf16>
    %180 = stablehlo.convert %179 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %181 = stablehlo.multiply %178, %180 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
    %182 = stablehlo.convert %181 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %183 = stablehlo.convert %182 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %184 = stablehlo.reshape %arg13 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %185 = stablehlo.reshape %184 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %186 = stablehlo.transpose %185, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %187 = stablehlo.dot_general %172, %186, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %188 = stablehlo.reshape %187 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %189 = stablehlo.convert %188 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %190 = stablehlo.multiply %183, %189 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
    %191 = stablehlo.convert %190 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %192 = stablehlo.reshape %191 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %193 = stablehlo.reshape %arg12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %194 = stablehlo.reshape %193 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %195 = stablehlo.transpose %194, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %196 = stablehlo.dot_general %192, %195, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %197 = sdy.all_reduce {"_axis_0"} %196 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %198 = stablehlo.reshape %197 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %199 = stablehlo.add %153, %198 : tensor<1x7x3072xbf16>
    %200 = stablehlo.convert %199 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %201 = stablehlo.power %200, %2 : tensor<1x7x3072xf32>
    %202 = stablehlo.reduce(%201 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %203 = stablehlo.multiply %202, %cst_4 : tensor<1x7xf32>
    %204 = stablehlo.reshape %203 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %205 = stablehlo.add %204, %26 : tensor<1x7x1xf32>
    %206 = stablehlo.rsqrt %205 : tensor<1x7x1xf32>
    %207 = stablehlo.reshape %206 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %208 = stablehlo.broadcast_in_dim %207, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %209 = stablehlo.multiply %200, %208 : tensor<1x7x3072xf32>
    %210 = stablehlo.convert %209 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %212 = stablehlo.multiply %85, %211 : tensor<1x7x3072xf32>
    %213 = stablehlo.convert %212 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %214 = stablehlo.reshape %213 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %215 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %216 = stablehlo.reshape %215 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %217 = stablehlo.transpose %216, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %218 = stablehlo.dot_general %214, %217, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %219 = stablehlo.reshape %218 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %73, %81, %218, %219 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump After CloseShardingsPass (sdy-close-shardings) ('builtin.module' operation: @SyncTensorsGraph.450) //----- //
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg1: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg2: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg3: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg4: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg5: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg7: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg8: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg9: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg10: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg11: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg12: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg13: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg14: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg15: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg16: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64>
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %7 = stablehlo.add %4, %6 : tensor<7xi64>
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64>
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64>
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32>
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32>
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32>
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32>
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32>
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %37 = stablehlo.reshape %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %38 = stablehlo.reshape %37 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %56 = stablehlo.multiply %43, %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x128xf32>
    %57 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.convert %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32>
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %69 = stablehlo.multiply %62, %68 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x128xf32>
    %70 = stablehlo.convert %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %71 = stablehlo.add %57, %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x128xbf16>
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %74 = stablehlo.reshape %arg9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %86 = stablehlo.reshape %arg17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %87 = stablehlo.reshape %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %90 = stablehlo.reshape %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>} : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %94 = stablehlo.multiply %92, %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x128xf32>
    %95 = stablehlo.convert %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %97 = stablehlo.negate %96 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x64xbf16>
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %99 = stablehlo.concatenate %97, %98, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %100 = stablehlo.convert %99 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.multiply %100, %101 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %95, %103 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x128xbf16>
    %105 = stablehlo.reshape %104 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %107 = stablehlo.reshape %106 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16>
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16>
    %109 = stablehlo.reshape %108 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16>
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16>
    %111 = stablehlo.reshape %110 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %112 = stablehlo.convert %111 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<f32>) -> tensor<1x24x7x16xf32>
    %114 = stablehlo.multiply %112, %113 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %115 = stablehlo.convert %114 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64>
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64>
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16>
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32>
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32>
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32>
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16>
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %130 = stablehlo.add %115, %129 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xbf16>
    %131 = stablehlo.convert %130 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %134 = stablehlo.subtract %131, %133 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %135 = stablehlo.exponential %134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %138 = stablehlo.divide %135, %137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %139 = stablehlo.convert %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %140 = stablehlo.reshape %139 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16>
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %142 = stablehlo.reshape %141 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16>
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16>
    %144 = stablehlo.reshape %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %146 = stablehlo.reshape %145 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %147 = stablehlo.reshape %arg14 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %148 = stablehlo.reshape %147 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %151 = sdy.all_reduce {"_axis_0"} %150 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %152 = stablehlo.reshape %151 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %153 = stablehlo.add %20, %152 : tensor<1x7x3072xbf16>
    %154 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %155 = stablehlo.reshape %154 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %156 = stablehlo.convert %155 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %157 = stablehlo.broadcast_in_dim %156, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %158 = stablehlo.convert %153 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %159 = stablehlo.power %158, %2 : tensor<1x7x3072xf32>
    %160 = stablehlo.reduce(%159 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %161 = stablehlo.multiply %160, %cst_4 : tensor<1x7xf32>
    %162 = stablehlo.reshape %161 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %163 = stablehlo.add %162, %26 : tensor<1x7x1xf32>
    %164 = stablehlo.rsqrt %163 : tensor<1x7x1xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %166 = stablehlo.broadcast_in_dim %165, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %167 = stablehlo.multiply %158, %166 : tensor<1x7x3072xf32>
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %169 = stablehlo.convert %168 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %170 = stablehlo.multiply %157, %169 : tensor<1x7x3072xf32>
    %171 = stablehlo.convert %170 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %172 = stablehlo.reshape %171 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %173 = stablehlo.reshape %arg19 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %174 = stablehlo.reshape %173 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %175 = stablehlo.transpose %174, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %176 = stablehlo.dot_general %172, %175, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %177 = stablehlo.reshape %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %178 = stablehlo.convert %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %179 = stablehlo.logistic %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : tensor<1x7x8192xbf16>
    %180 = stablehlo.convert %179 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %181 = stablehlo.multiply %178, %180 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : tensor<1x7x8192xf32>
    %182 = stablehlo.convert %181 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %183 = stablehlo.convert %182 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %184 = stablehlo.reshape %arg13 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %185 = stablehlo.reshape %184 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %186 = stablehlo.transpose %185, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %187 = stablehlo.dot_general %172, %186, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %188 = stablehlo.reshape %187 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %189 = stablehlo.convert %188 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %190 = stablehlo.multiply %183, %189 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : tensor<1x7x8192xf32>
    %191 = stablehlo.convert %190 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %192 = stablehlo.reshape %191 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %193 = stablehlo.reshape %arg12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %194 = stablehlo.reshape %193 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %195 = stablehlo.transpose %194, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %196 = stablehlo.dot_general %192, %195, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %197 = sdy.all_reduce {"_axis_0"} %196 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %198 = stablehlo.reshape %197 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %199 = stablehlo.add %153, %198 : tensor<1x7x3072xbf16>
    %200 = stablehlo.convert %199 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %201 = stablehlo.power %200, %2 : tensor<1x7x3072xf32>
    %202 = stablehlo.reduce(%201 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %203 = stablehlo.multiply %202, %cst_4 : tensor<1x7xf32>
    %204 = stablehlo.reshape %203 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %205 = stablehlo.add %204, %26 : tensor<1x7x1xf32>
    %206 = stablehlo.rsqrt %205 : tensor<1x7x1xf32>
    %207 = stablehlo.reshape %206 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %208 = stablehlo.broadcast_in_dim %207, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %209 = stablehlo.multiply %200, %208 : tensor<1x7x3072xf32>
    %210 = stablehlo.convert %209 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %212 = stablehlo.multiply %85, %211 : tensor<1x7x3072xf32>
    %213 = stablehlo.convert %212 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %214 = stablehlo.reshape %213 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %215 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %216 = stablehlo.reshape %215 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %217 = stablehlo.transpose %216, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %218 = stablehlo.dot_general %214, %217, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %219 = stablehlo.reshape %218 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %73, %81, %218, %219 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.450) //----- //
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg1: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg2: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg3: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg4: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg5: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg7: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg8: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg9: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg10: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg11: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg12: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg13: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg14: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg15: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg16: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64>
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %7 = stablehlo.add %4, %6 : tensor<7xi64>
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64>
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64>
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32>
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32>
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32>
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32>
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32>
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %37 = stablehlo.reshape %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %38 = stablehlo.reshape %37 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %56 = stablehlo.multiply %43, %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x128xf32>
    %57 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.convert %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32>
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %69 = stablehlo.multiply %62, %68 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x128xf32>
    %70 = stablehlo.convert %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %71 = stablehlo.add %57, %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x128xbf16>
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %74 = stablehlo.reshape %arg9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %86 = stablehlo.reshape %arg17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %87 = stablehlo.reshape %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %90 = stablehlo.reshape %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>} : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %94 = stablehlo.multiply %92, %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x128xf32>
    %95 = stablehlo.convert %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %97 = stablehlo.negate %96 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x64xbf16>
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %99 = stablehlo.concatenate %97, %98, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %100 = stablehlo.convert %99 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.multiply %100, %101 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %95, %103 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x128xbf16>
    %105 = stablehlo.reshape %104 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %107 = stablehlo.reshape %106 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16>
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16>
    %109 = stablehlo.reshape %108 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16>
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16>
    %111 = stablehlo.reshape %110 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %112 = stablehlo.convert %111 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<f32>) -> tensor<1x24x7x16xf32>
    %114 = stablehlo.multiply %112, %113 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %115 = stablehlo.convert %114 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64>
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64>
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16>
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32>
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32>
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32>
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16>
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %130 = stablehlo.add %115, %129 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xbf16>
    %131 = stablehlo.convert %130 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %134 = stablehlo.subtract %131, %133 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %135 = stablehlo.exponential %134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %138 = stablehlo.divide %135, %137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %139 = stablehlo.convert %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %140 = stablehlo.reshape %139 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16>
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %142 = stablehlo.reshape %141 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16>
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16>
    %144 = stablehlo.reshape %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %146 = stablehlo.reshape %145 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %147 = stablehlo.reshape %arg14 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %148 = stablehlo.reshape %147 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %151 = sdy.all_reduce {"_axis_0"} %150 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %152 = stablehlo.reshape %151 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %153 = stablehlo.add %20, %152 : tensor<1x7x3072xbf16>
    %154 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %155 = stablehlo.reshape %154 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %156 = stablehlo.convert %155 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %157 = stablehlo.broadcast_in_dim %156, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %158 = stablehlo.convert %153 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %159 = stablehlo.power %158, %2 : tensor<1x7x3072xf32>
    %160 = stablehlo.reduce(%159 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %161 = stablehlo.multiply %160, %cst_4 : tensor<1x7xf32>
    %162 = stablehlo.reshape %161 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %163 = stablehlo.add %162, %26 : tensor<1x7x1xf32>
    %164 = stablehlo.rsqrt %163 : tensor<1x7x1xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %166 = stablehlo.broadcast_in_dim %165, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %167 = stablehlo.multiply %158, %166 : tensor<1x7x3072xf32>
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %169 = stablehlo.convert %168 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %170 = stablehlo.multiply %157, %169 : tensor<1x7x3072xf32>
    %171 = stablehlo.convert %170 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %172 = stablehlo.reshape %171 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %173 = stablehlo.reshape %arg19 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %174 = stablehlo.reshape %173 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %175 = stablehlo.transpose %174, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %176 = stablehlo.dot_general %172, %175, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %177 = stablehlo.reshape %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %178 = stablehlo.convert %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %179 = stablehlo.logistic %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : tensor<1x7x8192xbf16>
    %180 = stablehlo.convert %179 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %181 = stablehlo.multiply %178, %180 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : tensor<1x7x8192xf32>
    %182 = stablehlo.convert %181 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %183 = stablehlo.convert %182 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %184 = stablehlo.reshape %arg13 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %185 = stablehlo.reshape %184 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %186 = stablehlo.transpose %185, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %187 = stablehlo.dot_general %172, %186, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %188 = stablehlo.reshape %187 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %189 = stablehlo.convert %188 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %190 = stablehlo.multiply %183, %189 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : tensor<1x7x8192xf32>
    %191 = stablehlo.convert %190 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %192 = stablehlo.reshape %191 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %193 = stablehlo.reshape %arg12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %194 = stablehlo.reshape %193 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %195 = stablehlo.transpose %194, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %196 = stablehlo.dot_general %192, %195, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %197 = sdy.all_reduce {"_axis_0"} %196 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %198 = stablehlo.reshape %197 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %199 = stablehlo.add %153, %198 : tensor<1x7x3072xbf16>
    %200 = stablehlo.convert %199 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %201 = stablehlo.power %200, %2 : tensor<1x7x3072xf32>
    %202 = stablehlo.reduce(%201 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %203 = stablehlo.multiply %202, %cst_4 : tensor<1x7xf32>
    %204 = stablehlo.reshape %203 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %205 = stablehlo.add %204, %26 : tensor<1x7x1xf32>
    %206 = stablehlo.rsqrt %205 : tensor<1x7x1xf32>
    %207 = stablehlo.reshape %206 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %208 = stablehlo.broadcast_in_dim %207, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %209 = stablehlo.multiply %200, %208 : tensor<1x7x3072xf32>
    %210 = stablehlo.convert %209 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %212 = stablehlo.multiply %85, %211 : tensor<1x7x3072xf32>
    %213 = stablehlo.convert %212 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %214 = stablehlo.reshape %213 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %215 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %216 = stablehlo.reshape %215 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %217 = stablehlo.transpose %216, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %218 = stablehlo.dot_general %214, %217, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %219 = stablehlo.reshape %218 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %73, %81, %218, %219 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}


#loc1 = loc("p0.3")
#loc2 = loc("p1.13")
#loc3 = loc("p2.31")
#loc4 = loc("p3.37")
#loc5 = loc("p4.39")
#loc6 = loc("p5.44")
#loc7 = loc("p6.81")
#loc8 = loc("p7.118")
#loc9 = loc("p8.130")
#loc10 = loc("p9.139")
#loc11 = loc("p10.160")
#loc12 = loc("p11.169")
#loc13 = loc("p12.177")
#loc14 = loc("p13.182")
#loc15 = loc("p14.190")
#loc16 = loc("p15.220")
#loc17 = loc("p16.254")
#loc18 = loc("p17.269")
#loc19 = loc("p18.371")
#loc20 = loc("p19.383")
#loc21 = loc("p20.435")
#loc91 = loc("scatter.136")
#loc99 = loc("scatter.166")
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]> loc(#loc)
  func.func @main(%arg0: tensor<7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"} loc("p0.3"), %arg1: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"} loc("p1.13"), %arg2: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"} loc("p2.31"), %arg3: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"} loc("p3.37"), %arg4: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"} loc("p4.39"), %arg5: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"} loc("p5.44"), %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"} loc("p6.81"), %arg7: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"} loc("p7.118"), %arg8: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"} loc("p8.130"), %arg9: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"} loc("p9.139"), %arg10: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"} loc("p10.160"), %arg11: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"} loc("p11.169"), %arg12: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"} loc("p12.177"), %arg13: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"} loc("p13.182"), %arg14: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"} loc("p14.190"), %arg15: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"} loc("p15.220"), %arg16: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"} loc("p16.254"), %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"} loc("p17.269"), %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"} loc("p18.371"), %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"} loc("p19.383"), %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"} loc("p20.435")) -> (tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64> loc(#loc)
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64> loc(#loc)
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64> loc(#loc)
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32> loc(#loc)
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32> loc(#loc)
    %c_5 = stablehlo.constant dense<1> : tensor<i64> loc(#loc)
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16> loc(#loc)
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64> loc(#loc)
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32> loc(#loc)
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64> loc(#loc22)
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64> loc(#loc23)
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1> loc(#loc24)
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64> loc(#loc25)
    %7 = stablehlo.add %4, %6 : tensor<7xi64> loc(#loc26)
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64> loc(#loc27)
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64> loc(#loc28)
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc29)
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc30)
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32> loc(#loc31)
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32> loc(#loc32)
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16> loc(#loc33)
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16> loc(#loc34)
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64> loc(#loc35)
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64> loc(#loc36)
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32> loc(#loc37)
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16> loc(#loc38)
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16> loc(#loc39)
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32> loc(#loc40)
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32> loc(#loc41)
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32> loc(#loc42)
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32> loc(#loc43)
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32> loc(#loc44)
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32> loc(#loc45)
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32> loc(#loc46)
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32> loc(#loc47)
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32> loc(#loc48)
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32> loc(#loc49)
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32> loc(#loc50)
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16> loc(#loc51)
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32> loc(#loc52)
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32> loc(#loc53)
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16> loc(#loc54)
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16> loc(#loc55)
    %37 = stablehlo.reshape %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16> loc(#loc56)
    %38 = stablehlo.reshape %37 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16> loc(#loc57)
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16> loc(#loc58)
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16> loc(#loc59)
    %41 = stablehlo.reshape %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16> loc(#loc60)
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16> loc(#loc61)
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32> loc(#loc62)
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32> loc(#loc63)
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32> loc(#loc64)
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32> loc(#loc65)
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32> loc(#loc66)
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32> loc(#loc67)
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32> loc(#loc68)
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32> loc(#loc69)
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16> loc(#loc70)
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16> loc(#loc71)
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32> loc(#loc72)
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32> loc(#loc73)
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32> loc(#loc74)
    %56 = stablehlo.multiply %43, %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x128xf32> loc(#loc75)
    %57 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16> loc(#loc76)
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16> loc(#loc77)
    %59 = stablehlo.negate %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x64xbf16> loc(#loc78)
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16> loc(#loc79)
    %61 = stablehlo.concatenate %59, %60, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16> loc(#loc80)
    %62 = stablehlo.convert %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32> loc(#loc81)
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32> loc(#loc82)
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16> loc(#loc83)
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16> loc(#loc84)
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32> loc(#loc85)
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32> loc(#loc86)
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32> loc(#loc87)
    %69 = stablehlo.multiply %62, %68 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x128xf32> loc(#loc88)
    %70 = stablehlo.convert %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16> loc(#loc89)
    %71 = stablehlo.add %57, %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x128xbf16> loc(#loc90)
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16> loc("scatter.136"), %arg22: tensor<bf16> loc("scatter.136")):
      stablehlo.return %arg22 : tensor<bf16> loc(#loc)
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16> loc(#loc91)
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16> loc(#loc92)
    %74 = stablehlo.reshape %arg9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16> loc(#loc93)
    %75 = stablehlo.reshape %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16> loc(#loc94)
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16> loc(#loc95)
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16> loc(#loc96)
    %78 = stablehlo.reshape %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16> loc(#loc97)
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16> loc(#loc98)
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16> loc("scatter.166"), %arg22: tensor<bf16> loc("scatter.166")):
      stablehlo.return %arg22 : tensor<bf16> loc(#loc)
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16> loc(#loc99)
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16> loc(#loc100)
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc101)
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc102)
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32> loc(#loc103)
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32> loc(#loc104)
    %86 = stablehlo.reshape %arg17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16> loc(#loc105)
    %87 = stablehlo.reshape %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16> loc(#loc106)
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16> loc(#loc107)
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16> loc(#loc108)
    %90 = stablehlo.reshape %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>} : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16> loc(#loc109)
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16> loc(#loc110)
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32> loc(#loc111)
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32> loc(#loc112)
    %94 = stablehlo.multiply %92, %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x128xf32> loc(#loc113)
    %95 = stablehlo.convert %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16> loc(#loc114)
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16> loc(#loc115)
    %97 = stablehlo.negate %96 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x64xbf16> loc(#loc116)
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16> loc(#loc117)
    %99 = stablehlo.concatenate %97, %98, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16> loc(#loc118)
    %100 = stablehlo.convert %99 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32> loc(#loc119)
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32> loc(#loc120)
    %102 = stablehlo.multiply %100, %101 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x128xf32> loc(#loc121)
    %103 = stablehlo.convert %102 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16> loc(#loc122)
    %104 = stablehlo.add %95, %103 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x128xbf16> loc(#loc123)
    %105 = stablehlo.reshape %104 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16> loc(#loc124)
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16> loc(#loc125)
    %107 = stablehlo.reshape %106 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16> loc(#loc126)
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16> loc(#loc127)
    %109 = stablehlo.reshape %108 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16> loc(#loc128)
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16> loc(#loc129)
    %111 = stablehlo.reshape %110 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16> loc(#loc130)
    %112 = stablehlo.convert %111 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32> loc(#loc131)
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<f32>) -> tensor<1x24x7x16xf32> loc(#loc132)
    %114 = stablehlo.multiply %112, %113 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32> loc(#loc133)
    %115 = stablehlo.convert %114 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16> loc(#loc134)
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64> loc(#loc135)
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64> loc(#loc136)
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64> loc(#loc137)
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1> loc(#loc138)
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16> loc(#loc139)
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16> loc(#loc140)
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32> loc(#loc141)
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64> loc(#loc142)
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1> loc(#loc143)
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32> loc(#loc144)
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32> loc(#loc145)
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16> loc(#loc146)
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16> loc(#loc147)
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16> loc(#loc148)
    %130 = stablehlo.add %115, %129 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xbf16> loc(#loc149)
    %131 = stablehlo.convert %130 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32> loc(#loc150)
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32> loc(#loc151)
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32> loc(#loc152)
    %134 = stablehlo.subtract %131, %133 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32> loc(#loc153)
    %135 = stablehlo.exponential %134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32> loc(#loc154)
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32> loc(#loc155)
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32> loc(#loc156)
    %138 = stablehlo.divide %135, %137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32> loc(#loc157)
    %139 = stablehlo.convert %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16> loc(#loc158)
    %140 = stablehlo.reshape %139 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16> loc(#loc159)
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16> loc(#loc160)
    %142 = stablehlo.reshape %141 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16> loc(#loc161)
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16> loc(#loc162)
    %144 = stablehlo.reshape %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16> loc(#loc163)
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16> loc(#loc164)
    %146 = stablehlo.reshape %145 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16> loc(#loc165)
    %147 = stablehlo.reshape %arg14 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16> loc(#loc166)
    %148 = stablehlo.reshape %147 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16> loc(#loc167)
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16> loc(#loc168)
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16> loc(#loc169)
    %151 = sdy.all_reduce {"_axis_0"} %150 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16> loc(#loc169)
    %152 = stablehlo.reshape %151 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16> loc(#loc170)
    %153 = stablehlo.add %20, %152 : tensor<1x7x3072xbf16> loc(#loc171)
    %154 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc172)
    %155 = stablehlo.reshape %154 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc173)
    %156 = stablehlo.convert %155 : (tensor<3072xbf16>) -> tensor<3072xf32> loc(#loc174)
    %157 = stablehlo.broadcast_in_dim %156, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32> loc(#loc175)
    %158 = stablehlo.convert %153 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32> loc(#loc176)
    %159 = stablehlo.power %158, %2 : tensor<1x7x3072xf32> loc(#loc177)
    %160 = stablehlo.reduce(%159 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32> loc(#loc178)
    %161 = stablehlo.multiply %160, %cst_4 : tensor<1x7xf32> loc(#loc179)
    %162 = stablehlo.reshape %161 : (tensor<1x7xf32>) -> tensor<1x7x1xf32> loc(#loc180)
    %163 = stablehlo.add %162, %26 : tensor<1x7x1xf32> loc(#loc181)
    %164 = stablehlo.rsqrt %163 : tensor<1x7x1xf32> loc(#loc182)
    %165 = stablehlo.reshape %164 : (tensor<1x7x1xf32>) -> tensor<1x7xf32> loc(#loc183)
    %166 = stablehlo.broadcast_in_dim %165, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32> loc(#loc184)
    %167 = stablehlo.multiply %158, %166 : tensor<1x7x3072xf32> loc(#loc185)
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16> loc(#loc186)
    %169 = stablehlo.convert %168 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32> loc(#loc187)
    %170 = stablehlo.multiply %157, %169 : tensor<1x7x3072xf32> loc(#loc188)
    %171 = stablehlo.convert %170 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16> loc(#loc189)
    %172 = stablehlo.reshape %171 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16> loc(#loc190)
    %173 = stablehlo.reshape %arg19 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16> loc(#loc191)
    %174 = stablehlo.reshape %173 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16> loc(#loc192)
    %175 = stablehlo.transpose %174, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16> loc(#loc193)
    %176 = stablehlo.dot_general %172, %175, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16> loc(#loc194)
    %177 = stablehlo.reshape %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16> loc(#loc195)
    %178 = stablehlo.convert %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32> loc(#loc196)
    %179 = stablehlo.logistic %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : tensor<1x7x8192xbf16> loc(#loc197)
    %180 = stablehlo.convert %179 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32> loc(#loc198)
    %181 = stablehlo.multiply %178, %180 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : tensor<1x7x8192xf32> loc(#loc199)
    %182 = stablehlo.convert %181 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16> loc(#loc200)
    %183 = stablehlo.convert %182 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32> loc(#loc201)
    %184 = stablehlo.reshape %arg13 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16> loc(#loc202)
    %185 = stablehlo.reshape %184 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16> loc(#loc203)
    %186 = stablehlo.transpose %185, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16> loc(#loc204)
    %187 = stablehlo.dot_general %172, %186, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16> loc(#loc205)
    %188 = stablehlo.reshape %187 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16> loc(#loc206)
    %189 = stablehlo.convert %188 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32> loc(#loc207)
    %190 = stablehlo.multiply %183, %189 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : tensor<1x7x8192xf32> loc(#loc208)
    %191 = stablehlo.convert %190 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16> loc(#loc209)
    %192 = stablehlo.reshape %191 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16> loc(#loc210)
    %193 = stablehlo.reshape %arg12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16> loc(#loc211)
    %194 = stablehlo.reshape %193 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16> loc(#loc212)
    %195 = stablehlo.transpose %194, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16> loc(#loc213)
    %196 = stablehlo.dot_general %192, %195, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16> loc(#loc214)
    %197 = sdy.all_reduce {"_axis_0"} %196 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16> loc(#loc214)
    %198 = stablehlo.reshape %197 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16> loc(#loc215)
    %199 = stablehlo.add %153, %198 : tensor<1x7x3072xbf16> loc(#loc216)
    %200 = stablehlo.convert %199 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32> loc(#loc217)
    %201 = stablehlo.power %200, %2 : tensor<1x7x3072xf32> loc(#loc218)
    %202 = stablehlo.reduce(%201 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32> loc(#loc219)
    %203 = stablehlo.multiply %202, %cst_4 : tensor<1x7xf32> loc(#loc220)
    %204 = stablehlo.reshape %203 : (tensor<1x7xf32>) -> tensor<1x7x1xf32> loc(#loc221)
    %205 = stablehlo.add %204, %26 : tensor<1x7x1xf32> loc(#loc222)
    %206 = stablehlo.rsqrt %205 : tensor<1x7x1xf32> loc(#loc223)
    %207 = stablehlo.reshape %206 : (tensor<1x7x1xf32>) -> tensor<1x7xf32> loc(#loc224)
    %208 = stablehlo.broadcast_in_dim %207, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32> loc(#loc225)
    %209 = stablehlo.multiply %200, %208 : tensor<1x7x3072xf32> loc(#loc226)
    %210 = stablehlo.convert %209 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16> loc(#loc227)
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32> loc(#loc228)
    %212 = stablehlo.multiply %85, %211 : tensor<1x7x3072xf32> loc(#loc229)
    %213 = stablehlo.convert %212 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16> loc(#loc230)
    %214 = stablehlo.reshape %213 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16> loc(#loc231)
    %215 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16> loc(#loc232)
    %216 = stablehlo.reshape %215 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16> loc(#loc233)
    %217 = stablehlo.transpose %216, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16> loc(#loc234)
    %218 = stablehlo.dot_general %214, %217, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16> loc(#loc235)
    %219 = stablehlo.reshape %218 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16> loc(#loc236)
    return %73, %81, %218, %219 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc22 = loc("reshape.4")
#loc23 = loc("reshape.6")
#loc24 = loc("compare.126")
#loc25 = loc("broadcast.122")
#loc26 = loc("add.123")
#loc27 = loc("select.127")
#loc28 = loc("reshape.128")
#loc29 = loc("reshape.82")
#loc30 = loc("reshape.84")
#loc31 = loc("convert.85")
#loc32 = loc("broadcast.86")
#loc33 = loc("reshape.45")
#loc34 = loc("reshape.47")
#loc35 = loc("reshape.40")
#loc36 = loc("reshape.43")
#loc37 = loc("convert.48")
#loc38 = loc("gather.49")
#loc39 = loc("reshape.50")
#loc40 = loc("convert.51")
#loc41 = loc("power.53")
#loc42 = loc("reduce.60")
#loc43 = loc("multiply.69")
#loc44 = loc("reshape.70")
#loc45 = loc("broadcast.73")
#loc46 = loc("add.74")
#loc47 = loc("rsqrt.75")
#loc48 = loc("reshape.76")
#loc49 = loc("broadcast.77")
#loc50 = loc("multiply.78")
#loc51 = loc("convert.79")
#loc52 = loc("convert.80")
#loc53 = loc("multiply.87")
#loc54 = loc("convert.88")
#loc55 = loc("reshape.89")
#loc56 = loc("reshape.32")
#loc57 = loc("reshape.34")
#loc58 = loc("transpose.35")
#loc59 = loc("dot.90")
#loc60 = loc("reshape.92")
#loc61 = loc("transpose.93")
#loc62 = loc("convert.110")
#loc63 = loc("reshape.14")
#loc64 = loc("reshape.19")
#loc65 = loc("convert.11")
#loc66 = loc("dot.22")
#loc67 = loc("transpose.23")
#loc68 = loc("concatenate.24")
#loc69 = loc("cosine.104")
#loc70 = loc("convert.107")
#loc71 = loc("reshape.108")
#loc72 = loc("convert.109")
#loc73 = loc("reshape.111")
#loc74 = loc("broadcast.112")
#loc75 = loc("multiply.113")
#loc76 = loc("convert.114")
#loc77 = loc("slice.95")
#loc78 = loc("negate.96")
#loc79 = loc("slice.94")
#loc80 = loc("concatenate.97")
#loc81 = loc("convert.98")
#loc82 = loc("sine.25")
#loc83 = loc("convert.28")
#loc84 = loc("reshape.29")
#loc85 = loc("convert.30")
#loc86 = loc("reshape.99")
#loc87 = loc("broadcast.100")
#loc88 = loc("multiply.101")
#loc89 = loc("convert.102")
#loc90 = loc("add.117")
#loc92 = loc("custom-call.138")
#loc93 = loc("reshape.140")
#loc94 = loc("reshape.142")
#loc95 = loc("transpose.143")
#loc96 = loc("dot.145")
#loc97 = loc("reshape.147")
#loc98 = loc("transpose.148")
#loc100 = loc("custom-call.168")
#loc101 = loc("reshape.436")
#loc102 = loc("reshape.438")
#loc103 = loc("convert.439")
#loc104 = loc("broadcast.440")
#loc105 = loc("reshape.270")
#loc106 = loc("reshape.272")
#loc107 = loc("transpose.273")
#loc108 = loc("dot.275")
#loc109 = loc("reshape.277")
#loc110 = loc("transpose.278")
#loc111 = loc("convert.289")
#loc112 = loc("broadcast.291")
#loc113 = loc("multiply.292")
#loc114 = loc("convert.293")
#loc115 = loc("slice.280")
#loc116 = loc("negate.281")
#loc117 = loc("slice.279")
#loc118 = loc("concatenate.282")
#loc119 = loc("convert.283")
#loc120 = loc("broadcast.285")
#loc121 = loc("multiply.286")
#loc122 = loc("convert.287")
#loc123 = loc("add.296")
#loc124 = loc("reshape.298")
#loc125 = loc("broadcast.262")
#loc126 = loc("reshape.263")
#loc127 = loc("transpose.264")
#loc128 = loc("reshape.266")
#loc129 = loc("dot.299")
#loc130 = loc("reshape.300")
#loc131 = loc("convert.301")
#loc132 = loc("broadcast.302")
#loc133 = loc("multiply.303")
#loc134 = loc("convert.304")
#loc135 = loc("broadcast.235")
#loc136 = loc("broadcast.237")
#loc137 = loc("subtract.238")
#loc138 = loc("compare.240")
#loc139 = loc("broadcast.224")
#loc140 = loc("select.242")
#loc141 = loc("convert.243")
#loc142 = loc("broadcast.211")
#loc143 = loc("compare.212")
#loc144 = loc("convert.213")
#loc145 = loc("multiply.244")
#loc146 = loc("convert.245")
#loc147 = loc("reshape.246")
#loc148 = loc("broadcast.308")
#loc149 = loc("add.309")
#loc150 = loc("convert.310")
#loc151 = loc("reduce.316")
#loc152 = loc("broadcast.317")
#loc153 = loc("subtract.318")
#loc154 = loc("exponential.319")
#loc155 = loc("reduce.325")
#loc156 = loc("broadcast.326")
#loc157 = loc("divide.327")
#loc158 = loc("convert.328")
#loc159 = loc("reshape.330")
#loc160 = loc("broadcast.202")
#loc161 = loc("reshape.205")
#loc162 = loc("dot.331")
#loc163 = loc("reshape.332")
#loc164 = loc("transpose.333")
#loc165 = loc("reshape.335")
#loc166 = loc("reshape.191")
#loc167 = loc("reshape.193")
#loc168 = loc("transpose.194")
#loc169 = loc("dot.336")
#loc170 = loc("reshape.337")
#loc171 = loc("add.340")
#loc172 = loc("reshape.372")
#loc173 = loc("reshape.374")
#loc174 = loc("convert.375")
#loc175 = loc("broadcast.376")
#loc176 = loc("convert.341")
#loc177 = loc("power.343")
#loc178 = loc("reduce.350")
#loc179 = loc("multiply.359")
#loc180 = loc("reshape.360")
#loc181 = loc("add.364")
#loc182 = loc("rsqrt.365")
#loc183 = loc("reshape.366")
#loc184 = loc("broadcast.367")
#loc185 = loc("multiply.368")
#loc186 = loc("convert.369")
#loc187 = loc("convert.370")
#loc188 = loc("multiply.377")
#loc189 = loc("convert.378")
#loc190 = loc("reshape.388")
#loc191 = loc("reshape.384")
#loc192 = loc("reshape.386")
#loc193 = loc("transpose.387")
#loc194 = loc("dot.389")
#loc195 = loc("reshape.390")
#loc196 = loc("convert.393")
#loc197 = loc("logistic.391")
#loc198 = loc("convert.392")
#loc199 = loc("multiply.394")
#loc200 = loc("convert.395")
#loc201 = loc("convert.396")
#loc202 = loc("reshape.183")
#loc203 = loc("reshape.185")
#loc204 = loc("transpose.186")
#loc205 = loc("dot.380")
#loc206 = loc("reshape.381")
#loc207 = loc("convert.382")
#loc208 = loc("multiply.397")
#loc209 = loc("convert.398")
#loc210 = loc("reshape.399")
#loc211 = loc("reshape.178")
#loc212 = loc("reshape.180")
#loc213 = loc("transpose.181")
#loc214 = loc("dot.400")
#loc215 = loc("reshape.401")
#loc216 = loc("add.404")
#loc217 = loc("convert.405")
#loc218 = loc("power.407")
#loc219 = loc("reduce.414")
#loc220 = loc("multiply.423")
#loc221 = loc("reshape.424")
#loc222 = loc("add.428")
#loc223 = loc("rsqrt.429")
#loc224 = loc("reshape.430")
#loc225 = loc("broadcast.431")
#loc226 = loc("multiply.432")
#loc227 = loc("convert.433")
#loc228 = loc("convert.434")
#loc229 = loc("multiply.441")
#loc230 = loc("convert.442")
#loc231 = loc("reshape.446")
#loc232 = loc("reshape.170")
#loc233 = loc("reshape.172")
#loc234 = loc("transpose.173")
#loc235 = loc("dot.447")
#loc236 = loc("reshape.448")
// -----// IR Dump Before ConvertArithToStableHLO (convert-arith-to-stablehlo) ('builtin.module' operation: @SyncTensorsGraph.450) //----- //
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg1: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg2: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg3: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg4: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg5: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg7: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg8: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg9: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg10: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg11: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg12: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg13: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg14: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg15: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg16: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64>
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %7 = stablehlo.add %4, %6 : tensor<7xi64>
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64>
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64>
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32>
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32>
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32>
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32>
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32>
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %37 = stablehlo.reshape %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %38 = stablehlo.reshape %37 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %56 = stablehlo.multiply %43, %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x128xf32>
    %57 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.convert %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32>
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %69 = stablehlo.multiply %62, %68 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x128xf32>
    %70 = stablehlo.convert %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %71 = stablehlo.add %57, %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x128xbf16>
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %74 = stablehlo.reshape %arg9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %86 = stablehlo.reshape %arg17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %87 = stablehlo.reshape %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %90 = stablehlo.reshape %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>} : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %94 = stablehlo.multiply %92, %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x128xf32>
    %95 = stablehlo.convert %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %97 = stablehlo.negate %96 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x64xbf16>
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %99 = stablehlo.concatenate %97, %98, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %100 = stablehlo.convert %99 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.multiply %100, %101 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %95, %103 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x128xbf16>
    %105 = stablehlo.reshape %104 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %107 = stablehlo.reshape %106 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16>
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16>
    %109 = stablehlo.reshape %108 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16>
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16>
    %111 = stablehlo.reshape %110 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %112 = stablehlo.convert %111 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<f32>) -> tensor<1x24x7x16xf32>
    %114 = stablehlo.multiply %112, %113 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %115 = stablehlo.convert %114 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64>
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64>
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16>
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32>
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32>
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32>
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16>
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %130 = stablehlo.add %115, %129 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xbf16>
    %131 = stablehlo.convert %130 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %134 = stablehlo.subtract %131, %133 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %135 = stablehlo.exponential %134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %138 = stablehlo.divide %135, %137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %139 = stablehlo.convert %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %140 = stablehlo.reshape %139 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16>
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %142 = stablehlo.reshape %141 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16>
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16>
    %144 = stablehlo.reshape %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %146 = stablehlo.reshape %145 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %147 = stablehlo.reshape %arg14 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %148 = stablehlo.reshape %147 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %151 = sdy.all_reduce {"_axis_0"} %150 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %152 = stablehlo.reshape %151 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %153 = stablehlo.add %20, %152 : tensor<1x7x3072xbf16>
    %154 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %155 = stablehlo.reshape %154 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %156 = stablehlo.convert %155 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %157 = stablehlo.broadcast_in_dim %156, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %158 = stablehlo.convert %153 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %159 = stablehlo.power %158, %2 : tensor<1x7x3072xf32>
    %160 = stablehlo.reduce(%159 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %161 = stablehlo.multiply %160, %cst_4 : tensor<1x7xf32>
    %162 = stablehlo.reshape %161 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %163 = stablehlo.add %162, %26 : tensor<1x7x1xf32>
    %164 = stablehlo.rsqrt %163 : tensor<1x7x1xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %166 = stablehlo.broadcast_in_dim %165, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %167 = stablehlo.multiply %158, %166 : tensor<1x7x3072xf32>
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %169 = stablehlo.convert %168 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %170 = stablehlo.multiply %157, %169 : tensor<1x7x3072xf32>
    %171 = stablehlo.convert %170 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %172 = stablehlo.reshape %171 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %173 = stablehlo.reshape %arg19 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %174 = stablehlo.reshape %173 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %175 = stablehlo.transpose %174, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %176 = stablehlo.dot_general %172, %175, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %177 = stablehlo.reshape %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %178 = stablehlo.convert %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %179 = stablehlo.logistic %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : tensor<1x7x8192xbf16>
    %180 = stablehlo.convert %179 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %181 = stablehlo.multiply %178, %180 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : tensor<1x7x8192xf32>
    %182 = stablehlo.convert %181 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %183 = stablehlo.convert %182 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %184 = stablehlo.reshape %arg13 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %185 = stablehlo.reshape %184 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %186 = stablehlo.transpose %185, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %187 = stablehlo.dot_general %172, %186, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %188 = stablehlo.reshape %187 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %189 = stablehlo.convert %188 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %190 = stablehlo.multiply %183, %189 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : tensor<1x7x8192xf32>
    %191 = stablehlo.convert %190 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %192 = stablehlo.reshape %191 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %193 = stablehlo.reshape %arg12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %194 = stablehlo.reshape %193 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %195 = stablehlo.transpose %194, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %196 = stablehlo.dot_general %192, %195, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %197 = sdy.all_reduce {"_axis_0"} %196 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %198 = stablehlo.reshape %197 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %199 = stablehlo.add %153, %198 : tensor<1x7x3072xbf16>
    %200 = stablehlo.convert %199 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %201 = stablehlo.power %200, %2 : tensor<1x7x3072xf32>
    %202 = stablehlo.reduce(%201 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %203 = stablehlo.multiply %202, %cst_4 : tensor<1x7xf32>
    %204 = stablehlo.reshape %203 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %205 = stablehlo.add %204, %26 : tensor<1x7x1xf32>
    %206 = stablehlo.rsqrt %205 : tensor<1x7x1xf32>
    %207 = stablehlo.reshape %206 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %208 = stablehlo.broadcast_in_dim %207, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %209 = stablehlo.multiply %200, %208 : tensor<1x7x3072xf32>
    %210 = stablehlo.convert %209 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %212 = stablehlo.multiply %85, %211 : tensor<1x7x3072xf32>
    %213 = stablehlo.convert %212 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %214 = stablehlo.reshape %213 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %215 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %216 = stablehlo.reshape %215 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %217 = stablehlo.transpose %216, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %218 = stablehlo.dot_general %214, %217, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %219 = stablehlo.reshape %218 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %73, %81, %218, %219 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump Before LegalizeStableHLOCompositeToTTIR (legalize-stablehlo-composite-to-ttir) ('builtin.module' operation: @SyncTensorsGraph.450) //----- //
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg1: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg2: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg3: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg4: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg5: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg7: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg8: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg9: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg10: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg11: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg12: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg13: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg14: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg15: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg16: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64>
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %7 = stablehlo.add %4, %6 : tensor<7xi64>
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64>
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64>
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32>
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32>
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32>
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32>
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32>
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %37 = stablehlo.reshape %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %38 = stablehlo.reshape %37 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %56 = stablehlo.multiply %43, %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x128xf32>
    %57 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.convert %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32>
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %69 = stablehlo.multiply %62, %68 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x128xf32>
    %70 = stablehlo.convert %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %71 = stablehlo.add %57, %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x128xbf16>
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %74 = stablehlo.reshape %arg9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %86 = stablehlo.reshape %arg17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %87 = stablehlo.reshape %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %90 = stablehlo.reshape %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>} : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %94 = stablehlo.multiply %92, %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x128xf32>
    %95 = stablehlo.convert %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %97 = stablehlo.negate %96 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x64xbf16>
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %99 = stablehlo.concatenate %97, %98, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %100 = stablehlo.convert %99 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.multiply %100, %101 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %95, %103 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x128xbf16>
    %105 = stablehlo.reshape %104 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %107 = stablehlo.reshape %106 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16>
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16>
    %109 = stablehlo.reshape %108 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16>
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16>
    %111 = stablehlo.reshape %110 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %112 = stablehlo.convert %111 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<f32>) -> tensor<1x24x7x16xf32>
    %114 = stablehlo.multiply %112, %113 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %115 = stablehlo.convert %114 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64>
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64>
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16>
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32>
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32>
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32>
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16>
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %130 = stablehlo.add %115, %129 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xbf16>
    %131 = stablehlo.convert %130 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %134 = stablehlo.subtract %131, %133 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %135 = stablehlo.exponential %134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %138 = stablehlo.divide %135, %137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %139 = stablehlo.convert %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %140 = stablehlo.reshape %139 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16>
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %142 = stablehlo.reshape %141 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16>
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16>
    %144 = stablehlo.reshape %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %146 = stablehlo.reshape %145 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %147 = stablehlo.reshape %arg14 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %148 = stablehlo.reshape %147 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %151 = sdy.all_reduce {"_axis_0"} %150 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %152 = stablehlo.reshape %151 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %153 = stablehlo.add %20, %152 : tensor<1x7x3072xbf16>
    %154 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %155 = stablehlo.reshape %154 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %156 = stablehlo.convert %155 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %157 = stablehlo.broadcast_in_dim %156, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %158 = stablehlo.convert %153 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %159 = stablehlo.power %158, %2 : tensor<1x7x3072xf32>
    %160 = stablehlo.reduce(%159 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %161 = stablehlo.multiply %160, %cst_4 : tensor<1x7xf32>
    %162 = stablehlo.reshape %161 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %163 = stablehlo.add %162, %26 : tensor<1x7x1xf32>
    %164 = stablehlo.rsqrt %163 : tensor<1x7x1xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %166 = stablehlo.broadcast_in_dim %165, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %167 = stablehlo.multiply %158, %166 : tensor<1x7x3072xf32>
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %169 = stablehlo.convert %168 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %170 = stablehlo.multiply %157, %169 : tensor<1x7x3072xf32>
    %171 = stablehlo.convert %170 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %172 = stablehlo.reshape %171 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %173 = stablehlo.reshape %arg19 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %174 = stablehlo.reshape %173 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %175 = stablehlo.transpose %174, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %176 = stablehlo.dot_general %172, %175, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %177 = stablehlo.reshape %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %178 = stablehlo.convert %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %179 = stablehlo.logistic %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : tensor<1x7x8192xbf16>
    %180 = stablehlo.convert %179 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %181 = stablehlo.multiply %178, %180 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : tensor<1x7x8192xf32>
    %182 = stablehlo.convert %181 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %183 = stablehlo.convert %182 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %184 = stablehlo.reshape %arg13 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %185 = stablehlo.reshape %184 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %186 = stablehlo.transpose %185, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %187 = stablehlo.dot_general %172, %186, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %188 = stablehlo.reshape %187 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %189 = stablehlo.convert %188 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %190 = stablehlo.multiply %183, %189 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : tensor<1x7x8192xf32>
    %191 = stablehlo.convert %190 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %192 = stablehlo.reshape %191 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %193 = stablehlo.reshape %arg12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %194 = stablehlo.reshape %193 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %195 = stablehlo.transpose %194, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %196 = stablehlo.dot_general %192, %195, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %197 = sdy.all_reduce {"_axis_0"} %196 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %198 = stablehlo.reshape %197 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %199 = stablehlo.add %153, %198 : tensor<1x7x3072xbf16>
    %200 = stablehlo.convert %199 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %201 = stablehlo.power %200, %2 : tensor<1x7x3072xf32>
    %202 = stablehlo.reduce(%201 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %203 = stablehlo.multiply %202, %cst_4 : tensor<1x7xf32>
    %204 = stablehlo.reshape %203 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %205 = stablehlo.add %204, %26 : tensor<1x7x1xf32>
    %206 = stablehlo.rsqrt %205 : tensor<1x7x1xf32>
    %207 = stablehlo.reshape %206 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %208 = stablehlo.broadcast_in_dim %207, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %209 = stablehlo.multiply %200, %208 : tensor<1x7x3072xf32>
    %210 = stablehlo.convert %209 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %212 = stablehlo.multiply %85, %211 : tensor<1x7x3072xf32>
    %213 = stablehlo.convert %212 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %214 = stablehlo.reshape %213 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %215 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %216 = stablehlo.reshape %215 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %217 = stablehlo.transpose %216, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %218 = stablehlo.dot_general %214, %217, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %219 = stablehlo.reshape %218 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %73, %81, %218, %219 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump Before StablehloLegalizeCompositeToCallPass (stablehlo-legalize-composite-to-call) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg1: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg2: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg3: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg4: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg5: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg7: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg8: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg9: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg10: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg11: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg12: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg13: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg14: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg15: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg16: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64>
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %7 = stablehlo.add %4, %6 : tensor<7xi64>
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64>
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64>
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32>
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32>
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32>
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32>
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32>
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %37 = stablehlo.reshape %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %38 = stablehlo.reshape %37 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %56 = stablehlo.multiply %43, %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x128xf32>
    %57 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.convert %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32>
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %69 = stablehlo.multiply %62, %68 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x128xf32>
    %70 = stablehlo.convert %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %71 = stablehlo.add %57, %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x128xbf16>
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %74 = stablehlo.reshape %arg9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %86 = stablehlo.reshape %arg17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %87 = stablehlo.reshape %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %90 = stablehlo.reshape %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>} : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %94 = stablehlo.multiply %92, %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x128xf32>
    %95 = stablehlo.convert %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %97 = stablehlo.negate %96 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x64xbf16>
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %99 = stablehlo.concatenate %97, %98, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %100 = stablehlo.convert %99 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.multiply %100, %101 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %95, %103 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x128xbf16>
    %105 = stablehlo.reshape %104 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %107 = stablehlo.reshape %106 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16>
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16>
    %109 = stablehlo.reshape %108 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16>
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16>
    %111 = stablehlo.reshape %110 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %112 = stablehlo.convert %111 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<f32>) -> tensor<1x24x7x16xf32>
    %114 = stablehlo.multiply %112, %113 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %115 = stablehlo.convert %114 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64>
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64>
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16>
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32>
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32>
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32>
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16>
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %130 = stablehlo.add %115, %129 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xbf16>
    %131 = stablehlo.convert %130 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %134 = stablehlo.subtract %131, %133 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %135 = stablehlo.exponential %134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %138 = stablehlo.divide %135, %137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %139 = stablehlo.convert %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %140 = stablehlo.reshape %139 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16>
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %142 = stablehlo.reshape %141 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16>
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16>
    %144 = stablehlo.reshape %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %146 = stablehlo.reshape %145 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %147 = stablehlo.reshape %arg14 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %148 = stablehlo.reshape %147 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %151 = sdy.all_reduce {"_axis_0"} %150 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %152 = stablehlo.reshape %151 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %153 = stablehlo.add %20, %152 : tensor<1x7x3072xbf16>
    %154 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %155 = stablehlo.reshape %154 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %156 = stablehlo.convert %155 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %157 = stablehlo.broadcast_in_dim %156, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %158 = stablehlo.convert %153 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %159 = stablehlo.power %158, %2 : tensor<1x7x3072xf32>
    %160 = stablehlo.reduce(%159 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %161 = stablehlo.multiply %160, %cst_4 : tensor<1x7xf32>
    %162 = stablehlo.reshape %161 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %163 = stablehlo.add %162, %26 : tensor<1x7x1xf32>
    %164 = stablehlo.rsqrt %163 : tensor<1x7x1xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %166 = stablehlo.broadcast_in_dim %165, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %167 = stablehlo.multiply %158, %166 : tensor<1x7x3072xf32>
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %169 = stablehlo.convert %168 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %170 = stablehlo.multiply %157, %169 : tensor<1x7x3072xf32>
    %171 = stablehlo.convert %170 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %172 = stablehlo.reshape %171 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %173 = stablehlo.reshape %arg19 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %174 = stablehlo.reshape %173 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %175 = stablehlo.transpose %174, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %176 = stablehlo.dot_general %172, %175, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %177 = stablehlo.reshape %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %178 = stablehlo.convert %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %179 = stablehlo.logistic %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : tensor<1x7x8192xbf16>
    %180 = stablehlo.convert %179 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %181 = stablehlo.multiply %178, %180 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : tensor<1x7x8192xf32>
    %182 = stablehlo.convert %181 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %183 = stablehlo.convert %182 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %184 = stablehlo.reshape %arg13 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %185 = stablehlo.reshape %184 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %186 = stablehlo.transpose %185, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %187 = stablehlo.dot_general %172, %186, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %188 = stablehlo.reshape %187 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %189 = stablehlo.convert %188 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %190 = stablehlo.multiply %183, %189 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : tensor<1x7x8192xf32>
    %191 = stablehlo.convert %190 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %192 = stablehlo.reshape %191 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %193 = stablehlo.reshape %arg12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %194 = stablehlo.reshape %193 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %195 = stablehlo.transpose %194, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %196 = stablehlo.dot_general %192, %195, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %197 = sdy.all_reduce {"_axis_0"} %196 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %198 = stablehlo.reshape %197 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %199 = stablehlo.add %153, %198 : tensor<1x7x3072xbf16>
    %200 = stablehlo.convert %199 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %201 = stablehlo.power %200, %2 : tensor<1x7x3072xf32>
    %202 = stablehlo.reduce(%201 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %203 = stablehlo.multiply %202, %cst_4 : tensor<1x7xf32>
    %204 = stablehlo.reshape %203 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %205 = stablehlo.add %204, %26 : tensor<1x7x1xf32>
    %206 = stablehlo.rsqrt %205 : tensor<1x7x1xf32>
    %207 = stablehlo.reshape %206 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %208 = stablehlo.broadcast_in_dim %207, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %209 = stablehlo.multiply %200, %208 : tensor<1x7x3072xf32>
    %210 = stablehlo.convert %209 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %212 = stablehlo.multiply %85, %211 : tensor<1x7x3072xf32>
    %213 = stablehlo.convert %212 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %214 = stablehlo.reshape %213 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %215 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %216 = stablehlo.reshape %215 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %217 = stablehlo.transpose %216, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %218 = stablehlo.dot_general %214, %217, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %219 = stablehlo.reshape %218 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %73, %81, %218, %219 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump Before Inliner (inline) ('builtin.module' operation: @SyncTensorsGraph.450) //----- //
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg1: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg2: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg3: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg4: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg5: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg7: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg8: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg9: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg10: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg11: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg12: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg13: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg14: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg15: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg16: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64>
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %7 = stablehlo.add %4, %6 : tensor<7xi64>
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64>
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64>
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32>
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32>
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32>
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32>
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32>
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %37 = stablehlo.reshape %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %38 = stablehlo.reshape %37 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %56 = stablehlo.multiply %43, %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x128xf32>
    %57 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.convert %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32>
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %69 = stablehlo.multiply %62, %68 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x128xf32>
    %70 = stablehlo.convert %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %71 = stablehlo.add %57, %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x128xbf16>
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %74 = stablehlo.reshape %arg9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %86 = stablehlo.reshape %arg17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %87 = stablehlo.reshape %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %90 = stablehlo.reshape %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>} : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %94 = stablehlo.multiply %92, %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x128xf32>
    %95 = stablehlo.convert %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %97 = stablehlo.negate %96 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x64xbf16>
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %99 = stablehlo.concatenate %97, %98, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %100 = stablehlo.convert %99 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.multiply %100, %101 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %95, %103 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x128xbf16>
    %105 = stablehlo.reshape %104 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %107 = stablehlo.reshape %106 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16>
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16>
    %109 = stablehlo.reshape %108 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16>
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16>
    %111 = stablehlo.reshape %110 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %112 = stablehlo.convert %111 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<f32>) -> tensor<1x24x7x16xf32>
    %114 = stablehlo.multiply %112, %113 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %115 = stablehlo.convert %114 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64>
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64>
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16>
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32>
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32>
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32>
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16>
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %130 = stablehlo.add %115, %129 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xbf16>
    %131 = stablehlo.convert %130 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %134 = stablehlo.subtract %131, %133 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %135 = stablehlo.exponential %134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %138 = stablehlo.divide %135, %137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %139 = stablehlo.convert %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %140 = stablehlo.reshape %139 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16>
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %142 = stablehlo.reshape %141 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16>
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16>
    %144 = stablehlo.reshape %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %146 = stablehlo.reshape %145 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %147 = stablehlo.reshape %arg14 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %148 = stablehlo.reshape %147 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %151 = sdy.all_reduce {"_axis_0"} %150 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %152 = stablehlo.reshape %151 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %153 = stablehlo.add %20, %152 : tensor<1x7x3072xbf16>
    %154 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %155 = stablehlo.reshape %154 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %156 = stablehlo.convert %155 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %157 = stablehlo.broadcast_in_dim %156, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %158 = stablehlo.convert %153 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %159 = stablehlo.power %158, %2 : tensor<1x7x3072xf32>
    %160 = stablehlo.reduce(%159 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %161 = stablehlo.multiply %160, %cst_4 : tensor<1x7xf32>
    %162 = stablehlo.reshape %161 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %163 = stablehlo.add %162, %26 : tensor<1x7x1xf32>
    %164 = stablehlo.rsqrt %163 : tensor<1x7x1xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %166 = stablehlo.broadcast_in_dim %165, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %167 = stablehlo.multiply %158, %166 : tensor<1x7x3072xf32>
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %169 = stablehlo.convert %168 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %170 = stablehlo.multiply %157, %169 : tensor<1x7x3072xf32>
    %171 = stablehlo.convert %170 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %172 = stablehlo.reshape %171 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %173 = stablehlo.reshape %arg19 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %174 = stablehlo.reshape %173 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %175 = stablehlo.transpose %174, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %176 = stablehlo.dot_general %172, %175, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %177 = stablehlo.reshape %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %178 = stablehlo.convert %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %179 = stablehlo.logistic %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : tensor<1x7x8192xbf16>
    %180 = stablehlo.convert %179 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %181 = stablehlo.multiply %178, %180 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : tensor<1x7x8192xf32>
    %182 = stablehlo.convert %181 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %183 = stablehlo.convert %182 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %184 = stablehlo.reshape %arg13 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %185 = stablehlo.reshape %184 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %186 = stablehlo.transpose %185, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %187 = stablehlo.dot_general %172, %186, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %188 = stablehlo.reshape %187 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %189 = stablehlo.convert %188 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %190 = stablehlo.multiply %183, %189 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : tensor<1x7x8192xf32>
    %191 = stablehlo.convert %190 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %192 = stablehlo.reshape %191 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %193 = stablehlo.reshape %arg12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %194 = stablehlo.reshape %193 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %195 = stablehlo.transpose %194, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %196 = stablehlo.dot_general %192, %195, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %197 = sdy.all_reduce {"_axis_0"} %196 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %198 = stablehlo.reshape %197 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %199 = stablehlo.add %153, %198 : tensor<1x7x3072xbf16>
    %200 = stablehlo.convert %199 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %201 = stablehlo.power %200, %2 : tensor<1x7x3072xf32>
    %202 = stablehlo.reduce(%201 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %203 = stablehlo.multiply %202, %cst_4 : tensor<1x7xf32>
    %204 = stablehlo.reshape %203 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %205 = stablehlo.add %204, %26 : tensor<1x7x1xf32>
    %206 = stablehlo.rsqrt %205 : tensor<1x7x1xf32>
    %207 = stablehlo.reshape %206 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %208 = stablehlo.broadcast_in_dim %207, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %209 = stablehlo.multiply %200, %208 : tensor<1x7x3072xf32>
    %210 = stablehlo.convert %209 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %212 = stablehlo.multiply %85, %211 : tensor<1x7x3072xf32>
    %213 = stablehlo.convert %212 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %214 = stablehlo.reshape %213 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %215 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %216 = stablehlo.reshape %215 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %217 = stablehlo.transpose %216, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %218 = stablehlo.dot_general %214, %217, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %219 = stablehlo.reshape %218 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %73, %81, %218, %219 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg1: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg2: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg3: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg4: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg5: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg7: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg8: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg9: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg10: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg11: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg12: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg13: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg14: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg15: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg16: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64>
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %7 = stablehlo.add %4, %6 : tensor<7xi64>
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64>
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64>
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32>
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32>
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32>
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32>
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32>
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %37 = stablehlo.reshape %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %38 = stablehlo.reshape %37 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %56 = stablehlo.multiply %43, %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x128xf32>
    %57 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.convert %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32>
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %69 = stablehlo.multiply %62, %68 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x128xf32>
    %70 = stablehlo.convert %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %71 = stablehlo.add %57, %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x128xbf16>
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %74 = stablehlo.reshape %arg9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %86 = stablehlo.reshape %arg17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %87 = stablehlo.reshape %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %90 = stablehlo.reshape %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>} : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %94 = stablehlo.multiply %92, %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x128xf32>
    %95 = stablehlo.convert %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %97 = stablehlo.negate %96 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x64xbf16>
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %99 = stablehlo.concatenate %97, %98, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %100 = stablehlo.convert %99 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.multiply %100, %101 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %95, %103 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x128xbf16>
    %105 = stablehlo.reshape %104 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %107 = stablehlo.reshape %106 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16>
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16>
    %109 = stablehlo.reshape %108 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16>
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16>
    %111 = stablehlo.reshape %110 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %112 = stablehlo.convert %111 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<f32>) -> tensor<1x24x7x16xf32>
    %114 = stablehlo.multiply %112, %113 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %115 = stablehlo.convert %114 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64>
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64>
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16>
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32>
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32>
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32>
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16>
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %130 = stablehlo.add %115, %129 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xbf16>
    %131 = stablehlo.convert %130 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %134 = stablehlo.subtract %131, %133 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %135 = stablehlo.exponential %134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %138 = stablehlo.divide %135, %137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %139 = stablehlo.convert %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %140 = stablehlo.reshape %139 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16>
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %142 = stablehlo.reshape %141 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16>
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16>
    %144 = stablehlo.reshape %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %146 = stablehlo.reshape %145 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %147 = stablehlo.reshape %arg14 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %148 = stablehlo.reshape %147 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %151 = sdy.all_reduce {"_axis_0"} %150 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %152 = stablehlo.reshape %151 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %153 = stablehlo.add %20, %152 : tensor<1x7x3072xbf16>
    %154 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %155 = stablehlo.reshape %154 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %156 = stablehlo.convert %155 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %157 = stablehlo.broadcast_in_dim %156, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %158 = stablehlo.convert %153 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %159 = stablehlo.power %158, %2 : tensor<1x7x3072xf32>
    %160 = stablehlo.reduce(%159 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %161 = stablehlo.multiply %160, %cst_4 : tensor<1x7xf32>
    %162 = stablehlo.reshape %161 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %163 = stablehlo.add %162, %26 : tensor<1x7x1xf32>
    %164 = stablehlo.rsqrt %163 : tensor<1x7x1xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %166 = stablehlo.broadcast_in_dim %165, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %167 = stablehlo.multiply %158, %166 : tensor<1x7x3072xf32>
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %169 = stablehlo.convert %168 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %170 = stablehlo.multiply %157, %169 : tensor<1x7x3072xf32>
    %171 = stablehlo.convert %170 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %172 = stablehlo.reshape %171 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %173 = stablehlo.reshape %arg19 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %174 = stablehlo.reshape %173 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %175 = stablehlo.transpose %174, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %176 = stablehlo.dot_general %172, %175, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %177 = stablehlo.reshape %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %178 = stablehlo.convert %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %179 = stablehlo.logistic %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : tensor<1x7x8192xbf16>
    %180 = stablehlo.convert %179 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %181 = stablehlo.multiply %178, %180 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : tensor<1x7x8192xf32>
    %182 = stablehlo.convert %181 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %183 = stablehlo.convert %182 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %184 = stablehlo.reshape %arg13 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %185 = stablehlo.reshape %184 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %186 = stablehlo.transpose %185, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %187 = stablehlo.dot_general %172, %186, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %188 = stablehlo.reshape %187 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %189 = stablehlo.convert %188 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %190 = stablehlo.multiply %183, %189 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : tensor<1x7x8192xf32>
    %191 = stablehlo.convert %190 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %192 = stablehlo.reshape %191 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %193 = stablehlo.reshape %arg12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %194 = stablehlo.reshape %193 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %195 = stablehlo.transpose %194, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %196 = stablehlo.dot_general %192, %195, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %197 = sdy.all_reduce {"_axis_0"} %196 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %198 = stablehlo.reshape %197 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %199 = stablehlo.add %153, %198 : tensor<1x7x3072xbf16>
    %200 = stablehlo.convert %199 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %201 = stablehlo.power %200, %2 : tensor<1x7x3072xf32>
    %202 = stablehlo.reduce(%201 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %203 = stablehlo.multiply %202, %cst_4 : tensor<1x7xf32>
    %204 = stablehlo.reshape %203 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %205 = stablehlo.add %204, %26 : tensor<1x7x1xf32>
    %206 = stablehlo.rsqrt %205 : tensor<1x7x1xf32>
    %207 = stablehlo.reshape %206 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %208 = stablehlo.broadcast_in_dim %207, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %209 = stablehlo.multiply %200, %208 : tensor<1x7x3072xf32>
    %210 = stablehlo.convert %209 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %212 = stablehlo.multiply %85, %211 : tensor<1x7x3072xf32>
    %213 = stablehlo.convert %212 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %214 = stablehlo.reshape %213 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %215 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %216 = stablehlo.reshape %215 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %217 = stablehlo.transpose %216, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %218 = stablehlo.dot_general %214, %217, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %219 = stablehlo.reshape %218 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %73, %81, %218, %219 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump Before ConvertStableHLOToTTIR (convert-stablehlo-to-ttir) ('builtin.module' operation: @SyncTensorsGraph.450) //----- //
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg1: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg2: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg3: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg4: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg5: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg7: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg8: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg9: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg10: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg11: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg12: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg13: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg14: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg15: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg16: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64>
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %7 = stablehlo.add %4, %6 : tensor<7xi64>
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64>
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64>
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32>
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32>
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32>
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32>
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32>
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %37 = stablehlo.reshape %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %38 = stablehlo.reshape %37 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %56 = stablehlo.multiply %43, %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x128xf32>
    %57 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.convert %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32>
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %69 = stablehlo.multiply %62, %68 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x128xf32>
    %70 = stablehlo.convert %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %71 = stablehlo.add %57, %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x128xbf16>
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %74 = stablehlo.reshape %arg9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %86 = stablehlo.reshape %arg17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %87 = stablehlo.reshape %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %90 = stablehlo.reshape %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>} : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %94 = stablehlo.multiply %92, %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x128xf32>
    %95 = stablehlo.convert %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %97 = stablehlo.negate %96 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x64xbf16>
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %99 = stablehlo.concatenate %97, %98, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %100 = stablehlo.convert %99 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.multiply %100, %101 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %95, %103 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x128xbf16>
    %105 = stablehlo.reshape %104 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %107 = stablehlo.reshape %106 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16>
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16>
    %109 = stablehlo.reshape %108 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16>
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16>
    %111 = stablehlo.reshape %110 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %112 = stablehlo.convert %111 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<f32>) -> tensor<1x24x7x16xf32>
    %114 = stablehlo.multiply %112, %113 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %115 = stablehlo.convert %114 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64>
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64>
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16>
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32>
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32>
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32>
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16>
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %130 = stablehlo.add %115, %129 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xbf16>
    %131 = stablehlo.convert %130 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %134 = stablehlo.subtract %131, %133 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %135 = stablehlo.exponential %134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %138 = stablehlo.divide %135, %137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %139 = stablehlo.convert %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %140 = stablehlo.reshape %139 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16>
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %142 = stablehlo.reshape %141 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16>
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16>
    %144 = stablehlo.reshape %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %146 = stablehlo.reshape %145 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %147 = stablehlo.reshape %arg14 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %148 = stablehlo.reshape %147 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %151 = sdy.all_reduce {"_axis_0"} %150 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %152 = stablehlo.reshape %151 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %153 = stablehlo.add %20, %152 : tensor<1x7x3072xbf16>
    %154 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %155 = stablehlo.reshape %154 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %156 = stablehlo.convert %155 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %157 = stablehlo.broadcast_in_dim %156, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %158 = stablehlo.convert %153 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %159 = stablehlo.power %158, %2 : tensor<1x7x3072xf32>
    %160 = stablehlo.reduce(%159 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %161 = stablehlo.multiply %160, %cst_4 : tensor<1x7xf32>
    %162 = stablehlo.reshape %161 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %163 = stablehlo.add %162, %26 : tensor<1x7x1xf32>
    %164 = stablehlo.rsqrt %163 : tensor<1x7x1xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %166 = stablehlo.broadcast_in_dim %165, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %167 = stablehlo.multiply %158, %166 : tensor<1x7x3072xf32>
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %169 = stablehlo.convert %168 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %170 = stablehlo.multiply %157, %169 : tensor<1x7x3072xf32>
    %171 = stablehlo.convert %170 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %172 = stablehlo.reshape %171 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %173 = stablehlo.reshape %arg19 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %174 = stablehlo.reshape %173 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %175 = stablehlo.transpose %174, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %176 = stablehlo.dot_general %172, %175, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %177 = stablehlo.reshape %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %178 = stablehlo.convert %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %179 = stablehlo.logistic %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : tensor<1x7x8192xbf16>
    %180 = stablehlo.convert %179 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %181 = stablehlo.multiply %178, %180 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : tensor<1x7x8192xf32>
    %182 = stablehlo.convert %181 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %183 = stablehlo.convert %182 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %184 = stablehlo.reshape %arg13 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %185 = stablehlo.reshape %184 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %186 = stablehlo.transpose %185, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %187 = stablehlo.dot_general %172, %186, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %188 = stablehlo.reshape %187 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %189 = stablehlo.convert %188 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %190 = stablehlo.multiply %183, %189 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : tensor<1x7x8192xf32>
    %191 = stablehlo.convert %190 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %192 = stablehlo.reshape %191 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %193 = stablehlo.reshape %arg12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %194 = stablehlo.reshape %193 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %195 = stablehlo.transpose %194, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %196 = stablehlo.dot_general %192, %195, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %197 = sdy.all_reduce {"_axis_0"} %196 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %198 = stablehlo.reshape %197 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %199 = stablehlo.add %153, %198 : tensor<1x7x3072xbf16>
    %200 = stablehlo.convert %199 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %201 = stablehlo.power %200, %2 : tensor<1x7x3072xf32>
    %202 = stablehlo.reduce(%201 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %203 = stablehlo.multiply %202, %cst_4 : tensor<1x7xf32>
    %204 = stablehlo.reshape %203 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %205 = stablehlo.add %204, %26 : tensor<1x7x1xf32>
    %206 = stablehlo.rsqrt %205 : tensor<1x7x1xf32>
    %207 = stablehlo.reshape %206 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %208 = stablehlo.broadcast_in_dim %207, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %209 = stablehlo.multiply %200, %208 : tensor<1x7x3072xf32>
    %210 = stablehlo.convert %209 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %212 = stablehlo.multiply %85, %211 : tensor<1x7x3072xf32>
    %213 = stablehlo.convert %212 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %214 = stablehlo.reshape %213 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %215 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %216 = stablehlo.reshape %215 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %217 = stablehlo.transpose %216, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %218 = stablehlo.dot_general %214, %217, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %219 = stablehlo.reshape %218 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %73, %81, %218, %219 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}


loc("dot.336"): error: failed to legalize operation 'sdy.all_reduce'
// -----// IR Dump After ConvertStableHLOToTTIR Failed (convert-stablehlo-to-ttir) ('builtin.module' operation: @SyncTensorsGraph.450) //----- //
module @SyncTensorsGraph.450 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg1: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg2: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg3: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg4: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg5: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg7: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg8: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg9: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg10: tensor<1x8x16x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg11: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg12: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg13: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg14: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg15: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg16: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x16x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]> : tensor<16xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c_2 = stablehlo.constant dense<0> : tensor<7xi64>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x16xi64>
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg0 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %4 = stablehlo.reshape %3 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %5 = stablehlo.compare  LT, %4, %c_2 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %6 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %7 = stablehlo.add %4, %6 : tensor<7xi64>
    %8 = stablehlo.select %5, %7, %4 : tensor<7xi1>, tensor<7xi64>
    %9 = stablehlo.reshape %8 : (tensor<7xi64>) -> tensor<7x1xi64>
    %10 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %13 = stablehlo.broadcast_in_dim %12, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %14 = stablehlo.reshape %arg5 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %15 = stablehlo.reshape %14 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = stablehlo.reshape %arg4 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %17 = stablehlo.reshape %16 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %18 = stablehlo.convert %17 : (tensor<7xi64>) -> tensor<7xui32>
    %19 = "stablehlo.gather"(%15, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %20 = stablehlo.reshape %19 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %21 = stablehlo.convert %20 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.power %21, %2 : tensor<1x7x3072xf32>
    %23 = stablehlo.reduce(%22 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %24 = stablehlo.multiply %23, %cst_4 : tensor<1x7xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %26 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.add %25, %26 : tensor<1x7x1xf32>
    %28 = stablehlo.rsqrt %27 : tensor<1x7x1xf32>
    %29 = stablehlo.reshape %28 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %30 = stablehlo.broadcast_in_dim %29, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %21, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %34 = stablehlo.multiply %13, %33 : tensor<1x7x3072xf32>
    %35 = stablehlo.convert %34 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %36 = stablehlo.reshape %35 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %37 = stablehlo.reshape %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %38 = stablehlo.reshape %37 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %40 = stablehlo.dot_general %36, %39, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.convert %42 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %44 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %45 = stablehlo.reshape %44 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %46 = stablehlo.convert %3 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %47 = stablehlo.dot_general %45, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %53 = stablehlo.convert %52 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %54 = stablehlo.reshape %53 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %56 = stablehlo.multiply %43, %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x128xf32>
    %57 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.convert %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %63 = stablehlo.sine %49 : tensor<1x7x128xf32>
    %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %67 = stablehlo.reshape %66 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %68 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %69 = stablehlo.multiply %62, %68 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x128xf32>
    %70 = stablehlo.convert %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %71 = stablehlo.add %57, %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x8x7x128xbf16>
    %72 = "stablehlo.scatter"(%arg8, %9, %71) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %73 = stablehlo.custom_call @Sharding(%72) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %74 = stablehlo.reshape %arg9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.reshape %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %77 = stablehlo.dot_general %36, %76, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %78 = stablehlo.reshape %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %80 = "stablehlo.scatter"(%arg10, %9, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x16x128xbf16>
    %81 = stablehlo.custom_call @Sharding(%80) {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>"}, mhlo.sharding = "{devices=[1,2,1,1]<=[2]}"} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x16x128xbf16>
    %82 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %84 = stablehlo.convert %83 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %86 = stablehlo.reshape %arg17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %87 = stablehlo.reshape %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %88 = stablehlo.transpose %87, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %89 = stablehlo.dot_general %36, %88, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %90 = stablehlo.reshape %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>} : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %92 = stablehlo.convert %91 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %94 = stablehlo.multiply %92, %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x128xf32>
    %95 = stablehlo.convert %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %96 = stablehlo.slice %91 [0:1, 0:24, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %97 = stablehlo.negate %96 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x64xbf16>
    %98 = stablehlo.slice %91 [0:1, 0:24, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %99 = stablehlo.concatenate %97, %98, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %100 = stablehlo.convert %99 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %101 = stablehlo.broadcast_in_dim %67, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.multiply %100, %101 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x128xf32>
    %103 = stablehlo.convert %102 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %95, %103 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x128xbf16>
    %105 = stablehlo.reshape %104 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %106 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %107 = stablehlo.reshape %106 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<1x24x16x128xbf16>
    %108 = stablehlo.transpose %107, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>, xla_shape = "bf16[1,24,128,16]{2,3,1,0}"} : (tensor<1x24x16x128xbf16>) -> tensor<1x24x128x16xbf16>
    %109 = stablehlo.reshape %108 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x24x128x16xbf16>) -> tensor<24x128x16xbf16>
    %110 = stablehlo.dot_general %105, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<24x7x128xbf16>, tensor<24x128x16xbf16>) -> tensor<24x7x16xbf16>
    %111 = stablehlo.reshape %110 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<24x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %112 = stablehlo.convert %111 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %113 = stablehlo.broadcast_in_dim %arg16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<f32>) -> tensor<1x24x7x16xf32>
    %114 = stablehlo.multiply %112, %113 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %115 = stablehlo.convert %114 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %116 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<16xi64>) -> tensor<7x16xi64>
    %117 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %118 = stablehlo.subtract %116, %117 : tensor<7x16xi64>
    %119 = stablehlo.compare  GE, %118, %1 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %120 = stablehlo.broadcast_in_dim %arg15, dims = [] : (tensor<bf16>) -> tensor<7x16xbf16>
    %121 = stablehlo.select %119, %120, %0 : tensor<7x16xi1>, tensor<7x16xbf16>
    %122 = stablehlo.convert %121 : (tensor<7x16xbf16>) -> tensor<7x16xf32>
    %123 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<7xi64>) -> tensor<7x16xi64>
    %124 = stablehlo.compare  GT, %116, %123 : (tensor<7x16xi64>, tensor<7x16xi64>) -> tensor<7x16xi1>
    %125 = stablehlo.convert %124 : (tensor<7x16xi1>) -> tensor<7x16xf32>
    %126 = stablehlo.multiply %122, %125 : tensor<7x16xf32>
    %127 = stablehlo.convert %126 : (tensor<7x16xf32>) -> tensor<7x16xbf16>
    %128 = stablehlo.reshape %127 : (tensor<7x16xbf16>) -> tensor<1x7x16xbf16>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x7x16xbf16>) -> tensor<1x24x7x16xbf16>
    %130 = stablehlo.add %115, %129 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xbf16>
    %131 = stablehlo.convert %130 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<1x24x7x16xf32>
    %132 = stablehlo.reduce(%131 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %134 = stablehlo.subtract %131, %133 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %135 = stablehlo.exponential %134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<1x24x7x16xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7xf32>) -> tensor<1x24x7x16xf32>
    %138 = stablehlo.divide %135, %137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : tensor<1x24x7x16xf32>
    %139 = stablehlo.convert %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xf32>) -> tensor<1x24x7x16xbf16>
    %140 = stablehlo.reshape %139 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x24x7x16xbf16>) -> tensor<24x7x16xbf16>
    %141 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}, {}]>]>} : (tensor<1x8x16x128xbf16>) -> tensor<1x8x3x16x128xbf16>
    %142 = stablehlo.reshape %141 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<1x8x3x16x128xbf16>) -> tensor<24x16x128xbf16>
    %143 = stablehlo.dot_general %140, %142, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}, {}]>]>} : (tensor<24x7x16xbf16>, tensor<24x16x128xbf16>) -> tensor<24x7x128xbf16>
    %144 = stablehlo.reshape %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}, {}]>]>} : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %145 = stablehlo.transpose %144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}, {}]>]>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %146 = stablehlo.reshape %145 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %147 = stablehlo.reshape %arg14 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %148 = stablehlo.reshape %147 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %149 = stablehlo.transpose %148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %150 = stablehlo.dot_general %146, %149, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %151 = sdy.all_reduce {"_axis_0"} %150 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %152 = stablehlo.reshape %151 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %153 = stablehlo.add %20, %152 : tensor<1x7x3072xbf16>
    %154 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %155 = stablehlo.reshape %154 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %156 = stablehlo.convert %155 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %157 = stablehlo.broadcast_in_dim %156, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %158 = stablehlo.convert %153 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %159 = stablehlo.power %158, %2 : tensor<1x7x3072xf32>
    %160 = stablehlo.reduce(%159 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %161 = stablehlo.multiply %160, %cst_4 : tensor<1x7xf32>
    %162 = stablehlo.reshape %161 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %163 = stablehlo.add %162, %26 : tensor<1x7x1xf32>
    %164 = stablehlo.rsqrt %163 : tensor<1x7x1xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %166 = stablehlo.broadcast_in_dim %165, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %167 = stablehlo.multiply %158, %166 : tensor<1x7x3072xf32>
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %169 = stablehlo.convert %168 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %170 = stablehlo.multiply %157, %169 : tensor<1x7x3072xf32>
    %171 = stablehlo.convert %170 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %172 = stablehlo.reshape %171 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %173 = stablehlo.reshape %arg19 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %174 = stablehlo.reshape %173 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %175 = stablehlo.transpose %174, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %176 = stablehlo.dot_general %172, %175, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %177 = stablehlo.reshape %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %178 = stablehlo.convert %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %179 = stablehlo.logistic %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : tensor<1x7x8192xbf16>
    %180 = stablehlo.convert %179 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %181 = stablehlo.multiply %178, %180 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : tensor<1x7x8192xf32>
    %182 = stablehlo.convert %181 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %183 = stablehlo.convert %182 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %184 = stablehlo.reshape %arg13 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}, {}]>]>} : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %185 = stablehlo.reshape %184 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>} : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %186 = stablehlo.transpose %185, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %187 = stablehlo.dot_general %172, %186, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %188 = stablehlo.reshape %187 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %189 = stablehlo.convert %188 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %190 = stablehlo.multiply %183, %189 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : tensor<1x7x8192xf32>
    %191 = stablehlo.convert %190 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %192 = stablehlo.reshape %191 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %193 = stablehlo.reshape %arg12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {"_axis_0"}]>]>} : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %194 = stablehlo.reshape %193 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"_axis_0"}]>]>} : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %195 = stablehlo.transpose %194, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {}]>]>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %196 = stablehlo.dot_general %192, %195, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %197 = sdy.all_reduce {"_axis_0"} %196 out_sharding=<@mesh, [{}, {}]> : tensor<7x3072xbf16>
    %198 = stablehlo.reshape %197 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %199 = stablehlo.add %153, %198 : tensor<1x7x3072xbf16>
    %200 = stablehlo.convert %199 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %201 = stablehlo.power %200, %2 : tensor<1x7x3072xf32>
    %202 = stablehlo.reduce(%201 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %203 = stablehlo.multiply %202, %cst_4 : tensor<1x7xf32>
    %204 = stablehlo.reshape %203 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %205 = stablehlo.add %204, %26 : tensor<1x7x1xf32>
    %206 = stablehlo.rsqrt %205 : tensor<1x7x1xf32>
    %207 = stablehlo.reshape %206 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %208 = stablehlo.broadcast_in_dim %207, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %209 = stablehlo.multiply %200, %208 : tensor<1x7x3072xf32>
    %210 = stablehlo.convert %209 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %211 = stablehlo.convert %210 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %212 = stablehlo.multiply %85, %211 : tensor<1x7x3072xf32>
    %213 = stablehlo.convert %212 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %214 = stablehlo.reshape %213 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %215 = stablehlo.reshape %arg11 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %216 = stablehlo.reshape %215 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %217 = stablehlo.transpose %216, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %218 = stablehlo.dot_general %214, %217, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %219 = stablehlo.reshape %218 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %73, %81, %218, %219 : tensor<1x8x16x128xbf16>, tensor<1x8x16x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}


isUsingShardy: 0
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[james] override use torch.export.export
program.graph_signature: ExportGraphSignature(input_specs=[InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_layers__modules__0___input_layernorm_weight'), target='L__self___model_layers__modules__0___input_layernorm_weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_layers__modules__0___post_attention_layernorm_weight'), target='L__self___model_layers__modules__0___post_attention_layernorm_weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_norm_weight'), target='L__self___model_norm_weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_embed_tokens_weight'), target='L__self___model_embed_tokens.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_layers__modules__0___self_attn_q_proj_weight'), target='L__self___model_layers__modules__0___self_attn_q_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_layers__modules__0___self_attn_k_proj_weight'), target='L__self___model_layers__modules__0___self_attn_k_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_layers__modules__0___self_attn_v_proj_weight'), target='L__self___model_layers__modules__0___self_attn_v_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_layers__modules__0___self_attn_o_proj_weight'), target='L__self___model_layers__modules__0___self_attn_o_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_layers__modules__0___mlp_gate_proj_weight'), target='L__self___model_layers__modules__0___mlp_gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_layers__modules__0___mlp_up_proj_weight'), target='L__self___model_layers__modules__0___mlp_up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_layers__modules__0___mlp_down_proj_weight'), target='L__self___model_layers__modules__0___mlp_down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_lm_head_weight'), target='L__self___lm_head.weight', persistent=None), InputSpec(kind=<InputKind.BUFFER: 3>, arg=TensorArgument(name='b_model_rotary_emb_inv_freq'), target='L__self___model_rotary_emb_inv_freq', persistent=True), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='args_0'), target=None, persistent=None), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='args_1'), target=None, persistent=None), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='args_2'), target=None, persistent=None), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='args_3'), target=None, persistent=None)], output_specs=[OutputSpec(kind=<OutputKind.USER_INPUT_MUTATION: 6>, arg=TensorArgument(name='index_put'), target='args_2'), OutputSpec(kind=<OutputKind.USER_INPUT_MUTATION: 6>, arg=TensorArgument(name='index_put_1'), target='args_3'), OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='view_31'), target=None)])
Traceback (most recent call last):
  File "/localdev/jameszianxu/tt-xla/examples/pytorch/llama.py", line 218, in <module>
    llama()
  File "/localdev/jameszianxu/tt-xla/examples/pytorch/llama.py", line 166, in llama
    output: CausalLMOutputWithPast = model(**input_args)
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1749, in _wrapped_call_impl
    return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py", line 655, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py", line 953, in wrapper
    @wraps(func)
  File "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/localdev/jameszianxu/tt-xla/python_package/tt_torch/backend/backend.py", line 95, in __call__
    torch_xla.sync()
  File "/usr/local/lib/python3.11/dist-packages/torch_xla/torch_xla.py", line 87, in sync
    torch_xla._XLAC._xla_step_marker(
ValueError: Error code: 13
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
