diff --git a/include/ttmlir/Dialect/StableHLO/Utils/ShardyUtils.h b/include/ttmlir/Dialect/StableHLO/Utils/ShardyUtils.h
index e050b2ec6..68861ebfa 100644
--- a/include/ttmlir/Dialect/StableHLO/Utils/ShardyUtils.h
+++ b/include/ttmlir/Dialect/StableHLO/Utils/ShardyUtils.h
@@ -87,6 +87,10 @@ convertCustomCallToShardingConstraint(mlir::ModuleOp &rootModule,
                                       mlir::MLIRContext *context,
                                       mlir::OpBuilder &builder);
 
+// Remove all stablehlo.custom_call @xla.sdy.FuncResultSharding and
+// @xla.sdy.FuncArgSharding ops by replacing their results with their operands.
+mlir::LogicalResult removeFuncShardingCustomCalls(mlir::ModuleOp &rootModule);
+
 // Check if the graph is solved.
 bool isGraphSolved(mlir::ModuleOp &module);
 
diff --git a/lib/Dialect/StableHLO/Pipelines/StableHLOPipelines.cpp b/lib/Dialect/StableHLO/Pipelines/StableHLOPipelines.cpp
index a1257e13e..01d6766d6 100644
--- a/lib/Dialect/StableHLO/Pipelines/StableHLOPipelines.cpp
+++ b/lib/Dialect/StableHLO/Pipelines/StableHLOPipelines.cpp
@@ -7,6 +7,7 @@
 #include "shardy/dialect/sdy/transforms/propagation/user_priority_propagation.h"
 
 #include "mlir/Transforms/Passes.h"
+#include <iostream>
 
 namespace mlir::tt::stablehlo {
 //===----------------------------------------------------------------------===//
@@ -23,6 +24,7 @@ void createStableHLOPipeline(OpPassManager &pm,
       mlir::tt::ttcore::createTTPopulateArgumentTypes(options.argumentTypeMap));
 
   // Annotate arguments with whether they are already pre-sharded or not.
+//   std::cout << "[james] Skipping argument shard status pass to remove ttcore attributes" << std::endl;
   pm.addPass(createApplyArgumentShardStatusPass());
 
   // Convert any xla.sdy ops to sdy ops.
diff --git a/lib/Dialect/StableHLO/Utils/GSPMDUtils.cpp b/lib/Dialect/StableHLO/Utils/GSPMDUtils.cpp
index ccf8d049c..c4ee56553 100644
--- a/lib/Dialect/StableHLO/Utils/GSPMDUtils.cpp
+++ b/lib/Dialect/StableHLO/Utils/GSPMDUtils.cpp
@@ -7,7 +7,7 @@
 #include "ttmlir/Dialect/StableHLO/Utils/ShardyUtils.h"
 #include "ttmlir/Dialect/TTCore/IR/TTCore.h"
 #include "ttmlir/Dialect/TTCore/IR/TTCoreOpsTypes.h"
-
+#include <iostream>
 #include "llvm/Support/Error.h"
 
 #include "stablehlo/dialect/StablehloOps.h"
@@ -162,6 +162,8 @@ parseMeshesFromGspmdModule(mlir::ModuleOp &module) {
 
 // Check if the module has any gspmd annotations.
 bool gspmdAnnotationsExist(mlir::ModuleOp &module) {
+  std::cout << "[james] gspmdAnnotationsExist force-set to false " << std::endl;
+  return false; 
   for (auto &op : module.getBody()->getOperations()) {
     if (!mlir::isa<func::FuncOp>(op)) {
       continue;
diff --git a/lib/Dialect/StableHLO/Utils/ShardyUtils.cpp b/lib/Dialect/StableHLO/Utils/ShardyUtils.cpp
index 23148069d..54ca6d85e 100644
--- a/lib/Dialect/StableHLO/Utils/ShardyUtils.cpp
+++ b/lib/Dialect/StableHLO/Utils/ShardyUtils.cpp
@@ -984,6 +984,39 @@ convertCustomCallToShardingConstraint(mlir::ModuleOp &rootModule,
   return mlir::success();
 }
 
+// Remove all stablehlo.custom_call @xla.sdy.FuncResultSharding and
+// @xla.sdy.FuncArgSharding ops by replacing their results with their operands.
+mlir::LogicalResult
+removeFuncShardingCustomCalls(mlir::ModuleOp &rootModule) {
+  rootModule.walk([&](mlir::Operation *op) {
+    if (!mlir::isa<mlir::stablehlo::CustomCallOp>(op)) {
+      return;
+    }
+
+    mlir::stablehlo::CustomCallOp customCallOp =
+        mlir::cast<mlir::stablehlo::CustomCallOp>(op);
+    auto callTargetName = customCallOp.getCallTargetName();
+    
+    // Check if this is a FuncResultSharding or FuncArgSharding custom call
+    if (callTargetName != "xla.sdy.FuncResultSharding" &&
+        callTargetName != "xla.sdy.FuncArgSharding") {
+      return;
+    }
+
+    // These custom calls should have exactly one operand and one result
+    if (customCallOp.getNumOperands() != 1 || customCallOp.getNumResults() != 1) {
+      return;
+    }
+
+    // Replace all uses of the result with the operand and erase the custom call
+    mlir::Value operand = customCallOp.getOperand(0);
+    customCallOp.getResult(0).replaceAllUsesWith(operand);
+    customCallOp.erase();
+  });
+
+  return mlir::success();
+}
+
 #endif // #ifdef TTMLIR_ENABLE_STABLEHLO
 
 } // namespace mlir::tt::shardy_utils
diff --git a/runtime/lib/ttnn/debug/debug_apis.cpp b/runtime/lib/ttnn/debug/debug_apis.cpp
index e76097405..c7f08434e 100644
--- a/runtime/lib/ttnn/debug/debug_apis.cpp
+++ b/runtime/lib/ttnn/debug/debug_apis.cpp
@@ -15,6 +15,8 @@ namespace tt::runtime::ttnn::debug {
 void checkTensorRefMatchesTTNNTensor(
     const ::tt::target::ttnn::TensorRef *tensorRef,
     const ::ttnn::Tensor &ttnnTensor) {
+
+        return;
   ::ttnn::Layout expectedLayout =
       ::tt::runtime::ttnn::utils::inferLayoutFromTileShape(tensorRef);
   ::ttnn::Layout actualLayout = ttnnTensor.layout();
