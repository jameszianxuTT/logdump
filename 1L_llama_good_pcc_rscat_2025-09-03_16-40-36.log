WARNING:root:Defaulting to PJRT_DEVICE=CPU
============================= test session starts ==============================
platform linux -- Python 3.10.12, pytest-8.4.1, pluggy-1.6.0 -- /localdev/jameszianxu/test2/tt-torch/env/venv/bin/python3.10
cachedir: .pytest_cache
rootdir: /localdev/jameszianxu/test2/tt-torch
configfile: pytest.ini
plugins: cov-6.2.1, forked-1.6.0, xdist-3.8.0, split-0.10.0
collecting ... collected 1 item

tests/models/llama/test_llama3_generative.py::test_llama3_generate Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 48.50it/s]
2025-09-03 16:39:15.559617: W torch_xla/csrc/runtime/profiler.cpp:88] Profiler API not found for PJRT plugin
Initial prompt: '<|begin_of_text|>I like taking walks in the'
Setting up XLA environment...
XLA environment configured.
Created device mesh: (1, 2) with 2 devices
<|begin_of_text|>I like taking walks in theNote: Using experimental XLA backend.
[XLA Debug] Processing 18 input specs:
[XLA Debug] Input 0: kind=InputKind.PARAMETER, arg=TensorArgument(name='p__fx_const_folded_attrs_0'), target=_FX_CONST_FOLDED_ATTRS.0

[XLA Debug] Input 1: kind=InputKind.PARAMETER, arg=TensorArgument(name='p__fx_const_folded_attrs_1'), target=_FX_CONST_FOLDED_ATTRS.1

[XLA Debug] Input 2: kind=InputKind.BUFFER, arg=TensorArgument(name='b_kwargs____past_key_values___key_cache_0'), target=kwargs____past_key_values___key_cache_0

[XLA Debug] Input 3: kind=InputKind.BUFFER, arg=TensorArgument(name='b_kwargs____past_key_values___value_cache_0'), target=kwargs____past_key_values___value_cache_0

[XLA Debug] Input 4: kind=InputKind.USER_INPUT, arg=TensorArgument(name='args_0'), target=None
[XLA Debug] Input 5: kind=InputKind.USER_INPUT, arg=TensorArgument(name='args_1'), target=None
[XLA Debug] Input 6: kind=InputKind.USER_INPUT, arg=TensorArgument(name='args_2'), target=None
[XLA Debug] Input 7: kind=InputKind.USER_INPUT, arg=TensorArgument(name='args_3'), target=None
[XLA Debug] Input 8: kind=InputKind.USER_INPUT, arg=TensorArgument(name='args_4'), target=None
[XLA Debug] Input 9: kind=InputKind.USER_INPUT, arg=TensorArgument(name='args_5'), target=None
[XLA Debug] Input 10: kind=InputKind.USER_INPUT, arg=TensorArgument(name='args_6'), target=None
[XLA Debug] Input 11: kind=InputKind.USER_INPUT, arg=TensorArgument(name='args_7'), target=None
[XLA Debug] Input 12: kind=InputKind.USER_INPUT, arg=TensorArgument(name='args_8'), target=None
[XLA Debug] Input 13: kind=InputKind.USER_INPUT, arg=TensorArgument(name='args_9'), target=None
[XLA Debug] Input 14: kind=InputKind.USER_INPUT, arg=TensorArgument(name='args_10'), target=None
[XLA Debug] Input 15: kind=InputKind.USER_INPUT, arg=TensorArgument(name='args_11'), target=None
[XLA Debug] Input 16: kind=InputKind.USER_INPUT, arg=TensorArgument(name='args_12'), target=None
[XLA Debug] Input 17: kind=InputKind.USER_INPUT, arg=TensorArgument(name='args_13'), target=None
[XLA Debug] Initialization complete - total inputs: 18, user input indices: [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
module @SyncTensorsGraph.374 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<7x!vhlo.i64_v1>, %arg1: !vhlo.tensor_v1<64x!vhlo.f32_v1>, %arg2: !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg4: !vhlo.tensor_v1<1x7x!vhlo.i64_v1>, %arg5: !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, %arg6: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg7: !vhlo.tensor_v1<!vhlo.i64_v1>, %arg8: !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, %arg9: !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>, %arg10: !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, %arg11: !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>, %arg13: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg14: !vhlo.tensor_v1<128x!vhlo.i64_v1>, %arg15: !vhlo.tensor_v1<7x128x!vhlo.bf16_v1>, %arg16: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg17: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg18: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg19: !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>, %arg20: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0> : tensor<7xi64>>}> : () -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF800000> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %3 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<3.25520843E-4> : tensor<1x7xf32>>}> : () -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %4 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %5 = "vhlo.broadcast_in_dim_v1"(%4) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %6 = "vhlo.compare_v1"(%arg0, %0) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 LT>}> : (!vhlo.tensor_v1<7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.bool_v1>
    %7 = "vhlo.broadcast_in_dim_v1"(%arg7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %8 = "vhlo.add_v1"(%arg0, %7) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %9 = "vhlo.select_v1"(%6, %8, %arg0) : (!vhlo.tensor_v1<7x!vhlo.bool_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %10 = "vhlo.reshape_v1"(%9) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x1x!vhlo.i64_v1>
    %11 = "vhlo.convert_v1"(%arg6) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %12 = "vhlo.broadcast_in_dim_v1"(%11) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %13 = "vhlo.reshape_v1"(%arg4) : (!vhlo.tensor_v1<1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %14 = "vhlo.convert_v1"(%13) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.ui32_v1>
    %15 = "vhlo.gather_v2"(%arg5, %14) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 3072]> : tensor<2xi64>>, start_index_map = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x!vhlo.ui32_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %16 = "vhlo.reshape_v1"(%15) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %17 = "vhlo.convert_v1"(%16) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %18 = "vhlo.power_v1"(%17, %5) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %19 = "vhlo.reduce_v1"(%18, %1) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %189 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%189) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %20 = "vhlo.multiply_v1"(%19, %3) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %21 = "vhlo.reshape_v1"(%20) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %22 = "vhlo.broadcast_in_dim_v1"(%arg3) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %23 = "vhlo.add_v1"(%21, %22) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %24 = "vhlo.rsqrt_v2"(%23) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %25 = "vhlo.reshape_v1"(%24) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %26 = "vhlo.broadcast_in_dim_v1"(%25) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %27 = "vhlo.multiply_v1"(%17, %26) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %28 = "vhlo.convert_v1"(%27) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %29 = "vhlo.convert_v1"(%28) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %30 = "vhlo.multiply_v1"(%12, %29) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %31 = "vhlo.convert_v1"(%30) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %32 = "vhlo.reshape_v1"(%31) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %33 = "vhlo.transpose_v1"(%arg2) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,1024]{0,1}">} : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>
    %34 = "vhlo.dot_general_v2"(%32, %33) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>
    %35 = "vhlo.reshape_v1"(%34) : (!vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>
    %36 = "vhlo.transpose_v1"(%35) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %37 = "vhlo.convert_v1"(%36) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %38 = "vhlo.reshape_v1"(%arg1) : (!vhlo.tensor_v1<64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>
    %39 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>
    %40 = "vhlo.convert_v1"(%39) : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.f32_v1>
    %41 = "vhlo.dot_general_v2"(%38, %40) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>
    %42 = "vhlo.transpose_v1"(%41) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1]> : tensor<3xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[1, 2, 0]> : tensor<3xindex>>, xla_shape = #vhlo.string_v1<"f32[1,7,64]{1,2,0}">} : (!vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>
    %43 = "vhlo.concatenate_v1"(%42, %42) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %44 = "vhlo.cosine_v2"(%43) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %45 = "vhlo.convert_v1"(%44) : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>
    %46 = "vhlo.reshape_v1"(%45) : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>
    %47 = "vhlo.convert_v1"(%46) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>
    %48 = "vhlo.reshape_v1"(%47) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %49 = "vhlo.broadcast_in_dim_v1"(%48) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %50 = "vhlo.multiply_v1"(%37, %49) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %51 = "vhlo.convert_v1"(%50) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %52 = "vhlo.slice_v1"(%36) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 7, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %53 = "vhlo.negate_v1"(%52) : (!vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %54 = "vhlo.slice_v1"(%36) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 7, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %55 = "vhlo.concatenate_v1"(%53, %54) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %56 = "vhlo.convert_v1"(%55) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %57 = "vhlo.sine_v2"(%43) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %58 = "vhlo.convert_v1"(%57) : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>
    %59 = "vhlo.reshape_v1"(%58) : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>
    %60 = "vhlo.convert_v1"(%59) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>
    %61 = "vhlo.reshape_v1"(%60) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %62 = "vhlo.broadcast_in_dim_v1"(%61) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %63 = "vhlo.multiply_v1"(%56, %62) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %64 = "vhlo.convert_v1"(%63) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %65 = "vhlo.add_v1"(%51, %64) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %66 = "vhlo.scatter_v2"(%arg8, %10, %65) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg22: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg22) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>
    %67 = "vhlo.custom_call_v1"(%66) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"Sharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>">}>, mhlo.sharding = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">} : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>
    %68 = "vhlo.transpose_v1"(%arg9) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,1024]{0,1}">} : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>
    %69 = "vhlo.dot_general_v2"(%32, %68) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>
    %70 = "vhlo.reshape_v1"(%69) : (!vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>
    %71 = "vhlo.transpose_v1"(%70) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %72 = "vhlo.scatter_v2"(%arg10, %10, %71) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg22: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg22) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>
    %73 = "vhlo.custom_call_v1"(%72) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"Sharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>">}>, mhlo.sharding = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">} : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>
    %74 = "vhlo.convert_v1"(%arg20) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %75 = "vhlo.broadcast_in_dim_v1"(%74) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %76 = "vhlo.transpose_v1"(%arg17) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,3072]{0,1}">} : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %77 = "vhlo.dot_general_v2"(%32, %76) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %78 = "vhlo.reshape_v1"(%77) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>
    %79 = "vhlo.transpose_v1"(%78) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,24,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %80 = "vhlo.convert_v1"(%79) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,24,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %81 = "vhlo.broadcast_in_dim_v1"(%48) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %82 = "vhlo.multiply_v1"(%80, %81) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %83 = "vhlo.convert_v1"(%82) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %84 = "vhlo.slice_v1"(%79) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 7, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>
    %85 = "vhlo.negate_v1"(%84) : (!vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>
    %86 = "vhlo.slice_v1"(%79) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 7, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>
    %87 = "vhlo.concatenate_v1"(%85, %86) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %88 = "vhlo.convert_v1"(%87) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %89 = "vhlo.broadcast_in_dim_v1"(%61) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %90 = "vhlo.multiply_v1"(%88, %89) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %91 = "vhlo.convert_v1"(%90) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %92 = "vhlo.add_v1"(%83, %91) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %93 = "vhlo.reshape_v1"(%92) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>
    %94 = "vhlo.broadcast_in_dim_v1"(%66) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>
    %95 = "vhlo.reshape_v1"(%94) : (!vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>
    %96 = "vhlo.transpose_v1"(%95) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 3, 1, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,24,128,128]{2,3,1,0}">} : (!vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>
    %97 = "vhlo.reshape_v1"(%96) : (!vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>
    %98 = "vhlo.dot_general_v2"(%93, %97) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>
    %99 = "vhlo.reshape_v1"(%98) : (!vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %100 = "vhlo.convert_v1"(%99) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %101 = "vhlo.broadcast_in_dim_v1"(%arg16) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %102 = "vhlo.multiply_v1"(%100, %101) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %103 = "vhlo.convert_v1"(%102) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %104 = "vhlo.convert_v1"(%arg15) : (!vhlo.tensor_v1<7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x128x!vhlo.f32_v1>
    %105 = "vhlo.broadcast_in_dim_v1"(%arg14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x128x!vhlo.i64_v1>
    %106 = "vhlo.broadcast_in_dim_v1"(%arg0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x128x!vhlo.i64_v1>
    %107 = "vhlo.compare_v1"(%105, %106) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<7x128x!vhlo.i64_v1>, !vhlo.tensor_v1<7x128x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x128x!vhlo.bool_v1>
    %108 = "vhlo.convert_v1"(%107) : (!vhlo.tensor_v1<7x128x!vhlo.bool_v1>) -> !vhlo.tensor_v1<7x128x!vhlo.f32_v1>
    %109 = "vhlo.multiply_v1"(%104, %108) : (!vhlo.tensor_v1<7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<7x128x!vhlo.f32_v1>
    %110 = "vhlo.convert_v1"(%109) : (!vhlo.tensor_v1<7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<7x128x!vhlo.bf16_v1>
    %111 = "vhlo.reshape_v1"(%110) : (!vhlo.tensor_v1<7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>
    %112 = "vhlo.broadcast_in_dim_v1"(%111) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %113 = "vhlo.add_v1"(%103, %112) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %114 = "vhlo.convert_v1"(%113) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %115 = "vhlo.reduce_v1"(%114, %2) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %189 = "vhlo.maximum_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%189) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>
    %116 = "vhlo.broadcast_in_dim_v1"(%115) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %117 = "vhlo.subtract_v1"(%114, %116) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %118 = "vhlo.exponential_v2"(%117) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %119 = "vhlo.reduce_v1"(%118, %1) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %189 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%189) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>
    %120 = "vhlo.broadcast_in_dim_v1"(%119) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %121 = "vhlo.divide_v1"(%118, %120) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %122 = "vhlo.convert_v1"(%121) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %123 = "vhlo.reshape_v1"(%122) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>
    %124 = "vhlo.broadcast_in_dim_v1"(%72) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>
    %125 = "vhlo.reshape_v1"(%124) : (!vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>
    %126 = "vhlo.dot_general_v2"(%123, %125) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>
    %127 = "vhlo.reshape_v1"(%126) : (!vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %128 = "vhlo.transpose_v1"(%127) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,7,24,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>
    %129 = "vhlo.reshape_v1"(%128) : (!vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %130 = "vhlo.transpose_v1"(%arg13) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,3072]{0,1}">} : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %131 = "vhlo.dot_general_v2"(%129, %130) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %132 = "vhlo.reshape_v1"(%131) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %133 = "vhlo.add_v1"(%16, %132) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %134 = "vhlo.convert_v1"(%arg18) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %135 = "vhlo.broadcast_in_dim_v1"(%134) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %136 = "vhlo.convert_v1"(%133) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %137 = "vhlo.power_v1"(%136, %5) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %138 = "vhlo.reduce_v1"(%137, %1) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %189 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%189) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %139 = "vhlo.multiply_v1"(%138, %3) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %140 = "vhlo.reshape_v1"(%139) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %141 = "vhlo.add_v1"(%140, %22) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %142 = "vhlo.rsqrt_v2"(%141) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %143 = "vhlo.reshape_v1"(%142) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %144 = "vhlo.broadcast_in_dim_v1"(%143) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %145 = "vhlo.multiply_v1"(%136, %144) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %146 = "vhlo.convert_v1"(%145) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %147 = "vhlo.convert_v1"(%146) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %148 = "vhlo.multiply_v1"(%135, %147) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %149 = "vhlo.convert_v1"(%148) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %150 = "vhlo.reshape_v1"(%149) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %151 = "vhlo.transpose_v1"(%arg19) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,8192]{0,1}">} : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>
    %152 = "vhlo.dot_general_v2"(%150, %151) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %153 = "vhlo.reshape_v1"(%152) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %154 = "vhlo.convert_v1"(%153) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %155 = "vhlo.logistic_v2"(%153) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %156 = "vhlo.convert_v1"(%155) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %157 = "vhlo.multiply_v1"(%154, %156) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %158 = "vhlo.convert_v1"(%157) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %159 = "vhlo.convert_v1"(%158) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %160 = "vhlo.transpose_v1"(%arg12) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,8192]{0,1}">} : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>
    %161 = "vhlo.dot_general_v2"(%150, %160) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %162 = "vhlo.reshape_v1"(%161) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %163 = "vhlo.convert_v1"(%162) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %164 = "vhlo.multiply_v1"(%159, %163) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %165 = "vhlo.convert_v1"(%164) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %166 = "vhlo.reshape_v1"(%165) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %167 = "vhlo.transpose_v1"(%arg11) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,3072]{0,1}">} : (!vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>
    %168 = "vhlo.dot_general_v2"(%166, %167) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %169 = "vhlo.reshape_v1"(%168) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %170 = "vhlo.add_v1"(%133, %169) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %171 = "vhlo.convert_v1"(%170) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %172 = "vhlo.power_v1"(%171, %5) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %173 = "vhlo.reduce_v1"(%172, %1) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %189 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%189) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %174 = "vhlo.multiply_v1"(%173, %3) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %175 = "vhlo.reshape_v1"(%174) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %176 = "vhlo.add_v1"(%175, %22) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %177 = "vhlo.rsqrt_v2"(%176) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %178 = "vhlo.reshape_v1"(%177) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %179 = "vhlo.broadcast_in_dim_v1"(%178) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %180 = "vhlo.multiply_v1"(%171, %179) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %181 = "vhlo.convert_v1"(%180) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %182 = "vhlo.convert_v1"(%181) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %183 = "vhlo.multiply_v1"(%75, %182) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %184 = "vhlo.convert_v1"(%183) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %185 = "vhlo.reshape_v1"(%184) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %186 = "vhlo.transpose_v1"(%arg5) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,128256]{0,1}">} : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x128256x!vhlo.bf16_v1>
    %187 = "vhlo.dot_general_v2"(%185, %186) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>
    %188 = "vhlo.reshape_v1"(%187) : (!vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>
    "vhlo.return_v1"(%67, %73, %187, %188) : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}
module @SyncTensorsGraph.374 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg1: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg2: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg3: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg4: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg5: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg7: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg8: tensor<1x8x128x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}, %arg9: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg10: tensor<1x8x128x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}, %arg11: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>}, %arg12: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg13: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>}, %arg14: tensor<128xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg15: tensor<7x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg16: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}) -> (tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>) {
    %c = stablehlo.constant dense<0> : tensor<7xi64>
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_1 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %0 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %1 = stablehlo.compare  LT, %arg0, %c : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %2 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %3 = stablehlo.add %arg0, %2 : tensor<7xi64>
    %4 = stablehlo.select %1, %3, %arg0 : tensor<7xi1>, tensor<7xi64>
    %5 = stablehlo.reshape %4 : (tensor<7xi64>) -> tensor<7x1xi64>
    %6 = stablehlo.convert %arg6 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %8 = stablehlo.convert %arg4 : (tensor<1x7xi64>) -> tensor<1x7xui32>
    %9 = stablehlo.reshape %8 : (tensor<1x7xui32>) -> tensor<7xui32>
    %10 = "stablehlo.gather"(%arg5, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %13 = stablehlo.power %12, %0 : tensor<1x7x3072xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %15 = stablehlo.multiply %14, %cst_1 : tensor<1x7xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x7x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x7x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x7x3072xf32>
    %23 = stablehlo.convert %22 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %24 = stablehlo.convert %23 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %25 = stablehlo.multiply %7, %24 : tensor<1x7x3072xf32>
    %26 = stablehlo.convert %25 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %27 = stablehlo.reshape %26 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %28 = stablehlo.transpose %arg2, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %29 = stablehlo.dot_general %27, %28, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %30 = stablehlo.reshape %29 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %31 = stablehlo.transpose %30, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %32 = stablehlo.convert %31 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %33 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x64x1xf32>
    %34 = stablehlo.convert %arg0 : (tensor<7xi64>) -> tensor<7xf32>
    %35 = stablehlo.reshape %34 : (tensor<7xf32>) -> tensor<1x1x7xf32>
    %36 = stablehlo.dot_general %33, %35, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %38 = stablehlo.concatenate %37, %37, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %39 = stablehlo.cosine %38 : tensor<1x7x128xf32>
    %40 = stablehlo.convert %39 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %41 = stablehlo.convert %40 : (tensor<1x7x128xbf16>) -> tensor<1x7x128xf32>
    %42 = stablehlo.broadcast_in_dim %41, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %43 = stablehlo.multiply %32, %42 : tensor<1x8x7x128xf32>
    %44 = stablehlo.convert %43 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %45 = stablehlo.slice %31 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %46 = stablehlo.negate %45 : tensor<1x8x7x64xbf16>
    %47 = stablehlo.slice %31 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %48 = stablehlo.concatenate %46, %47, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %50 = stablehlo.sine %38 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.convert %51 : (tensor<1x7x128xbf16>) -> tensor<1x7x128xf32>
    %53 = stablehlo.broadcast_in_dim %52, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %54 = stablehlo.multiply %49, %53 : tensor<1x8x7x128xf32>
    %55 = stablehlo.convert %54 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %56 = stablehlo.add %44, %55 : tensor<1x8x7x128xbf16>
    %57 = "stablehlo.scatter"(%arg8, %5, %56) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x128x128xbf16>
    %58 = sdy.sharding_constraint %57 <@mesh, [{}, {"_axis_0"}, {}, {}]> : tensor<1x8x128x128xbf16>
    %59 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %60 = stablehlo.dot_general %27, %59, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %61 = stablehlo.reshape %60 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %62 = stablehlo.transpose %61, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %63 = "stablehlo.scatter"(%arg10, %5, %62) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x128x128xbf16>
    %64 = sdy.sharding_constraint %63 <@mesh, [{}, {"_axis_0"}, {}, {}]> : tensor<1x8x128x128xbf16>
    %65 = stablehlo.convert %arg20 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %66 = stablehlo.broadcast_in_dim %65, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %67 = stablehlo.transpose %arg17, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %68 = stablehlo.dot_general %27, %67, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %69 = stablehlo.reshape %68 : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %70 = stablehlo.transpose %69, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %71 = stablehlo.convert %70 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %72 = stablehlo.broadcast_in_dim %41, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %73 = stablehlo.multiply %71, %72 : tensor<1x24x7x128xf32>
    %74 = stablehlo.convert %73 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %75 = stablehlo.slice %70 [0:1, 0:24, 0:7, 64:128] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %76 = stablehlo.negate %75 : tensor<1x24x7x64xbf16>
    %77 = stablehlo.slice %70 [0:1, 0:24, 0:7, 0:64] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %78 = stablehlo.concatenate %76, %77, dim = 3 : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %79 = stablehlo.convert %78 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %80 = stablehlo.broadcast_in_dim %52, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %81 = stablehlo.multiply %79, %80 : tensor<1x24x7x128xf32>
    %82 = stablehlo.convert %81 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %83 = stablehlo.add %74, %82 : tensor<1x24x7x128xbf16>
    %84 = stablehlo.reshape %83 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %85 = stablehlo.broadcast_in_dim %57, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %86 = stablehlo.reshape %85 : (tensor<1x8x3x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %87 = stablehlo.transpose %86, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,128]{2,3,1,0}"} : (tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %88 = stablehlo.reshape %87 : (tensor<1x24x128x128xbf16>) -> tensor<24x128x128xbf16>
    %89 = stablehlo.dot_general %84, %88, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x7x128xbf16>
    %90 = stablehlo.convert %89 : (tensor<24x7x128xbf16>) -> tensor<24x7x128xf32>
    %91 = stablehlo.reshape %90 : (tensor<24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %92 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.multiply %91, %92 : tensor<1x24x7x128xf32>
    %94 = stablehlo.convert %93 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %95 = stablehlo.convert %arg15 : (tensor<7x128xbf16>) -> tensor<7x128xf32>
    %96 = stablehlo.broadcast_in_dim %arg14, dims = [1] : (tensor<128xi64>) -> tensor<7x128xi64>
    %97 = stablehlo.broadcast_in_dim %arg0, dims = [0] : (tensor<7xi64>) -> tensor<7x128xi64>
    %98 = stablehlo.compare  GT, %96, %97 : (tensor<7x128xi64>, tensor<7x128xi64>) -> tensor<7x128xi1>
    %99 = stablehlo.convert %98 : (tensor<7x128xi1>) -> tensor<7x128xf32>
    %100 = stablehlo.multiply %95, %99 : tensor<7x128xf32>
    %101 = stablehlo.convert %100 : (tensor<7x128xf32>) -> tensor<7x128xbf16>
    %102 = stablehlo.reshape %101 : (tensor<7x128xbf16>) -> tensor<1x7x128xbf16>
    %103 = stablehlo.broadcast_in_dim %102, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %94, %103 : tensor<1x24x7x128xbf16>
    %105 = stablehlo.convert %104 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %106 = stablehlo.reduce(%105 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x7x128xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %107 = stablehlo.broadcast_in_dim %106, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x128xf32>
    %108 = stablehlo.subtract %105, %107 : tensor<1x24x7x128xf32>
    %109 = stablehlo.exponential %108 : tensor<1x24x7x128xf32>
    %110 = stablehlo.reduce(%109 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x7x128xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %111 = stablehlo.broadcast_in_dim %110, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x128xf32>
    %112 = stablehlo.divide %109, %111 : tensor<1x24x7x128xf32>
    %113 = stablehlo.convert %112 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %114 = stablehlo.reshape %113 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %115 = stablehlo.broadcast_in_dim %63, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %116 = stablehlo.reshape %115 : (tensor<1x8x3x128x128xbf16>) -> tensor<24x128x128xbf16>
    %117 = stablehlo.dot_general %114, %116, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x7x128xbf16>
    %118 = stablehlo.reshape %117 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %119 = stablehlo.transpose %118, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %120 = stablehlo.reshape %119 : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %121 = stablehlo.transpose %arg13, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %122 = stablehlo.dot_general %120, %121, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %123 = stablehlo.reshape %122 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %124 = stablehlo.add %11, %123 : tensor<1x7x3072xbf16>
    %125 = stablehlo.convert %arg18 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %126 = stablehlo.broadcast_in_dim %125, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %127 = stablehlo.convert %124 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %128 = stablehlo.power %127, %0 : tensor<1x7x3072xf32>
    %129 = stablehlo.reduce(%128 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %130 = stablehlo.multiply %129, %cst_1 : tensor<1x7xf32>
    %131 = stablehlo.reshape %130 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %132 = stablehlo.add %131, %17 : tensor<1x7x1xf32>
    %133 = stablehlo.rsqrt %132 : tensor<1x7x1xf32>
    %134 = stablehlo.reshape %133 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %136 = stablehlo.multiply %127, %135 : tensor<1x7x3072xf32>
    %137 = stablehlo.convert %136 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %138 = stablehlo.convert %137 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %139 = stablehlo.multiply %126, %138 : tensor<1x7x3072xf32>
    %140 = stablehlo.convert %139 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %141 = stablehlo.reshape %140 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %142 = stablehlo.transpose %arg19, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %143 = stablehlo.dot_general %141, %142, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %144 = stablehlo.reshape %143 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %145 = stablehlo.convert %144 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %146 = stablehlo.logistic %144 : tensor<1x7x8192xbf16>
    %147 = stablehlo.convert %146 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %148 = stablehlo.multiply %145, %147 : tensor<1x7x8192xf32>
    %149 = stablehlo.convert %148 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %150 = stablehlo.convert %149 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %151 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %152 = stablehlo.dot_general %141, %151, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %153 = stablehlo.convert %152 : (tensor<7x8192xbf16>) -> tensor<7x8192xf32>
    %154 = stablehlo.reshape %153 : (tensor<7x8192xf32>) -> tensor<1x7x8192xf32>
    %155 = stablehlo.multiply %150, %154 : tensor<1x7x8192xf32>
    %156 = stablehlo.convert %155 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %157 = stablehlo.reshape %156 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %158 = stablehlo.transpose %arg11, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %159 = stablehlo.dot_general %157, %158, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %160 = stablehlo.reshape %159 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %161 = stablehlo.add %124, %160 : tensor<1x7x3072xbf16>
    %162 = stablehlo.convert %161 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %163 = stablehlo.power %162, %0 : tensor<1x7x3072xf32>
    %164 = stablehlo.reduce(%163 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %165 = stablehlo.multiply %164, %cst_1 : tensor<1x7xf32>
    %166 = stablehlo.reshape %165 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %167 = stablehlo.add %166, %17 : tensor<1x7x1xf32>
    %168 = stablehlo.rsqrt %167 : tensor<1x7x1xf32>
    %169 = stablehlo.reshape %168 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %170 = stablehlo.broadcast_in_dim %169, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %171 = stablehlo.multiply %162, %170 : tensor<1x7x3072xf32>
    %172 = stablehlo.convert %171 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %173 = stablehlo.convert %172 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %174 = stablehlo.multiply %66, %173 : tensor<1x7x3072xf32>
    %175 = stablehlo.convert %174 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %176 = stablehlo.reshape %175 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %177 = stablehlo.transpose %arg5, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %178 = stablehlo.dot_general %176, %177, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %179 = stablehlo.reshape %178 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %58, %64, %178, %179 : tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}
module @SyncTensorsGraph.374 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg1: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg2: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg3: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg4: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg5: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg7: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg8: tensor<1x8x128x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}, %arg9: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg10: tensor<1x8x128x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}, %arg11: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>}, %arg12: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg13: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>}, %arg14: tensor<128xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg15: tensor<7x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg16: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}) -> (tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>) {
    %c = stablehlo.constant dense<0> : tensor<7xi64>
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_1 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %0 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %1 = stablehlo.compare  LT, %arg0, %c : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %2 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %3 = stablehlo.add %arg0, %2 : tensor<7xi64>
    %4 = stablehlo.select %1, %3, %arg0 : tensor<7xi1>, tensor<7xi64>
    %5 = stablehlo.reshape %4 : (tensor<7xi64>) -> tensor<7x1xi64>
    %6 = stablehlo.convert %arg6 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %8 = stablehlo.convert %arg4 : (tensor<1x7xi64>) -> tensor<1x7xui32>
    %9 = stablehlo.reshape %8 : (tensor<1x7xui32>) -> tensor<7xui32>
    %10 = "stablehlo.gather"(%arg5, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %13 = stablehlo.power %12, %0 : tensor<1x7x3072xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %15 = stablehlo.multiply %14, %cst_1 : tensor<1x7xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x7x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x7x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x7x3072xf32>
    %23 = stablehlo.convert %22 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %24 = stablehlo.convert %23 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %25 = stablehlo.multiply %7, %24 : tensor<1x7x3072xf32>
    %26 = stablehlo.convert %25 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %27 = stablehlo.reshape %26 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %28 = stablehlo.transpose %arg2, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %29 = stablehlo.dot_general %27, %28, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %30 = stablehlo.reshape %29 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %31 = stablehlo.transpose %30, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %32 = stablehlo.convert %31 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %33 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x64x1xf32>
    %34 = stablehlo.convert %arg0 : (tensor<7xi64>) -> tensor<7xf32>
    %35 = stablehlo.reshape %34 : (tensor<7xf32>) -> tensor<1x1x7xf32>
    %36 = stablehlo.dot_general %33, %35, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %38 = stablehlo.concatenate %37, %37, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %39 = stablehlo.cosine %38 : tensor<1x7x128xf32>
    %40 = stablehlo.convert %39 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %41 = stablehlo.convert %40 : (tensor<1x7x128xbf16>) -> tensor<1x7x128xf32>
    %42 = stablehlo.broadcast_in_dim %41, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %43 = stablehlo.multiply %32, %42 : tensor<1x8x7x128xf32>
    %44 = stablehlo.convert %43 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %45 = stablehlo.slice %31 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %46 = stablehlo.negate %45 : tensor<1x8x7x64xbf16>
    %47 = stablehlo.slice %31 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %48 = stablehlo.concatenate %46, %47, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %50 = stablehlo.sine %38 : tensor<1x7x128xf32>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %52 = stablehlo.convert %51 : (tensor<1x7x128xbf16>) -> tensor<1x7x128xf32>
    %53 = stablehlo.broadcast_in_dim %52, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %54 = stablehlo.multiply %49, %53 : tensor<1x8x7x128xf32>
    %55 = stablehlo.convert %54 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %56 = stablehlo.add %44, %55 : tensor<1x8x7x128xbf16>
    %57 = "stablehlo.scatter"(%arg8, %5, %56) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x128x128xbf16>
    %58 = sdy.sharding_constraint %57 <@mesh, [{}, {"_axis_0"}, {}, {}]> : tensor<1x8x128x128xbf16>
    %59 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %60 = stablehlo.dot_general %27, %59, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %61 = stablehlo.reshape %60 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %62 = stablehlo.transpose %61, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %63 = "stablehlo.scatter"(%arg10, %5, %62) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x128x128xbf16>
    %64 = sdy.sharding_constraint %63 <@mesh, [{}, {"_axis_0"}, {}, {}]> : tensor<1x8x128x128xbf16>
    %65 = stablehlo.convert %arg20 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %66 = stablehlo.broadcast_in_dim %65, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %67 = stablehlo.transpose %arg17, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %68 = stablehlo.dot_general %27, %67, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %69 = stablehlo.reshape %68 : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %70 = stablehlo.transpose %69, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %71 = stablehlo.convert %70 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %72 = stablehlo.broadcast_in_dim %41, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %73 = stablehlo.multiply %71, %72 : tensor<1x24x7x128xf32>
    %74 = stablehlo.convert %73 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %75 = stablehlo.slice %70 [0:1, 0:24, 0:7, 64:128] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %76 = stablehlo.negate %75 : tensor<1x24x7x64xbf16>
    %77 = stablehlo.slice %70 [0:1, 0:24, 0:7, 0:64] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %78 = stablehlo.concatenate %76, %77, dim = 3 : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %79 = stablehlo.convert %78 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %80 = stablehlo.broadcast_in_dim %52, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %81 = stablehlo.multiply %79, %80 : tensor<1x24x7x128xf32>
    %82 = stablehlo.convert %81 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %83 = stablehlo.add %74, %82 : tensor<1x24x7x128xbf16>
    %84 = stablehlo.reshape %83 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %85 = stablehlo.broadcast_in_dim %57, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %86 = stablehlo.reshape %85 : (tensor<1x8x3x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %87 = stablehlo.transpose %86, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,128]{2,3,1,0}"} : (tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %88 = stablehlo.reshape %87 : (tensor<1x24x128x128xbf16>) -> tensor<24x128x128xbf16>
    %89 = stablehlo.dot_general %84, %88, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x7x128xbf16>
    %90 = stablehlo.convert %89 : (tensor<24x7x128xbf16>) -> tensor<24x7x128xf32>
    %91 = stablehlo.reshape %90 : (tensor<24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %92 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<1x24x7x128xf32>
    %93 = stablehlo.multiply %91, %92 : tensor<1x24x7x128xf32>
    %94 = stablehlo.convert %93 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %95 = stablehlo.convert %arg15 : (tensor<7x128xbf16>) -> tensor<7x128xf32>
    %96 = stablehlo.broadcast_in_dim %arg14, dims = [1] : (tensor<128xi64>) -> tensor<7x128xi64>
    %97 = stablehlo.broadcast_in_dim %arg0, dims = [0] : (tensor<7xi64>) -> tensor<7x128xi64>
    %98 = stablehlo.compare  GT, %96, %97 : (tensor<7x128xi64>, tensor<7x128xi64>) -> tensor<7x128xi1>
    %99 = stablehlo.convert %98 : (tensor<7x128xi1>) -> tensor<7x128xf32>
    %100 = stablehlo.multiply %95, %99 : tensor<7x128xf32>
    %101 = stablehlo.convert %100 : (tensor<7x128xf32>) -> tensor<7x128xbf16>
    %102 = stablehlo.reshape %101 : (tensor<7x128xbf16>) -> tensor<1x7x128xbf16>
    %103 = stablehlo.broadcast_in_dim %102, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %104 = stablehlo.add %94, %103 : tensor<1x24x7x128xbf16>
    %105 = stablehlo.convert %104 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %106 = stablehlo.reduce(%105 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x7x128xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %107 = stablehlo.broadcast_in_dim %106, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x128xf32>
    %108 = stablehlo.subtract %105, %107 : tensor<1x24x7x128xf32>
    %109 = stablehlo.exponential %108 : tensor<1x24x7x128xf32>
    %110 = stablehlo.reduce(%109 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x7x128xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %111 = stablehlo.broadcast_in_dim %110, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x128xf32>
    %112 = stablehlo.divide %109, %111 : tensor<1x24x7x128xf32>
    %113 = stablehlo.convert %112 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %114 = stablehlo.reshape %113 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %115 = stablehlo.broadcast_in_dim %63, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %116 = stablehlo.reshape %115 : (tensor<1x8x3x128x128xbf16>) -> tensor<24x128x128xbf16>
    %117 = stablehlo.dot_general %114, %116, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x7x128xbf16>
    %118 = stablehlo.reshape %117 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %119 = stablehlo.transpose %118, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %120 = stablehlo.reshape %119 : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %121 = stablehlo.transpose %arg13, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %122 = stablehlo.dot_general %120, %121, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %123 = stablehlo.reshape %122 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %124 = stablehlo.add %11, %123 : tensor<1x7x3072xbf16>
    %125 = stablehlo.convert %arg18 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %126 = stablehlo.broadcast_in_dim %125, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %127 = stablehlo.convert %124 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %128 = stablehlo.power %127, %0 : tensor<1x7x3072xf32>
    %129 = stablehlo.reduce(%128 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %130 = stablehlo.multiply %129, %cst_1 : tensor<1x7xf32>
    %131 = stablehlo.reshape %130 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %132 = stablehlo.add %131, %17 : tensor<1x7x1xf32>
    %133 = stablehlo.rsqrt %132 : tensor<1x7x1xf32>
    %134 = stablehlo.reshape %133 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %136 = stablehlo.multiply %127, %135 : tensor<1x7x3072xf32>
    %137 = stablehlo.convert %136 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %138 = stablehlo.convert %137 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %139 = stablehlo.multiply %126, %138 : tensor<1x7x3072xf32>
    %140 = stablehlo.convert %139 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %141 = stablehlo.reshape %140 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %142 = stablehlo.transpose %arg19, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %143 = stablehlo.dot_general %141, %142, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %144 = stablehlo.reshape %143 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %145 = stablehlo.convert %144 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %146 = stablehlo.logistic %144 : tensor<1x7x8192xbf16>
    %147 = stablehlo.convert %146 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %148 = stablehlo.multiply %145, %147 : tensor<1x7x8192xf32>
    %149 = stablehlo.convert %148 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %150 = stablehlo.convert %149 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %151 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %152 = stablehlo.dot_general %141, %151, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %153 = stablehlo.convert %152 : (tensor<7x8192xbf16>) -> tensor<7x8192xf32>
    %154 = stablehlo.reshape %153 : (tensor<7x8192xf32>) -> tensor<1x7x8192xf32>
    %155 = stablehlo.multiply %150, %154 : tensor<1x7x8192xf32>
    %156 = stablehlo.convert %155 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %157 = stablehlo.reshape %156 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %158 = stablehlo.transpose %arg11, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %159 = stablehlo.dot_general %157, %158, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %160 = stablehlo.reshape %159 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %161 = stablehlo.add %124, %160 : tensor<1x7x3072xbf16>
    %162 = stablehlo.convert %161 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %163 = stablehlo.power %162, %0 : tensor<1x7x3072xf32>
    %164 = stablehlo.reduce(%163 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %165 = stablehlo.multiply %164, %cst_1 : tensor<1x7xf32>
    %166 = stablehlo.reshape %165 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %167 = stablehlo.add %166, %17 : tensor<1x7x1xf32>
    %168 = stablehlo.rsqrt %167 : tensor<1x7x1xf32>
    %169 = stablehlo.reshape %168 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %170 = stablehlo.broadcast_in_dim %169, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %171 = stablehlo.multiply %162, %170 : tensor<1x7x3072xf32>
    %172 = stablehlo.convert %171 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %173 = stablehlo.convert %172 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %174 = stablehlo.multiply %66, %173 : tensor<1x7x3072xf32>
    %175 = stablehlo.convert %174 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %176 = stablehlo.reshape %175 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %177 = stablehlo.transpose %arg5, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %178 = stablehlo.dot_general %176, %177, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %179 = stablehlo.reshape %178 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %58, %64, %178, %179 : tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}
module @SyncTensorsGraph.374 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<7xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<64xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1024x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1x7xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<128256x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<i64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<1024x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<3072x8192xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<8192x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<128xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<7x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<8192x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:4 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %arg16, %arg17, %arg18, %arg19, %arg20) in_shardings=[<@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, []>, <@mesh, [{}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}]>, <@mesh, []>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {}]>, <@mesh, []>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg21: tensor<7xi64>, %arg22: tensor<64xf32>, %arg23: tensor<512x3072xbf16>, %arg24: tensor<f32>, %arg25: tensor<1x7xi64>, %arg26: tensor<128256x3072xbf16>, %arg27: tensor<3072xbf16>, %arg28: tensor<i64>, %arg29: tensor<1x4x128x128xbf16>, %arg30: tensor<512x3072xbf16>, %arg31: tensor<1x4x128x128xbf16>, %arg32: tensor<3072x4096xbf16>, %arg33: tensor<4096x3072xbf16>, %arg34: tensor<3072x1536xbf16>, %arg35: tensor<128xi64>, %arg36: tensor<7x128xbf16>, %arg37: tensor<f32>, %arg38: tensor<1536x3072xbf16>, %arg39: tensor<3072xbf16>, %arg40: tensor<4096x3072xbf16>, %arg41: tensor<3072xbf16>) {
      %c = stablehlo.constant dense<0> : tensor<7xi64>
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
      %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
      %cst_1 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
      %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %1 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
      %2 = stablehlo.compare  LT, %arg21, %c : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
      %3 = stablehlo.broadcast_in_dim %arg28, dims = [] : (tensor<i64>) -> tensor<7xi64>
      %4 = stablehlo.add %arg21, %3 : tensor<7xi64>
      %5 = stablehlo.select %2, %4, %arg21 : tensor<7xi1>, tensor<7xi64>
      %6 = stablehlo.reshape %5 : (tensor<7xi64>) -> tensor<7x1xi64>
      %7 = stablehlo.convert %arg27 : (tensor<3072xbf16>) -> tensor<3072xf32>
      %8 = stablehlo.broadcast_in_dim %7, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
      %9 = stablehlo.convert %arg25 : (tensor<1x7xi64>) -> tensor<1x7xui32>
      %10 = stablehlo.reshape %9 : (tensor<1x7xui32>) -> tensor<7xui32>
      %11 = "stablehlo.gather"(%arg26, %10) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
      %12 = stablehlo.reshape %11 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
      %13 = stablehlo.convert %12 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
      %14 = stablehlo.power %13, %1 : tensor<1x7x3072xf32>
      %15 = stablehlo.reduce(%14 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
      %16 = stablehlo.multiply %15, %cst_1 : tensor<1x7xf32>
      %17 = stablehlo.reshape %16 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
      %18 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
      %19 = stablehlo.add %17, %18 : tensor<1x7x1xf32>
      %20 = stablehlo.rsqrt %19 : tensor<1x7x1xf32>
      %21 = stablehlo.reshape %20 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
      %22 = stablehlo.broadcast_in_dim %21, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
      %23 = stablehlo.multiply %13, %22 : tensor<1x7x3072xf32>
      %24 = stablehlo.convert %23 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
      %25 = stablehlo.convert %24 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
      %26 = stablehlo.multiply %8, %25 : tensor<1x7x3072xf32>
      %27 = stablehlo.convert %26 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
      %28 = stablehlo.reshape %27 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
      %29 = stablehlo.transpose %arg23, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<512x3072xbf16>) -> tensor<3072x512xbf16>
      %30 = stablehlo.dot_general %28, %29, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x512xbf16>) -> tensor<7x512xbf16>
      %31 = stablehlo.reshape %30 : (tensor<7x512xbf16>) -> tensor<1x7x4x128xbf16>
      %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x4x128xbf16>) -> tensor<1x4x7x128xbf16>
      %33 = stablehlo.convert %32 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x4x7x128xbf16>) -> tensor<1x4x7x128xf32>
      %34 = stablehlo.reshape %arg22 : (tensor<64xf32>) -> tensor<1x64x1xf32>
      %35 = stablehlo.convert %arg21 : (tensor<7xi64>) -> tensor<7xf32>
      %36 = stablehlo.reshape %35 : (tensor<7xf32>) -> tensor<1x1x7xf32>
      %37 = stablehlo.dot_general %34, %36, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
      %38 = stablehlo.transpose %37, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
      %39 = stablehlo.concatenate %38, %38, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
      %40 = stablehlo.cosine %39 : tensor<1x7x128xf32>
      %41 = stablehlo.convert %40 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
      %42 = stablehlo.convert %41 : (tensor<1x7x128xbf16>) -> tensor<1x7x128xf32>
      %43 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x4x7x128xf32>
      %44 = stablehlo.multiply %33, %43 : tensor<1x4x7x128xf32>
      %45 = stablehlo.convert %44 : (tensor<1x4x7x128xf32>) -> tensor<1x4x7x128xbf16>
      %46 = stablehlo.slice %32 [0:1, 0:4, 0:7, 64:128] : (tensor<1x4x7x128xbf16>) -> tensor<1x4x7x64xbf16>
      %47 = stablehlo.negate %46 : tensor<1x4x7x64xbf16>
      %48 = stablehlo.slice %32 [0:1, 0:4, 0:7, 0:64] : (tensor<1x4x7x128xbf16>) -> tensor<1x4x7x64xbf16>
      %49 = stablehlo.concatenate %47, %48, dim = 3 : (tensor<1x4x7x64xbf16>, tensor<1x4x7x64xbf16>) -> tensor<1x4x7x128xbf16>
      %50 = stablehlo.convert %49 : (tensor<1x4x7x128xbf16>) -> tensor<1x4x7x128xf32>
      %51 = stablehlo.sine %39 : tensor<1x7x128xf32>
      %52 = stablehlo.convert %51 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
      %53 = stablehlo.convert %52 : (tensor<1x7x128xbf16>) -> tensor<1x7x128xf32>
      %54 = stablehlo.broadcast_in_dim %53, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x4x7x128xf32>
      %55 = stablehlo.multiply %50, %54 : tensor<1x4x7x128xf32>
      %56 = stablehlo.convert %55 : (tensor<1x4x7x128xf32>) -> tensor<1x4x7x128xbf16>
      %57 = stablehlo.add %45, %56 : tensor<1x4x7x128xbf16>
      %58 = "stablehlo.scatter"(%arg29, %6, %57) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg42: tensor<bf16>, %arg43: tensor<bf16>):
        stablehlo.return %arg43 : tensor<bf16>
      }) : (tensor<1x4x128x128xbf16>, tensor<7x1xi64>, tensor<1x4x7x128xbf16>) -> tensor<1x4x128x128xbf16>
      %59 = stablehlo.transpose %arg30, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<512x3072xbf16>) -> tensor<3072x512xbf16>
      %60 = stablehlo.dot_general %28, %59, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x512xbf16>) -> tensor<7x512xbf16>
      %61 = stablehlo.reshape %60 : (tensor<7x512xbf16>) -> tensor<1x7x4x128xbf16>
      %62 = stablehlo.transpose %61, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x4x128xbf16>) -> tensor<1x4x7x128xbf16>
      %63 = "stablehlo.scatter"(%arg31, %6, %62) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg42: tensor<bf16>, %arg43: tensor<bf16>):
        stablehlo.return %arg43 : tensor<bf16>
      }) : (tensor<1x4x128x128xbf16>, tensor<7x1xi64>, tensor<1x4x7x128xbf16>) -> tensor<1x4x128x128xbf16>
      %64 = stablehlo.convert %arg41 : (tensor<3072xbf16>) -> tensor<3072xf32>
      %65 = stablehlo.broadcast_in_dim %64, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
      %66 = stablehlo.transpose %arg38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<1536x3072xbf16>) -> tensor<3072x1536xbf16>
      %67 = stablehlo.dot_general %28, %66, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1536xbf16>) -> tensor<7x1536xbf16>
      %68 = stablehlo.reshape %67 : (tensor<7x1536xbf16>) -> tensor<1x7x12x128xbf16>
      %69 = stablehlo.transpose %68, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x12x128xbf16>) -> tensor<1x12x7x128xbf16>
      %70 = stablehlo.convert %69 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x12x7x128xbf16>) -> tensor<1x12x7x128xf32>
      %71 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x12x7x128xf32>
      %72 = stablehlo.multiply %70, %71 : tensor<1x12x7x128xf32>
      %73 = stablehlo.convert %72 : (tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xbf16>
      %74 = stablehlo.slice %69 [0:1, 0:12, 0:7, 64:128] : (tensor<1x12x7x128xbf16>) -> tensor<1x12x7x64xbf16>
      %75 = stablehlo.negate %74 : tensor<1x12x7x64xbf16>
      %76 = stablehlo.slice %69 [0:1, 0:12, 0:7, 0:64] : (tensor<1x12x7x128xbf16>) -> tensor<1x12x7x64xbf16>
      %77 = stablehlo.concatenate %75, %76, dim = 3 : (tensor<1x12x7x64xbf16>, tensor<1x12x7x64xbf16>) -> tensor<1x12x7x128xbf16>
      %78 = stablehlo.convert %77 : (tensor<1x12x7x128xbf16>) -> tensor<1x12x7x128xf32>
      %79 = stablehlo.broadcast_in_dim %53, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x12x7x128xf32>
      %80 = stablehlo.multiply %78, %79 : tensor<1x12x7x128xf32>
      %81 = stablehlo.convert %80 : (tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xbf16>
      %82 = stablehlo.add %73, %81 : tensor<1x12x7x128xbf16>
      %83 = stablehlo.reshape %82 : (tensor<1x12x7x128xbf16>) -> tensor<12x7x128xbf16>
      %84 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 3, 4] : (tensor<1x4x128x128xbf16>) -> tensor<1x4x3x128x128xbf16>
      %85 = stablehlo.reshape %84 : (tensor<1x4x3x128x128xbf16>) -> tensor<1x12x128x128xbf16>
      %86 = stablehlo.transpose %85, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,128]{2,3,1,0}"} : (tensor<1x12x128x128xbf16>) -> tensor<1x12x128x128xbf16>
      %87 = stablehlo.reshape %86 : (tensor<1x12x128x128xbf16>) -> tensor<12x128x128xbf16>
      %88 = stablehlo.dot_general %83, %87, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<12x7x128xbf16>, tensor<12x128x128xbf16>) -> tensor<12x7x128xbf16>
      %89 = stablehlo.convert %88 : (tensor<12x7x128xbf16>) -> tensor<12x7x128xf32>
      %90 = stablehlo.reshape %89 : (tensor<12x7x128xf32>) -> tensor<1x12x7x128xf32>
      %91 = stablehlo.broadcast_in_dim %arg37, dims = [] : (tensor<f32>) -> tensor<1x12x7x128xf32>
      %92 = stablehlo.multiply %90, %91 : tensor<1x12x7x128xf32>
      %93 = stablehlo.convert %92 : (tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xbf16>
      %94 = stablehlo.convert %arg36 : (tensor<7x128xbf16>) -> tensor<7x128xf32>
      %95 = stablehlo.broadcast_in_dim %arg35, dims = [1] : (tensor<128xi64>) -> tensor<7x128xi64>
      %96 = stablehlo.broadcast_in_dim %arg21, dims = [0] : (tensor<7xi64>) -> tensor<7x128xi64>
      %97 = stablehlo.compare  GT, %95, %96 : (tensor<7x128xi64>, tensor<7x128xi64>) -> tensor<7x128xi1>
      %98 = stablehlo.convert %97 : (tensor<7x128xi1>) -> tensor<7x128xf32>
      %99 = stablehlo.multiply %94, %98 : tensor<7x128xf32>
      %100 = stablehlo.convert %99 : (tensor<7x128xf32>) -> tensor<7x128xbf16>
      %101 = stablehlo.reshape %100 : (tensor<7x128xbf16>) -> tensor<1x7x128xbf16>
      %102 = stablehlo.broadcast_in_dim %101, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x12x7x128xbf16>
      %103 = stablehlo.add %93, %102 : tensor<1x12x7x128xbf16>
      %104 = stablehlo.convert %103 : (tensor<1x12x7x128xbf16>) -> tensor<1x12x7x128xf32>
      %105 = stablehlo.reduce(%104 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x12x7x128xf32>, tensor<f32>) -> tensor<1x12x7xf32>
      %106 = stablehlo.broadcast_in_dim %105, dims = [0, 1, 2] : (tensor<1x12x7xf32>) -> tensor<1x12x7x128xf32>
      %107 = stablehlo.subtract %104, %106 : tensor<1x12x7x128xf32>
      %108 = stablehlo.exponential %107 : tensor<1x12x7x128xf32>
      %109 = stablehlo.reduce(%108 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x12x7x128xf32>, tensor<f32>) -> tensor<1x12x7xf32>
      %110 = stablehlo.broadcast_in_dim %109, dims = [0, 1, 2] : (tensor<1x12x7xf32>) -> tensor<1x12x7x128xf32>
      %111 = stablehlo.divide %108, %110 : tensor<1x12x7x128xf32>
      %112 = stablehlo.convert %111 : (tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xbf16>
      %113 = stablehlo.reshape %112 : (tensor<1x12x7x128xbf16>) -> tensor<12x7x128xbf16>
      %114 = stablehlo.broadcast_in_dim %63, dims = [0, 1, 3, 4] : (tensor<1x4x128x128xbf16>) -> tensor<1x4x3x128x128xbf16>
      %115 = stablehlo.reshape %114 : (tensor<1x4x3x128x128xbf16>) -> tensor<12x128x128xbf16>
      %116 = stablehlo.dot_general %113, %115, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<12x7x128xbf16>, tensor<12x128x128xbf16>) -> tensor<12x7x128xbf16>
      %117 = stablehlo.reshape %116 : (tensor<12x7x128xbf16>) -> tensor<1x12x7x128xbf16>
      %118 = stablehlo.transpose %117, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x12x7x128xbf16>) -> tensor<1x7x12x128xbf16>
      %119 = stablehlo.reshape %118 : (tensor<1x7x12x128xbf16>) -> tensor<7x1536xbf16>
      %120 = stablehlo.transpose %arg34, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x1536xbf16>) -> tensor<1536x3072xbf16>
      %121 = stablehlo.dot_general %119, %120, contracting_dims = [1] x [0] : (tensor<7x1536xbf16>, tensor<1536x3072xbf16>) -> tensor<7x3072xbf16>
      %122 = "stablehlo.all_reduce"(%121) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> ({
      ^bb0(%arg42: tensor<bf16>, %arg43: tensor<bf16>):
        %181 = stablehlo.add %arg42, %arg43 : tensor<bf16>
        stablehlo.return %181 : tensor<bf16>
      }) : (tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
      %123 = stablehlo.reshape %122 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
      %124 = stablehlo.add %12, %123 : tensor<1x7x3072xbf16>
      %125 = stablehlo.convert %arg39 : (tensor<3072xbf16>) -> tensor<3072xf32>
      %126 = stablehlo.broadcast_in_dim %125, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
      %127 = stablehlo.convert %124 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
      %128 = stablehlo.power %127, %1 : tensor<1x7x3072xf32>
      %129 = stablehlo.reduce(%128 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
      %130 = stablehlo.multiply %129, %cst_1 : tensor<1x7xf32>
      %131 = stablehlo.reshape %130 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
      %132 = stablehlo.add %131, %18 : tensor<1x7x1xf32>
      %133 = stablehlo.rsqrt %132 : tensor<1x7x1xf32>
      %134 = stablehlo.reshape %133 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
      %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
      %136 = stablehlo.multiply %127, %135 : tensor<1x7x3072xf32>
      %137 = stablehlo.convert %136 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
      %138 = stablehlo.convert %137 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
      %139 = stablehlo.multiply %126, %138 : tensor<1x7x3072xf32>
      %140 = stablehlo.convert %139 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
      %141 = stablehlo.reshape %140 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
      %142 = stablehlo.transpose %arg40, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<4096x3072xbf16>) -> tensor<3072x4096xbf16>
      %143 = stablehlo.dot_general %141, %142, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<7x4096xbf16>
      %144 = stablehlo.reshape %143 : (tensor<7x4096xbf16>) -> tensor<1x7x4096xbf16>
      %145 = stablehlo.convert %144 : (tensor<1x7x4096xbf16>) -> tensor<1x7x4096xf32>
      %146 = stablehlo.logistic %144 : tensor<1x7x4096xbf16>
      %147 = stablehlo.convert %146 : (tensor<1x7x4096xbf16>) -> tensor<1x7x4096xf32>
      %148 = stablehlo.multiply %145, %147 : tensor<1x7x4096xf32>
      %149 = stablehlo.convert %148 : (tensor<1x7x4096xf32>) -> tensor<1x7x4096xbf16>
      %150 = stablehlo.convert %149 : (tensor<1x7x4096xbf16>) -> tensor<1x7x4096xf32>
      %151 = stablehlo.transpose %arg33, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<4096x3072xbf16>) -> tensor<3072x4096xbf16>
      %152 = stablehlo.dot_general %141, %151, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<7x4096xbf16>
      %153 = stablehlo.convert %152 : (tensor<7x4096xbf16>) -> tensor<7x4096xf32>
      %154 = stablehlo.reshape %153 : (tensor<7x4096xf32>) -> tensor<1x7x4096xf32>
      %155 = stablehlo.multiply %150, %154 : tensor<1x7x4096xf32>
      %156 = stablehlo.convert %155 : (tensor<1x7x4096xf32>) -> tensor<1x7x4096xbf16>
      %157 = stablehlo.reshape %156 : (tensor<1x7x4096xbf16>) -> tensor<7x4096xbf16>
      %158 = stablehlo.transpose %arg32, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x4096xbf16>) -> tensor<4096x3072xbf16>
      %159 = stablehlo.dot_general %157, %158, contracting_dims = [1] x [0] : (tensor<7x4096xbf16>, tensor<4096x3072xbf16>) -> tensor<7x3072xbf16>
      %160 = "stablehlo.all_reduce"(%159) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> ({
      ^bb0(%arg42: tensor<bf16>, %arg43: tensor<bf16>):
        %181 = stablehlo.add %arg42, %arg43 : tensor<bf16>
        stablehlo.return %181 : tensor<bf16>
      }) : (tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
      %161 = stablehlo.reshape %160 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
      %162 = stablehlo.add %124, %161 : tensor<1x7x3072xbf16>
      %163 = stablehlo.convert %162 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
      %164 = stablehlo.power %163, %1 : tensor<1x7x3072xf32>
      %165 = stablehlo.reduce(%164 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
      %166 = stablehlo.multiply %165, %cst_1 : tensor<1x7xf32>
      %167 = stablehlo.reshape %166 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
      %168 = stablehlo.add %167, %18 : tensor<1x7x1xf32>
      %169 = stablehlo.rsqrt %168 : tensor<1x7x1xf32>
      %170 = stablehlo.reshape %169 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
      %171 = stablehlo.broadcast_in_dim %170, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
      %172 = stablehlo.multiply %163, %171 : tensor<1x7x3072xf32>
      %173 = stablehlo.convert %172 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
      %174 = stablehlo.convert %173 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
      %175 = stablehlo.multiply %65, %174 : tensor<1x7x3072xf32>
      %176 = stablehlo.convert %175 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
      %177 = stablehlo.reshape %176 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
      %178 = stablehlo.transpose %arg26, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
      %179 = stablehlo.dot_general %177, %178, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
      %180 = stablehlo.reshape %179 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
      sdy.return %58, %63, %179, %180 : tensor<1x4x128x128xbf16>, tensor<1x4x128x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
    } : (tensor<7xi64>, tensor<64xf32>, tensor<1024x3072xbf16>, tensor<f32>, tensor<1x7xi64>, tensor<128256x3072xbf16>, tensor<3072xbf16>, tensor<i64>, tensor<1x8x128x128xbf16>, tensor<1024x3072xbf16>, tensor<1x8x128x128xbf16>, tensor<3072x8192xbf16>, tensor<8192x3072xbf16>, tensor<3072x3072xbf16>, tensor<128xi64>, tensor<7x128xbf16>, tensor<f32>, tensor<3072x3072xbf16>, tensor<3072xbf16>, tensor<8192x3072xbf16>, tensor<3072xbf16>) -> (tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>)
    return %0#0, %0#1, %0#2, %0#3 : tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}
module @SyncTensorsGraph.374 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  func.func @main(%arg0: tensor<7xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<64xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1024x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1x7xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<128256x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<i64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<1024x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<3072x8192xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<8192x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<128xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<7x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<8192x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = ttir.empty() : tensor<7xi64>
    %1 = "ttir.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi64>
    %2 = ttir.empty() : tensor<64xf32>
    %3 = "ttir.mesh_shard"(%arg1, %2) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32>, tensor<64xf32>) -> tensor<64xf32>
    %4 = ttir.empty() : tensor<512x3072xbf16>
    %5 = "ttir.mesh_shard"(%arg2, %4) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16>, tensor<512x3072xbf16>) -> tensor<512x3072xbf16>
    %6 = ttir.empty() : tensor<f32>
    %7 = "ttir.mesh_shard"(%arg3, %6) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %8 = ttir.empty() : tensor<1x7xi64>
    %9 = "ttir.mesh_shard"(%arg4, %8) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x7xi64>, tensor<1x7xi64>) -> tensor<1x7xi64>
    %10 = ttir.empty() : tensor<128256x3072xbf16>
    %11 = "ttir.mesh_shard"(%arg5, %10) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16>, tensor<128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %12 = ttir.empty() : tensor<3072xbf16>
    %13 = "ttir.mesh_shard"(%arg6, %12) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16>, tensor<3072xbf16>) -> tensor<3072xbf16>
    %14 = ttir.empty() : tensor<i64>
    %15 = "ttir.mesh_shard"(%arg7, %14) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %16 = ttir.empty() : tensor<1x4x128x128xbf16>
    %17 = "ttir.mesh_shard"(%arg8, %16) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x128x128xbf16>, tensor<1x4x128x128xbf16>) -> tensor<1x4x128x128xbf16>
    %18 = ttir.empty() : tensor<512x3072xbf16>
    %19 = "ttir.mesh_shard"(%arg9, %18) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16>, tensor<512x3072xbf16>) -> tensor<512x3072xbf16>
    %20 = ttir.empty() : tensor<1x4x128x128xbf16>
    %21 = "ttir.mesh_shard"(%arg10, %20) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x128x128xbf16>, tensor<1x4x128x128xbf16>) -> tensor<1x4x128x128xbf16>
    %22 = ttir.empty() : tensor<3072x4096xbf16>
    %23 = "ttir.mesh_shard"(%arg11, %22) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x8192xbf16>, tensor<3072x4096xbf16>) -> tensor<3072x4096xbf16>
    %24 = ttir.empty() : tensor<4096x3072xbf16>
    %25 = "ttir.mesh_shard"(%arg12, %24) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16>, tensor<4096x3072xbf16>) -> tensor<4096x3072xbf16>
    %26 = ttir.empty() : tensor<3072x1536xbf16>
    %27 = "ttir.mesh_shard"(%arg13, %26) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16>, tensor<3072x1536xbf16>) -> tensor<3072x1536xbf16>
    %28 = ttir.empty() : tensor<128xi64>
    %29 = "ttir.mesh_shard"(%arg14, %28) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128xi64>, tensor<128xi64>) -> tensor<128xi64>
    %30 = ttir.empty() : tensor<7x128xbf16>
    %31 = "ttir.mesh_shard"(%arg15, %30) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<7x128xbf16>, tensor<7x128xbf16>) -> tensor<7x128xbf16>
    %32 = ttir.empty() : tensor<f32>
    %33 = "ttir.mesh_shard"(%arg16, %32) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %34 = ttir.empty() : tensor<1536x3072xbf16>
    %35 = "ttir.mesh_shard"(%arg17, %34) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16>, tensor<1536x3072xbf16>) -> tensor<1536x3072xbf16>
    %36 = ttir.empty() : tensor<3072xbf16>
    %37 = "ttir.mesh_shard"(%arg18, %36) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16>, tensor<3072xbf16>) -> tensor<3072xbf16>
    %38 = ttir.empty() : tensor<4096x3072xbf16>
    %39 = "ttir.mesh_shard"(%arg19, %38) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16>, tensor<4096x3072xbf16>) -> tensor<4096x3072xbf16>
    %40 = ttir.empty() : tensor<3072xbf16>
    %41 = "ttir.mesh_shard"(%arg20, %40) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16>, tensor<3072xbf16>) -> tensor<3072xbf16>
    %42 = "ttir.constant"() <{value = dense<0> : tensor<7xi64>}> : () -> tensor<7xi64>
    %43 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<f32>}> : () -> tensor<f32>
    %44 = "ttir.constant"() <{value = dense<0xFF800000> : tensor<f32>}> : () -> tensor<f32>
    %45 = "ttir.constant"() <{value = dense<3.25520843E-4> : tensor<1x7xf32>}> : () -> tensor<1x7xf32>
    %46 = "ttir.constant"() <{value = dense<2.000000e+00> : tensor<f32>}> : () -> tensor<f32>
    %47 = ttir.empty() : tensor<1x1x1xf32>
    %48 = "ttir.reshape"(%46, %47) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %49 = ttir.empty() : tensor<1x7x3072xf32>
    %50 = "ttir.broadcast"(%48, %49) <{broadcast_dimensions = array<i64: 1, 7, 3072>}> : (tensor<1x1x1xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %51 = ttir.empty() : tensor<7xi1>
    %52 = "ttir.lt"(%1, %42, %51) : (tensor<7xi64>, tensor<7xi64>, tensor<7xi1>) -> tensor<7xi1>
    %53 = ttir.empty() : tensor<1xi64>
    %54 = "ttir.reshape"(%15, %53) <{shape = [1 : i32]}> : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %55 = ttir.empty() : tensor<7xi64>
    %56 = "ttir.broadcast"(%54, %55) <{broadcast_dimensions = array<i64: 7>}> : (tensor<1xi64>, tensor<7xi64>) -> tensor<7xi64>
    %57 = ttir.empty() : tensor<7xi64>
    %58 = "ttir.add"(%1, %56, %57) : (tensor<7xi64>, tensor<7xi64>, tensor<7xi64>) -> tensor<7xi64>
    %59 = ttir.empty() : tensor<7xi64>
    %60 = "ttir.where"(%52, %58, %1, %59) : (tensor<7xi1>, tensor<7xi64>, tensor<7xi64>, tensor<7xi64>) -> tensor<7xi64>
    %61 = ttir.empty() : tensor<7x1xi64>
    %62 = "ttir.reshape"(%60, %61) <{shape = [7 : i32, 1 : i32]}> : (tensor<7xi64>, tensor<7x1xi64>) -> tensor<7x1xi64>
    %63 = ttir.empty() : tensor<3072xf32>
    %64 = "ttir.typecast"(%13, %63) <{conservative_folding = false}> : (tensor<3072xbf16>, tensor<3072xf32>) -> tensor<3072xf32>
    %65 = ttir.empty() : tensor<1x1x3072xf32>
    %66 = "ttir.reshape"(%64, %65) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %67 = ttir.empty() : tensor<1x7x3072xf32>
    %68 = "ttir.broadcast"(%66, %67) <{broadcast_dimensions = array<i64: 1, 7, 1>}> : (tensor<1x1x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %69 = ttir.empty() : tensor<1x7xui32>
    %70 = "ttir.typecast"(%9, %69) <{conservative_folding = false}> : (tensor<1x7xi64>, tensor<1x7xui32>) -> tensor<1x7xui32>
    %71 = ttir.empty() : tensor<7xui32>
    %72 = "ttir.reshape"(%70, %71) <{shape = [7 : i32]}> : (tensor<1x7xui32>, tensor<7xui32>) -> tensor<7xui32>
    %73 = ttir.empty() : tensor<7x3072xbf16>
    %74 = "ttir.gather"(%11, %72, %73) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 3072>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<128256x3072xbf16>, tensor<7xui32>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %75 = ttir.empty() : tensor<1x7x3072xbf16>
    %76 = "ttir.reshape"(%74, %75) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %77 = ttir.empty() : tensor<1x7x3072xf32>
    %78 = "ttir.typecast"(%76, %77) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %79 = ttir.empty() : tensor<1x7x3072xf32>
    %80 = "ttir.pow"(%78, %50, %79) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %81 = ttir.empty() : tensor<1x7xf32>
    %82 = "ttir.sum"(%80, %81) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %83 = ttir.empty() : tensor<1x7xf32>
    %84 = "ttir.multiply"(%82, %45, %83) : (tensor<1x7xf32>, tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %85 = ttir.empty() : tensor<1x7x1xf32>
    %86 = "ttir.reshape"(%84, %85) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %87 = ttir.empty() : tensor<1x1x1xf32>
    %88 = "ttir.reshape"(%7, %87) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %89 = ttir.empty() : tensor<1x7x1xf32>
    %90 = "ttir.broadcast"(%88, %89) <{broadcast_dimensions = array<i64: 1, 7, 1>}> : (tensor<1x1x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %91 = ttir.empty() : tensor<1x7x1xf32>
    %92 = "ttir.add"(%86, %90, %91) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %93 = ttir.empty() : tensor<1x7x1xf32>
    %94 = "ttir.rsqrt"(%92, %93) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %95 = ttir.empty() : tensor<1x7xf32>
    %96 = "ttir.reshape"(%94, %95) <{shape = [1 : i32, 7 : i32]}> : (tensor<1x7x1xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %97 = ttir.empty() : tensor<1x7x1xf32>
    %98 = "ttir.reshape"(%96, %97) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %99 = ttir.empty() : tensor<1x7x3072xf32>
    %100 = "ttir.broadcast"(%98, %99) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x7x1xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %101 = ttir.empty() : tensor<1x7x3072xf32>
    %102 = "ttir.multiply"(%78, %100, %101) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %103 = ttir.empty() : tensor<1x7x3072xbf16>
    %104 = "ttir.typecast"(%102, %103) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %105 = ttir.empty() : tensor<1x7x3072xf32>
    %106 = "ttir.typecast"(%104, %105) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %107 = ttir.empty() : tensor<1x7x3072xf32>
    %108 = "ttir.multiply"(%68, %106, %107) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %109 = ttir.empty() : tensor<1x7x3072xbf16>
    %110 = "ttir.typecast"(%108, %109) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %111 = ttir.empty() : tensor<7x3072xbf16>
    %112 = "ttir.reshape"(%110, %111) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x7x3072xbf16>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %113 = ttir.empty() : tensor<3072x512xbf16>
    %114 = "ttir.permute"(%5, %113) <{permutation = array<i64: 1, 0>}> : (tensor<512x3072xbf16>, tensor<3072x512xbf16>) -> tensor<3072x512xbf16>
    %115 = "ttir.dot_general"(%112, %114) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x512xbf16>) -> tensor<7x512xbf16>
    %116 = ttir.empty() : tensor<1x7x4x128xbf16>
    %117 = "ttir.reshape"(%115, %116) <{shape = [1 : i32, 7 : i32, 4 : i32, 128 : i32]}> : (tensor<7x512xbf16>, tensor<1x7x4x128xbf16>) -> tensor<1x7x4x128xbf16>
    %118 = ttir.empty() : tensor<1x4x7x128xbf16>
    %119 = "ttir.permute"(%117, %118) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x4x128xbf16>, tensor<1x4x7x128xbf16>) -> tensor<1x4x7x128xbf16>
    %120 = ttir.empty() : tensor<1x4x7x128xf32>
    %121 = "ttir.typecast"(%119, %120) <{conservative_folding = false}> : (tensor<1x4x7x128xbf16>, tensor<1x4x7x128xf32>) -> tensor<1x4x7x128xf32>
    %122 = ttir.empty() : tensor<1x64x1xf32>
    %123 = "ttir.reshape"(%3, %122) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
    %124 = ttir.empty() : tensor<7xf32>
    %125 = "ttir.typecast"(%1, %124) <{conservative_folding = false}> : (tensor<7xi64>, tensor<7xf32>) -> tensor<7xf32>
    %126 = ttir.empty() : tensor<1x1x7xf32>
    %127 = "ttir.reshape"(%125, %126) <{shape = [1 : i32, 1 : i32, 7 : i32]}> : (tensor<7xf32>, tensor<1x1x7xf32>) -> tensor<1x1x7xf32>
    %128 = "ttir.dot_general"(%123, %127) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %129 = ttir.empty() : tensor<1x7x64xf32>
    %130 = "ttir.permute"(%128, %129) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x7xf32>, tensor<1x7x64xf32>) -> tensor<1x7x64xf32>
    %131 = ttir.empty() : tensor<1x7x128xf32>
    %132 = "ttir.concat"(%130, %130, %131) <{dim = 2 : si32}> : (tensor<1x7x64xf32>, tensor<1x7x64xf32>, tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
    %133 = ttir.empty() : tensor<1x7x128xf32>
    %134 = "ttir.cos"(%132, %133) : (tensor<1x7x128xf32>, tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
    %135 = ttir.empty() : tensor<1x7x128xbf16>
    %136 = "ttir.typecast"(%134, %135) <{conservative_folding = false}> : (tensor<1x7x128xf32>, tensor<1x7x128xbf16>) -> tensor<1x7x128xbf16>
    %137 = ttir.empty() : tensor<1x7x128xf32>
    %138 = "ttir.typecast"(%136, %137) <{conservative_folding = false}> : (tensor<1x7x128xbf16>, tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
    %139 = ttir.empty() : tensor<1x1x7x128xf32>
    %140 = "ttir.reshape"(%138, %139) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32>, tensor<1x1x7x128xf32>) -> tensor<1x1x7x128xf32>
    %141 = ttir.empty() : tensor<1x4x7x128xf32>
    %142 = "ttir.broadcast"(%140, %141) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x7x128xf32>, tensor<1x4x7x128xf32>) -> tensor<1x4x7x128xf32>
    %143 = ttir.empty() : tensor<1x4x7x128xf32>
    %144 = "ttir.multiply"(%121, %142, %143) : (tensor<1x4x7x128xf32>, tensor<1x4x7x128xf32>, tensor<1x4x7x128xf32>) -> tensor<1x4x7x128xf32>
    %145 = ttir.empty() : tensor<1x4x7x128xbf16>
    %146 = "ttir.typecast"(%144, %145) <{conservative_folding = false}> : (tensor<1x4x7x128xf32>, tensor<1x4x7x128xbf16>) -> tensor<1x4x7x128xbf16>
    %147 = ttir.empty() : tensor<1x4x7x64xbf16>
    %148 = "ttir.slice_static"(%119, %147) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 7 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x7x128xbf16>, tensor<1x4x7x64xbf16>) -> tensor<1x4x7x64xbf16>
    %149 = ttir.empty() : tensor<1x4x7x64xbf16>
    %150 = "ttir.neg"(%148, %149) : (tensor<1x4x7x64xbf16>, tensor<1x4x7x64xbf16>) -> tensor<1x4x7x64xbf16>
    %151 = ttir.empty() : tensor<1x4x7x64xbf16>
    %152 = "ttir.slice_static"(%119, %151) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 7 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x7x128xbf16>, tensor<1x4x7x64xbf16>) -> tensor<1x4x7x64xbf16>
    %153 = ttir.empty() : tensor<1x4x7x128xbf16>
    %154 = "ttir.concat"(%150, %152, %153) <{dim = 3 : si32}> : (tensor<1x4x7x64xbf16>, tensor<1x4x7x64xbf16>, tensor<1x4x7x128xbf16>) -> tensor<1x4x7x128xbf16>
    %155 = ttir.empty() : tensor<1x4x7x128xf32>
    %156 = "ttir.typecast"(%154, %155) <{conservative_folding = false}> : (tensor<1x4x7x128xbf16>, tensor<1x4x7x128xf32>) -> tensor<1x4x7x128xf32>
    %157 = ttir.empty() : tensor<1x7x128xf32>
    %158 = "ttir.sin"(%132, %157) : (tensor<1x7x128xf32>, tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
    %159 = ttir.empty() : tensor<1x7x128xbf16>
    %160 = "ttir.typecast"(%158, %159) <{conservative_folding = false}> : (tensor<1x7x128xf32>, tensor<1x7x128xbf16>) -> tensor<1x7x128xbf16>
    %161 = ttir.empty() : tensor<1x7x128xf32>
    %162 = "ttir.typecast"(%160, %161) <{conservative_folding = false}> : (tensor<1x7x128xbf16>, tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
    %163 = ttir.empty() : tensor<1x1x7x128xf32>
    %164 = "ttir.reshape"(%162, %163) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32>, tensor<1x1x7x128xf32>) -> tensor<1x1x7x128xf32>
    %165 = ttir.empty() : tensor<1x4x7x128xf32>
    %166 = "ttir.broadcast"(%164, %165) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x7x128xf32>, tensor<1x4x7x128xf32>) -> tensor<1x4x7x128xf32>
    %167 = ttir.empty() : tensor<1x4x7x128xf32>
    %168 = "ttir.multiply"(%156, %166, %167) : (tensor<1x4x7x128xf32>, tensor<1x4x7x128xf32>, tensor<1x4x7x128xf32>) -> tensor<1x4x7x128xf32>
    %169 = ttir.empty() : tensor<1x4x7x128xbf16>
    %170 = "ttir.typecast"(%168, %169) <{conservative_folding = false}> : (tensor<1x4x7x128xf32>, tensor<1x4x7x128xbf16>) -> tensor<1x4x7x128xbf16>
    %171 = ttir.empty() : tensor<1x4x7x128xbf16>
    %172 = "ttir.add"(%146, %170, %171) : (tensor<1x4x7x128xbf16>, tensor<1x4x7x128xbf16>, tensor<1x4x7x128xbf16>) -> tensor<1x4x7x128xbf16>
    %173 = ttir.empty() : tensor<1x4x128x128xbf16>
    %174 = "ttir.scatter"(%17, %62, %172, %173) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x4x128x128xbf16>, tensor<7x1xi64>, tensor<1x4x7x128xbf16>, tensor<1x4x128x128xbf16>) -> tensor<1x4x128x128xbf16>
    %175 = ttir.empty() : tensor<3072x512xbf16>
    %176 = "ttir.permute"(%19, %175) <{permutation = array<i64: 1, 0>}> : (tensor<512x3072xbf16>, tensor<3072x512xbf16>) -> tensor<3072x512xbf16>
    %177 = "ttir.dot_general"(%112, %176) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x512xbf16>) -> tensor<7x512xbf16>
    %178 = ttir.empty() : tensor<1x7x4x128xbf16>
    %179 = "ttir.reshape"(%177, %178) <{shape = [1 : i32, 7 : i32, 4 : i32, 128 : i32]}> : (tensor<7x512xbf16>, tensor<1x7x4x128xbf16>) -> tensor<1x7x4x128xbf16>
    %180 = ttir.empty() : tensor<1x4x7x128xbf16>
    %181 = "ttir.permute"(%179, %180) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x4x128xbf16>, tensor<1x4x7x128xbf16>) -> tensor<1x4x7x128xbf16>
    %182 = ttir.empty() : tensor<1x4x128x128xbf16>
    %183 = "ttir.scatter"(%21, %62, %181, %182) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x4x128x128xbf16>, tensor<7x1xi64>, tensor<1x4x7x128xbf16>, tensor<1x4x128x128xbf16>) -> tensor<1x4x128x128xbf16>
    %184 = ttir.empty() : tensor<3072xf32>
    %185 = "ttir.typecast"(%41, %184) <{conservative_folding = false}> : (tensor<3072xbf16>, tensor<3072xf32>) -> tensor<3072xf32>
    %186 = ttir.empty() : tensor<1x1x3072xf32>
    %187 = "ttir.reshape"(%185, %186) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %188 = ttir.empty() : tensor<1x7x3072xf32>
    %189 = "ttir.broadcast"(%187, %188) <{broadcast_dimensions = array<i64: 1, 7, 1>}> : (tensor<1x1x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %190 = ttir.empty() : tensor<3072x1536xbf16>
    %191 = "ttir.permute"(%35, %190) <{permutation = array<i64: 1, 0>}> : (tensor<1536x3072xbf16>, tensor<3072x1536xbf16>) -> tensor<3072x1536xbf16>
    %192 = "ttir.dot_general"(%112, %191) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x1536xbf16>) -> tensor<7x1536xbf16>
    %193 = ttir.empty() : tensor<1x7x12x128xbf16>
    %194 = "ttir.reshape"(%192, %193) <{shape = [1 : i32, 7 : i32, 12 : i32, 128 : i32]}> : (tensor<7x1536xbf16>, tensor<1x7x12x128xbf16>) -> tensor<1x7x12x128xbf16>
    %195 = ttir.empty() : tensor<1x12x7x128xbf16>
    %196 = "ttir.permute"(%194, %195) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x12x128xbf16>, tensor<1x12x7x128xbf16>) -> tensor<1x12x7x128xbf16>
    %197 = ttir.empty() : tensor<1x12x7x128xf32>
    %198 = "ttir.typecast"(%196, %197) <{conservative_folding = false}> : (tensor<1x12x7x128xbf16>, tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xf32>
    %199 = ttir.empty() : tensor<1x1x7x128xf32>
    %200 = "ttir.reshape"(%138, %199) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32>, tensor<1x1x7x128xf32>) -> tensor<1x1x7x128xf32>
    %201 = ttir.empty() : tensor<1x12x7x128xf32>
    %202 = "ttir.broadcast"(%200, %201) <{broadcast_dimensions = array<i64: 1, 12, 1, 1>}> : (tensor<1x1x7x128xf32>, tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xf32>
    %203 = ttir.empty() : tensor<1x12x7x128xf32>
    %204 = "ttir.multiply"(%198, %202, %203) : (tensor<1x12x7x128xf32>, tensor<1x12x7x128xf32>, tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xf32>
    %205 = ttir.empty() : tensor<1x12x7x128xbf16>
    %206 = "ttir.typecast"(%204, %205) <{conservative_folding = false}> : (tensor<1x12x7x128xf32>, tensor<1x12x7x128xbf16>) -> tensor<1x12x7x128xbf16>
    %207 = ttir.empty() : tensor<1x12x7x64xbf16>
    %208 = "ttir.slice_static"(%196, %207) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 12 : i32, 7 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x7x128xbf16>, tensor<1x12x7x64xbf16>) -> tensor<1x12x7x64xbf16>
    %209 = ttir.empty() : tensor<1x12x7x64xbf16>
    %210 = "ttir.neg"(%208, %209) : (tensor<1x12x7x64xbf16>, tensor<1x12x7x64xbf16>) -> tensor<1x12x7x64xbf16>
    %211 = ttir.empty() : tensor<1x12x7x64xbf16>
    %212 = "ttir.slice_static"(%196, %211) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 12 : i32, 7 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x7x128xbf16>, tensor<1x12x7x64xbf16>) -> tensor<1x12x7x64xbf16>
    %213 = ttir.empty() : tensor<1x12x7x128xbf16>
    %214 = "ttir.concat"(%210, %212, %213) <{dim = 3 : si32}> : (tensor<1x12x7x64xbf16>, tensor<1x12x7x64xbf16>, tensor<1x12x7x128xbf16>) -> tensor<1x12x7x128xbf16>
    %215 = ttir.empty() : tensor<1x12x7x128xf32>
    %216 = "ttir.typecast"(%214, %215) <{conservative_folding = false}> : (tensor<1x12x7x128xbf16>, tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xf32>
    %217 = ttir.empty() : tensor<1x1x7x128xf32>
    %218 = "ttir.reshape"(%162, %217) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32>, tensor<1x1x7x128xf32>) -> tensor<1x1x7x128xf32>
    %219 = ttir.empty() : tensor<1x12x7x128xf32>
    %220 = "ttir.broadcast"(%218, %219) <{broadcast_dimensions = array<i64: 1, 12, 1, 1>}> : (tensor<1x1x7x128xf32>, tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xf32>
    %221 = ttir.empty() : tensor<1x12x7x128xf32>
    %222 = "ttir.multiply"(%216, %220, %221) : (tensor<1x12x7x128xf32>, tensor<1x12x7x128xf32>, tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xf32>
    %223 = ttir.empty() : tensor<1x12x7x128xbf16>
    %224 = "ttir.typecast"(%222, %223) <{conservative_folding = false}> : (tensor<1x12x7x128xf32>, tensor<1x12x7x128xbf16>) -> tensor<1x12x7x128xbf16>
    %225 = ttir.empty() : tensor<1x12x7x128xbf16>
    %226 = "ttir.add"(%206, %224, %225) : (tensor<1x12x7x128xbf16>, tensor<1x12x7x128xbf16>, tensor<1x12x7x128xbf16>) -> tensor<1x12x7x128xbf16>
    %227 = ttir.empty() : tensor<12x7x128xbf16>
    %228 = "ttir.reshape"(%226, %227) <{shape = [12 : i32, 7 : i32, 128 : i32]}> : (tensor<1x12x7x128xbf16>, tensor<12x7x128xbf16>) -> tensor<12x7x128xbf16>
    %229 = ttir.empty() : tensor<1x4x1x128x128xbf16>
    %230 = "ttir.reshape"(%174, %229) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x4x128x128xbf16>, tensor<1x4x1x128x128xbf16>) -> tensor<1x4x1x128x128xbf16>
    %231 = ttir.empty() : tensor<1x4x3x128x128xbf16>
    %232 = "ttir.broadcast"(%230, %231) <{broadcast_dimensions = array<i64: 1, 1, 3, 1, 1>}> : (tensor<1x4x1x128x128xbf16>, tensor<1x4x3x128x128xbf16>) -> tensor<1x4x3x128x128xbf16>
    %233 = ttir.empty() : tensor<1x12x128x128xbf16>
    %234 = "ttir.reshape"(%232, %233) <{shape = [1 : i32, 12 : i32, 128 : i32, 128 : i32]}> : (tensor<1x4x3x128x128xbf16>, tensor<1x12x128x128xbf16>) -> tensor<1x12x128x128xbf16>
    %235 = ttir.empty() : tensor<1x12x128x128xbf16>
    %236 = "ttir.permute"(%234, %235) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x12x128x128xbf16>, tensor<1x12x128x128xbf16>) -> tensor<1x12x128x128xbf16>
    %237 = ttir.empty() : tensor<12x128x128xbf16>
    %238 = "ttir.reshape"(%236, %237) <{shape = [12 : i32, 128 : i32, 128 : i32]}> : (tensor<1x12x128x128xbf16>, tensor<12x128x128xbf16>) -> tensor<12x128x128xbf16>
    %239 = "ttir.dot_general"(%228, %238) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<12x7x128xbf16>, tensor<12x128x128xbf16>) -> tensor<12x7x128xbf16>
    %240 = ttir.empty() : tensor<12x7x128xf32>
    %241 = "ttir.typecast"(%239, %240) <{conservative_folding = false}> : (tensor<12x7x128xbf16>, tensor<12x7x128xf32>) -> tensor<12x7x128xf32>
    %242 = ttir.empty() : tensor<1x12x7x128xf32>
    %243 = "ttir.reshape"(%241, %242) <{shape = [1 : i32, 12 : i32, 7 : i32, 128 : i32]}> : (tensor<12x7x128xf32>, tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xf32>
    %244 = ttir.empty() : tensor<1x1x1x1xf32>
    %245 = "ttir.reshape"(%33, %244) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %246 = ttir.empty() : tensor<1x12x7x128xf32>
    %247 = "ttir.broadcast"(%245, %246) <{broadcast_dimensions = array<i64: 1, 12, 7, 128>}> : (tensor<1x1x1x1xf32>, tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xf32>
    %248 = ttir.empty() : tensor<1x12x7x128xf32>
    %249 = "ttir.multiply"(%243, %247, %248) : (tensor<1x12x7x128xf32>, tensor<1x12x7x128xf32>, tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xf32>
    %250 = ttir.empty() : tensor<1x12x7x128xbf16>
    %251 = "ttir.typecast"(%249, %250) <{conservative_folding = false}> : (tensor<1x12x7x128xf32>, tensor<1x12x7x128xbf16>) -> tensor<1x12x7x128xbf16>
    %252 = ttir.empty() : tensor<7x128xf32>
    %253 = "ttir.typecast"(%31, %252) <{conservative_folding = false}> : (tensor<7x128xbf16>, tensor<7x128xf32>) -> tensor<7x128xf32>
    %254 = ttir.empty() : tensor<1x128xi64>
    %255 = "ttir.reshape"(%29, %254) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xi64>, tensor<1x128xi64>) -> tensor<1x128xi64>
    %256 = ttir.empty() : tensor<7x128xi64>
    %257 = "ttir.broadcast"(%255, %256) <{broadcast_dimensions = array<i64: 7, 1>}> : (tensor<1x128xi64>, tensor<7x128xi64>) -> tensor<7x128xi64>
    %258 = ttir.empty() : tensor<7x1xi64>
    %259 = "ttir.reshape"(%1, %258) <{shape = [7 : i32, 1 : i32]}> : (tensor<7xi64>, tensor<7x1xi64>) -> tensor<7x1xi64>
    %260 = ttir.empty() : tensor<7x128xi64>
    %261 = "ttir.broadcast"(%259, %260) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<7x1xi64>, tensor<7x128xi64>) -> tensor<7x128xi64>
    %262 = ttir.empty() : tensor<7x128xi1>
    %263 = "ttir.gt"(%257, %261, %262) : (tensor<7x128xi64>, tensor<7x128xi64>, tensor<7x128xi1>) -> tensor<7x128xi1>
    %264 = ttir.empty() : tensor<7x128xf32>
    %265 = "ttir.typecast"(%263, %264) <{conservative_folding = false}> : (tensor<7x128xi1>, tensor<7x128xf32>) -> tensor<7x128xf32>
    %266 = ttir.empty() : tensor<7x128xf32>
    %267 = "ttir.multiply"(%253, %265, %266) : (tensor<7x128xf32>, tensor<7x128xf32>, tensor<7x128xf32>) -> tensor<7x128xf32>
    %268 = ttir.empty() : tensor<7x128xbf16>
    %269 = "ttir.typecast"(%267, %268) <{conservative_folding = false}> : (tensor<7x128xf32>, tensor<7x128xbf16>) -> tensor<7x128xbf16>
    %270 = ttir.empty() : tensor<1x7x128xbf16>
    %271 = "ttir.reshape"(%269, %270) <{shape = [1 : i32, 7 : i32, 128 : i32]}> : (tensor<7x128xbf16>, tensor<1x7x128xbf16>) -> tensor<1x7x128xbf16>
    %272 = ttir.empty() : tensor<1x1x7x128xbf16>
    %273 = "ttir.reshape"(%271, %272) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xbf16>, tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %274 = ttir.empty() : tensor<1x12x7x128xbf16>
    %275 = "ttir.broadcast"(%273, %274) <{broadcast_dimensions = array<i64: 1, 12, 1, 1>}> : (tensor<1x1x7x128xbf16>, tensor<1x12x7x128xbf16>) -> tensor<1x12x7x128xbf16>
    %276 = ttir.empty() : tensor<1x12x7x128xbf16>
    %277 = "ttir.add"(%251, %275, %276) : (tensor<1x12x7x128xbf16>, tensor<1x12x7x128xbf16>, tensor<1x12x7x128xbf16>) -> tensor<1x12x7x128xbf16>
    %278 = ttir.empty() : tensor<1x12x7x128xf32>
    %279 = "ttir.typecast"(%277, %278) <{conservative_folding = false}> : (tensor<1x12x7x128xbf16>, tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xf32>
    %280 = ttir.empty() : tensor<1x12x7xf32>
    %281 = "ttir.max"(%279, %280) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x12x7x128xf32>, tensor<1x12x7xf32>) -> tensor<1x12x7xf32>
    %282 = ttir.empty() : tensor<1x12x7x1xf32>
    %283 = "ttir.reshape"(%281, %282) <{shape = [1 : i32, 12 : i32, 7 : i32, 1 : i32]}> : (tensor<1x12x7xf32>, tensor<1x12x7x1xf32>) -> tensor<1x12x7x1xf32>
    %284 = ttir.empty() : tensor<1x12x7x128xf32>
    %285 = "ttir.broadcast"(%283, %284) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<1x12x7x1xf32>, tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xf32>
    %286 = ttir.empty() : tensor<1x12x7x128xf32>
    %287 = "ttir.subtract"(%279, %285, %286) : (tensor<1x12x7x128xf32>, tensor<1x12x7x128xf32>, tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xf32>
    %288 = ttir.empty() : tensor<1x12x7x128xf32>
    %289 = "ttir.exp"(%287, %288) : (tensor<1x12x7x128xf32>, tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xf32>
    %290 = ttir.empty() : tensor<1x12x7xf32>
    %291 = "ttir.sum"(%289, %290) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x12x7x128xf32>, tensor<1x12x7xf32>) -> tensor<1x12x7xf32>
    %292 = ttir.empty() : tensor<1x12x7x1xf32>
    %293 = "ttir.reshape"(%291, %292) <{shape = [1 : i32, 12 : i32, 7 : i32, 1 : i32]}> : (tensor<1x12x7xf32>, tensor<1x12x7x1xf32>) -> tensor<1x12x7x1xf32>
    %294 = ttir.empty() : tensor<1x12x7x128xf32>
    %295 = "ttir.broadcast"(%293, %294) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<1x12x7x1xf32>, tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xf32>
    %296 = ttir.empty() : tensor<1x12x7x128xf32>
    %297 = "ttir.div"(%289, %295, %296) : (tensor<1x12x7x128xf32>, tensor<1x12x7x128xf32>, tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xf32>
    %298 = ttir.empty() : tensor<1x12x7x128xbf16>
    %299 = "ttir.typecast"(%297, %298) <{conservative_folding = false}> : (tensor<1x12x7x128xf32>, tensor<1x12x7x128xbf16>) -> tensor<1x12x7x128xbf16>
    %300 = ttir.empty() : tensor<12x7x128xbf16>
    %301 = "ttir.reshape"(%299, %300) <{shape = [12 : i32, 7 : i32, 128 : i32]}> : (tensor<1x12x7x128xbf16>, tensor<12x7x128xbf16>) -> tensor<12x7x128xbf16>
    %302 = ttir.empty() : tensor<1x4x1x128x128xbf16>
    %303 = "ttir.reshape"(%183, %302) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x4x128x128xbf16>, tensor<1x4x1x128x128xbf16>) -> tensor<1x4x1x128x128xbf16>
    %304 = ttir.empty() : tensor<1x4x3x128x128xbf16>
    %305 = "ttir.broadcast"(%303, %304) <{broadcast_dimensions = array<i64: 1, 1, 3, 1, 1>}> : (tensor<1x4x1x128x128xbf16>, tensor<1x4x3x128x128xbf16>) -> tensor<1x4x3x128x128xbf16>
    %306 = ttir.empty() : tensor<12x128x128xbf16>
    %307 = "ttir.reshape"(%305, %306) <{shape = [12 : i32, 128 : i32, 128 : i32]}> : (tensor<1x4x3x128x128xbf16>, tensor<12x128x128xbf16>) -> tensor<12x128x128xbf16>
    %308 = "ttir.dot_general"(%301, %307) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<12x7x128xbf16>, tensor<12x128x128xbf16>) -> tensor<12x7x128xbf16>
    %309 = ttir.empty() : tensor<1x12x7x128xbf16>
    %310 = "ttir.reshape"(%308, %309) <{shape = [1 : i32, 12 : i32, 7 : i32, 128 : i32]}> : (tensor<12x7x128xbf16>, tensor<1x12x7x128xbf16>) -> tensor<1x12x7x128xbf16>
    %311 = ttir.empty() : tensor<1x7x12x128xbf16>
    %312 = "ttir.permute"(%310, %311) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x12x7x128xbf16>, tensor<1x7x12x128xbf16>) -> tensor<1x7x12x128xbf16>
    %313 = ttir.empty() : tensor<7x1536xbf16>
    %314 = "ttir.reshape"(%312, %313) <{shape = [7 : i32, 1536 : i32]}> : (tensor<1x7x12x128xbf16>, tensor<7x1536xbf16>) -> tensor<7x1536xbf16>
    %315 = ttir.empty() : tensor<1536x3072xbf16>
    %316 = "ttir.permute"(%27, %315) <{permutation = array<i64: 1, 0>}> : (tensor<3072x1536xbf16>, tensor<1536x3072xbf16>) -> tensor<1536x3072xbf16>
    %317 = "ttir.dot_general"(%314, %316) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x1536xbf16>, tensor<1536x3072xbf16>) -> tensor<7x3072xbf16>
    %318 = ttir.empty() : tensor<7x3072xbf16>
    %319 = "ttir.all_reduce"(%317, %318) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<7x3072xbf16>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %320 = ttir.empty() : tensor<1x7x3072xbf16>
    %321 = "ttir.reshape"(%319, %320) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %322 = ttir.empty() : tensor<1x7x3072xbf16>
    %323 = "ttir.add"(%76, %321, %322) : (tensor<1x7x3072xbf16>, tensor<1x7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %324 = ttir.empty() : tensor<3072xf32>
    %325 = "ttir.typecast"(%37, %324) <{conservative_folding = false}> : (tensor<3072xbf16>, tensor<3072xf32>) -> tensor<3072xf32>
    %326 = ttir.empty() : tensor<1x1x3072xf32>
    %327 = "ttir.reshape"(%325, %326) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %328 = ttir.empty() : tensor<1x7x3072xf32>
    %329 = "ttir.broadcast"(%327, %328) <{broadcast_dimensions = array<i64: 1, 7, 1>}> : (tensor<1x1x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %330 = ttir.empty() : tensor<1x7x3072xf32>
    %331 = "ttir.typecast"(%323, %330) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %332 = ttir.empty() : tensor<1x7x3072xf32>
    %333 = "ttir.pow"(%331, %50, %332) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %334 = ttir.empty() : tensor<1x7xf32>
    %335 = "ttir.sum"(%333, %334) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %336 = ttir.empty() : tensor<1x7xf32>
    %337 = "ttir.multiply"(%335, %45, %336) : (tensor<1x7xf32>, tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %338 = ttir.empty() : tensor<1x7x1xf32>
    %339 = "ttir.reshape"(%337, %338) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %340 = ttir.empty() : tensor<1x7x1xf32>
    %341 = "ttir.add"(%339, %90, %340) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %342 = ttir.empty() : tensor<1x7x1xf32>
    %343 = "ttir.rsqrt"(%341, %342) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %344 = ttir.empty() : tensor<1x7xf32>
    %345 = "ttir.reshape"(%343, %344) <{shape = [1 : i32, 7 : i32]}> : (tensor<1x7x1xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %346 = ttir.empty() : tensor<1x7x1xf32>
    %347 = "ttir.reshape"(%345, %346) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %348 = ttir.empty() : tensor<1x7x3072xf32>
    %349 = "ttir.broadcast"(%347, %348) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x7x1xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %350 = ttir.empty() : tensor<1x7x3072xf32>
    %351 = "ttir.multiply"(%331, %349, %350) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %352 = ttir.empty() : tensor<1x7x3072xbf16>
    %353 = "ttir.typecast"(%351, %352) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %354 = ttir.empty() : tensor<1x7x3072xf32>
    %355 = "ttir.typecast"(%353, %354) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %356 = ttir.empty() : tensor<1x7x3072xf32>
    %357 = "ttir.multiply"(%329, %355, %356) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %358 = ttir.empty() : tensor<1x7x3072xbf16>
    %359 = "ttir.typecast"(%357, %358) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %360 = ttir.empty() : tensor<7x3072xbf16>
    %361 = "ttir.reshape"(%359, %360) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x7x3072xbf16>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %362 = ttir.empty() : tensor<3072x4096xbf16>
    %363 = "ttir.permute"(%39, %362) <{permutation = array<i64: 1, 0>}> : (tensor<4096x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<3072x4096xbf16>
    %364 = "ttir.dot_general"(%361, %363) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<7x4096xbf16>
    %365 = ttir.empty() : tensor<1x7x4096xbf16>
    %366 = "ttir.reshape"(%364, %365) <{shape = [1 : i32, 7 : i32, 4096 : i32]}> : (tensor<7x4096xbf16>, tensor<1x7x4096xbf16>) -> tensor<1x7x4096xbf16>
    %367 = ttir.empty() : tensor<1x7x4096xf32>
    %368 = "ttir.typecast"(%366, %367) <{conservative_folding = false}> : (tensor<1x7x4096xbf16>, tensor<1x7x4096xf32>) -> tensor<1x7x4096xf32>
    %369 = ttir.empty() : tensor<1x7x4096xbf16>
    %370 = "ttir.sigmoid"(%366, %369) : (tensor<1x7x4096xbf16>, tensor<1x7x4096xbf16>) -> tensor<1x7x4096xbf16>
    %371 = ttir.empty() : tensor<1x7x4096xf32>
    %372 = "ttir.typecast"(%370, %371) <{conservative_folding = false}> : (tensor<1x7x4096xbf16>, tensor<1x7x4096xf32>) -> tensor<1x7x4096xf32>
    %373 = ttir.empty() : tensor<1x7x4096xf32>
    %374 = "ttir.multiply"(%368, %372, %373) : (tensor<1x7x4096xf32>, tensor<1x7x4096xf32>, tensor<1x7x4096xf32>) -> tensor<1x7x4096xf32>
    %375 = ttir.empty() : tensor<1x7x4096xbf16>
    %376 = "ttir.typecast"(%374, %375) <{conservative_folding = false}> : (tensor<1x7x4096xf32>, tensor<1x7x4096xbf16>) -> tensor<1x7x4096xbf16>
    %377 = ttir.empty() : tensor<1x7x4096xf32>
    %378 = "ttir.typecast"(%376, %377) <{conservative_folding = false}> : (tensor<1x7x4096xbf16>, tensor<1x7x4096xf32>) -> tensor<1x7x4096xf32>
    %379 = ttir.empty() : tensor<3072x4096xbf16>
    %380 = "ttir.permute"(%25, %379) <{permutation = array<i64: 1, 0>}> : (tensor<4096x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<3072x4096xbf16>
    %381 = "ttir.dot_general"(%361, %380) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<7x4096xbf16>
    %382 = ttir.empty() : tensor<7x4096xf32>
    %383 = "ttir.typecast"(%381, %382) <{conservative_folding = false}> : (tensor<7x4096xbf16>, tensor<7x4096xf32>) -> tensor<7x4096xf32>
    %384 = ttir.empty() : tensor<1x7x4096xf32>
    %385 = "ttir.reshape"(%383, %384) <{shape = [1 : i32, 7 : i32, 4096 : i32]}> : (tensor<7x4096xf32>, tensor<1x7x4096xf32>) -> tensor<1x7x4096xf32>
    %386 = ttir.empty() : tensor<1x7x4096xf32>
    %387 = "ttir.multiply"(%378, %385, %386) : (tensor<1x7x4096xf32>, tensor<1x7x4096xf32>, tensor<1x7x4096xf32>) -> tensor<1x7x4096xf32>
    %388 = ttir.empty() : tensor<1x7x4096xbf16>
    %389 = "ttir.typecast"(%387, %388) <{conservative_folding = false}> : (tensor<1x7x4096xf32>, tensor<1x7x4096xbf16>) -> tensor<1x7x4096xbf16>
    %390 = ttir.empty() : tensor<7x4096xbf16>
    %391 = "ttir.reshape"(%389, %390) <{shape = [7 : i32, 4096 : i32]}> : (tensor<1x7x4096xbf16>, tensor<7x4096xbf16>) -> tensor<7x4096xbf16>
    %392 = ttir.empty() : tensor<4096x3072xbf16>
    %393 = "ttir.permute"(%23, %392) <{permutation = array<i64: 1, 0>}> : (tensor<3072x4096xbf16>, tensor<4096x3072xbf16>) -> tensor<4096x3072xbf16>
    %394 = "ttir.dot_general"(%391, %393) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x4096xbf16>, tensor<4096x3072xbf16>) -> tensor<7x3072xbf16>
    %395 = ttir.empty() : tensor<7x3072xbf16>
    %396 = "ttir.all_reduce"(%394, %395) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<7x3072xbf16>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %397 = ttir.empty() : tensor<1x7x3072xbf16>
    %398 = "ttir.reshape"(%396, %397) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %399 = ttir.empty() : tensor<1x7x3072xbf16>
    %400 = "ttir.add"(%323, %398, %399) : (tensor<1x7x3072xbf16>, tensor<1x7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %401 = ttir.empty() : tensor<1x7x3072xf32>
    %402 = "ttir.typecast"(%400, %401) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %403 = ttir.empty() : tensor<1x7x3072xf32>
    %404 = "ttir.pow"(%402, %50, %403) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %405 = ttir.empty() : tensor<1x7xf32>
    %406 = "ttir.sum"(%404, %405) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %407 = ttir.empty() : tensor<1x7xf32>
    %408 = "ttir.multiply"(%406, %45, %407) : (tensor<1x7xf32>, tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %409 = ttir.empty() : tensor<1x7x1xf32>
    %410 = "ttir.reshape"(%408, %409) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %411 = ttir.empty() : tensor<1x7x1xf32>
    %412 = "ttir.add"(%410, %90, %411) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %413 = ttir.empty() : tensor<1x7x1xf32>
    %414 = "ttir.rsqrt"(%412, %413) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %415 = ttir.empty() : tensor<1x7xf32>
    %416 = "ttir.reshape"(%414, %415) <{shape = [1 : i32, 7 : i32]}> : (tensor<1x7x1xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %417 = ttir.empty() : tensor<1x7x1xf32>
    %418 = "ttir.reshape"(%416, %417) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %419 = ttir.empty() : tensor<1x7x3072xf32>
    %420 = "ttir.broadcast"(%418, %419) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x7x1xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %421 = ttir.empty() : tensor<1x7x3072xf32>
    %422 = "ttir.multiply"(%402, %420, %421) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %423 = ttir.empty() : tensor<1x7x3072xbf16>
    %424 = "ttir.typecast"(%422, %423) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %425 = ttir.empty() : tensor<1x7x3072xf32>
    %426 = "ttir.typecast"(%424, %425) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %427 = ttir.empty() : tensor<1x7x3072xf32>
    %428 = "ttir.multiply"(%189, %426, %427) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %429 = ttir.empty() : tensor<1x7x3072xbf16>
    %430 = "ttir.typecast"(%428, %429) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %431 = ttir.empty() : tensor<7x3072xbf16>
    %432 = "ttir.reshape"(%430, %431) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x7x3072xbf16>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %433 = ttir.empty() : tensor<3072x128256xbf16>
    %434 = "ttir.permute"(%11, %433) <{permutation = array<i64: 1, 0>}> : (tensor<128256x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<3072x128256xbf16>
    %435 = "ttir.dot_general"(%432, %434) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %436 = ttir.empty() : tensor<1x7x128256xbf16>
    %437 = "ttir.reshape"(%435, %436) <{shape = [1 : i32, 7 : i32, 128256 : i32]}> : (tensor<7x128256xbf16>, tensor<1x7x128256xbf16>) -> tensor<1x7x128256xbf16>
    %438 = ttir.empty() : tensor<1x8x128x128xbf16>
    %439 = "ttir.mesh_shard"(%174, %438) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x4x128x128xbf16>, tensor<1x8x128x128xbf16>) -> tensor<1x8x128x128xbf16>
    %440 = ttir.empty() : tensor<1x8x128x128xbf16>
    %441 = "ttir.mesh_shard"(%183, %440) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x4x128x128xbf16>, tensor<1x8x128x128xbf16>) -> tensor<1x8x128x128xbf16>
    %442 = ttir.empty() : tensor<7x128256xbf16>
    %443 = "ttir.mesh_shard"(%435, %442) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<7x128256xbf16>, tensor<7x128256xbf16>) -> tensor<7x128256xbf16>
    %444 = ttir.empty() : tensor<1x7x128256xbf16>
    %445 = "ttir.mesh_shard"(%437, %444) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x7x128256xbf16>, tensor<1x7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %439, %441, %443, %445 : tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}
module @SyncTensorsGraph.374 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.374 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99776, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073184736, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99776, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073193056, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x16, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 2)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 2, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main_const_eval_0() -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %2 : tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main(%arg0: tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<128xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<7x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg7) <{force = false}> : (tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %2 = "ttnn.full"(%1) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 3.25520843E-4 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x7>}> : (!ttnn.device) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %3 = "ttnn.mesh_shard"(%arg0, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %4 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %5 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %6 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %7 = "ttnn.mesh_shard"(%arg4, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %8 = "ttnn.mesh_shard"(%arg5, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg5) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %9 = "ttnn.mesh_shard"(%arg6, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg6) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %10 = "ttnn.mesh_shard"(%arg8, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg8) <{force = false}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %11 = "ttnn.mesh_shard"(%arg9, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg9) <{force = false}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %12 = "ttnn.mesh_shard"(%arg10, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg10) <{force = false}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %13 = "ttnn.mesh_shard"(%arg11, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg11) <{force = false}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %14 = "ttnn.mesh_shard"(%arg12, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg12) <{force = false}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %15 = "ttnn.mesh_shard"(%arg13, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg13) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %16 = "ttnn.mesh_shard"(%arg14, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<128xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg14) <{force = false}> : (tensor<128xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %17 = "ttnn.mesh_shard"(%arg15, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<7x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<7x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg15) <{force = false}> : (tensor<7x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %18 = "ttnn.mesh_shard"(%arg16, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg16) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %19 = "ttnn.mesh_shard"(%arg17, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg17) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %20 = "ttnn.mesh_shard"(%arg18, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg18) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %21 = "ttnn.mesh_shard"(%arg19, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg19) <{force = false}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %22 = "ttnn.mesh_shard"(%arg20, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg20) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %23 = "ttnn.typecast"(%9) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %24 = "ttnn.reshape"(%23) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%23) <{force = false}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %25 = "ttnn.typecast"(%7) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %26 = "ttnn.reshape"(%25) <{shape = [7 : i32]}> : (tensor<1x7xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%25) <{force = false}> : (tensor<1x7xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %27 = "ttnn.from_device"(%26) : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%26) <{force = false}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %28 = "ttnn.to_layout"(%27) <{layout = #ttnn.layout<row_major>}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%27) <{force = false}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> ()
        %29 = "ttnn.to_device"(%28, %1) <{memory_config = #ttnn.memory_config<<dram>, <interleaved>>}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%28) <{force = false}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<system_memory>>>>) -> ()
        %30 = "ttnn.embedding"(%29, %8) : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%29) <{force = false}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %31 = "ttnn.typecast"(%30) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %32 = "ttnn.reshape"(%31) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %33 = "ttnn.pow"(%32, %0) : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%32) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %34 = "ttnn.sum"(%33) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%33) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %35 = "ttnn.multiply"(%34, %2) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%34) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %36 = "ttnn.reshape"(%35) <{shape = [7 : i32, 1 : i32]}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%35) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %37 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %38 = "ttnn.add"(%36, %37) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%36) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %39 = "ttnn.rsqrt"(%38) : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%38) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %40 = "ttnn.multiply"(%31, %39) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%39) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%31) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %41 = "ttnn.multiply"(%24, %40) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%40) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%24) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %42 = "ttnn.typecast"(%41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%41) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %43 = "ttnn.matmul"(%42, %5) <{transpose_a = false, transpose_b = true}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %44 = "ttnn.reshape"(%43) <{shape = [1 : i32, 7 : i32, 4 : i32, 128 : i32]}> : (tensor<7x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x4x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%43) <{force = false}> : (tensor<7x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %45 = "ttnn.permute"(%44) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x4x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%44) <{force = false}> : (tensor<1x7x4x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %46 = "ttnn.typecast"(%45) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %47 = "ttnn.reshape"(%4) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %48 = "ttnn.typecast"(%3) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %49 = "ttnn.reshape"(%48) <{shape = [1 : i32, 1 : i32, 7 : i32]}> : (tensor<7xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%48) <{force = false}> : (tensor<7xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %50 = "ttnn.matmul"(%47, %49) <{transpose_a = false, transpose_b = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%49) <{force = false}> : (tensor<1x1x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%47) <{force = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %51 = "ttnn.permute"(%50) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%50) <{force = false}> : (tensor<1x64x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %52 = "ttnn.concat"(%51, %51) <{dim = 2 : si32}> : (tensor<1x7x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%51) <{force = false}> : (tensor<1x7x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %53 = "ttnn.cos"(%52) : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %54 = "ttnn.reshape"(%53) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %55 = "ttnn.multiply"(%46, %54) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%54) <{force = false}> : (tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%46) <{force = false}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %56 = "ttnn.typecast"(%55) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%55) <{force = false}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %57 = "ttnn.slice_static"(%45) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 7 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %58 = "ttnn.neg"(%57) : (tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%57) <{force = false}> : (tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %59 = "ttnn.slice_static"(%45) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 7 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%45) <{force = false}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %60 = "ttnn.concat"(%58, %59) <{dim = 3 : si32}> : (tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%59) <{force = false}> : (tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%58) <{force = false}> : (tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %61 = "ttnn.typecast"(%60) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%60) <{force = false}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %62 = "ttnn.sin"(%52) : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%52) <{force = false}> : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %63 = "ttnn.reshape"(%62) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %64 = "ttnn.multiply"(%61, %63) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%63) <{force = false}> : (tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%61) <{force = false}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %65 = "ttnn.typecast"(%64) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%64) <{force = false}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %66 = "ttnn.add"(%56, %65) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%65) <{force = false}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%56) <{force = false}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.fill_cache"(%10, %66) <{batch_offset = 0 : i32}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%66) <{force = false}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %67 = "ttnn.matmul"(%42, %11) <{transpose_a = false, transpose_b = true}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %68 = "ttnn.reshape"(%67) <{shape = [1 : i32, 7 : i32, 4 : i32, 128 : i32]}> : (tensor<7x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x4x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%67) <{force = false}> : (tensor<7x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %69 = "ttnn.permute"(%68) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x4x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%68) <{force = false}> : (tensor<1x7x4x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.fill_cache"(%12, %69) <{batch_offset = 0 : i32}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%69) <{force = false}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %70 = "ttnn.typecast"(%22) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%22) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %71 = "ttnn.reshape"(%70) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%70) <{force = false}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %72 = "ttnn.matmul"(%42, %19) <{transpose_a = false, transpose_b = true}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%42) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %73 = "ttnn.reshape"(%72) <{shape = [1 : i32, 7 : i32, 12 : i32, 128 : i32]}> : (tensor<7x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x12x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%72) <{force = false}> : (tensor<7x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %74 = "ttnn.permute"(%73) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x12x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%73) <{force = false}> : (tensor<1x7x12x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %75 = "ttnn.typecast"(%74) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %76 = "ttnn.reshape"(%75) <{shape = [12 : i32, 7 : i32, 128 : i32]}> : (tensor<1x12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%75) <{force = false}> : (tensor<1x12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %77 = "ttnn.multiply"(%76, %53) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%76) <{force = false}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%53) <{force = false}> : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %78 = "ttnn.typecast"(%77) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%77) <{force = false}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %79 = "ttnn.slice_static"(%74) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 12 : i32, 7 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %80 = "ttnn.neg"(%79) : (tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%79) <{force = false}> : (tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %81 = "ttnn.reshape"(%80) <{shape = [12 : i32, 7 : i32, 64 : i32]}> : (tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%80) <{force = false}> : (tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %82 = "ttnn.slice_static"(%74) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 12 : i32, 7 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%74) <{force = false}> : (tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %83 = "ttnn.reshape"(%82) <{shape = [12 : i32, 7 : i32, 64 : i32]}> : (tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%82) <{force = false}> : (tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %84 = "ttnn.concat"(%81, %83) <{dim = 2 : si32}> : (tensor<12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%83) <{force = false}> : (tensor<12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%81) <{force = false}> : (tensor<12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %85 = "ttnn.typecast"(%84) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%84) <{force = false}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %86 = "ttnn.multiply"(%85, %62) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%85) <{force = false}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%62) <{force = false}> : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %87 = "ttnn.typecast"(%86) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%86) <{force = false}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %88 = "ttnn.add"(%78, %87) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%87) <{force = false}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%78) <{force = false}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %89 = "ttnn.reshape"(%10) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 512 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %90 = "ttnn.repeat"(%89) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x4x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 512 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1536 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%89) <{force = false}> : (tensor<1x4x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 512 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %91 = "ttnn.reshape"(%90) <{shape = [1 : i32, 12 : i32, 128 : i32, 128 : i32]}> : (tensor<1x4x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1536 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%90) <{force = false}> : (tensor<1x4x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1536 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %92 = "ttnn.permute"(%91) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%91) <{force = false}> : (tensor<1x12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %93 = "ttnn.reshape"(%92) <{shape = [12 : i32, 128 : i32, 128 : i32]}> : (tensor<1x12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%92) <{force = false}> : (tensor<1x12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %94 = "ttnn.matmul"(%88, %93) <{transpose_a = false, transpose_b = false}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%93) <{force = false}> : (tensor<12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%88) <{force = false}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %95 = "ttnn.typecast"(%94) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%94) <{force = false}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %96 = "ttnn.reshape"(%95) <{shape = [1 : i32, 12 : i32, 7 : i32, 128 : i32]}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%95) <{force = false}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %97 = "ttnn.reshape"(%18) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %98 = "ttnn.multiply"(%96, %97) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%97) <{force = false}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%96) <{force = false}> : (tensor<1x12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %99 = "ttnn.typecast"(%98) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%98) <{force = false}> : (tensor<1x12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %100 = "ttnn.typecast"(%17) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<7x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %101 = "ttnn.reshape"(%100) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<7x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%100) <{force = false}> : (tensor<7x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %102 = "ttnn.reshape"(%16) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<128xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %103 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 7 : i32, 1 : i32]}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %104 = "ttnn.typecast"(%102) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x128xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%102) <{force = false}> : (tensor<1x1x1x128xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %105 = "ttnn.typecast"(%103) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x7x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%103) <{force = false}> : (tensor<1x1x7x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %106 = "ttnn.gt"(%104, %105) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%105) <{force = false}> : (tensor<1x1x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%104) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %107 = "ttnn.typecast"(%106) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%106) <{force = false}> : (tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %108 = "ttnn.typecast"(%107) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%107) <{force = false}> : (tensor<1x1x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %109 = "ttnn.multiply"(%101, %108) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%108) <{force = false}> : (tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%101) <{force = false}> : (tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %110 = "ttnn.typecast"(%109) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%109) <{force = false}> : (tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %111 = "ttnn.add"(%99, %110) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%110) <{force = false}> : (tensor<1x1x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%99) <{force = false}> : (tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %112 = "ttnn.typecast"(%111) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%111) <{force = false}> : (tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %113 = "ttnn.max"(%112) <{dim_arg = [3 : i32], keep_dim = true}> : (tensor<1x12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %114 = "ttnn.neg"(%113) : (tensor<1x12x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%113) <{force = false}> : (tensor<1x12x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %115 = "ttnn.add"(%112, %114) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x12x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%114) <{force = false}> : (tensor<1x12x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%112) <{force = false}> : (tensor<1x12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %116 = "ttnn.softmax"(%115) <{dimension = 3 : si32}> : (tensor<1x12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%115) <{force = false}> : (tensor<1x12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %117 = "ttnn.typecast"(%116) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%116) <{force = false}> : (tensor<1x12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %118 = "ttnn.reshape"(%117) <{shape = [12 : i32, 7 : i32, 128 : i32]}> : (tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%117) <{force = false}> : (tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %119 = "ttnn.reshape"(%12) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 512 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %120 = "ttnn.repeat"(%119) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x4x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 512 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1536 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%119) <{force = false}> : (tensor<1x4x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 512 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %121 = "ttnn.reshape"(%120) <{shape = [12 : i32, 128 : i32, 128 : i32]}> : (tensor<1x4x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1536 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%120) <{force = false}> : (tensor<1x4x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1536 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %122 = "ttnn.matmul"(%118, %121) <{transpose_a = false, transpose_b = false}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%121) <{force = false}> : (tensor<12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%118) <{force = false}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %123 = "ttnn.reshape"(%122) <{shape = [1 : i32, 12 : i32, 7 : i32, 128 : i32]}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%122) <{force = false}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %124 = "ttnn.concatenate_heads"(%123) : (tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%123) <{force = false}> : (tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %125 = "ttnn.reshape"(%124) <{shape = [7 : i32, 1536 : i32]}> : (tensor<1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%124) <{force = false}> : (tensor<1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %126 = "ttnn.matmul"(%125, %15) <{transpose_a = false, transpose_b = true}> : (tensor<7x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%125) <{force = false}> : (tensor<7x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %127 = "ttnn.reshape"(%126) <{shape = [1 : i32, 1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%126) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %128 = "ttnn.reduce_scatter"(%127, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 3 : si32}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%127) <{force = false}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %129 = "ttnn.all_gather"(%128, %1) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%128) <{force = false}> : (tensor<1x1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %130 = "ttnn.reshape"(%129) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%129) <{force = false}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %131 = "ttnn.add"(%30, %130) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%130) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%30) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %132 = "ttnn.typecast"(%20) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %133 = "ttnn.reshape"(%132) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%132) <{force = false}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %134 = "ttnn.typecast"(%131) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %135 = "ttnn.reshape"(%134) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %136 = "ttnn.pow"(%135, %0) : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%135) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %137 = "ttnn.sum"(%136) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%136) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %138 = "ttnn.multiply"(%137, %2) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%137) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %139 = "ttnn.reshape"(%138) <{shape = [7 : i32, 1 : i32]}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%138) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %140 = "ttnn.add"(%139, %37) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%139) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %141 = "ttnn.rsqrt"(%140) : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%140) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %142 = "ttnn.multiply"(%134, %141) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%141) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%134) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %143 = "ttnn.multiply"(%133, %142) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%142) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%133) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %144 = "ttnn.typecast"(%143) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%143) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %145 = "ttnn.matmul"(%144, %21) <{transpose_a = false, transpose_b = true}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%21) <{force = false}> : (tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %146 = "ttnn.typecast"(%145) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %147 = "ttnn.sigmoid"(%145) : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%145) <{force = false}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %148 = "ttnn.typecast"(%147) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%147) <{force = false}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %149 = "ttnn.multiply"(%146, %148) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%148) <{force = false}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%146) <{force = false}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %150 = "ttnn.matmul"(%144, %14) <{transpose_a = false, transpose_b = true}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%144) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %151 = "ttnn.typecast"(%150) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%150) <{force = false}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %152 = "ttnn.multiply"(%149, %151) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%151) <{force = false}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%149) <{force = false}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %153 = "ttnn.typecast"(%152) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%152) <{force = false}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %154 = "ttnn.matmul"(%153, %13) <{transpose_a = false, transpose_b = true}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%153) <{force = false}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %155 = "ttnn.reshape"(%154) <{shape = [1 : i32, 1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%154) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %156 = "ttnn.reduce_scatter"(%155, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 3 : si32}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%155) <{force = false}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %157 = "ttnn.all_gather"(%156, %1) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%156) <{force = false}> : (tensor<1x1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %158 = "ttnn.reshape"(%157) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%157) <{force = false}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %159 = "ttnn.add"(%131, %158) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%158) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%131) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %160 = "ttnn.typecast"(%159) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%159) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %161 = "ttnn.reshape"(%160) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %162 = "ttnn.pow"(%161, %0) : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%161) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %163 = "ttnn.sum"(%162) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%162) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %164 = "ttnn.multiply"(%163, %2) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%163) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %165 = "ttnn.reshape"(%164) <{shape = [7 : i32, 1 : i32]}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%164) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %166 = "ttnn.add"(%165, %37) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%165) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%37) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %167 = "ttnn.rsqrt"(%166) : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%166) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %168 = "ttnn.multiply"(%160, %167) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%167) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%160) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %169 = "ttnn.multiply"(%71, %168) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%168) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%71) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %170 = "ttnn.typecast"(%169) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%169) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %171 = "ttnn.matmul"(%170, %8) <{transpose_a = false, transpose_b = true}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%170) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %172 = "ttnn.reshape"(%171) <{shape = [1 : i32, 7 : i32, 128256 : i32]}> : (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %173 = "ttnn.to_layout"(%10) <{layout = #ttnn.layout<row_major>}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %174 = "ttnn.from_device"(%173) : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%173) <{force = false}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %175 = "ttnn.mesh_shard"(%174, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%174) <{force = false}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<system_memory>>>>) -> ()
        %176 = "ttnn.to_layout"(%12) <{layout = #ttnn.layout<row_major>}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %177 = "ttnn.from_device"(%176) : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%176) <{force = false}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %178 = "ttnn.mesh_shard"(%177, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%177) <{force = false}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<system_memory>>>>) -> ()
        %179 = "ttnn.to_layout"(%171) <{layout = #ttnn.layout<row_major>}> : (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%171) <{force = false}> : (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %180 = "ttnn.from_device"(%179) : (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%179) <{force = false}> : (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %181 = "ttnn.mesh_shard"(%180, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%180) <{force = false}> : (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>) -> ()
        %182 = "ttnn.to_layout"(%172) <{layout = #ttnn.layout<row_major>}> : (tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%172) <{force = false}> : (tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %183 = "ttnn.from_device"(%182) : (tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%182) <{force = false}> : (tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %184 = "ttnn.mesh_shard"(%183, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%183) <{force = false}> : (tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>) -> ()
        return %175, %178, %181, %184 : tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>, tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>, tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>, tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>
      }
    }
  }
}
input 0: device = xla:0, shape torch.Size([128]) and shard spec {replicated}
input 1: device = xla:0, shape torch.Size([7, 128]) and shard spec {replicated}
input 2: device = xla:0, shape torch.Size([1, 8, 128, 128]) and shard spec {replicated}
[STATIC CACHE DUMP ]mean along seqlen for static cache @ input idx 2 and shape torch.Size([1, 8, 128, 128]): tensor([ 0.0576,  0.1055,  0.0986, -0.0315, -0.0776, -0.2383, -0.0459,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
       dtype=torch.bfloat16)
input 3: device = xla:0, shape torch.Size([1, 8, 128, 128]) and shard spec {replicated}
[STATIC CACHE DUMP ]mean along seqlen for static cache @ input idx 3 and shape torch.Size([1, 8, 128, 128]): tensor([ 3.0899e-04, -1.0620e-02,  6.9580e-03,  1.8597e-05, -6.0120e-03,
         1.4648e-03,  2.5940e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=torch.bfloat16)
input 4: device = xla:0, shape torch.Size([1, 7]) and shard spec {replicated}
input 5: device = xla:0, shape torch.Size([128256, 3072]) and shard spec {replicated}
input 6: device = xla:0, shape torch.Size([7]) and shard spec {replicated}
input 7: device = xla:0, shape torch.Size([64]) and shard spec {replicated}
input 8: device = xla:0, shape torch.Size([3072]) and shard spec {replicated}
input 9: device = xla:0, shape torch.Size([3072, 3072]) and shard spec {devices=[2,1]<=[2]}
input 10: device = xla:0, shape torch.Size([1024, 3072]) and shard spec {devices=[2,1]<=[2]}
input 11: device = xla:0, shape torch.Size([1024, 3072]) and shard spec {devices=[2,1]<=[2]}
input 12: device = xla:0, shape torch.Size([3072, 3072]) and shard spec {devices=[1,2]<=[2]}
input 13: device = xla:0, shape torch.Size([3072]) and shard spec {replicated}
input 14: device = xla:0, shape torch.Size([8192, 3072]) and shard spec {devices=[2,1]<=[2]}
input 15: device = xla:0, shape torch.Size([8192, 3072]) and shard spec {devices=[2,1]<=[2]}
input 16: device = xla:0, shape torch.Size([3072, 8192]) and shard spec {devices=[1,2]<=[2]}
input 17: device = xla:0, shape torch.Size([3072]) and shard spec {replicated}
CPU logit shape:  torch.Size([1, 7, 128256]) XLA logit shape:  torch.Size([1, 7, 128256])
 theGenerated tokens: [' the']
Note: Using experimental XLA backend.
[XLA Debug] Processing 18 input specs:
[XLA Debug] Input 0: kind=InputKind.PARAMETER, arg=TensorArgument(name='p__fx_const_folded_attrs_0'), target=_FX_CONST_FOLDED_ATTRS.0

[XLA Debug] Input 1: kind=InputKind.PARAMETER, arg=TensorArgument(name='p__fx_const_folded_attrs_1'), target=_FX_CONST_FOLDED_ATTRS.1

[XLA Debug] Input 2: kind=InputKind.BUFFER, arg=TensorArgument(name='b_kwargs____past_key_values___key_cache_0'), target=kwargs____past_key_values___key_cache_0
[XLA Debug] Input 3: kind=InputKind.BUFFER, arg=TensorArgument(name='b_kwargs____past_key_values___value_cache_0'), target=kwargs____past_key_values___value_cache_0
[XLA Debug] Input 4: kind=InputKind.USER_INPUT, arg=TensorArgument(name='args_0'), target=None
[XLA Debug] Input 5: kind=InputKind.USER_INPUT, arg=TensorArgument(name='args_1'), target=None
[XLA Debug] Input 6: kind=InputKind.USER_INPUT, arg=TensorArgument(name='args_2'), target=None
[XLA Debug] Input 7: kind=InputKind.USER_INPUT, arg=TensorArgument(name='args_3'), target=None
[XLA Debug] Input 8: kind=InputKind.USER_INPUT, arg=TensorArgument(name='args_4'), target=None
[XLA Debug] Input 9: kind=InputKind.USER_INPUT, arg=TensorArgument(name='args_5'), target=None
[XLA Debug] Input 10: kind=InputKind.USER_INPUT, arg=TensorArgument(name='args_6'), target=None
[XLA Debug] Input 11: kind=InputKind.USER_INPUT, arg=TensorArgument(name='args_7'), target=None
[XLA Debug] Input 12: kind=InputKind.USER_INPUT, arg=TensorArgument(name='args_8'), target=None
[XLA Debug] Input 13: kind=InputKind.USER_INPUT, arg=TensorArgument(name='args_9'), target=None
[XLA Debug] Input 14: kind=InputKind.USER_INPUT, arg=TensorArgument(name='args_10'), target=None
[XLA Debug] Input 15: kind=InputKind.USER_INPUT, arg=TensorArgument(name='args_11'), target=None
[XLA Debug] Input 16: kind=InputKind.USER_INPUT, arg=TensorArgument(name='args_12'), target=None
[XLA Debug] Input 17: kind=InputKind.USER_INPUT, arg=TensorArgument(name='args_13'), target=None
[XLA Debug] Initialization complete - total inputs: 18, user input indices: [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
module @SyncTensorsGraph.370 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<1x!vhlo.i64_v1>, %arg1: !vhlo.tensor_v1<64x!vhlo.f32_v1>, %arg2: !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg4: !vhlo.tensor_v1<1x1x!vhlo.i64_v1>, %arg5: !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, %arg6: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg7: !vhlo.tensor_v1<!vhlo.i64_v1>, %arg8: !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, %arg9: !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>, %arg10: !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, %arg11: !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>, %arg13: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg14: !vhlo.tensor_v1<128x!vhlo.i64_v1>, %arg15: !vhlo.tensor_v1<1x128x!vhlo.bf16_v1>, %arg16: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg17: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg18: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg19: !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>, %arg20: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128256x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x128256x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : () -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF800000> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %3 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<3.25520843E-4> : tensor<1x1xf32>>}> : () -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %4 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %5 = "vhlo.broadcast_in_dim_v1"(%4) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %6 = "vhlo.compare_v1"(%arg0, %0) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 LT>}> : (!vhlo.tensor_v1<1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.bool_v1>
    %7 = "vhlo.reshape_v1"(%arg7) : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %8 = "vhlo.add_v1"(%arg0, %7) : (!vhlo.tensor_v1<1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %9 = "vhlo.select_v1"(%6, %8, %arg0) : (!vhlo.tensor_v1<1x!vhlo.bool_v1>, !vhlo.tensor_v1<1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %10 = "vhlo.reshape_v1"(%9) : (!vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.i64_v1>
    %11 = "vhlo.convert_v1"(%arg6) {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}]>]>">}>, mhlo.sharding = #vhlo.string_v1<"{replicated}">} : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %12 = "vhlo.reshape_v1"(%11) : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %13 = "vhlo.reshape_v1"(%arg4) : (!vhlo.tensor_v1<1x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %14 = "vhlo.convert_v1"(%13) : (!vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.ui32_v1>
    %15 = "vhlo.gather_v2"(%arg5, %14) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 3072]> : tensor<2xi64>>, start_index_map = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x!vhlo.ui32_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %16 = "vhlo.reshape_v1"(%15) : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %17 = "vhlo.convert_v1"(%16) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %18 = "vhlo.power_v1"(%17, %5) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %19 = "vhlo.reduce_v1"(%18, %1) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %184 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%184) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %20 = "vhlo.multiply_v1"(%19, %3) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %21 = "vhlo.reshape_v1"(%20) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %22 = "vhlo.reshape_v1"(%arg3) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %23 = "vhlo.add_v1"(%21, %22) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %24 = "vhlo.rsqrt_v2"(%23) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %25 = "vhlo.reshape_v1"(%24) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %26 = "vhlo.broadcast_in_dim_v1"(%25) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %27 = "vhlo.multiply_v1"(%17, %26) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %28 = "vhlo.convert_v1"(%27) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %29 = "vhlo.convert_v1"(%28) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %30 = "vhlo.multiply_v1"(%12, %29) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %31 = "vhlo.convert_v1"(%30) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %32 = "vhlo.reshape_v1"(%31) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %33 = "vhlo.transpose_v1"(%arg2) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,1024]{0,1}">} : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>
    %34 = "vhlo.dot_general_v2"(%32, %33) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>
    %35 = "vhlo.reshape_v1"(%34) : (!vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %36 = "vhlo.convert_v1"(%35) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,8,1,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %37 = "vhlo.reshape_v1"(%arg1) : (!vhlo.tensor_v1<64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>
    %38 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.i64_v1>
    %39 = "vhlo.convert_v1"(%38) : (!vhlo.tensor_v1<1x1x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %40 = "vhlo.dot_general_v2"(%37, %39) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>
    %41 = "vhlo.reshape_v1"(%40) : (!vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>
    %42 = "vhlo.concatenate_v1"(%41, %41) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>
    %43 = "vhlo.cosine_v2"(%42) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>
    %44 = "vhlo.convert_v1"(%43) : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>
    %45 = "vhlo.reshape_v1"(%44) : (!vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1x128x!vhlo.bf16_v1>
    %46 = "vhlo.convert_v1"(%45) : (!vhlo.tensor_v1<1x1x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1x128x!vhlo.f32_v1>
    %47 = "vhlo.reshape_v1"(%46) : (!vhlo.tensor_v1<1x1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>
    %48 = "vhlo.broadcast_in_dim_v1"(%47) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %49 = "vhlo.multiply_v1"(%36, %48) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %50 = "vhlo.convert_v1"(%49) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %51 = "vhlo.slice_v1"(%35) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 1, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>
    %52 = "vhlo.negate_v1"(%51) : (!vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>
    %53 = "vhlo.slice_v1"(%35) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 1, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>
    %54 = "vhlo.concatenate_v1"(%52, %53) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %55 = "vhlo.convert_v1"(%54) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %56 = "vhlo.sine_v2"(%42) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>
    %57 = "vhlo.convert_v1"(%56) : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>
    %58 = "vhlo.reshape_v1"(%57) : (!vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1x128x!vhlo.bf16_v1>
    %59 = "vhlo.convert_v1"(%58) : (!vhlo.tensor_v1<1x1x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1x128x!vhlo.f32_v1>
    %60 = "vhlo.reshape_v1"(%59) : (!vhlo.tensor_v1<1x1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>
    %61 = "vhlo.broadcast_in_dim_v1"(%60) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %62 = "vhlo.multiply_v1"(%55, %61) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %63 = "vhlo.convert_v1"(%62) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %64 = "vhlo.add_v1"(%50, %63) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %65 = "vhlo.scatter_v2"(%arg8, %10, %64) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg22: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg22) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>
    %66 = "vhlo.custom_call_v1"(%65) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"Sharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>">}>, mhlo.sharding = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">} : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>
    %67 = "vhlo.transpose_v1"(%arg9) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,1024]{0,1}">} : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>
    %68 = "vhlo.dot_general_v2"(%32, %67) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>
    %69 = "vhlo.reshape_v1"(%68) : (!vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %70 = "vhlo.scatter_v2"(%arg10, %10, %69) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg22: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg22) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>
    %71 = "vhlo.custom_call_v1"(%70) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"Sharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>">}>, mhlo.sharding = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">} : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>
    %72 = "vhlo.convert_v1"(%arg20) {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}]>]>">}>, mhlo.sharding = #vhlo.string_v1<"{replicated}">} : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %73 = "vhlo.reshape_v1"(%72) : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %74 = "vhlo.transpose_v1"(%arg17) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,3072]{0,1}">} : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %75 = "vhlo.dot_general_v2"(%32, %74) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %76 = "vhlo.reshape_v1"(%75) : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %77 = "vhlo.convert_v1"(%76) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,24,1,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %78 = "vhlo.broadcast_in_dim_v1"(%47) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %79 = "vhlo.multiply_v1"(%77, %78) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %80 = "vhlo.convert_v1"(%79) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %81 = "vhlo.slice_v1"(%76) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 1, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>
    %82 = "vhlo.negate_v1"(%81) : (!vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>
    %83 = "vhlo.slice_v1"(%76) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 1, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>
    %84 = "vhlo.concatenate_v1"(%82, %83) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %85 = "vhlo.convert_v1"(%84) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %86 = "vhlo.broadcast_in_dim_v1"(%60) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %87 = "vhlo.multiply_v1"(%85, %86) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %88 = "vhlo.convert_v1"(%87) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %89 = "vhlo.add_v1"(%80, %88) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %90 = "vhlo.reshape_v1"(%89) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>
    %91 = "vhlo.broadcast_in_dim_v1"(%65) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>
    %92 = "vhlo.reshape_v1"(%91) : (!vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>
    %93 = "vhlo.transpose_v1"(%92) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 3, 1, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,24,128,128]{2,3,1,0}">} : (!vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>
    %94 = "vhlo.reshape_v1"(%93) : (!vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>
    %95 = "vhlo.dot_general_v2"(%90, %94) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>
    %96 = "vhlo.reshape_v1"(%95) : (!vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %97 = "vhlo.convert_v1"(%96) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %98 = "vhlo.broadcast_in_dim_v1"(%arg16) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %99 = "vhlo.multiply_v1"(%97, %98) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %100 = "vhlo.convert_v1"(%99) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %101 = "vhlo.convert_v1"(%arg15) : (!vhlo.tensor_v1<1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128x!vhlo.f32_v1>
    %102 = "vhlo.reshape_v1"(%arg14) : (!vhlo.tensor_v1<128x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x128x!vhlo.i64_v1>
    %103 = "vhlo.broadcast_in_dim_v1"(%arg0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x128x!vhlo.i64_v1>
    %104 = "vhlo.compare_v1"(%102, %103) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<1x128x!vhlo.i64_v1>, !vhlo.tensor_v1<1x128x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x128x!vhlo.bool_v1>
    %105 = "vhlo.convert_v1"(%104) : (!vhlo.tensor_v1<1x128x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x128x!vhlo.f32_v1>
    %106 = "vhlo.multiply_v1"(%101, %105) : (!vhlo.tensor_v1<1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x128x!vhlo.f32_v1>
    %107 = "vhlo.convert_v1"(%106) : (!vhlo.tensor_v1<1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x128x!vhlo.bf16_v1>
    %108 = "vhlo.reshape_v1"(%107) : (!vhlo.tensor_v1<1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>
    %109 = "vhlo.broadcast_in_dim_v1"(%108) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %110 = "vhlo.add_v1"(%100, %109) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %111 = "vhlo.convert_v1"(%110) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %112 = "vhlo.reduce_v1"(%111, %2) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %184 = "vhlo.maximum_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%184) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x!vhlo.f32_v1>
    %113 = "vhlo.broadcast_in_dim_v1"(%112) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %114 = "vhlo.subtract_v1"(%111, %113) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %115 = "vhlo.exponential_v2"(%114) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %116 = "vhlo.reduce_v1"(%115, %1) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %184 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%184) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x!vhlo.f32_v1>
    %117 = "vhlo.broadcast_in_dim_v1"(%116) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %118 = "vhlo.divide_v1"(%115, %117) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %119 = "vhlo.convert_v1"(%118) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %120 = "vhlo.reshape_v1"(%119) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>
    %121 = "vhlo.broadcast_in_dim_v1"(%70) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>
    %122 = "vhlo.reshape_v1"(%121) : (!vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>
    %123 = "vhlo.dot_general_v2"(%120, %122) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>
    %124 = "vhlo.reshape_v1"(%123) : (!vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %125 = "vhlo.transpose_v1"(%arg13) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,3072]{0,1}">} : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %126 = "vhlo.dot_general_v2"(%124, %125) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %127 = "vhlo.reshape_v1"(%126) : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %128 = "vhlo.add_v1"(%16, %127) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %129 = "vhlo.convert_v1"(%arg18) {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}]>]>">}>, mhlo.sharding = #vhlo.string_v1<"{replicated}">} : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %130 = "vhlo.reshape_v1"(%129) : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %131 = "vhlo.convert_v1"(%128) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %132 = "vhlo.power_v1"(%131, %5) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %133 = "vhlo.reduce_v1"(%132, %1) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %184 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%184) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %134 = "vhlo.multiply_v1"(%133, %3) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %135 = "vhlo.reshape_v1"(%134) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %136 = "vhlo.add_v1"(%135, %22) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %137 = "vhlo.rsqrt_v2"(%136) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %138 = "vhlo.reshape_v1"(%137) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %139 = "vhlo.broadcast_in_dim_v1"(%138) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %140 = "vhlo.multiply_v1"(%131, %139) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %141 = "vhlo.convert_v1"(%140) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %142 = "vhlo.convert_v1"(%141) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %143 = "vhlo.multiply_v1"(%130, %142) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %144 = "vhlo.convert_v1"(%143) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %145 = "vhlo.reshape_v1"(%144) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %146 = "vhlo.transpose_v1"(%arg19) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,8192]{0,1}">} : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>
    %147 = "vhlo.dot_general_v2"(%145, %146) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>
    %148 = "vhlo.reshape_v1"(%147) : (!vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %149 = "vhlo.convert_v1"(%148) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %150 = "vhlo.logistic_v2"(%148) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %151 = "vhlo.convert_v1"(%150) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %152 = "vhlo.multiply_v1"(%149, %151) : (!vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %153 = "vhlo.convert_v1"(%152) : (!vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %154 = "vhlo.convert_v1"(%153) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %155 = "vhlo.transpose_v1"(%arg12) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,8192]{0,1}">} : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>
    %156 = "vhlo.dot_general_v2"(%145, %155) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>
    %157 = "vhlo.reshape_v1"(%156) : (!vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %158 = "vhlo.convert_v1"(%157) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %159 = "vhlo.multiply_v1"(%154, %158) : (!vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %160 = "vhlo.convert_v1"(%159) : (!vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %161 = "vhlo.reshape_v1"(%160) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>
    %162 = "vhlo.transpose_v1"(%arg11) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,3072]{0,1}">} : (!vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>
    %163 = "vhlo.dot_general_v2"(%161, %162) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %164 = "vhlo.reshape_v1"(%163) : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %165 = "vhlo.add_v1"(%128, %164) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %166 = "vhlo.convert_v1"(%165) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %167 = "vhlo.power_v1"(%166, %5) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %168 = "vhlo.reduce_v1"(%167, %1) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %184 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%184) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %169 = "vhlo.multiply_v1"(%168, %3) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %170 = "vhlo.reshape_v1"(%169) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %171 = "vhlo.add_v1"(%170, %22) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %172 = "vhlo.rsqrt_v2"(%171) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %173 = "vhlo.reshape_v1"(%172) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %174 = "vhlo.broadcast_in_dim_v1"(%173) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %175 = "vhlo.multiply_v1"(%166, %174) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %176 = "vhlo.convert_v1"(%175) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %177 = "vhlo.convert_v1"(%176) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %178 = "vhlo.multiply_v1"(%73, %177) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %179 = "vhlo.convert_v1"(%178) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %180 = "vhlo.reshape_v1"(%179) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %181 = "vhlo.transpose_v1"(%arg5) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,128256]{0,1}">} : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x128256x!vhlo.bf16_v1>
    %182 = "vhlo.dot_general_v2"(%180, %181) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x!vhlo.bf16_v1>
    %183 = "vhlo.reshape_v1"(%182) : (!vhlo.tensor_v1<1x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x128256x!vhlo.bf16_v1>
    "vhlo.return_v1"(%66, %71, %182, %183) : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128256x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x128256x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}
module @SyncTensorsGraph.370 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<1xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg1: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg2: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg3: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg4: tensor<1x1xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg5: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg7: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg8: tensor<1x8x128x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}, %arg9: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg10: tensor<1x8x128x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}, %arg11: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>}, %arg12: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg13: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>}, %arg14: tensor<128xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg15: tensor<1x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg16: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}) -> (tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x128256xbf16>, tensor<1x1x128256xbf16>) {
    %c = stablehlo.constant dense<0> : tensor<1xi64>
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_1 = stablehlo.constant dense<3.25520843E-4> : tensor<1x1xf32>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %0 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x1x3072xf32>
    %1 = stablehlo.compare  LT, %arg0, %c : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
    %2 = stablehlo.reshape %arg7 : (tensor<i64>) -> tensor<1xi64>
    %3 = stablehlo.add %arg0, %2 : tensor<1xi64>
    %4 = stablehlo.select %1, %3, %arg0 : tensor<1xi1>, tensor<1xi64>
    %5 = stablehlo.reshape %4 : (tensor<1xi64>) -> tensor<1x1xi64>
    %6 = stablehlo.convert %arg6 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %7 = stablehlo.reshape %6 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
    %8 = stablehlo.convert %arg4 : (tensor<1x1xi64>) -> tensor<1x1xui32>
    %9 = stablehlo.reshape %8 : (tensor<1x1xui32>) -> tensor<1xui32>
    %10 = "stablehlo.gather"(%arg5, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<1xui32>) -> tensor<1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %13 = stablehlo.power %12, %0 : tensor<1x1x3072xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %15 = stablehlo.multiply %14, %cst_1 : tensor<1x1xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %17 = stablehlo.reshape %arg3 : (tensor<f32>) -> tensor<1x1x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x1x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x1x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x1x3072xf32>
    %23 = stablehlo.convert %22 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %24 = stablehlo.convert %23 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %25 = stablehlo.multiply %7, %24 : tensor<1x1x3072xf32>
    %26 = stablehlo.convert %25 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %27 = stablehlo.reshape %26 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %28 = stablehlo.transpose %arg2, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %29 = stablehlo.dot_general %27, %28, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %30 = stablehlo.reshape %29 : (tensor<1x1024xbf16>) -> tensor<1x8x1x128xbf16>
    %31 = stablehlo.convert %30 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,1,128]{3,1,2,0}"} : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xf32>
    %32 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x64x1xf32>
    %33 = stablehlo.convert %arg0 : (tensor<1xi64>) -> tensor<1xf32>
    %34 = stablehlo.reshape %33 : (tensor<1xf32>) -> tensor<1x1x1xf32>
    %35 = stablehlo.dot_general %32, %34, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x1xf32>) -> tensor<1x64x1xf32>
    %36 = stablehlo.reshape %35 : (tensor<1x64x1xf32>) -> tensor<1x1x64xf32>
    %37 = stablehlo.concatenate %36, %36, dim = 2 : (tensor<1x1x64xf32>, tensor<1x1x64xf32>) -> tensor<1x1x128xf32>
    %38 = stablehlo.cosine %37 : tensor<1x1x128xf32>
    %39 = stablehlo.convert %38 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
    %40 = stablehlo.convert %39 : (tensor<1x1x128xbf16>) -> tensor<1x1x128xf32>
    %41 = stablehlo.broadcast_in_dim %40, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x8x1x128xf32>
    %42 = stablehlo.multiply %31, %41 : tensor<1x8x1x128xf32>
    %43 = stablehlo.convert %42 : (tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xbf16>
    %44 = stablehlo.slice %30 [0:1, 0:8, 0:1, 64:128] : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x64xbf16>
    %45 = stablehlo.negate %44 : tensor<1x8x1x64xbf16>
    %46 = stablehlo.slice %30 [0:1, 0:8, 0:1, 0:64] : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x64xbf16>
    %47 = stablehlo.concatenate %45, %46, dim = 3 : (tensor<1x8x1x64xbf16>, tensor<1x8x1x64xbf16>) -> tensor<1x8x1x128xbf16>
    %48 = stablehlo.convert %47 : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xf32>
    %49 = stablehlo.sine %37 : tensor<1x1x128xf32>
    %50 = stablehlo.convert %49 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
    %51 = stablehlo.convert %50 : (tensor<1x1x128xbf16>) -> tensor<1x1x128xf32>
    %52 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x8x1x128xf32>
    %53 = stablehlo.multiply %48, %52 : tensor<1x8x1x128xf32>
    %54 = stablehlo.convert %53 : (tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xbf16>
    %55 = stablehlo.add %43, %54 : tensor<1x8x1x128xbf16>
    %56 = "stablehlo.scatter"(%arg8, %5, %55) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>) -> tensor<1x8x128x128xbf16>
    %57 = sdy.sharding_constraint %56 <@mesh, [{}, {"_axis_0"}, {}, {}]> : tensor<1x8x128x128xbf16>
    %58 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %59 = stablehlo.dot_general %27, %58, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %60 = stablehlo.reshape %59 : (tensor<1x1024xbf16>) -> tensor<1x8x1x128xbf16>
    %61 = "stablehlo.scatter"(%arg10, %5, %60) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>) -> tensor<1x8x128x128xbf16>
    %62 = sdy.sharding_constraint %61 <@mesh, [{}, {"_axis_0"}, {}, {}]> : tensor<1x8x128x128xbf16>
    %63 = stablehlo.convert %arg20 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %64 = stablehlo.reshape %63 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
    %65 = stablehlo.transpose %arg17, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %66 = stablehlo.dot_general %27, %65, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %67 = stablehlo.reshape %66 : (tensor<1x3072xbf16>) -> tensor<1x24x1x128xbf16>
    %68 = stablehlo.convert %67 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,1,128]{3,1,2,0}"} : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %69 = stablehlo.broadcast_in_dim %40, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x24x1x128xf32>
    %70 = stablehlo.multiply %68, %69 : tensor<1x24x1x128xf32>
    %71 = stablehlo.convert %70 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %72 = stablehlo.slice %67 [0:1, 0:24, 0:1, 64:128] : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x64xbf16>
    %73 = stablehlo.negate %72 : tensor<1x24x1x64xbf16>
    %74 = stablehlo.slice %67 [0:1, 0:24, 0:1, 0:64] : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x64xbf16>
    %75 = stablehlo.concatenate %73, %74, dim = 3 : (tensor<1x24x1x64xbf16>, tensor<1x24x1x64xbf16>) -> tensor<1x24x1x128xbf16>
    %76 = stablehlo.convert %75 : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %77 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x24x1x128xf32>
    %78 = stablehlo.multiply %76, %77 : tensor<1x24x1x128xf32>
    %79 = stablehlo.convert %78 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %80 = stablehlo.add %71, %79 : tensor<1x24x1x128xbf16>
    %81 = stablehlo.reshape %80 : (tensor<1x24x1x128xbf16>) -> tensor<24x1x128xbf16>
    %82 = stablehlo.broadcast_in_dim %56, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x8x3x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %84 = stablehlo.transpose %83, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,128]{2,3,1,0}"} : (tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %85 = stablehlo.reshape %84 : (tensor<1x24x128x128xbf16>) -> tensor<24x128x128xbf16>
    %86 = stablehlo.dot_general %81, %85, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x1x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x1x128xbf16>
    %87 = stablehlo.convert %86 : (tensor<24x1x128xbf16>) -> tensor<24x1x128xf32>
    %88 = stablehlo.reshape %87 : (tensor<24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %89 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<1x24x1x128xf32>
    %90 = stablehlo.multiply %88, %89 : tensor<1x24x1x128xf32>
    %91 = stablehlo.convert %90 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %92 = stablehlo.convert %arg15 : (tensor<1x128xbf16>) -> tensor<1x128xf32>
    %93 = stablehlo.reshape %arg14 : (tensor<128xi64>) -> tensor<1x128xi64>
    %94 = stablehlo.broadcast_in_dim %arg0, dims = [0] : (tensor<1xi64>) -> tensor<1x128xi64>
    %95 = stablehlo.compare  GT, %93, %94 : (tensor<1x128xi64>, tensor<1x128xi64>) -> tensor<1x128xi1>
    %96 = stablehlo.convert %95 : (tensor<1x128xi1>) -> tensor<1x128xf32>
    %97 = stablehlo.multiply %92, %96 : tensor<1x128xf32>
    %98 = stablehlo.convert %97 : (tensor<1x128xf32>) -> tensor<1x128xbf16>
    %99 = stablehlo.reshape %98 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16>
    %100 = stablehlo.broadcast_in_dim %99, dims = [0, 2, 3] : (tensor<1x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %101 = stablehlo.add %91, %100 : tensor<1x24x1x128xbf16>
    %102 = stablehlo.convert %101 : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %103 = stablehlo.reduce(%102 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x1x128xf32>, tensor<f32>) -> tensor<1x24x1xf32>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0, 1, 2] : (tensor<1x24x1xf32>) -> tensor<1x24x1x128xf32>
    %105 = stablehlo.subtract %102, %104 : tensor<1x24x1x128xf32>
    %106 = stablehlo.exponential %105 : tensor<1x24x1x128xf32>
    %107 = stablehlo.reduce(%106 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x1x128xf32>, tensor<f32>) -> tensor<1x24x1xf32>
    %108 = stablehlo.broadcast_in_dim %107, dims = [0, 1, 2] : (tensor<1x24x1xf32>) -> tensor<1x24x1x128xf32>
    %109 = stablehlo.divide %106, %108 : tensor<1x24x1x128xf32>
    %110 = stablehlo.convert %109 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %111 = stablehlo.reshape %110 : (tensor<1x24x1x128xbf16>) -> tensor<24x1x128xbf16>
    %112 = stablehlo.broadcast_in_dim %61, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %113 = stablehlo.reshape %112 : (tensor<1x8x3x128x128xbf16>) -> tensor<24x128x128xbf16>
    %114 = stablehlo.dot_general %111, %113, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x1x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x1x128xbf16>
    %115 = stablehlo.reshape %114 : (tensor<24x1x128xbf16>) -> tensor<1x3072xbf16>
    %116 = stablehlo.transpose %arg13, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %117 = stablehlo.dot_general %115, %116, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %118 = stablehlo.reshape %117 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %119 = stablehlo.add %11, %118 : tensor<1x1x3072xbf16>
    %120 = stablehlo.convert %arg18 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %121 = stablehlo.reshape %120 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
    %122 = stablehlo.convert %119 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %123 = stablehlo.power %122, %0 : tensor<1x1x3072xf32>
    %124 = stablehlo.reduce(%123 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %125 = stablehlo.multiply %124, %cst_1 : tensor<1x1xf32>
    %126 = stablehlo.reshape %125 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %127 = stablehlo.add %126, %17 : tensor<1x1x1xf32>
    %128 = stablehlo.rsqrt %127 : tensor<1x1x1xf32>
    %129 = stablehlo.reshape %128 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %130 = stablehlo.broadcast_in_dim %129, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %131 = stablehlo.multiply %122, %130 : tensor<1x1x3072xf32>
    %132 = stablehlo.convert %131 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %133 = stablehlo.convert %132 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %134 = stablehlo.multiply %121, %133 : tensor<1x1x3072xf32>
    %135 = stablehlo.convert %134 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %136 = stablehlo.reshape %135 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %137 = stablehlo.transpose %arg19, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %138 = stablehlo.dot_general %136, %137, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %139 = stablehlo.reshape %138 : (tensor<1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %140 = stablehlo.convert %139 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %141 = stablehlo.logistic %139 : tensor<1x1x8192xbf16>
    %142 = stablehlo.convert %141 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %143 = stablehlo.multiply %140, %142 : tensor<1x1x8192xf32>
    %144 = stablehlo.convert %143 : (tensor<1x1x8192xf32>) -> tensor<1x1x8192xbf16>
    %145 = stablehlo.convert %144 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %146 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %147 = stablehlo.dot_general %136, %146, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %148 = stablehlo.convert %147 : (tensor<1x8192xbf16>) -> tensor<1x8192xf32>
    %149 = stablehlo.reshape %148 : (tensor<1x8192xf32>) -> tensor<1x1x8192xf32>
    %150 = stablehlo.multiply %145, %149 : tensor<1x1x8192xf32>
    %151 = stablehlo.convert %150 : (tensor<1x1x8192xf32>) -> tensor<1x1x8192xbf16>
    %152 = stablehlo.reshape %151 : (tensor<1x1x8192xbf16>) -> tensor<1x8192xbf16>
    %153 = stablehlo.transpose %arg11, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %154 = stablehlo.dot_general %152, %153, contracting_dims = [1] x [0] : (tensor<1x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<1x3072xbf16>
    %155 = stablehlo.reshape %154 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %156 = stablehlo.add %119, %155 : tensor<1x1x3072xbf16>
    %157 = stablehlo.convert %156 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %158 = stablehlo.power %157, %0 : tensor<1x1x3072xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %160 = stablehlo.multiply %159, %cst_1 : tensor<1x1xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %162 = stablehlo.add %161, %17 : tensor<1x1x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x1x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x1x3072xf32>
    %167 = stablehlo.convert %166 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %168 = stablehlo.convert %167 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %169 = stablehlo.multiply %64, %168 : tensor<1x1x3072xf32>
    %170 = stablehlo.convert %169 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %172 = stablehlo.transpose %arg5, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %173 = stablehlo.dot_general %171, %172, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<1x128256xbf16>
    %174 = stablehlo.reshape %173 : (tensor<1x128256xbf16>) -> tensor<1x1x128256xbf16>
    return %57, %62, %173, %174 : tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x128256xbf16>, tensor<1x1x128256xbf16>
  }
}
module @SyncTensorsGraph.370 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<1xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg1: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg2: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg3: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg4: tensor<1x1xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg5: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg7: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg8: tensor<1x8x128x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}, %arg9: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg10: tensor<1x8x128x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}, %arg11: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>}, %arg12: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg13: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>}, %arg14: tensor<128xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg15: tensor<1x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg16: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}) -> (tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x128256xbf16>, tensor<1x1x128256xbf16>) {
    %c = stablehlo.constant dense<0> : tensor<1xi64>
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_1 = stablehlo.constant dense<3.25520843E-4> : tensor<1x1xf32>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %0 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x1x3072xf32>
    %1 = stablehlo.compare  LT, %arg0, %c : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
    %2 = stablehlo.reshape %arg7 : (tensor<i64>) -> tensor<1xi64>
    %3 = stablehlo.add %arg0, %2 : tensor<1xi64>
    %4 = stablehlo.select %1, %3, %arg0 : tensor<1xi1>, tensor<1xi64>
    %5 = stablehlo.reshape %4 : (tensor<1xi64>) -> tensor<1x1xi64>
    %6 = stablehlo.convert %arg6 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %7 = stablehlo.reshape %6 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
    %8 = stablehlo.convert %arg4 : (tensor<1x1xi64>) -> tensor<1x1xui32>
    %9 = stablehlo.reshape %8 : (tensor<1x1xui32>) -> tensor<1xui32>
    %10 = "stablehlo.gather"(%arg5, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<1xui32>) -> tensor<1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %13 = stablehlo.power %12, %0 : tensor<1x1x3072xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %15 = stablehlo.multiply %14, %cst_1 : tensor<1x1xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %17 = stablehlo.reshape %arg3 : (tensor<f32>) -> tensor<1x1x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x1x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x1x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x1x3072xf32>
    %23 = stablehlo.convert %22 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %24 = stablehlo.convert %23 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %25 = stablehlo.multiply %7, %24 : tensor<1x1x3072xf32>
    %26 = stablehlo.convert %25 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %27 = stablehlo.reshape %26 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %28 = stablehlo.transpose %arg2, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %29 = stablehlo.dot_general %27, %28, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %30 = stablehlo.reshape %29 : (tensor<1x1024xbf16>) -> tensor<1x8x1x128xbf16>
    %31 = stablehlo.convert %30 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,1,128]{3,1,2,0}"} : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xf32>
    %32 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x64x1xf32>
    %33 = stablehlo.convert %arg0 : (tensor<1xi64>) -> tensor<1xf32>
    %34 = stablehlo.reshape %33 : (tensor<1xf32>) -> tensor<1x1x1xf32>
    %35 = stablehlo.dot_general %32, %34, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x1xf32>) -> tensor<1x64x1xf32>
    %36 = stablehlo.reshape %35 : (tensor<1x64x1xf32>) -> tensor<1x1x64xf32>
    %37 = stablehlo.concatenate %36, %36, dim = 2 : (tensor<1x1x64xf32>, tensor<1x1x64xf32>) -> tensor<1x1x128xf32>
    %38 = stablehlo.cosine %37 : tensor<1x1x128xf32>
    %39 = stablehlo.convert %38 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
    %40 = stablehlo.convert %39 : (tensor<1x1x128xbf16>) -> tensor<1x1x128xf32>
    %41 = stablehlo.broadcast_in_dim %40, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x8x1x128xf32>
    %42 = stablehlo.multiply %31, %41 : tensor<1x8x1x128xf32>
    %43 = stablehlo.convert %42 : (tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xbf16>
    %44 = stablehlo.slice %30 [0:1, 0:8, 0:1, 64:128] : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x64xbf16>
    %45 = stablehlo.negate %44 : tensor<1x8x1x64xbf16>
    %46 = stablehlo.slice %30 [0:1, 0:8, 0:1, 0:64] : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x64xbf16>
    %47 = stablehlo.concatenate %45, %46, dim = 3 : (tensor<1x8x1x64xbf16>, tensor<1x8x1x64xbf16>) -> tensor<1x8x1x128xbf16>
    %48 = stablehlo.convert %47 : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xf32>
    %49 = stablehlo.sine %37 : tensor<1x1x128xf32>
    %50 = stablehlo.convert %49 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
    %51 = stablehlo.convert %50 : (tensor<1x1x128xbf16>) -> tensor<1x1x128xf32>
    %52 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x8x1x128xf32>
    %53 = stablehlo.multiply %48, %52 : tensor<1x8x1x128xf32>
    %54 = stablehlo.convert %53 : (tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xbf16>
    %55 = stablehlo.add %43, %54 : tensor<1x8x1x128xbf16>
    %56 = "stablehlo.scatter"(%arg8, %5, %55) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>) -> tensor<1x8x128x128xbf16>
    %57 = sdy.sharding_constraint %56 <@mesh, [{}, {"_axis_0"}, {}, {}]> : tensor<1x8x128x128xbf16>
    %58 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %59 = stablehlo.dot_general %27, %58, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %60 = stablehlo.reshape %59 : (tensor<1x1024xbf16>) -> tensor<1x8x1x128xbf16>
    %61 = "stablehlo.scatter"(%arg10, %5, %60) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>) -> tensor<1x8x128x128xbf16>
    %62 = sdy.sharding_constraint %61 <@mesh, [{}, {"_axis_0"}, {}, {}]> : tensor<1x8x128x128xbf16>
    %63 = stablehlo.convert %arg20 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %64 = stablehlo.reshape %63 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
    %65 = stablehlo.transpose %arg17, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %66 = stablehlo.dot_general %27, %65, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %67 = stablehlo.reshape %66 : (tensor<1x3072xbf16>) -> tensor<1x24x1x128xbf16>
    %68 = stablehlo.convert %67 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,1,128]{3,1,2,0}"} : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %69 = stablehlo.broadcast_in_dim %40, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x24x1x128xf32>
    %70 = stablehlo.multiply %68, %69 : tensor<1x24x1x128xf32>
    %71 = stablehlo.convert %70 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %72 = stablehlo.slice %67 [0:1, 0:24, 0:1, 64:128] : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x64xbf16>
    %73 = stablehlo.negate %72 : tensor<1x24x1x64xbf16>
    %74 = stablehlo.slice %67 [0:1, 0:24, 0:1, 0:64] : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x64xbf16>
    %75 = stablehlo.concatenate %73, %74, dim = 3 : (tensor<1x24x1x64xbf16>, tensor<1x24x1x64xbf16>) -> tensor<1x24x1x128xbf16>
    %76 = stablehlo.convert %75 : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %77 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x24x1x128xf32>
    %78 = stablehlo.multiply %76, %77 : tensor<1x24x1x128xf32>
    %79 = stablehlo.convert %78 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %80 = stablehlo.add %71, %79 : tensor<1x24x1x128xbf16>
    %81 = stablehlo.reshape %80 : (tensor<1x24x1x128xbf16>) -> tensor<24x1x128xbf16>
    %82 = stablehlo.broadcast_in_dim %56, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x8x3x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %84 = stablehlo.transpose %83, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,128]{2,3,1,0}"} : (tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %85 = stablehlo.reshape %84 : (tensor<1x24x128x128xbf16>) -> tensor<24x128x128xbf16>
    %86 = stablehlo.dot_general %81, %85, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x1x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x1x128xbf16>
    %87 = stablehlo.convert %86 : (tensor<24x1x128xbf16>) -> tensor<24x1x128xf32>
    %88 = stablehlo.reshape %87 : (tensor<24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %89 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<1x24x1x128xf32>
    %90 = stablehlo.multiply %88, %89 : tensor<1x24x1x128xf32>
    %91 = stablehlo.convert %90 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %92 = stablehlo.convert %arg15 : (tensor<1x128xbf16>) -> tensor<1x128xf32>
    %93 = stablehlo.reshape %arg14 : (tensor<128xi64>) -> tensor<1x128xi64>
    %94 = stablehlo.broadcast_in_dim %arg0, dims = [0] : (tensor<1xi64>) -> tensor<1x128xi64>
    %95 = stablehlo.compare  GT, %93, %94 : (tensor<1x128xi64>, tensor<1x128xi64>) -> tensor<1x128xi1>
    %96 = stablehlo.convert %95 : (tensor<1x128xi1>) -> tensor<1x128xf32>
    %97 = stablehlo.multiply %92, %96 : tensor<1x128xf32>
    %98 = stablehlo.convert %97 : (tensor<1x128xf32>) -> tensor<1x128xbf16>
    %99 = stablehlo.reshape %98 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16>
    %100 = stablehlo.broadcast_in_dim %99, dims = [0, 2, 3] : (tensor<1x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %101 = stablehlo.add %91, %100 : tensor<1x24x1x128xbf16>
    %102 = stablehlo.convert %101 : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %103 = stablehlo.reduce(%102 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x1x128xf32>, tensor<f32>) -> tensor<1x24x1xf32>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0, 1, 2] : (tensor<1x24x1xf32>) -> tensor<1x24x1x128xf32>
    %105 = stablehlo.subtract %102, %104 : tensor<1x24x1x128xf32>
    %106 = stablehlo.exponential %105 : tensor<1x24x1x128xf32>
    %107 = stablehlo.reduce(%106 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x1x128xf32>, tensor<f32>) -> tensor<1x24x1xf32>
    %108 = stablehlo.broadcast_in_dim %107, dims = [0, 1, 2] : (tensor<1x24x1xf32>) -> tensor<1x24x1x128xf32>
    %109 = stablehlo.divide %106, %108 : tensor<1x24x1x128xf32>
    %110 = stablehlo.convert %109 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %111 = stablehlo.reshape %110 : (tensor<1x24x1x128xbf16>) -> tensor<24x1x128xbf16>
    %112 = stablehlo.broadcast_in_dim %61, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %113 = stablehlo.reshape %112 : (tensor<1x8x3x128x128xbf16>) -> tensor<24x128x128xbf16>
    %114 = stablehlo.dot_general %111, %113, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x1x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x1x128xbf16>
    %115 = stablehlo.reshape %114 : (tensor<24x1x128xbf16>) -> tensor<1x3072xbf16>
    %116 = stablehlo.transpose %arg13, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %117 = stablehlo.dot_general %115, %116, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %118 = stablehlo.reshape %117 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %119 = stablehlo.add %11, %118 : tensor<1x1x3072xbf16>
    %120 = stablehlo.convert %arg18 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %121 = stablehlo.reshape %120 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
    %122 = stablehlo.convert %119 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %123 = stablehlo.power %122, %0 : tensor<1x1x3072xf32>
    %124 = stablehlo.reduce(%123 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %125 = stablehlo.multiply %124, %cst_1 : tensor<1x1xf32>
    %126 = stablehlo.reshape %125 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %127 = stablehlo.add %126, %17 : tensor<1x1x1xf32>
    %128 = stablehlo.rsqrt %127 : tensor<1x1x1xf32>
    %129 = stablehlo.reshape %128 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %130 = stablehlo.broadcast_in_dim %129, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %131 = stablehlo.multiply %122, %130 : tensor<1x1x3072xf32>
    %132 = stablehlo.convert %131 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %133 = stablehlo.convert %132 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %134 = stablehlo.multiply %121, %133 : tensor<1x1x3072xf32>
    %135 = stablehlo.convert %134 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %136 = stablehlo.reshape %135 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %137 = stablehlo.transpose %arg19, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %138 = stablehlo.dot_general %136, %137, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %139 = stablehlo.reshape %138 : (tensor<1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %140 = stablehlo.convert %139 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %141 = stablehlo.logistic %139 : tensor<1x1x8192xbf16>
    %142 = stablehlo.convert %141 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %143 = stablehlo.multiply %140, %142 : tensor<1x1x8192xf32>
    %144 = stablehlo.convert %143 : (tensor<1x1x8192xf32>) -> tensor<1x1x8192xbf16>
    %145 = stablehlo.convert %144 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %146 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %147 = stablehlo.dot_general %136, %146, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %148 = stablehlo.convert %147 : (tensor<1x8192xbf16>) -> tensor<1x8192xf32>
    %149 = stablehlo.reshape %148 : (tensor<1x8192xf32>) -> tensor<1x1x8192xf32>
    %150 = stablehlo.multiply %145, %149 : tensor<1x1x8192xf32>
    %151 = stablehlo.convert %150 : (tensor<1x1x8192xf32>) -> tensor<1x1x8192xbf16>
    %152 = stablehlo.reshape %151 : (tensor<1x1x8192xbf16>) -> tensor<1x8192xbf16>
    %153 = stablehlo.transpose %arg11, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %154 = stablehlo.dot_general %152, %153, contracting_dims = [1] x [0] : (tensor<1x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<1x3072xbf16>
    %155 = stablehlo.reshape %154 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %156 = stablehlo.add %119, %155 : tensor<1x1x3072xbf16>
    %157 = stablehlo.convert %156 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %158 = stablehlo.power %157, %0 : tensor<1x1x3072xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %160 = stablehlo.multiply %159, %cst_1 : tensor<1x1xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %162 = stablehlo.add %161, %17 : tensor<1x1x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x1x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x1x3072xf32>
    %167 = stablehlo.convert %166 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %168 = stablehlo.convert %167 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %169 = stablehlo.multiply %64, %168 : tensor<1x1x3072xf32>
    %170 = stablehlo.convert %169 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %172 = stablehlo.transpose %arg5, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %173 = stablehlo.dot_general %171, %172, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<1x128256xbf16>
    %174 = stablehlo.reshape %173 : (tensor<1x128256xbf16>) -> tensor<1x1x128256xbf16>
    return %57, %62, %173, %174 : tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x128256xbf16>, tensor<1x1x128256xbf16>
  }
}
module @SyncTensorsGraph.370 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<1xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<64xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1024x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1x1xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<128256x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<i64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<1024x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<3072x8192xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<8192x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<128xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<1x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<8192x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x1x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:4 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %arg16, %arg17, %arg18, %arg19, %arg20) in_shardings=[<@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, []>, <@mesh, [{}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}]>, <@mesh, []>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {}]>, <@mesh, []>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg21: tensor<1xi64>, %arg22: tensor<64xf32>, %arg23: tensor<512x3072xbf16>, %arg24: tensor<f32>, %arg25: tensor<1x1xi64>, %arg26: tensor<128256x3072xbf16>, %arg27: tensor<3072xbf16>, %arg28: tensor<i64>, %arg29: tensor<1x4x128x128xbf16>, %arg30: tensor<512x3072xbf16>, %arg31: tensor<1x4x128x128xbf16>, %arg32: tensor<3072x4096xbf16>, %arg33: tensor<4096x3072xbf16>, %arg34: tensor<3072x1536xbf16>, %arg35: tensor<128xi64>, %arg36: tensor<1x128xbf16>, %arg37: tensor<f32>, %arg38: tensor<1536x3072xbf16>, %arg39: tensor<3072xbf16>, %arg40: tensor<4096x3072xbf16>, %arg41: tensor<3072xbf16>) {
      %c = stablehlo.constant dense<0> : tensor<1xi64>
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
      %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
      %cst_1 = stablehlo.constant dense<3.25520843E-4> : tensor<1x1xf32>
      %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %1 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x1x3072xf32>
      %2 = stablehlo.compare  LT, %arg21, %c : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
      %3 = stablehlo.reshape %arg28 : (tensor<i64>) -> tensor<1xi64>
      %4 = stablehlo.add %arg21, %3 : tensor<1xi64>
      %5 = stablehlo.select %2, %4, %arg21 : tensor<1xi1>, tensor<1xi64>
      %6 = stablehlo.reshape %5 : (tensor<1xi64>) -> tensor<1x1xi64>
      %7 = stablehlo.convert %arg27 : (tensor<3072xbf16>) -> tensor<3072xf32>
      %8 = stablehlo.reshape %7 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
      %9 = stablehlo.convert %arg25 : (tensor<1x1xi64>) -> tensor<1x1xui32>
      %10 = stablehlo.reshape %9 : (tensor<1x1xui32>) -> tensor<1xui32>
      %11 = "stablehlo.gather"(%arg26, %10) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<1xui32>) -> tensor<1x3072xbf16>
      %12 = stablehlo.reshape %11 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
      %13 = stablehlo.convert %12 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %14 = stablehlo.power %13, %1 : tensor<1x1x3072xf32>
      %15 = stablehlo.reduce(%14 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
      %16 = stablehlo.multiply %15, %cst_1 : tensor<1x1xf32>
      %17 = stablehlo.reshape %16 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
      %18 = stablehlo.reshape %arg24 : (tensor<f32>) -> tensor<1x1x1xf32>
      %19 = stablehlo.add %17, %18 : tensor<1x1x1xf32>
      %20 = stablehlo.rsqrt %19 : tensor<1x1x1xf32>
      %21 = stablehlo.reshape %20 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
      %22 = stablehlo.broadcast_in_dim %21, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
      %23 = stablehlo.multiply %13, %22 : tensor<1x1x3072xf32>
      %24 = stablehlo.convert %23 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
      %25 = stablehlo.convert %24 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %26 = stablehlo.multiply %8, %25 : tensor<1x1x3072xf32>
      %27 = stablehlo.convert %26 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
      %28 = stablehlo.reshape %27 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
      %29 = stablehlo.transpose %arg23, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<512x3072xbf16>) -> tensor<3072x512xbf16>
      %30 = stablehlo.dot_general %28, %29, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x512xbf16>) -> tensor<1x512xbf16>
      %31 = stablehlo.reshape %30 : (tensor<1x512xbf16>) -> tensor<1x4x1x128xbf16>
      %32 = stablehlo.convert %31 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,1,128]{3,1,2,0}"} : (tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xf32>
      %33 = stablehlo.reshape %arg22 : (tensor<64xf32>) -> tensor<1x64x1xf32>
      %34 = stablehlo.convert %arg21 : (tensor<1xi64>) -> tensor<1xf32>
      %35 = stablehlo.reshape %34 : (tensor<1xf32>) -> tensor<1x1x1xf32>
      %36 = stablehlo.dot_general %33, %35, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x1xf32>) -> tensor<1x64x1xf32>
      %37 = stablehlo.reshape %36 : (tensor<1x64x1xf32>) -> tensor<1x1x64xf32>
      %38 = stablehlo.concatenate %37, %37, dim = 2 : (tensor<1x1x64xf32>, tensor<1x1x64xf32>) -> tensor<1x1x128xf32>
      %39 = stablehlo.cosine %38 : tensor<1x1x128xf32>
      %40 = stablehlo.convert %39 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
      %41 = stablehlo.convert %40 : (tensor<1x1x128xbf16>) -> tensor<1x1x128xf32>
      %42 = stablehlo.broadcast_in_dim %41, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x4x1x128xf32>
      %43 = stablehlo.multiply %32, %42 : tensor<1x4x1x128xf32>
      %44 = stablehlo.convert %43 : (tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xbf16>
      %45 = stablehlo.slice %31 [0:1, 0:4, 0:1, 64:128] : (tensor<1x4x1x128xbf16>) -> tensor<1x4x1x64xbf16>
      %46 = stablehlo.negate %45 : tensor<1x4x1x64xbf16>
      %47 = stablehlo.slice %31 [0:1, 0:4, 0:1, 0:64] : (tensor<1x4x1x128xbf16>) -> tensor<1x4x1x64xbf16>
      %48 = stablehlo.concatenate %46, %47, dim = 3 : (tensor<1x4x1x64xbf16>, tensor<1x4x1x64xbf16>) -> tensor<1x4x1x128xbf16>
      %49 = stablehlo.convert %48 : (tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xf32>
      %50 = stablehlo.sine %38 : tensor<1x1x128xf32>
      %51 = stablehlo.convert %50 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
      %52 = stablehlo.convert %51 : (tensor<1x1x128xbf16>) -> tensor<1x1x128xf32>
      %53 = stablehlo.broadcast_in_dim %52, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x4x1x128xf32>
      %54 = stablehlo.multiply %49, %53 : tensor<1x4x1x128xf32>
      %55 = stablehlo.convert %54 : (tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xbf16>
      %56 = stablehlo.add %44, %55 : tensor<1x4x1x128xbf16>
      %57 = "stablehlo.scatter"(%arg29, %6, %56) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg42: tensor<bf16>, %arg43: tensor<bf16>):
        stablehlo.return %arg43 : tensor<bf16>
      }) : (tensor<1x4x128x128xbf16>, tensor<1x1xi64>, tensor<1x4x1x128xbf16>) -> tensor<1x4x128x128xbf16>
      %58 = stablehlo.transpose %arg30, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<512x3072xbf16>) -> tensor<3072x512xbf16>
      %59 = stablehlo.dot_general %28, %58, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x512xbf16>) -> tensor<1x512xbf16>
      %60 = stablehlo.reshape %59 : (tensor<1x512xbf16>) -> tensor<1x4x1x128xbf16>
      %61 = "stablehlo.scatter"(%arg31, %6, %60) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg42: tensor<bf16>, %arg43: tensor<bf16>):
        stablehlo.return %arg43 : tensor<bf16>
      }) : (tensor<1x4x128x128xbf16>, tensor<1x1xi64>, tensor<1x4x1x128xbf16>) -> tensor<1x4x128x128xbf16>
      %62 = stablehlo.convert %arg41 : (tensor<3072xbf16>) -> tensor<3072xf32>
      %63 = stablehlo.reshape %62 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
      %64 = stablehlo.transpose %arg38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<1536x3072xbf16>) -> tensor<3072x1536xbf16>
      %65 = stablehlo.dot_general %28, %64, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1536xbf16>) -> tensor<1x1536xbf16>
      %66 = stablehlo.reshape %65 : (tensor<1x1536xbf16>) -> tensor<1x12x1x128xbf16>
      %67 = stablehlo.convert %66 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,1,128]{3,1,2,0}"} : (tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xf32>
      %68 = stablehlo.broadcast_in_dim %41, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x12x1x128xf32>
      %69 = stablehlo.multiply %67, %68 : tensor<1x12x1x128xf32>
      %70 = stablehlo.convert %69 : (tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xbf16>
      %71 = stablehlo.slice %66 [0:1, 0:12, 0:1, 64:128] : (tensor<1x12x1x128xbf16>) -> tensor<1x12x1x64xbf16>
      %72 = stablehlo.negate %71 : tensor<1x12x1x64xbf16>
      %73 = stablehlo.slice %66 [0:1, 0:12, 0:1, 0:64] : (tensor<1x12x1x128xbf16>) -> tensor<1x12x1x64xbf16>
      %74 = stablehlo.concatenate %72, %73, dim = 3 : (tensor<1x12x1x64xbf16>, tensor<1x12x1x64xbf16>) -> tensor<1x12x1x128xbf16>
      %75 = stablehlo.convert %74 : (tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xf32>
      %76 = stablehlo.broadcast_in_dim %52, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x12x1x128xf32>
      %77 = stablehlo.multiply %75, %76 : tensor<1x12x1x128xf32>
      %78 = stablehlo.convert %77 : (tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xbf16>
      %79 = stablehlo.add %70, %78 : tensor<1x12x1x128xbf16>
      %80 = stablehlo.reshape %79 : (tensor<1x12x1x128xbf16>) -> tensor<12x1x128xbf16>
      %81 = stablehlo.broadcast_in_dim %57, dims = [0, 1, 3, 4] : (tensor<1x4x128x128xbf16>) -> tensor<1x4x3x128x128xbf16>
      %82 = stablehlo.reshape %81 : (tensor<1x4x3x128x128xbf16>) -> tensor<1x12x128x128xbf16>
      %83 = stablehlo.transpose %82, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,128]{2,3,1,0}"} : (tensor<1x12x128x128xbf16>) -> tensor<1x12x128x128xbf16>
      %84 = stablehlo.reshape %83 : (tensor<1x12x128x128xbf16>) -> tensor<12x128x128xbf16>
      %85 = stablehlo.dot_general %80, %84, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<12x1x128xbf16>, tensor<12x128x128xbf16>) -> tensor<12x1x128xbf16>
      %86 = stablehlo.convert %85 : (tensor<12x1x128xbf16>) -> tensor<12x1x128xf32>
      %87 = stablehlo.reshape %86 : (tensor<12x1x128xf32>) -> tensor<1x12x1x128xf32>
      %88 = stablehlo.broadcast_in_dim %arg37, dims = [] : (tensor<f32>) -> tensor<1x12x1x128xf32>
      %89 = stablehlo.multiply %87, %88 : tensor<1x12x1x128xf32>
      %90 = stablehlo.convert %89 : (tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xbf16>
      %91 = stablehlo.convert %arg36 : (tensor<1x128xbf16>) -> tensor<1x128xf32>
      %92 = stablehlo.reshape %arg35 : (tensor<128xi64>) -> tensor<1x128xi64>
      %93 = stablehlo.broadcast_in_dim %arg21, dims = [0] : (tensor<1xi64>) -> tensor<1x128xi64>
      %94 = stablehlo.compare  GT, %92, %93 : (tensor<1x128xi64>, tensor<1x128xi64>) -> tensor<1x128xi1>
      %95 = stablehlo.convert %94 : (tensor<1x128xi1>) -> tensor<1x128xf32>
      %96 = stablehlo.multiply %91, %95 : tensor<1x128xf32>
      %97 = stablehlo.convert %96 : (tensor<1x128xf32>) -> tensor<1x128xbf16>
      %98 = stablehlo.reshape %97 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16>
      %99 = stablehlo.broadcast_in_dim %98, dims = [0, 2, 3] : (tensor<1x1x128xbf16>) -> tensor<1x12x1x128xbf16>
      %100 = stablehlo.add %90, %99 : tensor<1x12x1x128xbf16>
      %101 = stablehlo.convert %100 : (tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xf32>
      %102 = stablehlo.reduce(%101 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x12x1x128xf32>, tensor<f32>) -> tensor<1x12x1xf32>
      %103 = stablehlo.broadcast_in_dim %102, dims = [0, 1, 2] : (tensor<1x12x1xf32>) -> tensor<1x12x1x128xf32>
      %104 = stablehlo.subtract %101, %103 : tensor<1x12x1x128xf32>
      %105 = stablehlo.exponential %104 : tensor<1x12x1x128xf32>
      %106 = stablehlo.reduce(%105 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x12x1x128xf32>, tensor<f32>) -> tensor<1x12x1xf32>
      %107 = stablehlo.broadcast_in_dim %106, dims = [0, 1, 2] : (tensor<1x12x1xf32>) -> tensor<1x12x1x128xf32>
      %108 = stablehlo.divide %105, %107 : tensor<1x12x1x128xf32>
      %109 = stablehlo.convert %108 : (tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xbf16>
      %110 = stablehlo.reshape %109 : (tensor<1x12x1x128xbf16>) -> tensor<12x1x128xbf16>
      %111 = stablehlo.broadcast_in_dim %61, dims = [0, 1, 3, 4] : (tensor<1x4x128x128xbf16>) -> tensor<1x4x3x128x128xbf16>
      %112 = stablehlo.reshape %111 : (tensor<1x4x3x128x128xbf16>) -> tensor<12x128x128xbf16>
      %113 = stablehlo.dot_general %110, %112, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<12x1x128xbf16>, tensor<12x128x128xbf16>) -> tensor<12x1x128xbf16>
      %114 = stablehlo.reshape %113 : (tensor<12x1x128xbf16>) -> tensor<1x1536xbf16>
      %115 = stablehlo.transpose %arg34, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x1536xbf16>) -> tensor<1536x3072xbf16>
      %116 = stablehlo.dot_general %114, %115, contracting_dims = [1] x [0] : (tensor<1x1536xbf16>, tensor<1536x3072xbf16>) -> tensor<1x3072xbf16>
      %117 = "stablehlo.all_reduce"(%116) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> ({
      ^bb0(%arg42: tensor<bf16>, %arg43: tensor<bf16>):
        %176 = stablehlo.add %arg42, %arg43 : tensor<bf16>
        stablehlo.return %176 : tensor<bf16>
      }) : (tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
      %118 = stablehlo.reshape %117 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
      %119 = stablehlo.add %12, %118 : tensor<1x1x3072xbf16>
      %120 = stablehlo.convert %arg39 : (tensor<3072xbf16>) -> tensor<3072xf32>
      %121 = stablehlo.reshape %120 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
      %122 = stablehlo.convert %119 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %123 = stablehlo.power %122, %1 : tensor<1x1x3072xf32>
      %124 = stablehlo.reduce(%123 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
      %125 = stablehlo.multiply %124, %cst_1 : tensor<1x1xf32>
      %126 = stablehlo.reshape %125 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
      %127 = stablehlo.add %126, %18 : tensor<1x1x1xf32>
      %128 = stablehlo.rsqrt %127 : tensor<1x1x1xf32>
      %129 = stablehlo.reshape %128 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
      %130 = stablehlo.broadcast_in_dim %129, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
      %131 = stablehlo.multiply %122, %130 : tensor<1x1x3072xf32>
      %132 = stablehlo.convert %131 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
      %133 = stablehlo.convert %132 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %134 = stablehlo.multiply %121, %133 : tensor<1x1x3072xf32>
      %135 = stablehlo.convert %134 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
      %136 = stablehlo.reshape %135 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
      %137 = stablehlo.transpose %arg40, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<4096x3072xbf16>) -> tensor<3072x4096xbf16>
      %138 = stablehlo.dot_general %136, %137, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<1x4096xbf16>
      %139 = stablehlo.reshape %138 : (tensor<1x4096xbf16>) -> tensor<1x1x4096xbf16>
      %140 = stablehlo.convert %139 : (tensor<1x1x4096xbf16>) -> tensor<1x1x4096xf32>
      %141 = stablehlo.logistic %139 : tensor<1x1x4096xbf16>
      %142 = stablehlo.convert %141 : (tensor<1x1x4096xbf16>) -> tensor<1x1x4096xf32>
      %143 = stablehlo.multiply %140, %142 : tensor<1x1x4096xf32>
      %144 = stablehlo.convert %143 : (tensor<1x1x4096xf32>) -> tensor<1x1x4096xbf16>
      %145 = stablehlo.convert %144 : (tensor<1x1x4096xbf16>) -> tensor<1x1x4096xf32>
      %146 = stablehlo.transpose %arg33, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<4096x3072xbf16>) -> tensor<3072x4096xbf16>
      %147 = stablehlo.dot_general %136, %146, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<1x4096xbf16>
      %148 = stablehlo.convert %147 : (tensor<1x4096xbf16>) -> tensor<1x4096xf32>
      %149 = stablehlo.reshape %148 : (tensor<1x4096xf32>) -> tensor<1x1x4096xf32>
      %150 = stablehlo.multiply %145, %149 : tensor<1x1x4096xf32>
      %151 = stablehlo.convert %150 : (tensor<1x1x4096xf32>) -> tensor<1x1x4096xbf16>
      %152 = stablehlo.reshape %151 : (tensor<1x1x4096xbf16>) -> tensor<1x4096xbf16>
      %153 = stablehlo.transpose %arg32, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x4096xbf16>) -> tensor<4096x3072xbf16>
      %154 = stablehlo.dot_general %152, %153, contracting_dims = [1] x [0] : (tensor<1x4096xbf16>, tensor<4096x3072xbf16>) -> tensor<1x3072xbf16>
      %155 = "stablehlo.all_reduce"(%154) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> ({
      ^bb0(%arg42: tensor<bf16>, %arg43: tensor<bf16>):
        %176 = stablehlo.add %arg42, %arg43 : tensor<bf16>
        stablehlo.return %176 : tensor<bf16>
      }) : (tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
      %156 = stablehlo.reshape %155 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
      %157 = stablehlo.add %119, %156 : tensor<1x1x3072xbf16>
      %158 = stablehlo.convert %157 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %159 = stablehlo.power %158, %1 : tensor<1x1x3072xf32>
      %160 = stablehlo.reduce(%159 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
      %161 = stablehlo.multiply %160, %cst_1 : tensor<1x1xf32>
      %162 = stablehlo.reshape %161 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
      %163 = stablehlo.add %162, %18 : tensor<1x1x1xf32>
      %164 = stablehlo.rsqrt %163 : tensor<1x1x1xf32>
      %165 = stablehlo.reshape %164 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
      %166 = stablehlo.broadcast_in_dim %165, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
      %167 = stablehlo.multiply %158, %166 : tensor<1x1x3072xf32>
      %168 = stablehlo.convert %167 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
      %169 = stablehlo.convert %168 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %170 = stablehlo.multiply %63, %169 : tensor<1x1x3072xf32>
      %171 = stablehlo.convert %170 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
      %172 = stablehlo.reshape %171 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
      %173 = stablehlo.transpose %arg26, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
      %174 = stablehlo.dot_general %172, %173, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<1x128256xbf16>
      %175 = stablehlo.reshape %174 : (tensor<1x128256xbf16>) -> tensor<1x1x128256xbf16>
      sdy.return %57, %61, %174, %175 : tensor<1x4x128x128xbf16>, tensor<1x4x128x128xbf16>, tensor<1x128256xbf16>, tensor<1x1x128256xbf16>
    } : (tensor<1xi64>, tensor<64xf32>, tensor<1024x3072xbf16>, tensor<f32>, tensor<1x1xi64>, tensor<128256x3072xbf16>, tensor<3072xbf16>, tensor<i64>, tensor<1x8x128x128xbf16>, tensor<1024x3072xbf16>, tensor<1x8x128x128xbf16>, tensor<3072x8192xbf16>, tensor<8192x3072xbf16>, tensor<3072x3072xbf16>, tensor<128xi64>, tensor<1x128xbf16>, tensor<f32>, tensor<3072x3072xbf16>, tensor<3072xbf16>, tensor<8192x3072xbf16>, tensor<3072xbf16>) -> (tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x128256xbf16>, tensor<1x1x128256xbf16>)
    return %0#0, %0#1, %0#2, %0#3 : tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x128256xbf16>, tensor<1x1x128256xbf16>
  }
}
module @SyncTensorsGraph.370 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  func.func @main(%arg0: tensor<1xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<64xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1024x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1x1xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<128256x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<i64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<1024x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<3072x8192xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<8192x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<128xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<1x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<8192x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x1x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = ttir.empty() : tensor<1xi64>
    %1 = "ttir.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %2 = ttir.empty() : tensor<64xf32>
    %3 = "ttir.mesh_shard"(%arg1, %2) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32>, tensor<64xf32>) -> tensor<64xf32>
    %4 = ttir.empty() : tensor<512x3072xbf16>
    %5 = "ttir.mesh_shard"(%arg2, %4) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16>, tensor<512x3072xbf16>) -> tensor<512x3072xbf16>
    %6 = ttir.empty() : tensor<f32>
    %7 = "ttir.mesh_shard"(%arg3, %6) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %8 = ttir.empty() : tensor<1x1xi64>
    %9 = "ttir.mesh_shard"(%arg4, %8) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x1xi64>, tensor<1x1xi64>) -> tensor<1x1xi64>
    %10 = ttir.empty() : tensor<128256x3072xbf16>
    %11 = "ttir.mesh_shard"(%arg5, %10) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16>, tensor<128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %12 = ttir.empty() : tensor<3072xbf16>
    %13 = "ttir.mesh_shard"(%arg6, %12) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16>, tensor<3072xbf16>) -> tensor<3072xbf16>
    %14 = ttir.empty() : tensor<i64>
    %15 = "ttir.mesh_shard"(%arg7, %14) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %16 = ttir.empty() : tensor<1x4x128x128xbf16>
    %17 = "ttir.mesh_shard"(%arg8, %16) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x128x128xbf16>, tensor<1x4x128x128xbf16>) -> tensor<1x4x128x128xbf16>
    %18 = ttir.empty() : tensor<512x3072xbf16>
    %19 = "ttir.mesh_shard"(%arg9, %18) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16>, tensor<512x3072xbf16>) -> tensor<512x3072xbf16>
    %20 = ttir.empty() : tensor<1x4x128x128xbf16>
    %21 = "ttir.mesh_shard"(%arg10, %20) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x128x128xbf16>, tensor<1x4x128x128xbf16>) -> tensor<1x4x128x128xbf16>
    %22 = ttir.empty() : tensor<3072x4096xbf16>
    %23 = "ttir.mesh_shard"(%arg11, %22) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x8192xbf16>, tensor<3072x4096xbf16>) -> tensor<3072x4096xbf16>
    %24 = ttir.empty() : tensor<4096x3072xbf16>
    %25 = "ttir.mesh_shard"(%arg12, %24) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16>, tensor<4096x3072xbf16>) -> tensor<4096x3072xbf16>
    %26 = ttir.empty() : tensor<3072x1536xbf16>
    %27 = "ttir.mesh_shard"(%arg13, %26) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16>, tensor<3072x1536xbf16>) -> tensor<3072x1536xbf16>
    %28 = ttir.empty() : tensor<128xi64>
    %29 = "ttir.mesh_shard"(%arg14, %28) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128xi64>, tensor<128xi64>) -> tensor<128xi64>
    %30 = ttir.empty() : tensor<1x128xbf16>
    %31 = "ttir.mesh_shard"(%arg15, %30) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %32 = ttir.empty() : tensor<f32>
    %33 = "ttir.mesh_shard"(%arg16, %32) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %34 = ttir.empty() : tensor<1536x3072xbf16>
    %35 = "ttir.mesh_shard"(%arg17, %34) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16>, tensor<1536x3072xbf16>) -> tensor<1536x3072xbf16>
    %36 = ttir.empty() : tensor<3072xbf16>
    %37 = "ttir.mesh_shard"(%arg18, %36) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16>, tensor<3072xbf16>) -> tensor<3072xbf16>
    %38 = ttir.empty() : tensor<4096x3072xbf16>
    %39 = "ttir.mesh_shard"(%arg19, %38) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16>, tensor<4096x3072xbf16>) -> tensor<4096x3072xbf16>
    %40 = ttir.empty() : tensor<3072xbf16>
    %41 = "ttir.mesh_shard"(%arg20, %40) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16>, tensor<3072xbf16>) -> tensor<3072xbf16>
    %42 = "ttir.constant"() <{value = dense<0> : tensor<1xi64>}> : () -> tensor<1xi64>
    %43 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<f32>}> : () -> tensor<f32>
    %44 = "ttir.constant"() <{value = dense<0xFF800000> : tensor<f32>}> : () -> tensor<f32>
    %45 = "ttir.constant"() <{value = dense<3.25520843E-4> : tensor<1x1xf32>}> : () -> tensor<1x1xf32>
    %46 = "ttir.constant"() <{value = dense<2.000000e+00> : tensor<f32>}> : () -> tensor<f32>
    %47 = ttir.empty() : tensor<1x1x1xf32>
    %48 = "ttir.reshape"(%46, %47) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %49 = ttir.empty() : tensor<1x1x3072xf32>
    %50 = "ttir.broadcast"(%48, %49) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x1x1xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %51 = ttir.empty() : tensor<1xi1>
    %52 = "ttir.lt"(%1, %42, %51) : (tensor<1xi64>, tensor<1xi64>, tensor<1xi1>) -> tensor<1xi1>
    %53 = ttir.empty() : tensor<1xi64>
    %54 = "ttir.reshape"(%15, %53) <{shape = [1 : i32]}> : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %55 = ttir.empty() : tensor<1xi64>
    %56 = "ttir.add"(%1, %54, %55) : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %57 = ttir.empty() : tensor<1xi64>
    %58 = "ttir.where"(%52, %56, %1, %57) : (tensor<1xi1>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %59 = ttir.empty() : tensor<1x1xi64>
    %60 = "ttir.reshape"(%58, %59) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xi64>, tensor<1x1xi64>) -> tensor<1x1xi64>
    %61 = ttir.empty() : tensor<3072xf32>
    %62 = "ttir.typecast"(%13, %61) <{conservative_folding = false}> : (tensor<3072xbf16>, tensor<3072xf32>) -> tensor<3072xf32>
    %63 = ttir.empty() : tensor<1x1x3072xf32>
    %64 = "ttir.reshape"(%62, %63) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %65 = ttir.empty() : tensor<1x1xui32>
    %66 = "ttir.typecast"(%9, %65) <{conservative_folding = false}> : (tensor<1x1xi64>, tensor<1x1xui32>) -> tensor<1x1xui32>
    %67 = ttir.empty() : tensor<1xui32>
    %68 = "ttir.reshape"(%66, %67) <{shape = [1 : i32]}> : (tensor<1x1xui32>, tensor<1xui32>) -> tensor<1xui32>
    %69 = ttir.empty() : tensor<1x3072xbf16>
    %70 = "ttir.gather"(%11, %68, %69) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 3072>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<128256x3072xbf16>, tensor<1xui32>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %71 = ttir.empty() : tensor<1x1x3072xbf16>
    %72 = "ttir.reshape"(%70, %71) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %73 = ttir.empty() : tensor<1x1x3072xf32>
    %74 = "ttir.typecast"(%72, %73) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %75 = ttir.empty() : tensor<1x1x3072xf32>
    %76 = "ttir.pow"(%74, %50, %75) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %77 = ttir.empty() : tensor<1x1xf32>
    %78 = "ttir.sum"(%76, %77) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %79 = ttir.empty() : tensor<1x1xf32>
    %80 = "ttir.multiply"(%78, %45, %79) : (tensor<1x1xf32>, tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %81 = ttir.empty() : tensor<1x1x1xf32>
    %82 = "ttir.reshape"(%80, %81) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %83 = ttir.empty() : tensor<1x1x1xf32>
    %84 = "ttir.reshape"(%7, %83) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %85 = ttir.empty() : tensor<1x1x1xf32>
    %86 = "ttir.add"(%82, %84, %85) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %87 = ttir.empty() : tensor<1x1x1xf32>
    %88 = "ttir.rsqrt"(%86, %87) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %89 = ttir.empty() : tensor<1x1xf32>
    %90 = "ttir.reshape"(%88, %89) <{shape = [1 : i32, 1 : i32]}> : (tensor<1x1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %91 = ttir.empty() : tensor<1x1x1xf32>
    %92 = "ttir.reshape"(%90, %91) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %93 = ttir.empty() : tensor<1x1x3072xf32>
    %94 = "ttir.broadcast"(%92, %93) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x1x1xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %95 = ttir.empty() : tensor<1x1x3072xf32>
    %96 = "ttir.multiply"(%74, %94, %95) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %97 = ttir.empty() : tensor<1x1x3072xbf16>
    %98 = "ttir.typecast"(%96, %97) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %99 = ttir.empty() : tensor<1x1x3072xf32>
    %100 = "ttir.typecast"(%98, %99) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %101 = ttir.empty() : tensor<1x1x3072xf32>
    %102 = "ttir.multiply"(%64, %100, %101) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %103 = ttir.empty() : tensor<1x1x3072xbf16>
    %104 = "ttir.typecast"(%102, %103) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %105 = ttir.empty() : tensor<1x3072xbf16>
    %106 = "ttir.reshape"(%104, %105) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x3072xbf16>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %107 = ttir.empty() : tensor<3072x512xbf16>
    %108 = "ttir.permute"(%5, %107) <{permutation = array<i64: 1, 0>}> : (tensor<512x3072xbf16>, tensor<3072x512xbf16>) -> tensor<3072x512xbf16>
    %109 = "ttir.dot_general"(%106, %108) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x512xbf16>) -> tensor<1x512xbf16>
    %110 = ttir.empty() : tensor<1x4x1x128xbf16>
    %111 = "ttir.reshape"(%109, %110) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32]}> : (tensor<1x512xbf16>, tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xbf16>
    %112 = ttir.empty() : tensor<1x4x1x128xf32>
    %113 = "ttir.typecast"(%111, %112) <{conservative_folding = false}> : (tensor<1x4x1x128xbf16>, tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xf32>
    %114 = ttir.empty() : tensor<1x64x1xf32>
    %115 = "ttir.reshape"(%3, %114) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
    %116 = ttir.empty() : tensor<1xf32>
    %117 = "ttir.typecast"(%1, %116) <{conservative_folding = false}> : (tensor<1xi64>, tensor<1xf32>) -> tensor<1xf32>
    %118 = ttir.empty() : tensor<1x1x1xf32>
    %119 = "ttir.reshape"(%117, %118) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %120 = "ttir.dot_general"(%115, %119) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x64x1xf32>, tensor<1x1x1xf32>) -> tensor<1x64x1xf32>
    %121 = ttir.empty() : tensor<1x1x64xf32>
    %122 = "ttir.reshape"(%120, %121) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<1x64x1xf32>, tensor<1x1x64xf32>) -> tensor<1x1x64xf32>
    %123 = ttir.empty() : tensor<1x1x128xf32>
    %124 = "ttir.concat"(%122, %122, %123) <{dim = 2 : si32}> : (tensor<1x1x64xf32>, tensor<1x1x64xf32>, tensor<1x1x128xf32>) -> tensor<1x1x128xf32>
    %125 = ttir.empty() : tensor<1x1x128xf32>
    %126 = "ttir.cos"(%124, %125) : (tensor<1x1x128xf32>, tensor<1x1x128xf32>) -> tensor<1x1x128xf32>
    %127 = ttir.empty() : tensor<1x1x128xbf16>
    %128 = "ttir.typecast"(%126, %127) <{conservative_folding = false}> : (tensor<1x1x128xf32>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16>
    %129 = ttir.empty() : tensor<1x1x128xf32>
    %130 = "ttir.typecast"(%128, %129) <{conservative_folding = false}> : (tensor<1x1x128xbf16>, tensor<1x1x128xf32>) -> tensor<1x1x128xf32>
    %131 = ttir.empty() : tensor<1x1x1x128xf32>
    %132 = "ttir.reshape"(%130, %131) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32>, tensor<1x1x1x128xf32>) -> tensor<1x1x1x128xf32>
    %133 = ttir.empty() : tensor<1x4x1x128xf32>
    %134 = "ttir.broadcast"(%132, %133) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x1x128xf32>, tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xf32>
    %135 = ttir.empty() : tensor<1x4x1x128xf32>
    %136 = "ttir.multiply"(%113, %134, %135) : (tensor<1x4x1x128xf32>, tensor<1x4x1x128xf32>, tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xf32>
    %137 = ttir.empty() : tensor<1x4x1x128xbf16>
    %138 = "ttir.typecast"(%136, %137) <{conservative_folding = false}> : (tensor<1x4x1x128xf32>, tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xbf16>
    %139 = ttir.empty() : tensor<1x4x1x64xbf16>
    %140 = "ttir.slice_static"(%111, %139) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x1x128xbf16>, tensor<1x4x1x64xbf16>) -> tensor<1x4x1x64xbf16>
    %141 = ttir.empty() : tensor<1x4x1x64xbf16>
    %142 = "ttir.neg"(%140, %141) : (tensor<1x4x1x64xbf16>, tensor<1x4x1x64xbf16>) -> tensor<1x4x1x64xbf16>
    %143 = ttir.empty() : tensor<1x4x1x64xbf16>
    %144 = "ttir.slice_static"(%111, %143) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x1x128xbf16>, tensor<1x4x1x64xbf16>) -> tensor<1x4x1x64xbf16>
    %145 = ttir.empty() : tensor<1x4x1x128xbf16>
    %146 = "ttir.concat"(%142, %144, %145) <{dim = 3 : si32}> : (tensor<1x4x1x64xbf16>, tensor<1x4x1x64xbf16>, tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xbf16>
    %147 = ttir.empty() : tensor<1x4x1x128xf32>
    %148 = "ttir.typecast"(%146, %147) <{conservative_folding = false}> : (tensor<1x4x1x128xbf16>, tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xf32>
    %149 = ttir.empty() : tensor<1x1x128xf32>
    %150 = "ttir.sin"(%124, %149) : (tensor<1x1x128xf32>, tensor<1x1x128xf32>) -> tensor<1x1x128xf32>
    %151 = ttir.empty() : tensor<1x1x128xbf16>
    %152 = "ttir.typecast"(%150, %151) <{conservative_folding = false}> : (tensor<1x1x128xf32>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16>
    %153 = ttir.empty() : tensor<1x1x128xf32>
    %154 = "ttir.typecast"(%152, %153) <{conservative_folding = false}> : (tensor<1x1x128xbf16>, tensor<1x1x128xf32>) -> tensor<1x1x128xf32>
    %155 = ttir.empty() : tensor<1x1x1x128xf32>
    %156 = "ttir.reshape"(%154, %155) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32>, tensor<1x1x1x128xf32>) -> tensor<1x1x1x128xf32>
    %157 = ttir.empty() : tensor<1x4x1x128xf32>
    %158 = "ttir.broadcast"(%156, %157) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x1x128xf32>, tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xf32>
    %159 = ttir.empty() : tensor<1x4x1x128xf32>
    %160 = "ttir.multiply"(%148, %158, %159) : (tensor<1x4x1x128xf32>, tensor<1x4x1x128xf32>, tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xf32>
    %161 = ttir.empty() : tensor<1x4x1x128xbf16>
    %162 = "ttir.typecast"(%160, %161) <{conservative_folding = false}> : (tensor<1x4x1x128xf32>, tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xbf16>
    %163 = ttir.empty() : tensor<1x4x1x128xbf16>
    %164 = "ttir.add"(%138, %162, %163) : (tensor<1x4x1x128xbf16>, tensor<1x4x1x128xbf16>, tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xbf16>
    %165 = ttir.empty() : tensor<1x4x128x128xbf16>
    %166 = "ttir.scatter"(%17, %60, %164, %165) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x4x128x128xbf16>, tensor<1x1xi64>, tensor<1x4x1x128xbf16>, tensor<1x4x128x128xbf16>) -> tensor<1x4x128x128xbf16>
    %167 = ttir.empty() : tensor<3072x512xbf16>
    %168 = "ttir.permute"(%19, %167) <{permutation = array<i64: 1, 0>}> : (tensor<512x3072xbf16>, tensor<3072x512xbf16>) -> tensor<3072x512xbf16>
    %169 = "ttir.dot_general"(%106, %168) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x512xbf16>) -> tensor<1x512xbf16>
    %170 = ttir.empty() : tensor<1x4x1x128xbf16>
    %171 = "ttir.reshape"(%169, %170) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32]}> : (tensor<1x512xbf16>, tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xbf16>
    %172 = ttir.empty() : tensor<1x4x128x128xbf16>
    %173 = "ttir.scatter"(%21, %60, %171, %172) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x4x128x128xbf16>, tensor<1x1xi64>, tensor<1x4x1x128xbf16>, tensor<1x4x128x128xbf16>) -> tensor<1x4x128x128xbf16>
    %174 = ttir.empty() : tensor<3072xf32>
    %175 = "ttir.typecast"(%41, %174) <{conservative_folding = false}> : (tensor<3072xbf16>, tensor<3072xf32>) -> tensor<3072xf32>
    %176 = ttir.empty() : tensor<1x1x3072xf32>
    %177 = "ttir.reshape"(%175, %176) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %178 = ttir.empty() : tensor<3072x1536xbf16>
    %179 = "ttir.permute"(%35, %178) <{permutation = array<i64: 1, 0>}> : (tensor<1536x3072xbf16>, tensor<3072x1536xbf16>) -> tensor<3072x1536xbf16>
    %180 = "ttir.dot_general"(%106, %179) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x1536xbf16>) -> tensor<1x1536xbf16>
    %181 = ttir.empty() : tensor<1x12x1x128xbf16>
    %182 = "ttir.reshape"(%180, %181) <{shape = [1 : i32, 12 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1536xbf16>, tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xbf16>
    %183 = ttir.empty() : tensor<1x12x1x128xf32>
    %184 = "ttir.typecast"(%182, %183) <{conservative_folding = false}> : (tensor<1x12x1x128xbf16>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %185 = ttir.empty() : tensor<1x1x1x128xf32>
    %186 = "ttir.reshape"(%130, %185) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32>, tensor<1x1x1x128xf32>) -> tensor<1x1x1x128xf32>
    %187 = ttir.empty() : tensor<1x12x1x128xf32>
    %188 = "ttir.broadcast"(%186, %187) <{broadcast_dimensions = array<i64: 1, 12, 1, 1>}> : (tensor<1x1x1x128xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %189 = ttir.empty() : tensor<1x12x1x128xf32>
    %190 = "ttir.multiply"(%184, %188, %189) : (tensor<1x12x1x128xf32>, tensor<1x12x1x128xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %191 = ttir.empty() : tensor<1x12x1x128xbf16>
    %192 = "ttir.typecast"(%190, %191) <{conservative_folding = false}> : (tensor<1x12x1x128xf32>, tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xbf16>
    %193 = ttir.empty() : tensor<1x12x1x64xbf16>
    %194 = "ttir.slice_static"(%182, %193) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 12 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x1x128xbf16>, tensor<1x12x1x64xbf16>) -> tensor<1x12x1x64xbf16>
    %195 = ttir.empty() : tensor<1x12x1x64xbf16>
    %196 = "ttir.neg"(%194, %195) : (tensor<1x12x1x64xbf16>, tensor<1x12x1x64xbf16>) -> tensor<1x12x1x64xbf16>
    %197 = ttir.empty() : tensor<1x12x1x64xbf16>
    %198 = "ttir.slice_static"(%182, %197) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 12 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x1x128xbf16>, tensor<1x12x1x64xbf16>) -> tensor<1x12x1x64xbf16>
    %199 = ttir.empty() : tensor<1x12x1x128xbf16>
    %200 = "ttir.concat"(%196, %198, %199) <{dim = 3 : si32}> : (tensor<1x12x1x64xbf16>, tensor<1x12x1x64xbf16>, tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xbf16>
    %201 = ttir.empty() : tensor<1x12x1x128xf32>
    %202 = "ttir.typecast"(%200, %201) <{conservative_folding = false}> : (tensor<1x12x1x128xbf16>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %203 = ttir.empty() : tensor<1x1x1x128xf32>
    %204 = "ttir.reshape"(%154, %203) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32>, tensor<1x1x1x128xf32>) -> tensor<1x1x1x128xf32>
    %205 = ttir.empty() : tensor<1x12x1x128xf32>
    %206 = "ttir.broadcast"(%204, %205) <{broadcast_dimensions = array<i64: 1, 12, 1, 1>}> : (tensor<1x1x1x128xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %207 = ttir.empty() : tensor<1x12x1x128xf32>
    %208 = "ttir.multiply"(%202, %206, %207) : (tensor<1x12x1x128xf32>, tensor<1x12x1x128xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %209 = ttir.empty() : tensor<1x12x1x128xbf16>
    %210 = "ttir.typecast"(%208, %209) <{conservative_folding = false}> : (tensor<1x12x1x128xf32>, tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xbf16>
    %211 = ttir.empty() : tensor<1x12x1x128xbf16>
    %212 = "ttir.add"(%192, %210, %211) : (tensor<1x12x1x128xbf16>, tensor<1x12x1x128xbf16>, tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xbf16>
    %213 = ttir.empty() : tensor<12x1x128xbf16>
    %214 = "ttir.reshape"(%212, %213) <{shape = [12 : i32, 1 : i32, 128 : i32]}> : (tensor<1x12x1x128xbf16>, tensor<12x1x128xbf16>) -> tensor<12x1x128xbf16>
    %215 = ttir.empty() : tensor<1x4x1x128x128xbf16>
    %216 = "ttir.reshape"(%166, %215) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x4x128x128xbf16>, tensor<1x4x1x128x128xbf16>) -> tensor<1x4x1x128x128xbf16>
    %217 = ttir.empty() : tensor<1x4x3x128x128xbf16>
    %218 = "ttir.broadcast"(%216, %217) <{broadcast_dimensions = array<i64: 1, 1, 3, 1, 1>}> : (tensor<1x4x1x128x128xbf16>, tensor<1x4x3x128x128xbf16>) -> tensor<1x4x3x128x128xbf16>
    %219 = ttir.empty() : tensor<1x12x128x128xbf16>
    %220 = "ttir.reshape"(%218, %219) <{shape = [1 : i32, 12 : i32, 128 : i32, 128 : i32]}> : (tensor<1x4x3x128x128xbf16>, tensor<1x12x128x128xbf16>) -> tensor<1x12x128x128xbf16>
    %221 = ttir.empty() : tensor<1x12x128x128xbf16>
    %222 = "ttir.permute"(%220, %221) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x12x128x128xbf16>, tensor<1x12x128x128xbf16>) -> tensor<1x12x128x128xbf16>
    %223 = ttir.empty() : tensor<12x128x128xbf16>
    %224 = "ttir.reshape"(%222, %223) <{shape = [12 : i32, 128 : i32, 128 : i32]}> : (tensor<1x12x128x128xbf16>, tensor<12x128x128xbf16>) -> tensor<12x128x128xbf16>
    %225 = "ttir.dot_general"(%214, %224) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<12x1x128xbf16>, tensor<12x128x128xbf16>) -> tensor<12x1x128xbf16>
    %226 = ttir.empty() : tensor<12x1x128xf32>
    %227 = "ttir.typecast"(%225, %226) <{conservative_folding = false}> : (tensor<12x1x128xbf16>, tensor<12x1x128xf32>) -> tensor<12x1x128xf32>
    %228 = ttir.empty() : tensor<1x12x1x128xf32>
    %229 = "ttir.reshape"(%227, %228) <{shape = [1 : i32, 12 : i32, 1 : i32, 128 : i32]}> : (tensor<12x1x128xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %230 = ttir.empty() : tensor<1x1x1x1xf32>
    %231 = "ttir.reshape"(%33, %230) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %232 = ttir.empty() : tensor<1x12x1x128xf32>
    %233 = "ttir.broadcast"(%231, %232) <{broadcast_dimensions = array<i64: 1, 12, 1, 128>}> : (tensor<1x1x1x1xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %234 = ttir.empty() : tensor<1x12x1x128xf32>
    %235 = "ttir.multiply"(%229, %233, %234) : (tensor<1x12x1x128xf32>, tensor<1x12x1x128xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %236 = ttir.empty() : tensor<1x12x1x128xbf16>
    %237 = "ttir.typecast"(%235, %236) <{conservative_folding = false}> : (tensor<1x12x1x128xf32>, tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xbf16>
    %238 = ttir.empty() : tensor<1x128xf32>
    %239 = "ttir.typecast"(%31, %238) <{conservative_folding = false}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %240 = ttir.empty() : tensor<1x128xi64>
    %241 = "ttir.reshape"(%29, %240) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xi64>, tensor<1x128xi64>) -> tensor<1x128xi64>
    %242 = ttir.empty() : tensor<1x1xi64>
    %243 = "ttir.reshape"(%1, %242) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xi64>, tensor<1x1xi64>) -> tensor<1x1xi64>
    %244 = ttir.empty() : tensor<1x128xi64>
    %245 = "ttir.broadcast"(%243, %244) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xi64>, tensor<1x128xi64>) -> tensor<1x128xi64>
    %246 = ttir.empty() : tensor<1x128xi1>
    %247 = "ttir.gt"(%241, %245, %246) : (tensor<1x128xi64>, tensor<1x128xi64>, tensor<1x128xi1>) -> tensor<1x128xi1>
    %248 = ttir.empty() : tensor<1x128xf32>
    %249 = "ttir.typecast"(%247, %248) <{conservative_folding = false}> : (tensor<1x128xi1>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %250 = ttir.empty() : tensor<1x128xf32>
    %251 = "ttir.multiply"(%239, %249, %250) : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %252 = ttir.empty() : tensor<1x128xbf16>
    %253 = "ttir.typecast"(%251, %252) <{conservative_folding = false}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %254 = ttir.empty() : tensor<1x1x128xbf16>
    %255 = "ttir.reshape"(%253, %254) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16>
    %256 = ttir.empty() : tensor<1x1x1x128xbf16>
    %257 = "ttir.reshape"(%255, %256) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xbf16>, tensor<1x1x1x128xbf16>) -> tensor<1x1x1x128xbf16>
    %258 = ttir.empty() : tensor<1x12x1x128xbf16>
    %259 = "ttir.broadcast"(%257, %258) <{broadcast_dimensions = array<i64: 1, 12, 1, 1>}> : (tensor<1x1x1x128xbf16>, tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xbf16>
    %260 = ttir.empty() : tensor<1x12x1x128xbf16>
    %261 = "ttir.add"(%237, %259, %260) : (tensor<1x12x1x128xbf16>, tensor<1x12x1x128xbf16>, tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xbf16>
    %262 = ttir.empty() : tensor<1x12x1x128xf32>
    %263 = "ttir.typecast"(%261, %262) <{conservative_folding = false}> : (tensor<1x12x1x128xbf16>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %264 = ttir.empty() : tensor<1x12x1xf32>
    %265 = "ttir.max"(%263, %264) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x12x1x128xf32>, tensor<1x12x1xf32>) -> tensor<1x12x1xf32>
    %266 = ttir.empty() : tensor<1x12x1x1xf32>
    %267 = "ttir.reshape"(%265, %266) <{shape = [1 : i32, 12 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x1xf32>, tensor<1x12x1x1xf32>) -> tensor<1x12x1x1xf32>
    %268 = ttir.empty() : tensor<1x12x1x128xf32>
    %269 = "ttir.broadcast"(%267, %268) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<1x12x1x1xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %270 = ttir.empty() : tensor<1x12x1x128xf32>
    %271 = "ttir.subtract"(%263, %269, %270) : (tensor<1x12x1x128xf32>, tensor<1x12x1x128xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %272 = ttir.empty() : tensor<1x12x1x128xf32>
    %273 = "ttir.exp"(%271, %272) : (tensor<1x12x1x128xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %274 = ttir.empty() : tensor<1x12x1xf32>
    %275 = "ttir.sum"(%273, %274) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x12x1x128xf32>, tensor<1x12x1xf32>) -> tensor<1x12x1xf32>
    %276 = ttir.empty() : tensor<1x12x1x1xf32>
    %277 = "ttir.reshape"(%275, %276) <{shape = [1 : i32, 12 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x1xf32>, tensor<1x12x1x1xf32>) -> tensor<1x12x1x1xf32>
    %278 = ttir.empty() : tensor<1x12x1x128xf32>
    %279 = "ttir.broadcast"(%277, %278) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<1x12x1x1xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %280 = ttir.empty() : tensor<1x12x1x128xf32>
    %281 = "ttir.div"(%273, %279, %280) : (tensor<1x12x1x128xf32>, tensor<1x12x1x128xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %282 = ttir.empty() : tensor<1x12x1x128xbf16>
    %283 = "ttir.typecast"(%281, %282) <{conservative_folding = false}> : (tensor<1x12x1x128xf32>, tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xbf16>
    %284 = ttir.empty() : tensor<12x1x128xbf16>
    %285 = "ttir.reshape"(%283, %284) <{shape = [12 : i32, 1 : i32, 128 : i32]}> : (tensor<1x12x1x128xbf16>, tensor<12x1x128xbf16>) -> tensor<12x1x128xbf16>
    %286 = ttir.empty() : tensor<1x4x1x128x128xbf16>
    %287 = "ttir.reshape"(%173, %286) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x4x128x128xbf16>, tensor<1x4x1x128x128xbf16>) -> tensor<1x4x1x128x128xbf16>
    %288 = ttir.empty() : tensor<1x4x3x128x128xbf16>
    %289 = "ttir.broadcast"(%287, %288) <{broadcast_dimensions = array<i64: 1, 1, 3, 1, 1>}> : (tensor<1x4x1x128x128xbf16>, tensor<1x4x3x128x128xbf16>) -> tensor<1x4x3x128x128xbf16>
    %290 = ttir.empty() : tensor<12x128x128xbf16>
    %291 = "ttir.reshape"(%289, %290) <{shape = [12 : i32, 128 : i32, 128 : i32]}> : (tensor<1x4x3x128x128xbf16>, tensor<12x128x128xbf16>) -> tensor<12x128x128xbf16>
    %292 = "ttir.dot_general"(%285, %291) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<12x1x128xbf16>, tensor<12x128x128xbf16>) -> tensor<12x1x128xbf16>
    %293 = ttir.empty() : tensor<1x1536xbf16>
    %294 = "ttir.reshape"(%292, %293) <{shape = [1 : i32, 1536 : i32]}> : (tensor<12x1x128xbf16>, tensor<1x1536xbf16>) -> tensor<1x1536xbf16>
    %295 = ttir.empty() : tensor<1536x3072xbf16>
    %296 = "ttir.permute"(%27, %295) <{permutation = array<i64: 1, 0>}> : (tensor<3072x1536xbf16>, tensor<1536x3072xbf16>) -> tensor<1536x3072xbf16>
    %297 = "ttir.dot_general"(%294, %296) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x1536xbf16>, tensor<1536x3072xbf16>) -> tensor<1x3072xbf16>
    %298 = ttir.empty() : tensor<1x3072xbf16>
    %299 = "ttir.all_reduce"(%297, %298) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<1x3072xbf16>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %300 = ttir.empty() : tensor<1x1x3072xbf16>
    %301 = "ttir.reshape"(%299, %300) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %302 = ttir.empty() : tensor<1x1x3072xbf16>
    %303 = "ttir.add"(%72, %301, %302) : (tensor<1x1x3072xbf16>, tensor<1x1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %304 = ttir.empty() : tensor<3072xf32>
    %305 = "ttir.typecast"(%37, %304) <{conservative_folding = false}> : (tensor<3072xbf16>, tensor<3072xf32>) -> tensor<3072xf32>
    %306 = ttir.empty() : tensor<1x1x3072xf32>
    %307 = "ttir.reshape"(%305, %306) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %308 = ttir.empty() : tensor<1x1x3072xf32>
    %309 = "ttir.typecast"(%303, %308) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %310 = ttir.empty() : tensor<1x1x3072xf32>
    %311 = "ttir.pow"(%309, %50, %310) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %312 = ttir.empty() : tensor<1x1xf32>
    %313 = "ttir.sum"(%311, %312) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %314 = ttir.empty() : tensor<1x1xf32>
    %315 = "ttir.multiply"(%313, %45, %314) : (tensor<1x1xf32>, tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %316 = ttir.empty() : tensor<1x1x1xf32>
    %317 = "ttir.reshape"(%315, %316) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %318 = ttir.empty() : tensor<1x1x1xf32>
    %319 = "ttir.add"(%317, %84, %318) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %320 = ttir.empty() : tensor<1x1x1xf32>
    %321 = "ttir.rsqrt"(%319, %320) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %322 = ttir.empty() : tensor<1x1xf32>
    %323 = "ttir.reshape"(%321, %322) <{shape = [1 : i32, 1 : i32]}> : (tensor<1x1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %324 = ttir.empty() : tensor<1x1x1xf32>
    %325 = "ttir.reshape"(%323, %324) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %326 = ttir.empty() : tensor<1x1x3072xf32>
    %327 = "ttir.broadcast"(%325, %326) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x1x1xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %328 = ttir.empty() : tensor<1x1x3072xf32>
    %329 = "ttir.multiply"(%309, %327, %328) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %330 = ttir.empty() : tensor<1x1x3072xbf16>
    %331 = "ttir.typecast"(%329, %330) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %332 = ttir.empty() : tensor<1x1x3072xf32>
    %333 = "ttir.typecast"(%331, %332) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %334 = ttir.empty() : tensor<1x1x3072xf32>
    %335 = "ttir.multiply"(%307, %333, %334) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %336 = ttir.empty() : tensor<1x1x3072xbf16>
    %337 = "ttir.typecast"(%335, %336) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %338 = ttir.empty() : tensor<1x3072xbf16>
    %339 = "ttir.reshape"(%337, %338) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x3072xbf16>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %340 = ttir.empty() : tensor<3072x4096xbf16>
    %341 = "ttir.permute"(%39, %340) <{permutation = array<i64: 1, 0>}> : (tensor<4096x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<3072x4096xbf16>
    %342 = "ttir.dot_general"(%339, %341) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<1x4096xbf16>
    %343 = ttir.empty() : tensor<1x1x4096xbf16>
    %344 = "ttir.reshape"(%342, %343) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<1x4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
    %345 = ttir.empty() : tensor<1x1x4096xf32>
    %346 = "ttir.typecast"(%344, %345) <{conservative_folding = false}> : (tensor<1x1x4096xbf16>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %347 = ttir.empty() : tensor<1x1x4096xbf16>
    %348 = "ttir.sigmoid"(%344, %347) : (tensor<1x1x4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
    %349 = ttir.empty() : tensor<1x1x4096xf32>
    %350 = "ttir.typecast"(%348, %349) <{conservative_folding = false}> : (tensor<1x1x4096xbf16>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %351 = ttir.empty() : tensor<1x1x4096xf32>
    %352 = "ttir.multiply"(%346, %350, %351) : (tensor<1x1x4096xf32>, tensor<1x1x4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %353 = ttir.empty() : tensor<1x1x4096xbf16>
    %354 = "ttir.typecast"(%352, %353) <{conservative_folding = false}> : (tensor<1x1x4096xf32>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
    %355 = ttir.empty() : tensor<1x1x4096xf32>
    %356 = "ttir.typecast"(%354, %355) <{conservative_folding = false}> : (tensor<1x1x4096xbf16>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %357 = ttir.empty() : tensor<3072x4096xbf16>
    %358 = "ttir.permute"(%25, %357) <{permutation = array<i64: 1, 0>}> : (tensor<4096x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<3072x4096xbf16>
    %359 = "ttir.dot_general"(%339, %358) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<1x4096xbf16>
    %360 = ttir.empty() : tensor<1x4096xf32>
    %361 = "ttir.typecast"(%359, %360) <{conservative_folding = false}> : (tensor<1x4096xbf16>, tensor<1x4096xf32>) -> tensor<1x4096xf32>
    %362 = ttir.empty() : tensor<1x1x4096xf32>
    %363 = "ttir.reshape"(%361, %362) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<1x4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %364 = ttir.empty() : tensor<1x1x4096xf32>
    %365 = "ttir.multiply"(%356, %363, %364) : (tensor<1x1x4096xf32>, tensor<1x1x4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %366 = ttir.empty() : tensor<1x1x4096xbf16>
    %367 = "ttir.typecast"(%365, %366) <{conservative_folding = false}> : (tensor<1x1x4096xf32>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
    %368 = ttir.empty() : tensor<1x4096xbf16>
    %369 = "ttir.reshape"(%367, %368) <{shape = [1 : i32, 4096 : i32]}> : (tensor<1x1x4096xbf16>, tensor<1x4096xbf16>) -> tensor<1x4096xbf16>
    %370 = ttir.empty() : tensor<4096x3072xbf16>
    %371 = "ttir.permute"(%23, %370) <{permutation = array<i64: 1, 0>}> : (tensor<3072x4096xbf16>, tensor<4096x3072xbf16>) -> tensor<4096x3072xbf16>
    %372 = "ttir.dot_general"(%369, %371) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x4096xbf16>, tensor<4096x3072xbf16>) -> tensor<1x3072xbf16>
    %373 = ttir.empty() : tensor<1x3072xbf16>
    %374 = "ttir.all_reduce"(%372, %373) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<1x3072xbf16>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %375 = ttir.empty() : tensor<1x1x3072xbf16>
    %376 = "ttir.reshape"(%374, %375) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %377 = ttir.empty() : tensor<1x1x3072xbf16>
    %378 = "ttir.add"(%303, %376, %377) : (tensor<1x1x3072xbf16>, tensor<1x1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %379 = ttir.empty() : tensor<1x1x3072xf32>
    %380 = "ttir.typecast"(%378, %379) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %381 = ttir.empty() : tensor<1x1x3072xf32>
    %382 = "ttir.pow"(%380, %50, %381) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %383 = ttir.empty() : tensor<1x1xf32>
    %384 = "ttir.sum"(%382, %383) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %385 = ttir.empty() : tensor<1x1xf32>
    %386 = "ttir.multiply"(%384, %45, %385) : (tensor<1x1xf32>, tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %387 = ttir.empty() : tensor<1x1x1xf32>
    %388 = "ttir.reshape"(%386, %387) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %389 = ttir.empty() : tensor<1x1x1xf32>
    %390 = "ttir.add"(%388, %84, %389) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %391 = ttir.empty() : tensor<1x1x1xf32>
    %392 = "ttir.rsqrt"(%390, %391) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %393 = ttir.empty() : tensor<1x1xf32>
    %394 = "ttir.reshape"(%392, %393) <{shape = [1 : i32, 1 : i32]}> : (tensor<1x1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %395 = ttir.empty() : tensor<1x1x1xf32>
    %396 = "ttir.reshape"(%394, %395) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %397 = ttir.empty() : tensor<1x1x3072xf32>
    %398 = "ttir.broadcast"(%396, %397) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x1x1xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %399 = ttir.empty() : tensor<1x1x3072xf32>
    %400 = "ttir.multiply"(%380, %398, %399) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %401 = ttir.empty() : tensor<1x1x3072xbf16>
    %402 = "ttir.typecast"(%400, %401) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %403 = ttir.empty() : tensor<1x1x3072xf32>
    %404 = "ttir.typecast"(%402, %403) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %405 = ttir.empty() : tensor<1x1x3072xf32>
    %406 = "ttir.multiply"(%177, %404, %405) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %407 = ttir.empty() : tensor<1x1x3072xbf16>
    %408 = "ttir.typecast"(%406, %407) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %409 = ttir.empty() : tensor<1x3072xbf16>
    %410 = "ttir.reshape"(%408, %409) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x3072xbf16>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %411 = ttir.empty() : tensor<3072x128256xbf16>
    %412 = "ttir.permute"(%11, %411) <{permutation = array<i64: 1, 0>}> : (tensor<128256x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<3072x128256xbf16>
    %413 = "ttir.dot_general"(%410, %412) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<1x128256xbf16>
    %414 = ttir.empty() : tensor<1x1x128256xbf16>
    %415 = "ttir.reshape"(%413, %414) <{shape = [1 : i32, 1 : i32, 128256 : i32]}> : (tensor<1x128256xbf16>, tensor<1x1x128256xbf16>) -> tensor<1x1x128256xbf16>
    %416 = ttir.empty() : tensor<1x8x128x128xbf16>
    %417 = "ttir.mesh_shard"(%166, %416) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x4x128x128xbf16>, tensor<1x8x128x128xbf16>) -> tensor<1x8x128x128xbf16>
    %418 = ttir.empty() : tensor<1x8x128x128xbf16>
    %419 = "ttir.mesh_shard"(%173, %418) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x4x128x128xbf16>, tensor<1x8x128x128xbf16>) -> tensor<1x8x128x128xbf16>
    %420 = ttir.empty() : tensor<1x128256xbf16>
    %421 = "ttir.mesh_shard"(%413, %420) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x128256xbf16>, tensor<1x128256xbf16>) -> tensor<1x128256xbf16>
    %422 = ttir.empty() : tensor<1x1x128256xbf16>
    %423 = "ttir.mesh_shard"(%415, %422) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x1x128256xbf16>, tensor<1x1x128256xbf16>) -> tensor<1x1x128256xbf16>
    return %417, %419, %421, %423 : tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x128256xbf16>, tensor<1x1x128256xbf16>
  }
}
module @SyncTensorsGraph.370 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.370 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99776, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073184736, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99776, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073193056, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x16, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 2)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 2, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main_const_eval_0() -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %2 : tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main(%arg0: tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<128xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg7) <{force = false}> : (tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %2 = "ttnn.full"(%1) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 3.25520843E-4 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %3 = "ttnn.mesh_shard"(%arg0, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %4 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %5 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %6 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %7 = "ttnn.mesh_shard"(%arg4, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %8 = "ttnn.mesh_shard"(%arg5, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg5) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %9 = "ttnn.mesh_shard"(%arg6, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg6) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %10 = "ttnn.mesh_shard"(%arg8, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg8) <{force = false}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %11 = "ttnn.mesh_shard"(%arg9, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg9) <{force = false}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %12 = "ttnn.mesh_shard"(%arg10, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg10) <{force = false}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %13 = "ttnn.mesh_shard"(%arg11, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg11) <{force = false}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %14 = "ttnn.mesh_shard"(%arg12, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg12) <{force = false}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %15 = "ttnn.mesh_shard"(%arg13, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg13) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %16 = "ttnn.mesh_shard"(%arg14, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<128xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg14) <{force = false}> : (tensor<128xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %17 = "ttnn.mesh_shard"(%arg15, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg15) <{force = false}> : (tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %18 = "ttnn.mesh_shard"(%arg16, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg16) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %19 = "ttnn.mesh_shard"(%arg17, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg17) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %20 = "ttnn.mesh_shard"(%arg18, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg18) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %21 = "ttnn.mesh_shard"(%arg19, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg19) <{force = false}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %22 = "ttnn.mesh_shard"(%arg20, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg20) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %23 = "ttnn.typecast"(%9) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %24 = "ttnn.reshape"(%23) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%23) <{force = false}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %25 = "ttnn.typecast"(%7) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %26 = "ttnn.reshape"(%25) <{shape = [1 : i32]}> : (tensor<1x1xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%25) <{force = false}> : (tensor<1x1xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %27 = "ttnn.from_device"(%26) : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%26) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %28 = "ttnn.to_layout"(%27) <{layout = #ttnn.layout<row_major>}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%27) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> ()
        %29 = "ttnn.to_device"(%28, %1) <{memory_config = #ttnn.memory_config<<dram>, <interleaved>>}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%28) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<system_memory>>>>) -> ()
        %30 = "ttnn.embedding"(%29, %8) : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%29) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %31 = "ttnn.typecast"(%30) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %32 = "ttnn.reshape"(%31) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %33 = "ttnn.pow"(%32, %0) : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%32) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %34 = "ttnn.sum"(%33) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%33) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %35 = "ttnn.multiply"(%34, %2) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%34) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %36 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %37 = "ttnn.add"(%35, %36) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%36) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%35) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %38 = "ttnn.rsqrt"(%37) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%37) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %39 = "ttnn.multiply"(%31, %38) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%38) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%31) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %40 = "ttnn.multiply"(%24, %39) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%39) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%24) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %41 = "ttnn.typecast"(%40) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%40) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %42 = "ttnn.matmul"(%41, %5) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %43 = "ttnn.reshape"(%42) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32]}> : (tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%42) <{force = false}> : (tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %44 = "ttnn.typecast"(%43) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %45 = "ttnn.reshape"(%4) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %46 = "ttnn.typecast"(%3) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %47 = "ttnn.reshape"(%46) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%46) <{force = false}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %48 = "ttnn.matmul"(%45, %47) <{transpose_a = false, transpose_b = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%47) <{force = false}> : (tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%45) <{force = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %49 = "ttnn.reshape"(%48) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%48) <{force = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %50 = "ttnn.concat"(%49, %49) <{dim = 2 : si32}> : (tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%49) <{force = false}> : (tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %51 = "ttnn.cos"(%50) : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %52 = "ttnn.reshape"(%51) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %53 = "ttnn.multiply"(%44, %52) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%52) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%44) <{force = false}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %54 = "ttnn.typecast"(%53) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%53) <{force = false}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %55 = "ttnn.slice_static"(%43) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %56 = "ttnn.neg"(%55) : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%55) <{force = false}> : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %57 = "ttnn.slice_static"(%43) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%43) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %58 = "ttnn.concat"(%56, %57) <{dim = 3 : si32}> : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%57) <{force = false}> : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%56) <{force = false}> : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %59 = "ttnn.typecast"(%58) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%58) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %60 = "ttnn.sin"(%50) : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%50) <{force = false}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %61 = "ttnn.reshape"(%60) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %62 = "ttnn.multiply"(%59, %61) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%61) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%59) <{force = false}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %63 = "ttnn.typecast"(%62) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%62) <{force = false}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %64 = "ttnn.add"(%54, %63) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%63) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%54) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %65 = "ttnn.typecast"(%arg0) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.update_cache"(%10, %64, %65) <{batch_offset = 0 : i32}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%65) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%64) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %66 = "ttnn.matmul"(%41, %11) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %67 = "ttnn.reshape"(%66) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32]}> : (tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%66) <{force = false}> : (tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %68 = "ttnn.typecast"(%arg0) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.update_cache"(%12, %67, %68) <{batch_offset = 0 : i32}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%68) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%67) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %69 = "ttnn.typecast"(%22) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%22) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %70 = "ttnn.reshape"(%69) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%69) <{force = false}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %71 = "ttnn.matmul"(%41, %19) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%41) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %72 = "ttnn.reshape"(%71) <{shape = [1 : i32, 12 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %73 = "ttnn.typecast"(%71) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1536xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%71) <{force = false}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %74 = "ttnn.reshape"(%73) <{shape = [12 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1536xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%73) <{force = false}> : (tensor<1x1536xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %75 = "ttnn.multiply"(%74, %51) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%74) <{force = false}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%51) <{force = false}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %76 = "ttnn.typecast"(%75) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%75) <{force = false}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %77 = "ttnn.slice_static"(%72) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 12 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %78 = "ttnn.neg"(%77) : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%77) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %79 = "ttnn.reshape"(%78) <{shape = [12 : i32, 1 : i32, 64 : i32]}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%78) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %80 = "ttnn.slice_static"(%72) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 12 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%72) <{force = false}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %81 = "ttnn.reshape"(%80) <{shape = [12 : i32, 1 : i32, 64 : i32]}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%80) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %82 = "ttnn.concat"(%79, %81) <{dim = 2 : si32}> : (tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%81) <{force = false}> : (tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%79) <{force = false}> : (tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %83 = "ttnn.typecast"(%82) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%82) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %84 = "ttnn.multiply"(%83, %60) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%83) <{force = false}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%60) <{force = false}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %85 = "ttnn.typecast"(%84) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%84) <{force = false}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %86 = "ttnn.add"(%76, %85) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%85) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%76) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %87 = "ttnn.reshape"(%10) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 512 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %88 = "ttnn.repeat"(%87) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x4x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 512 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1536 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%87) <{force = false}> : (tensor<1x4x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 512 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %89 = "ttnn.reshape"(%88) <{shape = [1 : i32, 12 : i32, 128 : i32, 128 : i32]}> : (tensor<1x4x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1536 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%88) <{force = false}> : (tensor<1x4x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1536 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %90 = "ttnn.permute"(%89) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%89) <{force = false}> : (tensor<1x12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %91 = "ttnn.reshape"(%90) <{shape = [12 : i32, 128 : i32, 128 : i32]}> : (tensor<1x12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%90) <{force = false}> : (tensor<1x12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %92 = "ttnn.matmul"(%86, %91) <{transpose_a = false, transpose_b = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%91) <{force = false}> : (tensor<12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%86) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %93 = "ttnn.typecast"(%92) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%92) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %94 = "ttnn.reshape"(%93) <{shape = [1 : i32, 12 : i32, 1 : i32, 128 : i32]}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%93) <{force = false}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %95 = "ttnn.reshape"(%18) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %96 = "ttnn.multiply"(%94, %95) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%95) <{force = false}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%94) <{force = false}> : (tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %97 = "ttnn.typecast"(%96) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%96) <{force = false}> : (tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %98 = "ttnn.typecast"(%17) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %99 = "ttnn.reshape"(%98) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%98) <{force = false}> : (tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %100 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %101 = "ttnn.reshape"(%16) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<128xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %102 = "ttnn.typecast"(%101) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x128xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%101) <{force = false}> : (tensor<1x1x1x128xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %103 = "ttnn.typecast"(%100) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%100) <{force = false}> : (tensor<1x1x1x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %104 = "ttnn.gt"(%102, %103) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%103) <{force = false}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%102) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %105 = "ttnn.typecast"(%104) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%104) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %106 = "ttnn.typecast"(%105) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%105) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %107 = "ttnn.multiply"(%99, %106) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%106) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%99) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %108 = "ttnn.typecast"(%107) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%107) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %109 = "ttnn.add"(%97, %108) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%108) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%97) <{force = false}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %110 = "ttnn.typecast"(%109) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%109) <{force = false}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %111 = "ttnn.max"(%110) <{dim_arg = [3 : i32], keep_dim = true}> : (tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %112 = "ttnn.neg"(%111) : (tensor<1x12x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%111) <{force = false}> : (tensor<1x12x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %113 = "ttnn.add"(%110, %112) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x12x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%112) <{force = false}> : (tensor<1x12x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%110) <{force = false}> : (tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %114 = "ttnn.softmax"(%113) <{dimension = 3 : si32}> : (tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%113) <{force = false}> : (tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %115 = "ttnn.typecast"(%114) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%114) <{force = false}> : (tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %116 = "ttnn.reshape"(%115) <{shape = [12 : i32, 1 : i32, 128 : i32]}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%115) <{force = false}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %117 = "ttnn.reshape"(%12) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 512 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %118 = "ttnn.repeat"(%117) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x4x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 512 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1536 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%117) <{force = false}> : (tensor<1x4x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 512 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %119 = "ttnn.reshape"(%118) <{shape = [12 : i32, 128 : i32, 128 : i32]}> : (tensor<1x4x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1536 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%118) <{force = false}> : (tensor<1x4x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1536 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %120 = "ttnn.matmul"(%116, %119) <{transpose_a = false, transpose_b = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%119) <{force = false}> : (tensor<12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%116) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %121 = "ttnn.reshape"(%120) <{shape = [1 : i32, 1536 : i32]}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%120) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %122 = "ttnn.matmul"(%121, %15) <{transpose_a = false, transpose_b = true}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%121) <{force = false}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %123 = "ttnn.reshape"(%122) <{shape = [1 : i32, 1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%122) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %124 = "ttnn.reduce_scatter"(%123, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 3 : si32}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%123) <{force = false}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %125 = "ttnn.all_gather"(%124, %1) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%124) <{force = false}> : (tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %126 = "ttnn.reshape"(%125) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%125) <{force = false}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %127 = "ttnn.add"(%30, %126) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%126) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%30) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %128 = "ttnn.typecast"(%20) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %129 = "ttnn.reshape"(%128) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%128) <{force = false}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %130 = "ttnn.typecast"(%127) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %131 = "ttnn.reshape"(%130) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %132 = "ttnn.pow"(%131, %0) : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%131) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %133 = "ttnn.sum"(%132) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%132) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %134 = "ttnn.multiply"(%133, %2) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%133) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %135 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %136 = "ttnn.add"(%134, %135) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%135) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%134) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %137 = "ttnn.rsqrt"(%136) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%136) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %138 = "ttnn.multiply"(%130, %137) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%137) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%130) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %139 = "ttnn.multiply"(%129, %138) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%138) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%129) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %140 = "ttnn.typecast"(%139) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%139) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %141 = "ttnn.matmul"(%140, %21) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%21) <{force = false}> : (tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %142 = "ttnn.typecast"(%141) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %143 = "ttnn.sigmoid"(%141) : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%141) <{force = false}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %144 = "ttnn.typecast"(%143) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%143) <{force = false}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %145 = "ttnn.multiply"(%142, %144) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%144) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%142) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %146 = "ttnn.matmul"(%140, %14) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%140) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %147 = "ttnn.typecast"(%146) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%146) <{force = false}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %148 = "ttnn.multiply"(%145, %147) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%147) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%145) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %149 = "ttnn.typecast"(%148) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%148) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %150 = "ttnn.matmul"(%149, %13) <{transpose_a = false, transpose_b = true}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%149) <{force = false}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %151 = "ttnn.reshape"(%150) <{shape = [1 : i32, 1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%150) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %152 = "ttnn.reduce_scatter"(%151, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 3 : si32}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%151) <{force = false}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %153 = "ttnn.all_gather"(%152, %1) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%152) <{force = false}> : (tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %154 = "ttnn.reshape"(%153) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%153) <{force = false}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %155 = "ttnn.add"(%127, %154) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%154) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%127) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %156 = "ttnn.typecast"(%155) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%155) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %157 = "ttnn.reshape"(%156) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %158 = "ttnn.pow"(%157, %0) : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%157) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %159 = "ttnn.sum"(%158) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%158) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %160 = "ttnn.multiply"(%159, %2) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%159) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %161 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %162 = "ttnn.add"(%160, %161) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%161) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%160) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %163 = "ttnn.rsqrt"(%162) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%162) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %164 = "ttnn.multiply"(%156, %163) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%163) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%156) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %165 = "ttnn.multiply"(%70, %164) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%164) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%70) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %166 = "ttnn.typecast"(%165) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%165) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %167 = "ttnn.matmul"(%166, %8) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%166) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %168 = "ttnn.reshape"(%167) <{shape = [1 : i32, 1 : i32, 128256 : i32]}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %169 = "ttnn.to_layout"(%10) <{layout = #ttnn.layout<row_major>}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %170 = "ttnn.from_device"(%169) : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%169) <{force = false}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %171 = "ttnn.mesh_shard"(%170, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%170) <{force = false}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<system_memory>>>>) -> ()
        %172 = "ttnn.to_layout"(%12) <{layout = #ttnn.layout<row_major>}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %173 = "ttnn.from_device"(%172) : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%172) <{force = false}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %174 = "ttnn.mesh_shard"(%173, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%173) <{force = false}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<system_memory>>>>) -> ()
        %175 = "ttnn.to_layout"(%167) <{layout = #ttnn.layout<row_major>}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%167) <{force = false}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %176 = "ttnn.from_device"(%175) : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%175) <{force = false}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %177 = "ttnn.mesh_shard"(%176, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%176) <{force = false}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>) -> ()
        %178 = "ttnn.to_layout"(%168) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%168) <{force = false}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %179 = "ttnn.from_device"(%178) : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%178) <{force = false}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %180 = "ttnn.mesh_shard"(%179, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%179) <{force = false}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>) -> ()
        return %171, %174, %177, %180 : tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>, tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>, tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>, tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>
      }
    }
  }
}
input 0: device = xla:0, shape torch.Size([128]) and shard spec {replicated}
input 1: device = xla:0, shape torch.Size([1, 128]) and shard spec {replicated}
input 2: device = xla:0, shape torch.Size([1, 8, 128, 128]) and shard spec {replicated}
[STATIC CACHE DUMP ]mean along seqlen for static cache @ input idx 2 and shape torch.Size([1, 8, 128, 128]): tensor([ 0.0576,  0.1055,  0.0986, -0.0315, -0.0776, -0.2383, -0.0459,  0.0461,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
       dtype=torch.bfloat16)
input 3: device = xla:0, shape torch.Size([1, 8, 128, 128]) and shard spec {replicated}
[STATIC CACHE DUMP ]mean along seqlen for static cache @ input idx 3 and shape torch.Size([1, 8, 128, 128]): tensor([ 3.0899e-04, -1.0620e-02,  6.9580e-03,  1.8597e-05, -6.0120e-03,
         1.4648e-03,  2.5940e-03,  2.5787e-03,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=torch.bfloat16)
input 4: device = xla:0, shape torch.Size([1, 1]) and shard spec {replicated}
input 5: device = xla:0, shape torch.Size([128256, 3072]) and shard spec {replicated}
input 6: device = xla:0, shape torch.Size([1]) and shard spec {replicated}
input 7: device = xla:0, shape torch.Size([64]) and shard spec {replicated}
input 8: device = xla:0, shape torch.Size([3072]) and shard spec {replicated}
input 9: device = xla:0, shape torch.Size([3072, 3072]) and shard spec {devices=[2,1]<=[2]}
input 10: device = xla:0, shape torch.Size([1024, 3072]) and shard spec {devices=[2,1]<=[2]}
input 11: device = xla:0, shape torch.Size([1024, 3072]) and shard spec {devices=[2,1]<=[2]}
input 12: device = xla:0, shape torch.Size([3072, 3072]) and shard spec {devices=[1,2]<=[2]}
input 13: device = xla:0, shape torch.Size([3072]) and shard spec {replicated}
input 14: device = xla:0, shape torch.Size([8192, 3072]) and shard spec {devices=[2,1]<=[2]}
input 15: device = xla:0, shape torch.Size([8192, 3072]) and shard spec {devices=[2,1]<=[2]}
input 16: device = xla:0, shape torch.Size([3072, 8192]) and shard spec {devices=[1,2]<=[2]}
input 17: device = xla:0, shape torch.Size([3072]) and shard spec {replicated}
CPU logit shape:  torch.Size([1, 7, 128256]) XLA logit shape:  torch.Size([1, 1, 128256])
 the[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
module @SyncTensorsGraph.370 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<1x!vhlo.i64_v1>, %arg1: !vhlo.tensor_v1<64x!vhlo.f32_v1>, %arg2: !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg4: !vhlo.tensor_v1<1x1x!vhlo.i64_v1>, %arg5: !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, %arg6: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg7: !vhlo.tensor_v1<!vhlo.i64_v1>, %arg8: !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, %arg9: !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>, %arg10: !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, %arg11: !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>, %arg13: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg14: !vhlo.tensor_v1<128x!vhlo.i64_v1>, %arg15: !vhlo.tensor_v1<1x128x!vhlo.bf16_v1>, %arg16: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg17: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg18: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg19: !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>, %arg20: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128256x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x128256x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : () -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF800000> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %3 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<3.25520843E-4> : tensor<1x1xf32>>}> : () -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %4 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %5 = "vhlo.broadcast_in_dim_v1"(%4) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %6 = "vhlo.compare_v1"(%arg0, %0) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 LT>}> : (!vhlo.tensor_v1<1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.bool_v1>
    %7 = "vhlo.reshape_v1"(%arg7) : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %8 = "vhlo.add_v1"(%arg0, %7) : (!vhlo.tensor_v1<1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %9 = "vhlo.select_v1"(%6, %8, %arg0) : (!vhlo.tensor_v1<1x!vhlo.bool_v1>, !vhlo.tensor_v1<1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %10 = "vhlo.reshape_v1"(%9) : (!vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.i64_v1>
    %11 = "vhlo.convert_v1"(%arg6) {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}]>]>">}>, mhlo.sharding = #vhlo.string_v1<"{replicated}">} : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %12 = "vhlo.reshape_v1"(%11) : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %13 = "vhlo.reshape_v1"(%arg4) : (!vhlo.tensor_v1<1x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %14 = "vhlo.convert_v1"(%13) : (!vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.ui32_v1>
    %15 = "vhlo.gather_v2"(%arg5, %14) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 3072]> : tensor<2xi64>>, start_index_map = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x!vhlo.ui32_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %16 = "vhlo.reshape_v1"(%15) : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %17 = "vhlo.convert_v1"(%16) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %18 = "vhlo.power_v1"(%17, %5) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %19 = "vhlo.reduce_v1"(%18, %1) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %184 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%184) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %20 = "vhlo.multiply_v1"(%19, %3) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %21 = "vhlo.reshape_v1"(%20) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %22 = "vhlo.reshape_v1"(%arg3) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %23 = "vhlo.add_v1"(%21, %22) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %24 = "vhlo.rsqrt_v2"(%23) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %25 = "vhlo.reshape_v1"(%24) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %26 = "vhlo.broadcast_in_dim_v1"(%25) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %27 = "vhlo.multiply_v1"(%17, %26) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %28 = "vhlo.convert_v1"(%27) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %29 = "vhlo.convert_v1"(%28) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %30 = "vhlo.multiply_v1"(%12, %29) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %31 = "vhlo.convert_v1"(%30) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %32 = "vhlo.reshape_v1"(%31) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %33 = "vhlo.transpose_v1"(%arg2) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,1024]{0,1}">} : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>
    %34 = "vhlo.dot_general_v2"(%32, %33) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>
    %35 = "vhlo.reshape_v1"(%34) : (!vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %36 = "vhlo.convert_v1"(%35) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,8,1,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %37 = "vhlo.reshape_v1"(%arg1) : (!vhlo.tensor_v1<64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>
    %38 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.i64_v1>
    %39 = "vhlo.convert_v1"(%38) : (!vhlo.tensor_v1<1x1x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %40 = "vhlo.dot_general_v2"(%37, %39) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>
    %41 = "vhlo.reshape_v1"(%40) : (!vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>
    %42 = "vhlo.concatenate_v1"(%41, %41) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>
    %43 = "vhlo.cosine_v2"(%42) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>
    %44 = "vhlo.convert_v1"(%43) : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>
    %45 = "vhlo.reshape_v1"(%44) : (!vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1x128x!vhlo.bf16_v1>
    %46 = "vhlo.convert_v1"(%45) : (!vhlo.tensor_v1<1x1x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1x128x!vhlo.f32_v1>
    %47 = "vhlo.reshape_v1"(%46) : (!vhlo.tensor_v1<1x1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>
    %48 = "vhlo.broadcast_in_dim_v1"(%47) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %49 = "vhlo.multiply_v1"(%36, %48) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %50 = "vhlo.convert_v1"(%49) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %51 = "vhlo.slice_v1"(%35) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 1, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>
    %52 = "vhlo.negate_v1"(%51) : (!vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>
    %53 = "vhlo.slice_v1"(%35) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 1, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>
    %54 = "vhlo.concatenate_v1"(%52, %53) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %55 = "vhlo.convert_v1"(%54) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %56 = "vhlo.sine_v2"(%42) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>
    %57 = "vhlo.convert_v1"(%56) : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>
    %58 = "vhlo.reshape_v1"(%57) : (!vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1x128x!vhlo.bf16_v1>
    %59 = "vhlo.convert_v1"(%58) : (!vhlo.tensor_v1<1x1x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1x128x!vhlo.f32_v1>
    %60 = "vhlo.reshape_v1"(%59) : (!vhlo.tensor_v1<1x1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>
    %61 = "vhlo.broadcast_in_dim_v1"(%60) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %62 = "vhlo.multiply_v1"(%55, %61) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %63 = "vhlo.convert_v1"(%62) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %64 = "vhlo.add_v1"(%50, %63) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %65 = "vhlo.scatter_v2"(%arg8, %10, %64) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg22: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg22) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>
    %66 = "vhlo.custom_call_v1"(%65) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"Sharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>">}>, mhlo.sharding = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">} : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>
    %67 = "vhlo.transpose_v1"(%arg9) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,1024]{0,1}">} : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>
    %68 = "vhlo.dot_general_v2"(%32, %67) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>
    %69 = "vhlo.reshape_v1"(%68) : (!vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %70 = "vhlo.scatter_v2"(%arg10, %10, %69) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg22: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg22) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>
    %71 = "vhlo.custom_call_v1"(%70) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"Sharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}, {}, {}]>]>">}>, mhlo.sharding = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">} : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>
    %72 = "vhlo.convert_v1"(%arg20) {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}]>]>">}>, mhlo.sharding = #vhlo.string_v1<"{replicated}">} : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %73 = "vhlo.reshape_v1"(%72) : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %74 = "vhlo.transpose_v1"(%arg17) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,3072]{0,1}">} : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %75 = "vhlo.dot_general_v2"(%32, %74) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %76 = "vhlo.reshape_v1"(%75) : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %77 = "vhlo.convert_v1"(%76) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,24,1,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %78 = "vhlo.broadcast_in_dim_v1"(%47) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %79 = "vhlo.multiply_v1"(%77, %78) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %80 = "vhlo.convert_v1"(%79) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %81 = "vhlo.slice_v1"(%76) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 1, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>
    %82 = "vhlo.negate_v1"(%81) : (!vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>
    %83 = "vhlo.slice_v1"(%76) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 1, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>
    %84 = "vhlo.concatenate_v1"(%82, %83) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %85 = "vhlo.convert_v1"(%84) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %86 = "vhlo.broadcast_in_dim_v1"(%60) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %87 = "vhlo.multiply_v1"(%85, %86) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %88 = "vhlo.convert_v1"(%87) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %89 = "vhlo.add_v1"(%80, %88) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %90 = "vhlo.reshape_v1"(%89) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>
    %91 = "vhlo.broadcast_in_dim_v1"(%65) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>
    %92 = "vhlo.reshape_v1"(%91) : (!vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>
    %93 = "vhlo.transpose_v1"(%92) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 3, 1, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,24,128,128]{2,3,1,0}">} : (!vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>
    %94 = "vhlo.reshape_v1"(%93) : (!vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>
    %95 = "vhlo.dot_general_v2"(%90, %94) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>
    %96 = "vhlo.reshape_v1"(%95) : (!vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %97 = "vhlo.convert_v1"(%96) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %98 = "vhlo.broadcast_in_dim_v1"(%arg16) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %99 = "vhlo.multiply_v1"(%97, %98) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %100 = "vhlo.convert_v1"(%99) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %101 = "vhlo.convert_v1"(%arg15) {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>">}>, mhlo.sharding = #vhlo.string_v1<"{replicated}">} : (!vhlo.tensor_v1<1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128x!vhlo.f32_v1>
    %102 = "vhlo.reshape_v1"(%arg14) : (!vhlo.tensor_v1<128x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x128x!vhlo.i64_v1>
    %103 = "vhlo.broadcast_in_dim_v1"(%arg0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x128x!vhlo.i64_v1>
    %104 = "vhlo.compare_v1"(%102, %103) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<1x128x!vhlo.i64_v1>, !vhlo.tensor_v1<1x128x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x128x!vhlo.bool_v1>
    %105 = "vhlo.convert_v1"(%104) : (!vhlo.tensor_v1<1x128x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x128x!vhlo.f32_v1>
    %106 = "vhlo.multiply_v1"(%101, %105) : (!vhlo.tensor_v1<1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x128x!vhlo.f32_v1>
    %107 = "vhlo.convert_v1"(%106) : (!vhlo.tensor_v1<1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x128x!vhlo.bf16_v1>
    %108 = "vhlo.reshape_v1"(%107) : (!vhlo.tensor_v1<1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>
    %109 = "vhlo.broadcast_in_dim_v1"(%108) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %110 = "vhlo.add_v1"(%100, %109) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %111 = "vhlo.convert_v1"(%110) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %112 = "vhlo.reduce_v1"(%111, %2) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %184 = "vhlo.maximum_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%184) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x!vhlo.f32_v1>
    %113 = "vhlo.broadcast_in_dim_v1"(%112) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %114 = "vhlo.subtract_v1"(%111, %113) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %115 = "vhlo.exponential_v2"(%114) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %116 = "vhlo.reduce_v1"(%115, %1) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %184 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%184) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x!vhlo.f32_v1>
    %117 = "vhlo.broadcast_in_dim_v1"(%116) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %118 = "vhlo.divide_v1"(%115, %117) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %119 = "vhlo.convert_v1"(%118) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %120 = "vhlo.reshape_v1"(%119) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>
    %121 = "vhlo.broadcast_in_dim_v1"(%70) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>
    %122 = "vhlo.reshape_v1"(%121) : (!vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>
    %123 = "vhlo.dot_general_v2"(%120, %122) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>
    %124 = "vhlo.reshape_v1"(%123) : (!vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %125 = "vhlo.transpose_v1"(%arg13) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,3072]{0,1}">} : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %126 = "vhlo.dot_general_v2"(%124, %125) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %127 = "vhlo.reshape_v1"(%126) : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %128 = "vhlo.add_v1"(%16, %127) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %129 = "vhlo.convert_v1"(%arg18) {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}]>]>">}>, mhlo.sharding = #vhlo.string_v1<"{replicated}">} : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %130 = "vhlo.reshape_v1"(%129) : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %131 = "vhlo.convert_v1"(%128) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %132 = "vhlo.power_v1"(%131, %5) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %133 = "vhlo.reduce_v1"(%132, %1) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %184 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%184) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %134 = "vhlo.multiply_v1"(%133, %3) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %135 = "vhlo.reshape_v1"(%134) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %136 = "vhlo.add_v1"(%135, %22) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %137 = "vhlo.rsqrt_v2"(%136) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %138 = "vhlo.reshape_v1"(%137) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %139 = "vhlo.broadcast_in_dim_v1"(%138) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %140 = "vhlo.multiply_v1"(%131, %139) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %141 = "vhlo.convert_v1"(%140) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %142 = "vhlo.convert_v1"(%141) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %143 = "vhlo.multiply_v1"(%130, %142) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %144 = "vhlo.convert_v1"(%143) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %145 = "vhlo.reshape_v1"(%144) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %146 = "vhlo.transpose_v1"(%arg19) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,8192]{0,1}">} : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>
    %147 = "vhlo.dot_general_v2"(%145, %146) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>
    %148 = "vhlo.reshape_v1"(%147) : (!vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %149 = "vhlo.convert_v1"(%148) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %150 = "vhlo.logistic_v2"(%148) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %151 = "vhlo.convert_v1"(%150) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %152 = "vhlo.multiply_v1"(%149, %151) : (!vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %153 = "vhlo.convert_v1"(%152) : (!vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %154 = "vhlo.convert_v1"(%153) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %155 = "vhlo.transpose_v1"(%arg12) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,8192]{0,1}">} : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>
    %156 = "vhlo.dot_general_v2"(%145, %155) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>
    %157 = "vhlo.reshape_v1"(%156) : (!vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %158 = "vhlo.convert_v1"(%157) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %159 = "vhlo.multiply_v1"(%154, %158) : (!vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %160 = "vhlo.convert_v1"(%159) : (!vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %161 = "vhlo.reshape_v1"(%160) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>
    %162 = "vhlo.transpose_v1"(%arg11) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,3072]{0,1}">} : (!vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>
    %163 = "vhlo.dot_general_v2"(%161, %162) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %164 = "vhlo.reshape_v1"(%163) : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %165 = "vhlo.add_v1"(%128, %164) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %166 = "vhlo.convert_v1"(%165) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %167 = "vhlo.power_v1"(%166, %5) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %168 = "vhlo.reduce_v1"(%167, %1) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %184 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%184) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %169 = "vhlo.multiply_v1"(%168, %3) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %170 = "vhlo.reshape_v1"(%169) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %171 = "vhlo.add_v1"(%170, %22) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %172 = "vhlo.rsqrt_v2"(%171) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %173 = "vhlo.reshape_v1"(%172) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %174 = "vhlo.broadcast_in_dim_v1"(%173) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %175 = "vhlo.multiply_v1"(%166, %174) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %176 = "vhlo.convert_v1"(%175) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %177 = "vhlo.convert_v1"(%176) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %178 = "vhlo.multiply_v1"(%73, %177) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %179 = "vhlo.convert_v1"(%178) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %180 = "vhlo.reshape_v1"(%179) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %181 = "vhlo.transpose_v1"(%arg5) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,128256]{0,1}">} : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x128256x!vhlo.bf16_v1>
    %182 = "vhlo.dot_general_v2"(%180, %181) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x!vhlo.bf16_v1>
    %183 = "vhlo.reshape_v1"(%182) : (!vhlo.tensor_v1<1x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x128256x!vhlo.bf16_v1>
    "vhlo.return_v1"(%66, %71, %182, %183) : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128256x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x128256x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}
module @SyncTensorsGraph.370 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<1xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg1: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg2: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg3: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg4: tensor<1x1xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg5: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg7: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg8: tensor<1x8x128x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}, %arg9: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg10: tensor<1x8x128x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}, %arg11: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>}, %arg12: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg13: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>}, %arg14: tensor<128xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg15: tensor<1x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg16: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}) -> (tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x128256xbf16>, tensor<1x1x128256xbf16>) {
    %c = stablehlo.constant dense<0> : tensor<1xi64>
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_1 = stablehlo.constant dense<3.25520843E-4> : tensor<1x1xf32>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %0 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x1x3072xf32>
    %1 = stablehlo.compare  LT, %arg0, %c : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
    %2 = stablehlo.reshape %arg7 : (tensor<i64>) -> tensor<1xi64>
    %3 = stablehlo.add %arg0, %2 : tensor<1xi64>
    %4 = stablehlo.select %1, %3, %arg0 : tensor<1xi1>, tensor<1xi64>
    %5 = stablehlo.reshape %4 : (tensor<1xi64>) -> tensor<1x1xi64>
    %6 = stablehlo.convert %arg6 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %7 = stablehlo.reshape %6 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
    %8 = stablehlo.convert %arg4 : (tensor<1x1xi64>) -> tensor<1x1xui32>
    %9 = stablehlo.reshape %8 : (tensor<1x1xui32>) -> tensor<1xui32>
    %10 = "stablehlo.gather"(%arg5, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<1xui32>) -> tensor<1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %13 = stablehlo.power %12, %0 : tensor<1x1x3072xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %15 = stablehlo.multiply %14, %cst_1 : tensor<1x1xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %17 = stablehlo.reshape %arg3 : (tensor<f32>) -> tensor<1x1x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x1x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x1x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x1x3072xf32>
    %23 = stablehlo.convert %22 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %24 = stablehlo.convert %23 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %25 = stablehlo.multiply %7, %24 : tensor<1x1x3072xf32>
    %26 = stablehlo.convert %25 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %27 = stablehlo.reshape %26 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %28 = stablehlo.transpose %arg2, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %29 = stablehlo.dot_general %27, %28, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %30 = stablehlo.reshape %29 : (tensor<1x1024xbf16>) -> tensor<1x8x1x128xbf16>
    %31 = stablehlo.convert %30 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,1,128]{3,1,2,0}"} : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xf32>
    %32 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x64x1xf32>
    %33 = stablehlo.convert %arg0 : (tensor<1xi64>) -> tensor<1xf32>
    %34 = stablehlo.reshape %33 : (tensor<1xf32>) -> tensor<1x1x1xf32>
    %35 = stablehlo.dot_general %32, %34, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x1xf32>) -> tensor<1x64x1xf32>
    %36 = stablehlo.reshape %35 : (tensor<1x64x1xf32>) -> tensor<1x1x64xf32>
    %37 = stablehlo.concatenate %36, %36, dim = 2 : (tensor<1x1x64xf32>, tensor<1x1x64xf32>) -> tensor<1x1x128xf32>
    %38 = stablehlo.cosine %37 : tensor<1x1x128xf32>
    %39 = stablehlo.convert %38 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
    %40 = stablehlo.convert %39 : (tensor<1x1x128xbf16>) -> tensor<1x1x128xf32>
    %41 = stablehlo.broadcast_in_dim %40, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x8x1x128xf32>
    %42 = stablehlo.multiply %31, %41 : tensor<1x8x1x128xf32>
    %43 = stablehlo.convert %42 : (tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xbf16>
    %44 = stablehlo.slice %30 [0:1, 0:8, 0:1, 64:128] : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x64xbf16>
    %45 = stablehlo.negate %44 : tensor<1x8x1x64xbf16>
    %46 = stablehlo.slice %30 [0:1, 0:8, 0:1, 0:64] : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x64xbf16>
    %47 = stablehlo.concatenate %45, %46, dim = 3 : (tensor<1x8x1x64xbf16>, tensor<1x8x1x64xbf16>) -> tensor<1x8x1x128xbf16>
    %48 = stablehlo.convert %47 : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xf32>
    %49 = stablehlo.sine %37 : tensor<1x1x128xf32>
    %50 = stablehlo.convert %49 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
    %51 = stablehlo.convert %50 : (tensor<1x1x128xbf16>) -> tensor<1x1x128xf32>
    %52 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x8x1x128xf32>
    %53 = stablehlo.multiply %48, %52 : tensor<1x8x1x128xf32>
    %54 = stablehlo.convert %53 : (tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xbf16>
    %55 = stablehlo.add %43, %54 : tensor<1x8x1x128xbf16>
    %56 = "stablehlo.scatter"(%arg8, %5, %55) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>) -> tensor<1x8x128x128xbf16>
    %57 = sdy.sharding_constraint %56 <@mesh, [{}, {"_axis_0"}, {}, {}]> : tensor<1x8x128x128xbf16>
    %58 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %59 = stablehlo.dot_general %27, %58, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %60 = stablehlo.reshape %59 : (tensor<1x1024xbf16>) -> tensor<1x8x1x128xbf16>
    %61 = "stablehlo.scatter"(%arg10, %5, %60) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>) -> tensor<1x8x128x128xbf16>
    %62 = sdy.sharding_constraint %61 <@mesh, [{}, {"_axis_0"}, {}, {}]> : tensor<1x8x128x128xbf16>
    %63 = stablehlo.convert %arg20 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %64 = stablehlo.reshape %63 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
    %65 = stablehlo.transpose %arg17, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %66 = stablehlo.dot_general %27, %65, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %67 = stablehlo.reshape %66 : (tensor<1x3072xbf16>) -> tensor<1x24x1x128xbf16>
    %68 = stablehlo.convert %67 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,1,128]{3,1,2,0}"} : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %69 = stablehlo.broadcast_in_dim %40, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x24x1x128xf32>
    %70 = stablehlo.multiply %68, %69 : tensor<1x24x1x128xf32>
    %71 = stablehlo.convert %70 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %72 = stablehlo.slice %67 [0:1, 0:24, 0:1, 64:128] : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x64xbf16>
    %73 = stablehlo.negate %72 : tensor<1x24x1x64xbf16>
    %74 = stablehlo.slice %67 [0:1, 0:24, 0:1, 0:64] : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x64xbf16>
    %75 = stablehlo.concatenate %73, %74, dim = 3 : (tensor<1x24x1x64xbf16>, tensor<1x24x1x64xbf16>) -> tensor<1x24x1x128xbf16>
    %76 = stablehlo.convert %75 : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %77 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x24x1x128xf32>
    %78 = stablehlo.multiply %76, %77 : tensor<1x24x1x128xf32>
    %79 = stablehlo.convert %78 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %80 = stablehlo.add %71, %79 : tensor<1x24x1x128xbf16>
    %81 = stablehlo.reshape %80 : (tensor<1x24x1x128xbf16>) -> tensor<24x1x128xbf16>
    %82 = stablehlo.broadcast_in_dim %56, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x8x3x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %84 = stablehlo.transpose %83, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,128]{2,3,1,0}"} : (tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %85 = stablehlo.reshape %84 : (tensor<1x24x128x128xbf16>) -> tensor<24x128x128xbf16>
    %86 = stablehlo.dot_general %81, %85, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x1x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x1x128xbf16>
    %87 = stablehlo.convert %86 : (tensor<24x1x128xbf16>) -> tensor<24x1x128xf32>
    %88 = stablehlo.reshape %87 : (tensor<24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %89 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<1x24x1x128xf32>
    %90 = stablehlo.multiply %88, %89 : tensor<1x24x1x128xf32>
    %91 = stablehlo.convert %90 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %92 = stablehlo.convert %arg15 : (tensor<1x128xbf16>) -> tensor<1x128xf32>
    %93 = stablehlo.reshape %arg14 : (tensor<128xi64>) -> tensor<1x128xi64>
    %94 = stablehlo.broadcast_in_dim %arg0, dims = [0] : (tensor<1xi64>) -> tensor<1x128xi64>
    %95 = stablehlo.compare  GT, %93, %94 : (tensor<1x128xi64>, tensor<1x128xi64>) -> tensor<1x128xi1>
    %96 = stablehlo.convert %95 : (tensor<1x128xi1>) -> tensor<1x128xf32>
    %97 = stablehlo.multiply %92, %96 : tensor<1x128xf32>
    %98 = stablehlo.convert %97 : (tensor<1x128xf32>) -> tensor<1x128xbf16>
    %99 = stablehlo.reshape %98 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16>
    %100 = stablehlo.broadcast_in_dim %99, dims = [0, 2, 3] : (tensor<1x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %101 = stablehlo.add %91, %100 : tensor<1x24x1x128xbf16>
    %102 = stablehlo.convert %101 : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %103 = stablehlo.reduce(%102 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x1x128xf32>, tensor<f32>) -> tensor<1x24x1xf32>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0, 1, 2] : (tensor<1x24x1xf32>) -> tensor<1x24x1x128xf32>
    %105 = stablehlo.subtract %102, %104 : tensor<1x24x1x128xf32>
    %106 = stablehlo.exponential %105 : tensor<1x24x1x128xf32>
    %107 = stablehlo.reduce(%106 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x1x128xf32>, tensor<f32>) -> tensor<1x24x1xf32>
    %108 = stablehlo.broadcast_in_dim %107, dims = [0, 1, 2] : (tensor<1x24x1xf32>) -> tensor<1x24x1x128xf32>
    %109 = stablehlo.divide %106, %108 : tensor<1x24x1x128xf32>
    %110 = stablehlo.convert %109 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %111 = stablehlo.reshape %110 : (tensor<1x24x1x128xbf16>) -> tensor<24x1x128xbf16>
    %112 = stablehlo.broadcast_in_dim %61, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %113 = stablehlo.reshape %112 : (tensor<1x8x3x128x128xbf16>) -> tensor<24x128x128xbf16>
    %114 = stablehlo.dot_general %111, %113, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x1x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x1x128xbf16>
    %115 = stablehlo.reshape %114 : (tensor<24x1x128xbf16>) -> tensor<1x3072xbf16>
    %116 = stablehlo.transpose %arg13, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %117 = stablehlo.dot_general %115, %116, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %118 = stablehlo.reshape %117 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %119 = stablehlo.add %11, %118 : tensor<1x1x3072xbf16>
    %120 = stablehlo.convert %arg18 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %121 = stablehlo.reshape %120 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
    %122 = stablehlo.convert %119 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %123 = stablehlo.power %122, %0 : tensor<1x1x3072xf32>
    %124 = stablehlo.reduce(%123 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %125 = stablehlo.multiply %124, %cst_1 : tensor<1x1xf32>
    %126 = stablehlo.reshape %125 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %127 = stablehlo.add %126, %17 : tensor<1x1x1xf32>
    %128 = stablehlo.rsqrt %127 : tensor<1x1x1xf32>
    %129 = stablehlo.reshape %128 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %130 = stablehlo.broadcast_in_dim %129, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %131 = stablehlo.multiply %122, %130 : tensor<1x1x3072xf32>
    %132 = stablehlo.convert %131 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %133 = stablehlo.convert %132 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %134 = stablehlo.multiply %121, %133 : tensor<1x1x3072xf32>
    %135 = stablehlo.convert %134 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %136 = stablehlo.reshape %135 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %137 = stablehlo.transpose %arg19, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %138 = stablehlo.dot_general %136, %137, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %139 = stablehlo.reshape %138 : (tensor<1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %140 = stablehlo.convert %139 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %141 = stablehlo.logistic %139 : tensor<1x1x8192xbf16>
    %142 = stablehlo.convert %141 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %143 = stablehlo.multiply %140, %142 : tensor<1x1x8192xf32>
    %144 = stablehlo.convert %143 : (tensor<1x1x8192xf32>) -> tensor<1x1x8192xbf16>
    %145 = stablehlo.convert %144 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %146 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %147 = stablehlo.dot_general %136, %146, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %148 = stablehlo.convert %147 : (tensor<1x8192xbf16>) -> tensor<1x8192xf32>
    %149 = stablehlo.reshape %148 : (tensor<1x8192xf32>) -> tensor<1x1x8192xf32>
    %150 = stablehlo.multiply %145, %149 : tensor<1x1x8192xf32>
    %151 = stablehlo.convert %150 : (tensor<1x1x8192xf32>) -> tensor<1x1x8192xbf16>
    %152 = stablehlo.reshape %151 : (tensor<1x1x8192xbf16>) -> tensor<1x8192xbf16>
    %153 = stablehlo.transpose %arg11, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %154 = stablehlo.dot_general %152, %153, contracting_dims = [1] x [0] : (tensor<1x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<1x3072xbf16>
    %155 = stablehlo.reshape %154 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %156 = stablehlo.add %119, %155 : tensor<1x1x3072xbf16>
    %157 = stablehlo.convert %156 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %158 = stablehlo.power %157, %0 : tensor<1x1x3072xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %160 = stablehlo.multiply %159, %cst_1 : tensor<1x1xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %162 = stablehlo.add %161, %17 : tensor<1x1x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x1x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x1x3072xf32>
    %167 = stablehlo.convert %166 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %168 = stablehlo.convert %167 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %169 = stablehlo.multiply %64, %168 : tensor<1x1x3072xf32>
    %170 = stablehlo.convert %169 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %172 = stablehlo.transpose %arg5, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %173 = stablehlo.dot_general %171, %172, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<1x128256xbf16>
    %174 = stablehlo.reshape %173 : (tensor<1x128256xbf16>) -> tensor<1x1x128256xbf16>
    return %57, %62, %173, %174 : tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x128256xbf16>, tensor<1x1x128256xbf16>
  }
}
module @SyncTensorsGraph.370 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<1xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg1: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg2: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg3: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg4: tensor<1x1xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg5: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg7: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg8: tensor<1x8x128x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}, %arg9: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg10: tensor<1x8x128x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}, %arg11: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>}, %arg12: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg13: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>}, %arg14: tensor<128xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg15: tensor<1x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg16: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}) -> (tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x128256xbf16>, tensor<1x1x128256xbf16>) {
    %c = stablehlo.constant dense<0> : tensor<1xi64>
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_1 = stablehlo.constant dense<3.25520843E-4> : tensor<1x1xf32>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %0 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x1x3072xf32>
    %1 = stablehlo.compare  LT, %arg0, %c : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
    %2 = stablehlo.reshape %arg7 : (tensor<i64>) -> tensor<1xi64>
    %3 = stablehlo.add %arg0, %2 : tensor<1xi64>
    %4 = stablehlo.select %1, %3, %arg0 : tensor<1xi1>, tensor<1xi64>
    %5 = stablehlo.reshape %4 : (tensor<1xi64>) -> tensor<1x1xi64>
    %6 = stablehlo.convert %arg6 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %7 = stablehlo.reshape %6 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
    %8 = stablehlo.convert %arg4 : (tensor<1x1xi64>) -> tensor<1x1xui32>
    %9 = stablehlo.reshape %8 : (tensor<1x1xui32>) -> tensor<1xui32>
    %10 = "stablehlo.gather"(%arg5, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<1xui32>) -> tensor<1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %13 = stablehlo.power %12, %0 : tensor<1x1x3072xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %15 = stablehlo.multiply %14, %cst_1 : tensor<1x1xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %17 = stablehlo.reshape %arg3 : (tensor<f32>) -> tensor<1x1x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x1x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x1x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x1x3072xf32>
    %23 = stablehlo.convert %22 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %24 = stablehlo.convert %23 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %25 = stablehlo.multiply %7, %24 : tensor<1x1x3072xf32>
    %26 = stablehlo.convert %25 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %27 = stablehlo.reshape %26 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %28 = stablehlo.transpose %arg2, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %29 = stablehlo.dot_general %27, %28, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %30 = stablehlo.reshape %29 : (tensor<1x1024xbf16>) -> tensor<1x8x1x128xbf16>
    %31 = stablehlo.convert %30 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,1,128]{3,1,2,0}"} : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xf32>
    %32 = stablehlo.reshape %arg1 : (tensor<64xf32>) -> tensor<1x64x1xf32>
    %33 = stablehlo.convert %arg0 : (tensor<1xi64>) -> tensor<1xf32>
    %34 = stablehlo.reshape %33 : (tensor<1xf32>) -> tensor<1x1x1xf32>
    %35 = stablehlo.dot_general %32, %34, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x1xf32>) -> tensor<1x64x1xf32>
    %36 = stablehlo.reshape %35 : (tensor<1x64x1xf32>) -> tensor<1x1x64xf32>
    %37 = stablehlo.concatenate %36, %36, dim = 2 : (tensor<1x1x64xf32>, tensor<1x1x64xf32>) -> tensor<1x1x128xf32>
    %38 = stablehlo.cosine %37 : tensor<1x1x128xf32>
    %39 = stablehlo.convert %38 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
    %40 = stablehlo.convert %39 : (tensor<1x1x128xbf16>) -> tensor<1x1x128xf32>
    %41 = stablehlo.broadcast_in_dim %40, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x8x1x128xf32>
    %42 = stablehlo.multiply %31, %41 : tensor<1x8x1x128xf32>
    %43 = stablehlo.convert %42 : (tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xbf16>
    %44 = stablehlo.slice %30 [0:1, 0:8, 0:1, 64:128] : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x64xbf16>
    %45 = stablehlo.negate %44 : tensor<1x8x1x64xbf16>
    %46 = stablehlo.slice %30 [0:1, 0:8, 0:1, 0:64] : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x64xbf16>
    %47 = stablehlo.concatenate %45, %46, dim = 3 : (tensor<1x8x1x64xbf16>, tensor<1x8x1x64xbf16>) -> tensor<1x8x1x128xbf16>
    %48 = stablehlo.convert %47 : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xf32>
    %49 = stablehlo.sine %37 : tensor<1x1x128xf32>
    %50 = stablehlo.convert %49 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
    %51 = stablehlo.convert %50 : (tensor<1x1x128xbf16>) -> tensor<1x1x128xf32>
    %52 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x8x1x128xf32>
    %53 = stablehlo.multiply %48, %52 : tensor<1x8x1x128xf32>
    %54 = stablehlo.convert %53 : (tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xbf16>
    %55 = stablehlo.add %43, %54 : tensor<1x8x1x128xbf16>
    %56 = "stablehlo.scatter"(%arg8, %5, %55) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>) -> tensor<1x8x128x128xbf16>
    %57 = sdy.sharding_constraint %56 <@mesh, [{}, {"_axis_0"}, {}, {}]> : tensor<1x8x128x128xbf16>
    %58 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %59 = stablehlo.dot_general %27, %58, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %60 = stablehlo.reshape %59 : (tensor<1x1024xbf16>) -> tensor<1x8x1x128xbf16>
    %61 = "stablehlo.scatter"(%arg10, %5, %60) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>) -> tensor<1x8x128x128xbf16>
    %62 = sdy.sharding_constraint %61 <@mesh, [{}, {"_axis_0"}, {}, {}]> : tensor<1x8x128x128xbf16>
    %63 = stablehlo.convert %arg20 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %64 = stablehlo.reshape %63 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
    %65 = stablehlo.transpose %arg17, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %66 = stablehlo.dot_general %27, %65, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %67 = stablehlo.reshape %66 : (tensor<1x3072xbf16>) -> tensor<1x24x1x128xbf16>
    %68 = stablehlo.convert %67 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,1,128]{3,1,2,0}"} : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %69 = stablehlo.broadcast_in_dim %40, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x24x1x128xf32>
    %70 = stablehlo.multiply %68, %69 : tensor<1x24x1x128xf32>
    %71 = stablehlo.convert %70 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %72 = stablehlo.slice %67 [0:1, 0:24, 0:1, 64:128] : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x64xbf16>
    %73 = stablehlo.negate %72 : tensor<1x24x1x64xbf16>
    %74 = stablehlo.slice %67 [0:1, 0:24, 0:1, 0:64] : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x64xbf16>
    %75 = stablehlo.concatenate %73, %74, dim = 3 : (tensor<1x24x1x64xbf16>, tensor<1x24x1x64xbf16>) -> tensor<1x24x1x128xbf16>
    %76 = stablehlo.convert %75 : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %77 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x24x1x128xf32>
    %78 = stablehlo.multiply %76, %77 : tensor<1x24x1x128xf32>
    %79 = stablehlo.convert %78 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %80 = stablehlo.add %71, %79 : tensor<1x24x1x128xbf16>
    %81 = stablehlo.reshape %80 : (tensor<1x24x1x128xbf16>) -> tensor<24x1x128xbf16>
    %82 = stablehlo.broadcast_in_dim %56, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x8x3x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %84 = stablehlo.transpose %83, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,128]{2,3,1,0}"} : (tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %85 = stablehlo.reshape %84 : (tensor<1x24x128x128xbf16>) -> tensor<24x128x128xbf16>
    %86 = stablehlo.dot_general %81, %85, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x1x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x1x128xbf16>
    %87 = stablehlo.convert %86 : (tensor<24x1x128xbf16>) -> tensor<24x1x128xf32>
    %88 = stablehlo.reshape %87 : (tensor<24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %89 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<1x24x1x128xf32>
    %90 = stablehlo.multiply %88, %89 : tensor<1x24x1x128xf32>
    %91 = stablehlo.convert %90 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %92 = stablehlo.convert %arg15 : (tensor<1x128xbf16>) -> tensor<1x128xf32>
    %93 = stablehlo.reshape %arg14 : (tensor<128xi64>) -> tensor<1x128xi64>
    %94 = stablehlo.broadcast_in_dim %arg0, dims = [0] : (tensor<1xi64>) -> tensor<1x128xi64>
    %95 = stablehlo.compare  GT, %93, %94 : (tensor<1x128xi64>, tensor<1x128xi64>) -> tensor<1x128xi1>
    %96 = stablehlo.convert %95 : (tensor<1x128xi1>) -> tensor<1x128xf32>
    %97 = stablehlo.multiply %92, %96 : tensor<1x128xf32>
    %98 = stablehlo.convert %97 : (tensor<1x128xf32>) -> tensor<1x128xbf16>
    %99 = stablehlo.reshape %98 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16>
    %100 = stablehlo.broadcast_in_dim %99, dims = [0, 2, 3] : (tensor<1x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %101 = stablehlo.add %91, %100 : tensor<1x24x1x128xbf16>
    %102 = stablehlo.convert %101 : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %103 = stablehlo.reduce(%102 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x1x128xf32>, tensor<f32>) -> tensor<1x24x1xf32>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0, 1, 2] : (tensor<1x24x1xf32>) -> tensor<1x24x1x128xf32>
    %105 = stablehlo.subtract %102, %104 : tensor<1x24x1x128xf32>
    %106 = stablehlo.exponential %105 : tensor<1x24x1x128xf32>
    %107 = stablehlo.reduce(%106 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x1x128xf32>, tensor<f32>) -> tensor<1x24x1xf32>
    %108 = stablehlo.broadcast_in_dim %107, dims = [0, 1, 2] : (tensor<1x24x1xf32>) -> tensor<1x24x1x128xf32>
    %109 = stablehlo.divide %106, %108 : tensor<1x24x1x128xf32>
    %110 = stablehlo.convert %109 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %111 = stablehlo.reshape %110 : (tensor<1x24x1x128xbf16>) -> tensor<24x1x128xbf16>
    %112 = stablehlo.broadcast_in_dim %61, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %113 = stablehlo.reshape %112 : (tensor<1x8x3x128x128xbf16>) -> tensor<24x128x128xbf16>
    %114 = stablehlo.dot_general %111, %113, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x1x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x1x128xbf16>
    %115 = stablehlo.reshape %114 : (tensor<24x1x128xbf16>) -> tensor<1x3072xbf16>
    %116 = stablehlo.transpose %arg13, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %117 = stablehlo.dot_general %115, %116, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %118 = stablehlo.reshape %117 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %119 = stablehlo.add %11, %118 : tensor<1x1x3072xbf16>
    %120 = stablehlo.convert %arg18 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %121 = stablehlo.reshape %120 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
    %122 = stablehlo.convert %119 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %123 = stablehlo.power %122, %0 : tensor<1x1x3072xf32>
    %124 = stablehlo.reduce(%123 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %125 = stablehlo.multiply %124, %cst_1 : tensor<1x1xf32>
    %126 = stablehlo.reshape %125 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %127 = stablehlo.add %126, %17 : tensor<1x1x1xf32>
    %128 = stablehlo.rsqrt %127 : tensor<1x1x1xf32>
    %129 = stablehlo.reshape %128 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %130 = stablehlo.broadcast_in_dim %129, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %131 = stablehlo.multiply %122, %130 : tensor<1x1x3072xf32>
    %132 = stablehlo.convert %131 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %133 = stablehlo.convert %132 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %134 = stablehlo.multiply %121, %133 : tensor<1x1x3072xf32>
    %135 = stablehlo.convert %134 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %136 = stablehlo.reshape %135 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %137 = stablehlo.transpose %arg19, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %138 = stablehlo.dot_general %136, %137, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %139 = stablehlo.reshape %138 : (tensor<1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %140 = stablehlo.convert %139 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %141 = stablehlo.logistic %139 : tensor<1x1x8192xbf16>
    %142 = stablehlo.convert %141 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %143 = stablehlo.multiply %140, %142 : tensor<1x1x8192xf32>
    %144 = stablehlo.convert %143 : (tensor<1x1x8192xf32>) -> tensor<1x1x8192xbf16>
    %145 = stablehlo.convert %144 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %146 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %147 = stablehlo.dot_general %136, %146, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %148 = stablehlo.convert %147 : (tensor<1x8192xbf16>) -> tensor<1x8192xf32>
    %149 = stablehlo.reshape %148 : (tensor<1x8192xf32>) -> tensor<1x1x8192xf32>
    %150 = stablehlo.multiply %145, %149 : tensor<1x1x8192xf32>
    %151 = stablehlo.convert %150 : (tensor<1x1x8192xf32>) -> tensor<1x1x8192xbf16>
    %152 = stablehlo.reshape %151 : (tensor<1x1x8192xbf16>) -> tensor<1x8192xbf16>
    %153 = stablehlo.transpose %arg11, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %154 = stablehlo.dot_general %152, %153, contracting_dims = [1] x [0] : (tensor<1x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<1x3072xbf16>
    %155 = stablehlo.reshape %154 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %156 = stablehlo.add %119, %155 : tensor<1x1x3072xbf16>
    %157 = stablehlo.convert %156 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %158 = stablehlo.power %157, %0 : tensor<1x1x3072xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %160 = stablehlo.multiply %159, %cst_1 : tensor<1x1xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %162 = stablehlo.add %161, %17 : tensor<1x1x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x1x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x1x3072xf32>
    %167 = stablehlo.convert %166 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %168 = stablehlo.convert %167 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %169 = stablehlo.multiply %64, %168 : tensor<1x1x3072xf32>
    %170 = stablehlo.convert %169 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %172 = stablehlo.transpose %arg5, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %173 = stablehlo.dot_general %171, %172, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<1x128256xbf16>
    %174 = stablehlo.reshape %173 : (tensor<1x128256xbf16>) -> tensor<1x1x128256xbf16>
    return %57, %62, %173, %174 : tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x128256xbf16>, tensor<1x1x128256xbf16>
  }
}
module @SyncTensorsGraph.370 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<1xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<64xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1024x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1x1xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<128256x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<i64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<1024x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<3072x8192xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<8192x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<128xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<1x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<8192x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x1x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:4 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %arg16, %arg17, %arg18, %arg19, %arg20) in_shardings=[<@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, []>, <@mesh, [{}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}]>, <@mesh, []>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {}]>, <@mesh, []>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg21: tensor<1xi64>, %arg22: tensor<64xf32>, %arg23: tensor<512x3072xbf16>, %arg24: tensor<f32>, %arg25: tensor<1x1xi64>, %arg26: tensor<128256x3072xbf16>, %arg27: tensor<3072xbf16>, %arg28: tensor<i64>, %arg29: tensor<1x4x128x128xbf16>, %arg30: tensor<512x3072xbf16>, %arg31: tensor<1x4x128x128xbf16>, %arg32: tensor<3072x4096xbf16>, %arg33: tensor<4096x3072xbf16>, %arg34: tensor<3072x1536xbf16>, %arg35: tensor<128xi64>, %arg36: tensor<1x128xbf16>, %arg37: tensor<f32>, %arg38: tensor<1536x3072xbf16>, %arg39: tensor<3072xbf16>, %arg40: tensor<4096x3072xbf16>, %arg41: tensor<3072xbf16>) {
      %c = stablehlo.constant dense<0> : tensor<1xi64>
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
      %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
      %cst_1 = stablehlo.constant dense<3.25520843E-4> : tensor<1x1xf32>
      %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %1 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x1x3072xf32>
      %2 = stablehlo.compare  LT, %arg21, %c : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
      %3 = stablehlo.reshape %arg28 : (tensor<i64>) -> tensor<1xi64>
      %4 = stablehlo.add %arg21, %3 : tensor<1xi64>
      %5 = stablehlo.select %2, %4, %arg21 : tensor<1xi1>, tensor<1xi64>
      %6 = stablehlo.reshape %5 : (tensor<1xi64>) -> tensor<1x1xi64>
      %7 = stablehlo.convert %arg27 : (tensor<3072xbf16>) -> tensor<3072xf32>
      %8 = stablehlo.reshape %7 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
      %9 = stablehlo.convert %arg25 : (tensor<1x1xi64>) -> tensor<1x1xui32>
      %10 = stablehlo.reshape %9 : (tensor<1x1xui32>) -> tensor<1xui32>
      %11 = "stablehlo.gather"(%arg26, %10) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<1xui32>) -> tensor<1x3072xbf16>
      %12 = stablehlo.reshape %11 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
      %13 = stablehlo.convert %12 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %14 = stablehlo.power %13, %1 : tensor<1x1x3072xf32>
      %15 = stablehlo.reduce(%14 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
      %16 = stablehlo.multiply %15, %cst_1 : tensor<1x1xf32>
      %17 = stablehlo.reshape %16 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
      %18 = stablehlo.reshape %arg24 : (tensor<f32>) -> tensor<1x1x1xf32>
      %19 = stablehlo.add %17, %18 : tensor<1x1x1xf32>
      %20 = stablehlo.rsqrt %19 : tensor<1x1x1xf32>
      %21 = stablehlo.reshape %20 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
      %22 = stablehlo.broadcast_in_dim %21, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
      %23 = stablehlo.multiply %13, %22 : tensor<1x1x3072xf32>
      %24 = stablehlo.convert %23 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
      %25 = stablehlo.convert %24 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %26 = stablehlo.multiply %8, %25 : tensor<1x1x3072xf32>
      %27 = stablehlo.convert %26 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
      %28 = stablehlo.reshape %27 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
      %29 = stablehlo.transpose %arg23, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<512x3072xbf16>) -> tensor<3072x512xbf16>
      %30 = stablehlo.dot_general %28, %29, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x512xbf16>) -> tensor<1x512xbf16>
      %31 = stablehlo.reshape %30 : (tensor<1x512xbf16>) -> tensor<1x4x1x128xbf16>
      %32 = stablehlo.convert %31 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,1,128]{3,1,2,0}"} : (tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xf32>
      %33 = stablehlo.reshape %arg22 : (tensor<64xf32>) -> tensor<1x64x1xf32>
      %34 = stablehlo.convert %arg21 : (tensor<1xi64>) -> tensor<1xf32>
      %35 = stablehlo.reshape %34 : (tensor<1xf32>) -> tensor<1x1x1xf32>
      %36 = stablehlo.dot_general %33, %35, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x1xf32>) -> tensor<1x64x1xf32>
      %37 = stablehlo.reshape %36 : (tensor<1x64x1xf32>) -> tensor<1x1x64xf32>
      %38 = stablehlo.concatenate %37, %37, dim = 2 : (tensor<1x1x64xf32>, tensor<1x1x64xf32>) -> tensor<1x1x128xf32>
      %39 = stablehlo.cosine %38 : tensor<1x1x128xf32>
      %40 = stablehlo.convert %39 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
      %41 = stablehlo.convert %40 : (tensor<1x1x128xbf16>) -> tensor<1x1x128xf32>
      %42 = stablehlo.broadcast_in_dim %41, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x4x1x128xf32>
      %43 = stablehlo.multiply %32, %42 : tensor<1x4x1x128xf32>
      %44 = stablehlo.convert %43 : (tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xbf16>
      %45 = stablehlo.slice %31 [0:1, 0:4, 0:1, 64:128] : (tensor<1x4x1x128xbf16>) -> tensor<1x4x1x64xbf16>
      %46 = stablehlo.negate %45 : tensor<1x4x1x64xbf16>
      %47 = stablehlo.slice %31 [0:1, 0:4, 0:1, 0:64] : (tensor<1x4x1x128xbf16>) -> tensor<1x4x1x64xbf16>
      %48 = stablehlo.concatenate %46, %47, dim = 3 : (tensor<1x4x1x64xbf16>, tensor<1x4x1x64xbf16>) -> tensor<1x4x1x128xbf16>
      %49 = stablehlo.convert %48 : (tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xf32>
      %50 = stablehlo.sine %38 : tensor<1x1x128xf32>
      %51 = stablehlo.convert %50 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
      %52 = stablehlo.convert %51 : (tensor<1x1x128xbf16>) -> tensor<1x1x128xf32>
      %53 = stablehlo.broadcast_in_dim %52, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x4x1x128xf32>
      %54 = stablehlo.multiply %49, %53 : tensor<1x4x1x128xf32>
      %55 = stablehlo.convert %54 : (tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xbf16>
      %56 = stablehlo.add %44, %55 : tensor<1x4x1x128xbf16>
      %57 = "stablehlo.scatter"(%arg29, %6, %56) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg42: tensor<bf16>, %arg43: tensor<bf16>):
        stablehlo.return %arg43 : tensor<bf16>
      }) : (tensor<1x4x128x128xbf16>, tensor<1x1xi64>, tensor<1x4x1x128xbf16>) -> tensor<1x4x128x128xbf16>
      %58 = stablehlo.transpose %arg30, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<512x3072xbf16>) -> tensor<3072x512xbf16>
      %59 = stablehlo.dot_general %28, %58, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x512xbf16>) -> tensor<1x512xbf16>
      %60 = stablehlo.reshape %59 : (tensor<1x512xbf16>) -> tensor<1x4x1x128xbf16>
      %61 = "stablehlo.scatter"(%arg31, %6, %60) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg42: tensor<bf16>, %arg43: tensor<bf16>):
        stablehlo.return %arg43 : tensor<bf16>
      }) : (tensor<1x4x128x128xbf16>, tensor<1x1xi64>, tensor<1x4x1x128xbf16>) -> tensor<1x4x128x128xbf16>
      %62 = stablehlo.convert %arg41 : (tensor<3072xbf16>) -> tensor<3072xf32>
      %63 = stablehlo.reshape %62 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
      %64 = stablehlo.transpose %arg38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<1536x3072xbf16>) -> tensor<3072x1536xbf16>
      %65 = stablehlo.dot_general %28, %64, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1536xbf16>) -> tensor<1x1536xbf16>
      %66 = stablehlo.reshape %65 : (tensor<1x1536xbf16>) -> tensor<1x12x1x128xbf16>
      %67 = stablehlo.convert %66 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,1,128]{3,1,2,0}"} : (tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xf32>
      %68 = stablehlo.broadcast_in_dim %41, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x12x1x128xf32>
      %69 = stablehlo.multiply %67, %68 : tensor<1x12x1x128xf32>
      %70 = stablehlo.convert %69 : (tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xbf16>
      %71 = stablehlo.slice %66 [0:1, 0:12, 0:1, 64:128] : (tensor<1x12x1x128xbf16>) -> tensor<1x12x1x64xbf16>
      %72 = stablehlo.negate %71 : tensor<1x12x1x64xbf16>
      %73 = stablehlo.slice %66 [0:1, 0:12, 0:1, 0:64] : (tensor<1x12x1x128xbf16>) -> tensor<1x12x1x64xbf16>
      %74 = stablehlo.concatenate %72, %73, dim = 3 : (tensor<1x12x1x64xbf16>, tensor<1x12x1x64xbf16>) -> tensor<1x12x1x128xbf16>
      %75 = stablehlo.convert %74 : (tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xf32>
      %76 = stablehlo.broadcast_in_dim %52, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x12x1x128xf32>
      %77 = stablehlo.multiply %75, %76 : tensor<1x12x1x128xf32>
      %78 = stablehlo.convert %77 : (tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xbf16>
      %79 = stablehlo.add %70, %78 : tensor<1x12x1x128xbf16>
      %80 = stablehlo.reshape %79 : (tensor<1x12x1x128xbf16>) -> tensor<12x1x128xbf16>
      %81 = stablehlo.broadcast_in_dim %57, dims = [0, 1, 3, 4] : (tensor<1x4x128x128xbf16>) -> tensor<1x4x3x128x128xbf16>
      %82 = stablehlo.reshape %81 : (tensor<1x4x3x128x128xbf16>) -> tensor<1x12x128x128xbf16>
      %83 = stablehlo.transpose %82, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,128]{2,3,1,0}"} : (tensor<1x12x128x128xbf16>) -> tensor<1x12x128x128xbf16>
      %84 = stablehlo.reshape %83 : (tensor<1x12x128x128xbf16>) -> tensor<12x128x128xbf16>
      %85 = stablehlo.dot_general %80, %84, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<12x1x128xbf16>, tensor<12x128x128xbf16>) -> tensor<12x1x128xbf16>
      %86 = stablehlo.convert %85 : (tensor<12x1x128xbf16>) -> tensor<12x1x128xf32>
      %87 = stablehlo.reshape %86 : (tensor<12x1x128xf32>) -> tensor<1x12x1x128xf32>
      %88 = stablehlo.broadcast_in_dim %arg37, dims = [] : (tensor<f32>) -> tensor<1x12x1x128xf32>
      %89 = stablehlo.multiply %87, %88 : tensor<1x12x1x128xf32>
      %90 = stablehlo.convert %89 : (tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xbf16>
      %91 = stablehlo.convert %arg36 : (tensor<1x128xbf16>) -> tensor<1x128xf32>
      %92 = stablehlo.reshape %arg35 : (tensor<128xi64>) -> tensor<1x128xi64>
      %93 = stablehlo.broadcast_in_dim %arg21, dims = [0] : (tensor<1xi64>) -> tensor<1x128xi64>
      %94 = stablehlo.compare  GT, %92, %93 : (tensor<1x128xi64>, tensor<1x128xi64>) -> tensor<1x128xi1>
      %95 = stablehlo.convert %94 : (tensor<1x128xi1>) -> tensor<1x128xf32>
      %96 = stablehlo.multiply %91, %95 : tensor<1x128xf32>
      %97 = stablehlo.convert %96 : (tensor<1x128xf32>) -> tensor<1x128xbf16>
      %98 = stablehlo.reshape %97 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16>
      %99 = stablehlo.broadcast_in_dim %98, dims = [0, 2, 3] : (tensor<1x1x128xbf16>) -> tensor<1x12x1x128xbf16>
      %100 = stablehlo.add %90, %99 : tensor<1x12x1x128xbf16>
      %101 = stablehlo.convert %100 : (tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xf32>
      %102 = stablehlo.reduce(%101 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x12x1x128xf32>, tensor<f32>) -> tensor<1x12x1xf32>
      %103 = stablehlo.broadcast_in_dim %102, dims = [0, 1, 2] : (tensor<1x12x1xf32>) -> tensor<1x12x1x128xf32>
      %104 = stablehlo.subtract %101, %103 : tensor<1x12x1x128xf32>
      %105 = stablehlo.exponential %104 : tensor<1x12x1x128xf32>
      %106 = stablehlo.reduce(%105 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x12x1x128xf32>, tensor<f32>) -> tensor<1x12x1xf32>
      %107 = stablehlo.broadcast_in_dim %106, dims = [0, 1, 2] : (tensor<1x12x1xf32>) -> tensor<1x12x1x128xf32>
      %108 = stablehlo.divide %105, %107 : tensor<1x12x1x128xf32>
      %109 = stablehlo.convert %108 : (tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xbf16>
      %110 = stablehlo.reshape %109 : (tensor<1x12x1x128xbf16>) -> tensor<12x1x128xbf16>
      %111 = stablehlo.broadcast_in_dim %61, dims = [0, 1, 3, 4] : (tensor<1x4x128x128xbf16>) -> tensor<1x4x3x128x128xbf16>
      %112 = stablehlo.reshape %111 : (tensor<1x4x3x128x128xbf16>) -> tensor<12x128x128xbf16>
      %113 = stablehlo.dot_general %110, %112, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<12x1x128xbf16>, tensor<12x128x128xbf16>) -> tensor<12x1x128xbf16>
      %114 = stablehlo.reshape %113 : (tensor<12x1x128xbf16>) -> tensor<1x1536xbf16>
      %115 = stablehlo.transpose %arg34, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x1536xbf16>) -> tensor<1536x3072xbf16>
      %116 = stablehlo.dot_general %114, %115, contracting_dims = [1] x [0] : (tensor<1x1536xbf16>, tensor<1536x3072xbf16>) -> tensor<1x3072xbf16>
      %117 = "stablehlo.all_reduce"(%116) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> ({
      ^bb0(%arg42: tensor<bf16>, %arg43: tensor<bf16>):
        %176 = stablehlo.add %arg42, %arg43 : tensor<bf16>
        stablehlo.return %176 : tensor<bf16>
      }) : (tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
      %118 = stablehlo.reshape %117 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
      %119 = stablehlo.add %12, %118 : tensor<1x1x3072xbf16>
      %120 = stablehlo.convert %arg39 : (tensor<3072xbf16>) -> tensor<3072xf32>
      %121 = stablehlo.reshape %120 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
      %122 = stablehlo.convert %119 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %123 = stablehlo.power %122, %1 : tensor<1x1x3072xf32>
      %124 = stablehlo.reduce(%123 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
      %125 = stablehlo.multiply %124, %cst_1 : tensor<1x1xf32>
      %126 = stablehlo.reshape %125 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
      %127 = stablehlo.add %126, %18 : tensor<1x1x1xf32>
      %128 = stablehlo.rsqrt %127 : tensor<1x1x1xf32>
      %129 = stablehlo.reshape %128 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
      %130 = stablehlo.broadcast_in_dim %129, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
      %131 = stablehlo.multiply %122, %130 : tensor<1x1x3072xf32>
      %132 = stablehlo.convert %131 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
      %133 = stablehlo.convert %132 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %134 = stablehlo.multiply %121, %133 : tensor<1x1x3072xf32>
      %135 = stablehlo.convert %134 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
      %136 = stablehlo.reshape %135 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
      %137 = stablehlo.transpose %arg40, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<4096x3072xbf16>) -> tensor<3072x4096xbf16>
      %138 = stablehlo.dot_general %136, %137, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<1x4096xbf16>
      %139 = stablehlo.reshape %138 : (tensor<1x4096xbf16>) -> tensor<1x1x4096xbf16>
      %140 = stablehlo.convert %139 : (tensor<1x1x4096xbf16>) -> tensor<1x1x4096xf32>
      %141 = stablehlo.logistic %139 : tensor<1x1x4096xbf16>
      %142 = stablehlo.convert %141 : (tensor<1x1x4096xbf16>) -> tensor<1x1x4096xf32>
      %143 = stablehlo.multiply %140, %142 : tensor<1x1x4096xf32>
      %144 = stablehlo.convert %143 : (tensor<1x1x4096xf32>) -> tensor<1x1x4096xbf16>
      %145 = stablehlo.convert %144 : (tensor<1x1x4096xbf16>) -> tensor<1x1x4096xf32>
      %146 = stablehlo.transpose %arg33, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<4096x3072xbf16>) -> tensor<3072x4096xbf16>
      %147 = stablehlo.dot_general %136, %146, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<1x4096xbf16>
      %148 = stablehlo.convert %147 : (tensor<1x4096xbf16>) -> tensor<1x4096xf32>
      %149 = stablehlo.reshape %148 : (tensor<1x4096xf32>) -> tensor<1x1x4096xf32>
      %150 = stablehlo.multiply %145, %149 : tensor<1x1x4096xf32>
      %151 = stablehlo.convert %150 : (tensor<1x1x4096xf32>) -> tensor<1x1x4096xbf16>
      %152 = stablehlo.reshape %151 : (tensor<1x1x4096xbf16>) -> tensor<1x4096xbf16>
      %153 = stablehlo.transpose %arg32, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x4096xbf16>) -> tensor<4096x3072xbf16>
      %154 = stablehlo.dot_general %152, %153, contracting_dims = [1] x [0] : (tensor<1x4096xbf16>, tensor<4096x3072xbf16>) -> tensor<1x3072xbf16>
      %155 = "stablehlo.all_reduce"(%154) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> ({
      ^bb0(%arg42: tensor<bf16>, %arg43: tensor<bf16>):
        %176 = stablehlo.add %arg42, %arg43 : tensor<bf16>
        stablehlo.return %176 : tensor<bf16>
      }) : (tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
      %156 = stablehlo.reshape %155 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
      %157 = stablehlo.add %119, %156 : tensor<1x1x3072xbf16>
      %158 = stablehlo.convert %157 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %159 = stablehlo.power %158, %1 : tensor<1x1x3072xf32>
      %160 = stablehlo.reduce(%159 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
      %161 = stablehlo.multiply %160, %cst_1 : tensor<1x1xf32>
      %162 = stablehlo.reshape %161 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
      %163 = stablehlo.add %162, %18 : tensor<1x1x1xf32>
      %164 = stablehlo.rsqrt %163 : tensor<1x1x1xf32>
      %165 = stablehlo.reshape %164 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
      %166 = stablehlo.broadcast_in_dim %165, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
      %167 = stablehlo.multiply %158, %166 : tensor<1x1x3072xf32>
      %168 = stablehlo.convert %167 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
      %169 = stablehlo.convert %168 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %170 = stablehlo.multiply %63, %169 : tensor<1x1x3072xf32>
      %171 = stablehlo.convert %170 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
      %172 = stablehlo.reshape %171 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
      %173 = stablehlo.transpose %arg26, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
      %174 = stablehlo.dot_general %172, %173, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<1x128256xbf16>
      %175 = stablehlo.reshape %174 : (tensor<1x128256xbf16>) -> tensor<1x1x128256xbf16>
      sdy.return %57, %61, %174, %175 : tensor<1x4x128x128xbf16>, tensor<1x4x128x128xbf16>, tensor<1x128256xbf16>, tensor<1x1x128256xbf16>
    } : (tensor<1xi64>, tensor<64xf32>, tensor<1024x3072xbf16>, tensor<f32>, tensor<1x1xi64>, tensor<128256x3072xbf16>, tensor<3072xbf16>, tensor<i64>, tensor<1x8x128x128xbf16>, tensor<1024x3072xbf16>, tensor<1x8x128x128xbf16>, tensor<3072x8192xbf16>, tensor<8192x3072xbf16>, tensor<3072x3072xbf16>, tensor<128xi64>, tensor<1x128xbf16>, tensor<f32>, tensor<3072x3072xbf16>, tensor<3072xbf16>, tensor<8192x3072xbf16>, tensor<3072xbf16>) -> (tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x128256xbf16>, tensor<1x1x128256xbf16>)
    return %0#0, %0#1, %0#2, %0#3 : tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x128256xbf16>, tensor<1x1x128256xbf16>
  }
}
module @SyncTensorsGraph.370 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  func.func @main(%arg0: tensor<1xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<64xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1024x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1x1xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<128256x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<i64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<1024x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<3072x8192xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<8192x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<128xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<1x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<8192x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x1x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = ttir.empty() : tensor<1xi64>
    %1 = "ttir.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %2 = ttir.empty() : tensor<64xf32>
    %3 = "ttir.mesh_shard"(%arg1, %2) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32>, tensor<64xf32>) -> tensor<64xf32>
    %4 = ttir.empty() : tensor<512x3072xbf16>
    %5 = "ttir.mesh_shard"(%arg2, %4) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16>, tensor<512x3072xbf16>) -> tensor<512x3072xbf16>
    %6 = ttir.empty() : tensor<f32>
    %7 = "ttir.mesh_shard"(%arg3, %6) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %8 = ttir.empty() : tensor<1x1xi64>
    %9 = "ttir.mesh_shard"(%arg4, %8) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x1xi64>, tensor<1x1xi64>) -> tensor<1x1xi64>
    %10 = ttir.empty() : tensor<128256x3072xbf16>
    %11 = "ttir.mesh_shard"(%arg5, %10) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16>, tensor<128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %12 = ttir.empty() : tensor<3072xbf16>
    %13 = "ttir.mesh_shard"(%arg6, %12) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16>, tensor<3072xbf16>) -> tensor<3072xbf16>
    %14 = ttir.empty() : tensor<i64>
    %15 = "ttir.mesh_shard"(%arg7, %14) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %16 = ttir.empty() : tensor<1x4x128x128xbf16>
    %17 = "ttir.mesh_shard"(%arg8, %16) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x128x128xbf16>, tensor<1x4x128x128xbf16>) -> tensor<1x4x128x128xbf16>
    %18 = ttir.empty() : tensor<512x3072xbf16>
    %19 = "ttir.mesh_shard"(%arg9, %18) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16>, tensor<512x3072xbf16>) -> tensor<512x3072xbf16>
    %20 = ttir.empty() : tensor<1x4x128x128xbf16>
    %21 = "ttir.mesh_shard"(%arg10, %20) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x128x128xbf16>, tensor<1x4x128x128xbf16>) -> tensor<1x4x128x128xbf16>
    %22 = ttir.empty() : tensor<3072x4096xbf16>
    %23 = "ttir.mesh_shard"(%arg11, %22) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x8192xbf16>, tensor<3072x4096xbf16>) -> tensor<3072x4096xbf16>
    %24 = ttir.empty() : tensor<4096x3072xbf16>
    %25 = "ttir.mesh_shard"(%arg12, %24) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16>, tensor<4096x3072xbf16>) -> tensor<4096x3072xbf16>
    %26 = ttir.empty() : tensor<3072x1536xbf16>
    %27 = "ttir.mesh_shard"(%arg13, %26) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16>, tensor<3072x1536xbf16>) -> tensor<3072x1536xbf16>
    %28 = ttir.empty() : tensor<128xi64>
    %29 = "ttir.mesh_shard"(%arg14, %28) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128xi64>, tensor<128xi64>) -> tensor<128xi64>
    %30 = ttir.empty() : tensor<1x128xbf16>
    %31 = "ttir.mesh_shard"(%arg15, %30) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %32 = ttir.empty() : tensor<f32>
    %33 = "ttir.mesh_shard"(%arg16, %32) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %34 = ttir.empty() : tensor<1536x3072xbf16>
    %35 = "ttir.mesh_shard"(%arg17, %34) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16>, tensor<1536x3072xbf16>) -> tensor<1536x3072xbf16>
    %36 = ttir.empty() : tensor<3072xbf16>
    %37 = "ttir.mesh_shard"(%arg18, %36) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16>, tensor<3072xbf16>) -> tensor<3072xbf16>
    %38 = ttir.empty() : tensor<4096x3072xbf16>
    %39 = "ttir.mesh_shard"(%arg19, %38) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16>, tensor<4096x3072xbf16>) -> tensor<4096x3072xbf16>
    %40 = ttir.empty() : tensor<3072xbf16>
    %41 = "ttir.mesh_shard"(%arg20, %40) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16>, tensor<3072xbf16>) -> tensor<3072xbf16>
    %42 = "ttir.constant"() <{value = dense<0> : tensor<1xi64>}> : () -> tensor<1xi64>
    %43 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<f32>}> : () -> tensor<f32>
    %44 = "ttir.constant"() <{value = dense<0xFF800000> : tensor<f32>}> : () -> tensor<f32>
    %45 = "ttir.constant"() <{value = dense<3.25520843E-4> : tensor<1x1xf32>}> : () -> tensor<1x1xf32>
    %46 = "ttir.constant"() <{value = dense<2.000000e+00> : tensor<f32>}> : () -> tensor<f32>
    %47 = ttir.empty() : tensor<1x1x1xf32>
    %48 = "ttir.reshape"(%46, %47) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %49 = ttir.empty() : tensor<1x1x3072xf32>
    %50 = "ttir.broadcast"(%48, %49) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x1x1xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %51 = ttir.empty() : tensor<1xi1>
    %52 = "ttir.lt"(%1, %42, %51) : (tensor<1xi64>, tensor<1xi64>, tensor<1xi1>) -> tensor<1xi1>
    %53 = ttir.empty() : tensor<1xi64>
    %54 = "ttir.reshape"(%15, %53) <{shape = [1 : i32]}> : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %55 = ttir.empty() : tensor<1xi64>
    %56 = "ttir.add"(%1, %54, %55) : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %57 = ttir.empty() : tensor<1xi64>
    %58 = "ttir.where"(%52, %56, %1, %57) : (tensor<1xi1>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %59 = ttir.empty() : tensor<1x1xi64>
    %60 = "ttir.reshape"(%58, %59) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xi64>, tensor<1x1xi64>) -> tensor<1x1xi64>
    %61 = ttir.empty() : tensor<3072xf32>
    %62 = "ttir.typecast"(%13, %61) <{conservative_folding = false}> : (tensor<3072xbf16>, tensor<3072xf32>) -> tensor<3072xf32>
    %63 = ttir.empty() : tensor<1x1x3072xf32>
    %64 = "ttir.reshape"(%62, %63) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %65 = ttir.empty() : tensor<1x1xui32>
    %66 = "ttir.typecast"(%9, %65) <{conservative_folding = false}> : (tensor<1x1xi64>, tensor<1x1xui32>) -> tensor<1x1xui32>
    %67 = ttir.empty() : tensor<1xui32>
    %68 = "ttir.reshape"(%66, %67) <{shape = [1 : i32]}> : (tensor<1x1xui32>, tensor<1xui32>) -> tensor<1xui32>
    %69 = ttir.empty() : tensor<1x3072xbf16>
    %70 = "ttir.gather"(%11, %68, %69) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 3072>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<128256x3072xbf16>, tensor<1xui32>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %71 = ttir.empty() : tensor<1x1x3072xbf16>
    %72 = "ttir.reshape"(%70, %71) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %73 = ttir.empty() : tensor<1x1x3072xf32>
    %74 = "ttir.typecast"(%72, %73) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %75 = ttir.empty() : tensor<1x1x3072xf32>
    %76 = "ttir.pow"(%74, %50, %75) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %77 = ttir.empty() : tensor<1x1xf32>
    %78 = "ttir.sum"(%76, %77) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %79 = ttir.empty() : tensor<1x1xf32>
    %80 = "ttir.multiply"(%78, %45, %79) : (tensor<1x1xf32>, tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %81 = ttir.empty() : tensor<1x1x1xf32>
    %82 = "ttir.reshape"(%80, %81) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %83 = ttir.empty() : tensor<1x1x1xf32>
    %84 = "ttir.reshape"(%7, %83) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %85 = ttir.empty() : tensor<1x1x1xf32>
    %86 = "ttir.add"(%82, %84, %85) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %87 = ttir.empty() : tensor<1x1x1xf32>
    %88 = "ttir.rsqrt"(%86, %87) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %89 = ttir.empty() : tensor<1x1xf32>
    %90 = "ttir.reshape"(%88, %89) <{shape = [1 : i32, 1 : i32]}> : (tensor<1x1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %91 = ttir.empty() : tensor<1x1x1xf32>
    %92 = "ttir.reshape"(%90, %91) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %93 = ttir.empty() : tensor<1x1x3072xf32>
    %94 = "ttir.broadcast"(%92, %93) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x1x1xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %95 = ttir.empty() : tensor<1x1x3072xf32>
    %96 = "ttir.multiply"(%74, %94, %95) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %97 = ttir.empty() : tensor<1x1x3072xbf16>
    %98 = "ttir.typecast"(%96, %97) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %99 = ttir.empty() : tensor<1x1x3072xf32>
    %100 = "ttir.typecast"(%98, %99) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %101 = ttir.empty() : tensor<1x1x3072xf32>
    %102 = "ttir.multiply"(%64, %100, %101) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %103 = ttir.empty() : tensor<1x1x3072xbf16>
    %104 = "ttir.typecast"(%102, %103) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %105 = ttir.empty() : tensor<1x3072xbf16>
    %106 = "ttir.reshape"(%104, %105) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x3072xbf16>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %107 = ttir.empty() : tensor<3072x512xbf16>
    %108 = "ttir.permute"(%5, %107) <{permutation = array<i64: 1, 0>}> : (tensor<512x3072xbf16>, tensor<3072x512xbf16>) -> tensor<3072x512xbf16>
    %109 = "ttir.dot_general"(%106, %108) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x512xbf16>) -> tensor<1x512xbf16>
    %110 = ttir.empty() : tensor<1x4x1x128xbf16>
    %111 = "ttir.reshape"(%109, %110) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32]}> : (tensor<1x512xbf16>, tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xbf16>
    %112 = ttir.empty() : tensor<1x4x1x128xf32>
    %113 = "ttir.typecast"(%111, %112) <{conservative_folding = false}> : (tensor<1x4x1x128xbf16>, tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xf32>
    %114 = ttir.empty() : tensor<1x64x1xf32>
    %115 = "ttir.reshape"(%3, %114) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
    %116 = ttir.empty() : tensor<1xf32>
    %117 = "ttir.typecast"(%1, %116) <{conservative_folding = false}> : (tensor<1xi64>, tensor<1xf32>) -> tensor<1xf32>
    %118 = ttir.empty() : tensor<1x1x1xf32>
    %119 = "ttir.reshape"(%117, %118) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %120 = "ttir.dot_general"(%115, %119) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x64x1xf32>, tensor<1x1x1xf32>) -> tensor<1x64x1xf32>
    %121 = ttir.empty() : tensor<1x1x64xf32>
    %122 = "ttir.reshape"(%120, %121) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<1x64x1xf32>, tensor<1x1x64xf32>) -> tensor<1x1x64xf32>
    %123 = ttir.empty() : tensor<1x1x128xf32>
    %124 = "ttir.concat"(%122, %122, %123) <{dim = 2 : si32}> : (tensor<1x1x64xf32>, tensor<1x1x64xf32>, tensor<1x1x128xf32>) -> tensor<1x1x128xf32>
    %125 = ttir.empty() : tensor<1x1x128xf32>
    %126 = "ttir.cos"(%124, %125) : (tensor<1x1x128xf32>, tensor<1x1x128xf32>) -> tensor<1x1x128xf32>
    %127 = ttir.empty() : tensor<1x1x128xbf16>
    %128 = "ttir.typecast"(%126, %127) <{conservative_folding = false}> : (tensor<1x1x128xf32>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16>
    %129 = ttir.empty() : tensor<1x1x128xf32>
    %130 = "ttir.typecast"(%128, %129) <{conservative_folding = false}> : (tensor<1x1x128xbf16>, tensor<1x1x128xf32>) -> tensor<1x1x128xf32>
    %131 = ttir.empty() : tensor<1x1x1x128xf32>
    %132 = "ttir.reshape"(%130, %131) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32>, tensor<1x1x1x128xf32>) -> tensor<1x1x1x128xf32>
    %133 = ttir.empty() : tensor<1x4x1x128xf32>
    %134 = "ttir.broadcast"(%132, %133) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x1x128xf32>, tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xf32>
    %135 = ttir.empty() : tensor<1x4x1x128xf32>
    %136 = "ttir.multiply"(%113, %134, %135) : (tensor<1x4x1x128xf32>, tensor<1x4x1x128xf32>, tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xf32>
    %137 = ttir.empty() : tensor<1x4x1x128xbf16>
    %138 = "ttir.typecast"(%136, %137) <{conservative_folding = false}> : (tensor<1x4x1x128xf32>, tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xbf16>
    %139 = ttir.empty() : tensor<1x4x1x64xbf16>
    %140 = "ttir.slice_static"(%111, %139) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x1x128xbf16>, tensor<1x4x1x64xbf16>) -> tensor<1x4x1x64xbf16>
    %141 = ttir.empty() : tensor<1x4x1x64xbf16>
    %142 = "ttir.neg"(%140, %141) : (tensor<1x4x1x64xbf16>, tensor<1x4x1x64xbf16>) -> tensor<1x4x1x64xbf16>
    %143 = ttir.empty() : tensor<1x4x1x64xbf16>
    %144 = "ttir.slice_static"(%111, %143) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x1x128xbf16>, tensor<1x4x1x64xbf16>) -> tensor<1x4x1x64xbf16>
    %145 = ttir.empty() : tensor<1x4x1x128xbf16>
    %146 = "ttir.concat"(%142, %144, %145) <{dim = 3 : si32}> : (tensor<1x4x1x64xbf16>, tensor<1x4x1x64xbf16>, tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xbf16>
    %147 = ttir.empty() : tensor<1x4x1x128xf32>
    %148 = "ttir.typecast"(%146, %147) <{conservative_folding = false}> : (tensor<1x4x1x128xbf16>, tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xf32>
    %149 = ttir.empty() : tensor<1x1x128xf32>
    %150 = "ttir.sin"(%124, %149) : (tensor<1x1x128xf32>, tensor<1x1x128xf32>) -> tensor<1x1x128xf32>
    %151 = ttir.empty() : tensor<1x1x128xbf16>
    %152 = "ttir.typecast"(%150, %151) <{conservative_folding = false}> : (tensor<1x1x128xf32>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16>
    %153 = ttir.empty() : tensor<1x1x128xf32>
    %154 = "ttir.typecast"(%152, %153) <{conservative_folding = false}> : (tensor<1x1x128xbf16>, tensor<1x1x128xf32>) -> tensor<1x1x128xf32>
    %155 = ttir.empty() : tensor<1x1x1x128xf32>
    %156 = "ttir.reshape"(%154, %155) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32>, tensor<1x1x1x128xf32>) -> tensor<1x1x1x128xf32>
    %157 = ttir.empty() : tensor<1x4x1x128xf32>
    %158 = "ttir.broadcast"(%156, %157) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x1x128xf32>, tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xf32>
    %159 = ttir.empty() : tensor<1x4x1x128xf32>
    %160 = "ttir.multiply"(%148, %158, %159) : (tensor<1x4x1x128xf32>, tensor<1x4x1x128xf32>, tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xf32>
    %161 = ttir.empty() : tensor<1x4x1x128xbf16>
    %162 = "ttir.typecast"(%160, %161) <{conservative_folding = false}> : (tensor<1x4x1x128xf32>, tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xbf16>
    %163 = ttir.empty() : tensor<1x4x1x128xbf16>
    %164 = "ttir.add"(%138, %162, %163) : (tensor<1x4x1x128xbf16>, tensor<1x4x1x128xbf16>, tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xbf16>
    %165 = ttir.empty() : tensor<1x4x128x128xbf16>
    %166 = "ttir.scatter"(%17, %60, %164, %165) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x4x128x128xbf16>, tensor<1x1xi64>, tensor<1x4x1x128xbf16>, tensor<1x4x128x128xbf16>) -> tensor<1x4x128x128xbf16>
    %167 = ttir.empty() : tensor<3072x512xbf16>
    %168 = "ttir.permute"(%19, %167) <{permutation = array<i64: 1, 0>}> : (tensor<512x3072xbf16>, tensor<3072x512xbf16>) -> tensor<3072x512xbf16>
    %169 = "ttir.dot_general"(%106, %168) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x512xbf16>) -> tensor<1x512xbf16>
    %170 = ttir.empty() : tensor<1x4x1x128xbf16>
    %171 = "ttir.reshape"(%169, %170) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32]}> : (tensor<1x512xbf16>, tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xbf16>
    %172 = ttir.empty() : tensor<1x4x128x128xbf16>
    %173 = "ttir.scatter"(%21, %60, %171, %172) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x4x128x128xbf16>, tensor<1x1xi64>, tensor<1x4x1x128xbf16>, tensor<1x4x128x128xbf16>) -> tensor<1x4x128x128xbf16>
    %174 = ttir.empty() : tensor<3072xf32>
    %175 = "ttir.typecast"(%41, %174) <{conservative_folding = false}> : (tensor<3072xbf16>, tensor<3072xf32>) -> tensor<3072xf32>
    %176 = ttir.empty() : tensor<1x1x3072xf32>
    %177 = "ttir.reshape"(%175, %176) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %178 = ttir.empty() : tensor<3072x1536xbf16>
    %179 = "ttir.permute"(%35, %178) <{permutation = array<i64: 1, 0>}> : (tensor<1536x3072xbf16>, tensor<3072x1536xbf16>) -> tensor<3072x1536xbf16>
    %180 = "ttir.dot_general"(%106, %179) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x1536xbf16>) -> tensor<1x1536xbf16>
    %181 = ttir.empty() : tensor<1x12x1x128xbf16>
    %182 = "ttir.reshape"(%180, %181) <{shape = [1 : i32, 12 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1536xbf16>, tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xbf16>
    %183 = ttir.empty() : tensor<1x12x1x128xf32>
    %184 = "ttir.typecast"(%182, %183) <{conservative_folding = false}> : (tensor<1x12x1x128xbf16>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %185 = ttir.empty() : tensor<1x1x1x128xf32>
    %186 = "ttir.reshape"(%130, %185) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32>, tensor<1x1x1x128xf32>) -> tensor<1x1x1x128xf32>
    %187 = ttir.empty() : tensor<1x12x1x128xf32>
    %188 = "ttir.broadcast"(%186, %187) <{broadcast_dimensions = array<i64: 1, 12, 1, 1>}> : (tensor<1x1x1x128xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %189 = ttir.empty() : tensor<1x12x1x128xf32>
    %190 = "ttir.multiply"(%184, %188, %189) : (tensor<1x12x1x128xf32>, tensor<1x12x1x128xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %191 = ttir.empty() : tensor<1x12x1x128xbf16>
    %192 = "ttir.typecast"(%190, %191) <{conservative_folding = false}> : (tensor<1x12x1x128xf32>, tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xbf16>
    %193 = ttir.empty() : tensor<1x12x1x64xbf16>
    %194 = "ttir.slice_static"(%182, %193) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 12 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x1x128xbf16>, tensor<1x12x1x64xbf16>) -> tensor<1x12x1x64xbf16>
    %195 = ttir.empty() : tensor<1x12x1x64xbf16>
    %196 = "ttir.neg"(%194, %195) : (tensor<1x12x1x64xbf16>, tensor<1x12x1x64xbf16>) -> tensor<1x12x1x64xbf16>
    %197 = ttir.empty() : tensor<1x12x1x64xbf16>
    %198 = "ttir.slice_static"(%182, %197) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 12 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x1x128xbf16>, tensor<1x12x1x64xbf16>) -> tensor<1x12x1x64xbf16>
    %199 = ttir.empty() : tensor<1x12x1x128xbf16>
    %200 = "ttir.concat"(%196, %198, %199) <{dim = 3 : si32}> : (tensor<1x12x1x64xbf16>, tensor<1x12x1x64xbf16>, tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xbf16>
    %201 = ttir.empty() : tensor<1x12x1x128xf32>
    %202 = "ttir.typecast"(%200, %201) <{conservative_folding = false}> : (tensor<1x12x1x128xbf16>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %203 = ttir.empty() : tensor<1x1x1x128xf32>
    %204 = "ttir.reshape"(%154, %203) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32>, tensor<1x1x1x128xf32>) -> tensor<1x1x1x128xf32>
    %205 = ttir.empty() : tensor<1x12x1x128xf32>
    %206 = "ttir.broadcast"(%204, %205) <{broadcast_dimensions = array<i64: 1, 12, 1, 1>}> : (tensor<1x1x1x128xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %207 = ttir.empty() : tensor<1x12x1x128xf32>
    %208 = "ttir.multiply"(%202, %206, %207) : (tensor<1x12x1x128xf32>, tensor<1x12x1x128xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %209 = ttir.empty() : tensor<1x12x1x128xbf16>
    %210 = "ttir.typecast"(%208, %209) <{conservative_folding = false}> : (tensor<1x12x1x128xf32>, tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xbf16>
    %211 = ttir.empty() : tensor<1x12x1x128xbf16>
    %212 = "ttir.add"(%192, %210, %211) : (tensor<1x12x1x128xbf16>, tensor<1x12x1x128xbf16>, tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xbf16>
    %213 = ttir.empty() : tensor<12x1x128xbf16>
    %214 = "ttir.reshape"(%212, %213) <{shape = [12 : i32, 1 : i32, 128 : i32]}> : (tensor<1x12x1x128xbf16>, tensor<12x1x128xbf16>) -> tensor<12x1x128xbf16>
    %215 = ttir.empty() : tensor<1x4x1x128x128xbf16>
    %216 = "ttir.reshape"(%166, %215) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x4x128x128xbf16>, tensor<1x4x1x128x128xbf16>) -> tensor<1x4x1x128x128xbf16>
    %217 = ttir.empty() : tensor<1x4x3x128x128xbf16>
    %218 = "ttir.broadcast"(%216, %217) <{broadcast_dimensions = array<i64: 1, 1, 3, 1, 1>}> : (tensor<1x4x1x128x128xbf16>, tensor<1x4x3x128x128xbf16>) -> tensor<1x4x3x128x128xbf16>
    %219 = ttir.empty() : tensor<1x12x128x128xbf16>
    %220 = "ttir.reshape"(%218, %219) <{shape = [1 : i32, 12 : i32, 128 : i32, 128 : i32]}> : (tensor<1x4x3x128x128xbf16>, tensor<1x12x128x128xbf16>) -> tensor<1x12x128x128xbf16>
    %221 = ttir.empty() : tensor<1x12x128x128xbf16>
    %222 = "ttir.permute"(%220, %221) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x12x128x128xbf16>, tensor<1x12x128x128xbf16>) -> tensor<1x12x128x128xbf16>
    %223 = ttir.empty() : tensor<12x128x128xbf16>
    %224 = "ttir.reshape"(%222, %223) <{shape = [12 : i32, 128 : i32, 128 : i32]}> : (tensor<1x12x128x128xbf16>, tensor<12x128x128xbf16>) -> tensor<12x128x128xbf16>
    %225 = "ttir.dot_general"(%214, %224) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<12x1x128xbf16>, tensor<12x128x128xbf16>) -> tensor<12x1x128xbf16>
    %226 = ttir.empty() : tensor<12x1x128xf32>
    %227 = "ttir.typecast"(%225, %226) <{conservative_folding = false}> : (tensor<12x1x128xbf16>, tensor<12x1x128xf32>) -> tensor<12x1x128xf32>
    %228 = ttir.empty() : tensor<1x12x1x128xf32>
    %229 = "ttir.reshape"(%227, %228) <{shape = [1 : i32, 12 : i32, 1 : i32, 128 : i32]}> : (tensor<12x1x128xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %230 = ttir.empty() : tensor<1x1x1x1xf32>
    %231 = "ttir.reshape"(%33, %230) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %232 = ttir.empty() : tensor<1x12x1x128xf32>
    %233 = "ttir.broadcast"(%231, %232) <{broadcast_dimensions = array<i64: 1, 12, 1, 128>}> : (tensor<1x1x1x1xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %234 = ttir.empty() : tensor<1x12x1x128xf32>
    %235 = "ttir.multiply"(%229, %233, %234) : (tensor<1x12x1x128xf32>, tensor<1x12x1x128xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %236 = ttir.empty() : tensor<1x12x1x128xbf16>
    %237 = "ttir.typecast"(%235, %236) <{conservative_folding = false}> : (tensor<1x12x1x128xf32>, tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xbf16>
    %238 = ttir.empty() : tensor<1x128xf32>
    %239 = "ttir.typecast"(%31, %238) <{conservative_folding = false}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %240 = ttir.empty() : tensor<1x128xi64>
    %241 = "ttir.reshape"(%29, %240) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xi64>, tensor<1x128xi64>) -> tensor<1x128xi64>
    %242 = ttir.empty() : tensor<1x1xi64>
    %243 = "ttir.reshape"(%1, %242) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xi64>, tensor<1x1xi64>) -> tensor<1x1xi64>
    %244 = ttir.empty() : tensor<1x128xi64>
    %245 = "ttir.broadcast"(%243, %244) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xi64>, tensor<1x128xi64>) -> tensor<1x128xi64>
    %246 = ttir.empty() : tensor<1x128xi1>
    %247 = "ttir.gt"(%241, %245, %246) : (tensor<1x128xi64>, tensor<1x128xi64>, tensor<1x128xi1>) -> tensor<1x128xi1>
    %248 = ttir.empty() : tensor<1x128xf32>
    %249 = "ttir.typecast"(%247, %248) <{conservative_folding = false}> : (tensor<1x128xi1>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %250 = ttir.empty() : tensor<1x128xf32>
    %251 = "ttir.multiply"(%239, %249, %250) : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %252 = ttir.empty() : tensor<1x128xbf16>
    %253 = "ttir.typecast"(%251, %252) <{conservative_folding = false}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %254 = ttir.empty() : tensor<1x1x128xbf16>
    %255 = "ttir.reshape"(%253, %254) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16>
    %256 = ttir.empty() : tensor<1x1x1x128xbf16>
    %257 = "ttir.reshape"(%255, %256) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xbf16>, tensor<1x1x1x128xbf16>) -> tensor<1x1x1x128xbf16>
    %258 = ttir.empty() : tensor<1x12x1x128xbf16>
    %259 = "ttir.broadcast"(%257, %258) <{broadcast_dimensions = array<i64: 1, 12, 1, 1>}> : (tensor<1x1x1x128xbf16>, tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xbf16>
    %260 = ttir.empty() : tensor<1x12x1x128xbf16>
    %261 = "ttir.add"(%237, %259, %260) : (tensor<1x12x1x128xbf16>, tensor<1x12x1x128xbf16>, tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xbf16>
    %262 = ttir.empty() : tensor<1x12x1x128xf32>
    %263 = "ttir.typecast"(%261, %262) <{conservative_folding = false}> : (tensor<1x12x1x128xbf16>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %264 = ttir.empty() : tensor<1x12x1xf32>
    %265 = "ttir.max"(%263, %264) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x12x1x128xf32>, tensor<1x12x1xf32>) -> tensor<1x12x1xf32>
    %266 = ttir.empty() : tensor<1x12x1x1xf32>
    %267 = "ttir.reshape"(%265, %266) <{shape = [1 : i32, 12 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x1xf32>, tensor<1x12x1x1xf32>) -> tensor<1x12x1x1xf32>
    %268 = ttir.empty() : tensor<1x12x1x128xf32>
    %269 = "ttir.broadcast"(%267, %268) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<1x12x1x1xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %270 = ttir.empty() : tensor<1x12x1x128xf32>
    %271 = "ttir.subtract"(%263, %269, %270) : (tensor<1x12x1x128xf32>, tensor<1x12x1x128xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %272 = ttir.empty() : tensor<1x12x1x128xf32>
    %273 = "ttir.exp"(%271, %272) : (tensor<1x12x1x128xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %274 = ttir.empty() : tensor<1x12x1xf32>
    %275 = "ttir.sum"(%273, %274) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x12x1x128xf32>, tensor<1x12x1xf32>) -> tensor<1x12x1xf32>
    %276 = ttir.empty() : tensor<1x12x1x1xf32>
    %277 = "ttir.reshape"(%275, %276) <{shape = [1 : i32, 12 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x1xf32>, tensor<1x12x1x1xf32>) -> tensor<1x12x1x1xf32>
    %278 = ttir.empty() : tensor<1x12x1x128xf32>
    %279 = "ttir.broadcast"(%277, %278) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<1x12x1x1xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %280 = ttir.empty() : tensor<1x12x1x128xf32>
    %281 = "ttir.div"(%273, %279, %280) : (tensor<1x12x1x128xf32>, tensor<1x12x1x128xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %282 = ttir.empty() : tensor<1x12x1x128xbf16>
    %283 = "ttir.typecast"(%281, %282) <{conservative_folding = false}> : (tensor<1x12x1x128xf32>, tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xbf16>
    %284 = ttir.empty() : tensor<12x1x128xbf16>
    %285 = "ttir.reshape"(%283, %284) <{shape = [12 : i32, 1 : i32, 128 : i32]}> : (tensor<1x12x1x128xbf16>, tensor<12x1x128xbf16>) -> tensor<12x1x128xbf16>
    %286 = ttir.empty() : tensor<1x4x1x128x128xbf16>
    %287 = "ttir.reshape"(%173, %286) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x4x128x128xbf16>, tensor<1x4x1x128x128xbf16>) -> tensor<1x4x1x128x128xbf16>
    %288 = ttir.empty() : tensor<1x4x3x128x128xbf16>
    %289 = "ttir.broadcast"(%287, %288) <{broadcast_dimensions = array<i64: 1, 1, 3, 1, 1>}> : (tensor<1x4x1x128x128xbf16>, tensor<1x4x3x128x128xbf16>) -> tensor<1x4x3x128x128xbf16>
    %290 = ttir.empty() : tensor<12x128x128xbf16>
    %291 = "ttir.reshape"(%289, %290) <{shape = [12 : i32, 128 : i32, 128 : i32]}> : (tensor<1x4x3x128x128xbf16>, tensor<12x128x128xbf16>) -> tensor<12x128x128xbf16>
    %292 = "ttir.dot_general"(%285, %291) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<12x1x128xbf16>, tensor<12x128x128xbf16>) -> tensor<12x1x128xbf16>
    %293 = ttir.empty() : tensor<1x1536xbf16>
    %294 = "ttir.reshape"(%292, %293) <{shape = [1 : i32, 1536 : i32]}> : (tensor<12x1x128xbf16>, tensor<1x1536xbf16>) -> tensor<1x1536xbf16>
    %295 = ttir.empty() : tensor<1536x3072xbf16>
    %296 = "ttir.permute"(%27, %295) <{permutation = array<i64: 1, 0>}> : (tensor<3072x1536xbf16>, tensor<1536x3072xbf16>) -> tensor<1536x3072xbf16>
    %297 = "ttir.dot_general"(%294, %296) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x1536xbf16>, tensor<1536x3072xbf16>) -> tensor<1x3072xbf16>
    %298 = ttir.empty() : tensor<1x3072xbf16>
    %299 = "ttir.all_reduce"(%297, %298) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<1x3072xbf16>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %300 = ttir.empty() : tensor<1x1x3072xbf16>
    %301 = "ttir.reshape"(%299, %300) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %302 = ttir.empty() : tensor<1x1x3072xbf16>
    %303 = "ttir.add"(%72, %301, %302) : (tensor<1x1x3072xbf16>, tensor<1x1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %304 = ttir.empty() : tensor<3072xf32>
    %305 = "ttir.typecast"(%37, %304) <{conservative_folding = false}> : (tensor<3072xbf16>, tensor<3072xf32>) -> tensor<3072xf32>
    %306 = ttir.empty() : tensor<1x1x3072xf32>
    %307 = "ttir.reshape"(%305, %306) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %308 = ttir.empty() : tensor<1x1x3072xf32>
    %309 = "ttir.typecast"(%303, %308) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %310 = ttir.empty() : tensor<1x1x3072xf32>
    %311 = "ttir.pow"(%309, %50, %310) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %312 = ttir.empty() : tensor<1x1xf32>
    %313 = "ttir.sum"(%311, %312) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %314 = ttir.empty() : tensor<1x1xf32>
    %315 = "ttir.multiply"(%313, %45, %314) : (tensor<1x1xf32>, tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %316 = ttir.empty() : tensor<1x1x1xf32>
    %317 = "ttir.reshape"(%315, %316) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %318 = ttir.empty() : tensor<1x1x1xf32>
    %319 = "ttir.add"(%317, %84, %318) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %320 = ttir.empty() : tensor<1x1x1xf32>
    %321 = "ttir.rsqrt"(%319, %320) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %322 = ttir.empty() : tensor<1x1xf32>
    %323 = "ttir.reshape"(%321, %322) <{shape = [1 : i32, 1 : i32]}> : (tensor<1x1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %324 = ttir.empty() : tensor<1x1x1xf32>
    %325 = "ttir.reshape"(%323, %324) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %326 = ttir.empty() : tensor<1x1x3072xf32>
    %327 = "ttir.broadcast"(%325, %326) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x1x1xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %328 = ttir.empty() : tensor<1x1x3072xf32>
    %329 = "ttir.multiply"(%309, %327, %328) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %330 = ttir.empty() : tensor<1x1x3072xbf16>
    %331 = "ttir.typecast"(%329, %330) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %332 = ttir.empty() : tensor<1x1x3072xf32>
    %333 = "ttir.typecast"(%331, %332) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %334 = ttir.empty() : tensor<1x1x3072xf32>
    %335 = "ttir.multiply"(%307, %333, %334) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %336 = ttir.empty() : tensor<1x1x3072xbf16>
    %337 = "ttir.typecast"(%335, %336) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %338 = ttir.empty() : tensor<1x3072xbf16>
    %339 = "ttir.reshape"(%337, %338) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x3072xbf16>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %340 = ttir.empty() : tensor<3072x4096xbf16>
    %341 = "ttir.permute"(%39, %340) <{permutation = array<i64: 1, 0>}> : (tensor<4096x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<3072x4096xbf16>
    %342 = "ttir.dot_general"(%339, %341) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<1x4096xbf16>
    %343 = ttir.empty() : tensor<1x1x4096xbf16>
    %344 = "ttir.reshape"(%342, %343) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<1x4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
    %345 = ttir.empty() : tensor<1x1x4096xf32>
    %346 = "ttir.typecast"(%344, %345) <{conservative_folding = false}> : (tensor<1x1x4096xbf16>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %347 = ttir.empty() : tensor<1x1x4096xbf16>
    %348 = "ttir.sigmoid"(%344, %347) : (tensor<1x1x4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
    %349 = ttir.empty() : tensor<1x1x4096xf32>
    %350 = "ttir.typecast"(%348, %349) <{conservative_folding = false}> : (tensor<1x1x4096xbf16>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %351 = ttir.empty() : tensor<1x1x4096xf32>
    %352 = "ttir.multiply"(%346, %350, %351) : (tensor<1x1x4096xf32>, tensor<1x1x4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %353 = ttir.empty() : tensor<1x1x4096xbf16>
    %354 = "ttir.typecast"(%352, %353) <{conservative_folding = false}> : (tensor<1x1x4096xf32>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
    %355 = ttir.empty() : tensor<1x1x4096xf32>
    %356 = "ttir.typecast"(%354, %355) <{conservative_folding = false}> : (tensor<1x1x4096xbf16>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %357 = ttir.empty() : tensor<3072x4096xbf16>
    %358 = "ttir.permute"(%25, %357) <{permutation = array<i64: 1, 0>}> : (tensor<4096x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<3072x4096xbf16>
    %359 = "ttir.dot_general"(%339, %358) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<1x4096xbf16>
    %360 = ttir.empty() : tensor<1x4096xf32>
    %361 = "ttir.typecast"(%359, %360) <{conservative_folding = false}> : (tensor<1x4096xbf16>, tensor<1x4096xf32>) -> tensor<1x4096xf32>
    %362 = ttir.empty() : tensor<1x1x4096xf32>
    %363 = "ttir.reshape"(%361, %362) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<1x4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %364 = ttir.empty() : tensor<1x1x4096xf32>
    %365 = "ttir.multiply"(%356, %363, %364) : (tensor<1x1x4096xf32>, tensor<1x1x4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %366 = ttir.empty() : tensor<1x1x4096xbf16>
    %367 = "ttir.typecast"(%365, %366) <{conservative_folding = false}> : (tensor<1x1x4096xf32>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
    %368 = ttir.empty() : tensor<1x4096xbf16>
    %369 = "ttir.reshape"(%367, %368) <{shape = [1 : i32, 4096 : i32]}> : (tensor<1x1x4096xbf16>, tensor<1x4096xbf16>) -> tensor<1x4096xbf16>
    %370 = ttir.empty() : tensor<4096x3072xbf16>
    %371 = "ttir.permute"(%23, %370) <{permutation = array<i64: 1, 0>}> : (tensor<3072x4096xbf16>, tensor<4096x3072xbf16>) -> tensor<4096x3072xbf16>
    %372 = "ttir.dot_general"(%369, %371) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x4096xbf16>, tensor<4096x3072xbf16>) -> tensor<1x3072xbf16>
    %373 = ttir.empty() : tensor<1x3072xbf16>
    %374 = "ttir.all_reduce"(%372, %373) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<1x3072xbf16>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %375 = ttir.empty() : tensor<1x1x3072xbf16>
    %376 = "ttir.reshape"(%374, %375) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %377 = ttir.empty() : tensor<1x1x3072xbf16>
    %378 = "ttir.add"(%303, %376, %377) : (tensor<1x1x3072xbf16>, tensor<1x1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %379 = ttir.empty() : tensor<1x1x3072xf32>
    %380 = "ttir.typecast"(%378, %379) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %381 = ttir.empty() : tensor<1x1x3072xf32>
    %382 = "ttir.pow"(%380, %50, %381) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %383 = ttir.empty() : tensor<1x1xf32>
    %384 = "ttir.sum"(%382, %383) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %385 = ttir.empty() : tensor<1x1xf32>
    %386 = "ttir.multiply"(%384, %45, %385) : (tensor<1x1xf32>, tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %387 = ttir.empty() : tensor<1x1x1xf32>
    %388 = "ttir.reshape"(%386, %387) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %389 = ttir.empty() : tensor<1x1x1xf32>
    %390 = "ttir.add"(%388, %84, %389) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %391 = ttir.empty() : tensor<1x1x1xf32>
    %392 = "ttir.rsqrt"(%390, %391) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %393 = ttir.empty() : tensor<1x1xf32>
    %394 = "ttir.reshape"(%392, %393) <{shape = [1 : i32, 1 : i32]}> : (tensor<1x1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %395 = ttir.empty() : tensor<1x1x1xf32>
    %396 = "ttir.reshape"(%394, %395) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %397 = ttir.empty() : tensor<1x1x3072xf32>
    %398 = "ttir.broadcast"(%396, %397) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x1x1xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %399 = ttir.empty() : tensor<1x1x3072xf32>
    %400 = "ttir.multiply"(%380, %398, %399) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %401 = ttir.empty() : tensor<1x1x3072xbf16>
    %402 = "ttir.typecast"(%400, %401) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %403 = ttir.empty() : tensor<1x1x3072xf32>
    %404 = "ttir.typecast"(%402, %403) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %405 = ttir.empty() : tensor<1x1x3072xf32>
    %406 = "ttir.multiply"(%177, %404, %405) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %407 = ttir.empty() : tensor<1x1x3072xbf16>
    %408 = "ttir.typecast"(%406, %407) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %409 = ttir.empty() : tensor<1x3072xbf16>
    %410 = "ttir.reshape"(%408, %409) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x3072xbf16>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %411 = ttir.empty() : tensor<3072x128256xbf16>
    %412 = "ttir.permute"(%11, %411) <{permutation = array<i64: 1, 0>}> : (tensor<128256x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<3072x128256xbf16>
    %413 = "ttir.dot_general"(%410, %412) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<1x128256xbf16>
    %414 = ttir.empty() : tensor<1x1x128256xbf16>
    %415 = "ttir.reshape"(%413, %414) <{shape = [1 : i32, 1 : i32, 128256 : i32]}> : (tensor<1x128256xbf16>, tensor<1x1x128256xbf16>) -> tensor<1x1x128256xbf16>
    %416 = ttir.empty() : tensor<1x8x128x128xbf16>
    %417 = "ttir.mesh_shard"(%166, %416) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x4x128x128xbf16>, tensor<1x8x128x128xbf16>) -> tensor<1x8x128x128xbf16>
    %418 = ttir.empty() : tensor<1x8x128x128xbf16>
    %419 = "ttir.mesh_shard"(%173, %418) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x4x128x128xbf16>, tensor<1x8x128x128xbf16>) -> tensor<1x8x128x128xbf16>
    %420 = ttir.empty() : tensor<1x128256xbf16>
    %421 = "ttir.mesh_shard"(%413, %420) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x128256xbf16>, tensor<1x128256xbf16>) -> tensor<1x128256xbf16>
    %422 = ttir.empty() : tensor<1x1x128256xbf16>
    %423 = "ttir.mesh_shard"(%415, %422) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x1x128256xbf16>, tensor<1x1x128256xbf16>) -> tensor<1x1x128256xbf16>
    return %417, %419, %421, %423 : tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x128256xbf16>, tensor<1x1x128256xbf16>
  }
}
module @SyncTensorsGraph.370 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.370 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99776, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073184736, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99776, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073193056, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x16, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 2)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 2, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main_const_eval_0() -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %2 : tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main(%arg0: tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<128xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg7) <{force = false}> : (tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %2 = "ttnn.full"(%1) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 3.25520843E-4 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %3 = "ttnn.mesh_shard"(%arg0, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %4 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %5 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %6 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %7 = "ttnn.mesh_shard"(%arg4, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %8 = "ttnn.mesh_shard"(%arg5, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg5) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %9 = "ttnn.mesh_shard"(%arg6, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg6) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %10 = "ttnn.mesh_shard"(%arg8, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg8) <{force = false}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %11 = "ttnn.mesh_shard"(%arg9, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg9) <{force = false}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %12 = "ttnn.mesh_shard"(%arg10, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg10) <{force = false}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %13 = "ttnn.mesh_shard"(%arg11, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg11) <{force = false}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %14 = "ttnn.mesh_shard"(%arg12, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg12) <{force = false}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %15 = "ttnn.mesh_shard"(%arg13, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg13) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %16 = "ttnn.mesh_shard"(%arg14, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<128xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg14) <{force = false}> : (tensor<128xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %17 = "ttnn.mesh_shard"(%arg15, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg15) <{force = false}> : (tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %18 = "ttnn.mesh_shard"(%arg16, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg16) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %19 = "ttnn.mesh_shard"(%arg17, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg17) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %20 = "ttnn.mesh_shard"(%arg18, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg18) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %21 = "ttnn.mesh_shard"(%arg19, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg19) <{force = false}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %22 = "ttnn.mesh_shard"(%arg20, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg20) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %23 = "ttnn.typecast"(%9) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %24 = "ttnn.reshape"(%23) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%23) <{force = false}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %25 = "ttnn.typecast"(%7) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %26 = "ttnn.reshape"(%25) <{shape = [1 : i32]}> : (tensor<1x1xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%25) <{force = false}> : (tensor<1x1xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %27 = "ttnn.from_device"(%26) : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%26) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %28 = "ttnn.to_layout"(%27) <{layout = #ttnn.layout<row_major>}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%27) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> ()
        %29 = "ttnn.to_device"(%28, %1) <{memory_config = #ttnn.memory_config<<dram>, <interleaved>>}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%28) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<system_memory>>>>) -> ()
        %30 = "ttnn.embedding"(%29, %8) : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%29) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %31 = "ttnn.typecast"(%30) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %32 = "ttnn.reshape"(%31) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %33 = "ttnn.pow"(%32, %0) : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%32) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %34 = "ttnn.sum"(%33) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%33) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %35 = "ttnn.multiply"(%34, %2) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%34) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %36 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %37 = "ttnn.add"(%35, %36) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%36) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%35) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %38 = "ttnn.rsqrt"(%37) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%37) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %39 = "ttnn.multiply"(%31, %38) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%38) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%31) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %40 = "ttnn.multiply"(%24, %39) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%39) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%24) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %41 = "ttnn.typecast"(%40) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%40) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %42 = "ttnn.matmul"(%41, %5) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %43 = "ttnn.reshape"(%42) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32]}> : (tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%42) <{force = false}> : (tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %44 = "ttnn.typecast"(%43) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %45 = "ttnn.reshape"(%4) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %46 = "ttnn.typecast"(%3) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %47 = "ttnn.reshape"(%46) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%46) <{force = false}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %48 = "ttnn.matmul"(%45, %47) <{transpose_a = false, transpose_b = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%47) <{force = false}> : (tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%45) <{force = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %49 = "ttnn.reshape"(%48) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%48) <{force = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %50 = "ttnn.concat"(%49, %49) <{dim = 2 : si32}> : (tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%49) <{force = false}> : (tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %51 = "ttnn.cos"(%50) : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %52 = "ttnn.reshape"(%51) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %53 = "ttnn.multiply"(%44, %52) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%52) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%44) <{force = false}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %54 = "ttnn.typecast"(%53) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%53) <{force = false}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %55 = "ttnn.slice_static"(%43) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %56 = "ttnn.neg"(%55) : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%55) <{force = false}> : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %57 = "ttnn.slice_static"(%43) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%43) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %58 = "ttnn.concat"(%56, %57) <{dim = 3 : si32}> : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%57) <{force = false}> : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%56) <{force = false}> : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %59 = "ttnn.typecast"(%58) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%58) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %60 = "ttnn.sin"(%50) : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%50) <{force = false}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %61 = "ttnn.reshape"(%60) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %62 = "ttnn.multiply"(%59, %61) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%61) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%59) <{force = false}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %63 = "ttnn.typecast"(%62) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%62) <{force = false}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %64 = "ttnn.add"(%54, %63) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%63) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%54) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %65 = "ttnn.typecast"(%arg0) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.update_cache"(%10, %64, %65) <{batch_offset = 0 : i32}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%65) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%64) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %66 = "ttnn.matmul"(%41, %11) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %67 = "ttnn.reshape"(%66) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32]}> : (tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%66) <{force = false}> : (tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %68 = "ttnn.typecast"(%arg0) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.update_cache"(%12, %67, %68) <{batch_offset = 0 : i32}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%68) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%67) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %69 = "ttnn.typecast"(%22) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%22) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %70 = "ttnn.reshape"(%69) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%69) <{force = false}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %71 = "ttnn.matmul"(%41, %19) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%41) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %72 = "ttnn.reshape"(%71) <{shape = [1 : i32, 12 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %73 = "ttnn.typecast"(%71) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1536xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%71) <{force = false}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %74 = "ttnn.reshape"(%73) <{shape = [12 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1536xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%73) <{force = false}> : (tensor<1x1536xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %75 = "ttnn.multiply"(%74, %51) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%74) <{force = false}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%51) <{force = false}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %76 = "ttnn.typecast"(%75) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%75) <{force = false}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %77 = "ttnn.slice_static"(%72) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 12 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %78 = "ttnn.neg"(%77) : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%77) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %79 = "ttnn.reshape"(%78) <{shape = [12 : i32, 1 : i32, 64 : i32]}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%78) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %80 = "ttnn.slice_static"(%72) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 12 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%72) <{force = false}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %81 = "ttnn.reshape"(%80) <{shape = [12 : i32, 1 : i32, 64 : i32]}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%80) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %82 = "ttnn.concat"(%79, %81) <{dim = 2 : si32}> : (tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%81) <{force = false}> : (tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%79) <{force = false}> : (tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %83 = "ttnn.typecast"(%82) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%82) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %84 = "ttnn.multiply"(%83, %60) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%83) <{force = false}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%60) <{force = false}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %85 = "ttnn.typecast"(%84) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%84) <{force = false}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %86 = "ttnn.add"(%76, %85) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%85) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%76) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %87 = "ttnn.reshape"(%10) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 512 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %88 = "ttnn.repeat"(%87) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x4x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 512 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1536 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%87) <{force = false}> : (tensor<1x4x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 512 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %89 = "ttnn.reshape"(%88) <{shape = [1 : i32, 12 : i32, 128 : i32, 128 : i32]}> : (tensor<1x4x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1536 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%88) <{force = false}> : (tensor<1x4x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1536 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %90 = "ttnn.permute"(%89) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%89) <{force = false}> : (tensor<1x12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %91 = "ttnn.reshape"(%90) <{shape = [12 : i32, 128 : i32, 128 : i32]}> : (tensor<1x12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%90) <{force = false}> : (tensor<1x12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %92 = "ttnn.matmul"(%86, %91) <{transpose_a = false, transpose_b = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%91) <{force = false}> : (tensor<12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%86) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %93 = "ttnn.typecast"(%92) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%92) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %94 = "ttnn.reshape"(%93) <{shape = [1 : i32, 12 : i32, 1 : i32, 128 : i32]}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%93) <{force = false}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %95 = "ttnn.reshape"(%18) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %96 = "ttnn.multiply"(%94, %95) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%95) <{force = false}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%94) <{force = false}> : (tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %97 = "ttnn.typecast"(%96) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%96) <{force = false}> : (tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %98 = "ttnn.typecast"(%17) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %99 = "ttnn.reshape"(%98) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%98) <{force = false}> : (tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %100 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %101 = "ttnn.reshape"(%16) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<128xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %102 = "ttnn.typecast"(%101) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x128xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%101) <{force = false}> : (tensor<1x1x1x128xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %103 = "ttnn.typecast"(%100) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%100) <{force = false}> : (tensor<1x1x1x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %104 = "ttnn.gt"(%102, %103) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%103) <{force = false}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%102) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %105 = "ttnn.typecast"(%104) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%104) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %106 = "ttnn.typecast"(%105) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%105) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %107 = "ttnn.multiply"(%99, %106) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%106) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%99) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %108 = "ttnn.typecast"(%107) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%107) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %109 = "ttnn.add"(%97, %108) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%108) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%97) <{force = false}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %110 = "ttnn.typecast"(%109) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%109) <{force = false}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %111 = "ttnn.max"(%110) <{dim_arg = [3 : i32], keep_dim = true}> : (tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %112 = "ttnn.neg"(%111) : (tensor<1x12x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%111) <{force = false}> : (tensor<1x12x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %113 = "ttnn.add"(%110, %112) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x12x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%112) <{force = false}> : (tensor<1x12x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%110) <{force = false}> : (tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %114 = "ttnn.softmax"(%113) <{dimension = 3 : si32}> : (tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%113) <{force = false}> : (tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %115 = "ttnn.typecast"(%114) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%114) <{force = false}> : (tensor<1x12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %116 = "ttnn.reshape"(%115) <{shape = [12 : i32, 1 : i32, 128 : i32]}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%115) <{force = false}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %117 = "ttnn.reshape"(%12) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 512 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %118 = "ttnn.repeat"(%117) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x4x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 512 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1536 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%117) <{force = false}> : (tensor<1x4x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 512 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %119 = "ttnn.reshape"(%118) <{shape = [12 : i32, 128 : i32, 128 : i32]}> : (tensor<1x4x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1536 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%118) <{force = false}> : (tensor<1x4x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1536 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %120 = "ttnn.matmul"(%116, %119) <{transpose_a = false, transpose_b = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%119) <{force = false}> : (tensor<12x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%116) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %121 = "ttnn.reshape"(%120) <{shape = [1 : i32, 1536 : i32]}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%120) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %122 = "ttnn.matmul"(%121, %15) <{transpose_a = false, transpose_b = true}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%121) <{force = false}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %123 = "ttnn.reshape"(%122) <{shape = [1 : i32, 1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%122) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %124 = "ttnn.reduce_scatter"(%123, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 3 : si32}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%123) <{force = false}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %125 = "ttnn.all_gather"(%124, %1) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%124) <{force = false}> : (tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %126 = "ttnn.reshape"(%125) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%125) <{force = false}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %127 = "ttnn.add"(%30, %126) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%126) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%30) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %128 = "ttnn.typecast"(%20) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %129 = "ttnn.reshape"(%128) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%128) <{force = false}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %130 = "ttnn.typecast"(%127) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %131 = "ttnn.reshape"(%130) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %132 = "ttnn.pow"(%131, %0) : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%131) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %133 = "ttnn.sum"(%132) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%132) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %134 = "ttnn.multiply"(%133, %2) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%133) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %135 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %136 = "ttnn.add"(%134, %135) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%135) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%134) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %137 = "ttnn.rsqrt"(%136) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%136) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %138 = "ttnn.multiply"(%130, %137) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%137) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%130) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %139 = "ttnn.multiply"(%129, %138) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%138) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%129) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %140 = "ttnn.typecast"(%139) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%139) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %141 = "ttnn.matmul"(%140, %21) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%21) <{force = false}> : (tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %142 = "ttnn.typecast"(%141) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %143 = "ttnn.sigmoid"(%141) : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%141) <{force = false}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %144 = "ttnn.typecast"(%143) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%143) <{force = false}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %145 = "ttnn.multiply"(%142, %144) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%144) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%142) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %146 = "ttnn.matmul"(%140, %14) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%140) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %147 = "ttnn.typecast"(%146) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%146) <{force = false}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %148 = "ttnn.multiply"(%145, %147) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%147) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%145) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %149 = "ttnn.typecast"(%148) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%148) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %150 = "ttnn.matmul"(%149, %13) <{transpose_a = false, transpose_b = true}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%149) <{force = false}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %151 = "ttnn.reshape"(%150) <{shape = [1 : i32, 1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%150) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %152 = "ttnn.reduce_scatter"(%151, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 3 : si32}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%151) <{force = false}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %153 = "ttnn.all_gather"(%152, %1) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%152) <{force = false}> : (tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %154 = "ttnn.reshape"(%153) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%153) <{force = false}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %155 = "ttnn.add"(%127, %154) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%154) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%127) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %156 = "ttnn.typecast"(%155) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%155) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %157 = "ttnn.reshape"(%156) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %158 = "ttnn.pow"(%157, %0) : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%157) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %159 = "ttnn.sum"(%158) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%158) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %160 = "ttnn.multiply"(%159, %2) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%159) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %161 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %162 = "ttnn.add"(%160, %161) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%161) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%160) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %163 = "ttnn.rsqrt"(%162) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%162) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %164 = "ttnn.multiply"(%156, %163) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%163) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%156) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %165 = "ttnn.multiply"(%70, %164) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%164) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%70) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %166 = "ttnn.typecast"(%165) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%165) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %167 = "ttnn.matmul"(%166, %8) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%166) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %168 = "ttnn.reshape"(%167) <{shape = [1 : i32, 1 : i32, 128256 : i32]}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %169 = "ttnn.to_layout"(%10) <{layout = #ttnn.layout<row_major>}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %170 = "ttnn.from_device"(%169) : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%169) <{force = false}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %171 = "ttnn.mesh_shard"(%170, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%170) <{force = false}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<system_memory>>>>) -> ()
        %172 = "ttnn.to_layout"(%12) <{layout = #ttnn.layout<row_major>}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %173 = "ttnn.from_device"(%172) : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%172) <{force = false}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %174 = "ttnn.mesh_shard"(%173, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%173) <{force = false}> : (tensor<1x4x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 128 + d2, d3), <1x1>, memref<512x128xbf16, #ttnn.buffer_type<system_memory>>>>) -> ()
        %175 = "ttnn.to_layout"(%167) <{layout = #ttnn.layout<row_major>}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%167) <{force = false}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %176 = "ttnn.from_device"(%175) : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%175) <{force = false}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %177 = "ttnn.mesh_shard"(%176, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%176) <{force = false}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>) -> ()
        %178 = "ttnn.to_layout"(%168) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%168) <{force = false}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %179 = "ttnn.from_device"(%178) : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%178) <{force = false}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %180 = "ttnn.mesh_shard"(%179, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%179) <{force = false}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>) -> ()
        return %171, %174, %177, %180 : tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>, tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>, tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>, tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>
      }
    }
  }
}
Generated tokens: [' the', ' the']
input 0: device = xla:0, shape torch.Size([128]) and shard spec {replicated}
input 1: device = xla:0, shape torch.Size([1, 128]) and shard spec {replicated}
input 2: device = xla:0, shape torch.Size([1, 8, 128, 128]) and shard spec {replicated}
[STATIC CACHE DUMP ]mean along seqlen for static cache @ input idx 2 and shape torch.Size([1, 8, 128, 128]): tensor([ 0.0576,  0.1055,  0.0986, -0.0315, -0.0776, -0.2383, -0.0459,  0.0461,
         0.0938,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
       dtype=torch.bfloat16)
input 3: device = xla:0, shape torch.Size([1, 8, 128, 128]) and shard spec {replicated}
[STATIC CACHE DUMP ]mean along seqlen for static cache @ input idx 3 and shape torch.Size([1, 8, 128, 128]): tensor([ 3.0899e-04, -1.0620e-02,  6.9580e-03,  1.8597e-05, -6.0120e-03,
         1.4648e-03,  2.5940e-03,  2.5787e-03,  2.5787e-03,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=torch.bfloat16)
input 4: device = xla:0, shape torch.Size([1, 1]) and shard spec {replicated}
input 5: device = xla:0, shape torch.Size([128256, 3072]) and shard spec {replicated}
input 6: device = xla:0, shape torch.Size([1]) and shard spec {replicated}
input 7: device = xla:0, shape torch.Size([64]) and shard spec {replicated}
input 8: device = xla:0, shape torch.Size([3072]) and shard spec {replicated}
input 9: device = xla:0, shape torch.Size([3072, 3072]) and shard spec {devices=[2,1]<=[2]}
input 10: device = xla:0, shape torch.Size([1024, 3072]) and shard spec {devices=[2,1]<=[2]}
input 11: device = xla:0, shape torch.Size([1024, 3072]) and shard spec {devices=[2,1]<=[2]}
input 12: device = xla:0, shape torch.Size([3072, 3072]) and shard spec {devices=[1,2]<=[2]}
input 13: device = xla:0, shape torch.Size([3072]) and shard spec {replicated}
input 14: device = xla:0, shape torch.Size([8192, 3072]) and shard spec {devices=[2,1]<=[2]}
input 15: device = xla:0, shape torch.Size([8192, 3072]) and shard spec {devices=[2,1]<=[2]}
input 16: device = xla:0, shape torch.Size([3072, 8192]) and shard spec {devices=[1,2]<=[2]}
input 17: device = xla:0, shape torch.Size([3072]) and shard spec {replicated}
CPU logit shape:  torch.Size([1, 7, 128256]) XLA logit shape:  torch.Size([1, 1, 128256])
 the[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
Generated tokens: [' the', ' the', ' the']
input 0: device = xla:0, shape torch.Size([128]) and shard spec {replicated}
input 1: device = xla:0, shape torch.Size([1, 128]) and shard spec {replicated}
input 2: device = xla:0, shape torch.Size([1, 8, 128, 128]) and shard spec {replicated}
[STATIC CACHE DUMP ]mean along seqlen for static cache @ input idx 2 and shape torch.Size([1, 8, 128, 128]): tensor([ 0.0576,  0.1055,  0.0986, -0.0315, -0.0776, -0.2383, -0.0459,  0.0461,
         0.0938,  0.0771,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
       dtype=torch.bfloat16)
input 3: device = xla:0, shape torch.Size([1, 8, 128, 128]) and shard spec {replicated}
[STATIC CACHE DUMP ]mean along seqlen for static cache @ input idx 3 and shape torch.Size([1, 8, 128, 128]): tensor([ 3.0899e-04, -1.0620e-02,  6.9580e-03,  1.8597e-05, -6.0120e-03,
         1.4648e-03,  2.5940e-03,  2.5787e-03,  2.5787e-03,  2.5787e-03,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=torch.bfloat16)
input 4: device = xla:0, shape torch.Size([1, 1]) and shard spec {replicated}
input 5: device = xla:0, shape torch.Size([128256, 3072]) and shard spec {replicated}
input 6: device = xla:0, shape torch.Size([1]) and shard spec {replicated}
input 7: device = xla:0, shape torch.Size([64]) and shard spec {replicated}
input 8: device = xla:0, shape torch.Size([3072]) and shard spec {replicated}
input 9: device = xla:0, shape torch.Size([3072, 3072]) and shard spec {devices=[2,1]<=[2]}
input 10: device = xla:0, shape torch.Size([1024, 3072]) and shard spec {devices=[2,1]<=[2]}
input 11: device = xla:0, shape torch.Size([1024, 3072]) and shard spec {devices=[2,1]<=[2]}
input 12: device = xla:0, shape torch.Size([3072, 3072]) and shard spec {devices=[1,2]<=[2]}
input 13: device = xla:0, shape torch.Size([3072]) and shard spec {replicated}
input 14: device = xla:0, shape torch.Size([8192, 3072]) and shard spec {devices=[2,1]<=[2]}
input 15: device = xla:0, shape torch.Size([8192, 3072]) and shard spec {devices=[2,1]<=[2]}
input 16: device = xla:0, shape torch.Size([3072, 8192]) and shard spec {devices=[1,2]<=[2]}
input 17: device = xla:0, shape torch.Size([3072]) and shard spec {replicated}
CPU logit shape:  torch.Size([1, 7, 128256]) XLA logit shape:  torch.Size([1, 1, 128256])
 the[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
Generated tokens: [' the', ' the', ' the', ' the']
input 0: device = xla:0, shape torch.Size([128]) and shard spec {replicated}
input 1: device = xla:0, shape torch.Size([1, 128]) and shard spec {replicated}
input 2: device = xla:0, shape torch.Size([1, 8, 128, 128]) and shard spec {replicated}
[STATIC CACHE DUMP ]mean along seqlen for static cache @ input idx 2 and shape torch.Size([1, 8, 128, 128]): tensor([ 0.0576,  0.1055,  0.0986, -0.0315, -0.0776, -0.2383, -0.0459,  0.0461,
         0.0938,  0.0771,  0.0194,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
       dtype=torch.bfloat16)
input 3: device = xla:0, shape torch.Size([1, 8, 128, 128]) and shard spec {replicated}
[STATIC CACHE DUMP ]mean along seqlen for static cache @ input idx 3 and shape torch.Size([1, 8, 128, 128]): tensor([ 3.0899e-04, -1.0620e-02,  6.9580e-03,  1.8597e-05, -6.0120e-03,
         1.4648e-03,  2.5940e-03,  2.5787e-03,  2.5787e-03,  2.5787e-03,
         2.5787e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=torch.bfloat16)
input 4: device = xla:0, shape torch.Size([1, 1]) and shard spec {replicated}
input 5: device = xla:0, shape torch.Size([128256, 3072]) and shard spec {replicated}
input 6: device = xla:0, shape torch.Size([1]) and shard spec {replicated}
input 7: device = xla:0, shape torch.Size([64]) and shard spec {replicated}
input 8: device = xla:0, shape torch.Size([3072]) and shard spec {replicated}
input 9: device = xla:0, shape torch.Size([3072, 3072]) and shard spec {devices=[2,1]<=[2]}
input 10: device = xla:0, shape torch.Size([1024, 3072]) and shard spec {devices=[2,1]<=[2]}
input 11: device = xla:0, shape torch.Size([1024, 3072]) and shard spec {devices=[2,1]<=[2]}
input 12: device = xla:0, shape torch.Size([3072, 3072]) and shard spec {devices=[1,2]<=[2]}
input 13: device = xla:0, shape torch.Size([3072]) and shard spec {replicated}
input 14: device = xla:0, shape torch.Size([8192, 3072]) and shard spec {devices=[2,1]<=[2]}
input 15: device = xla:0, shape torch.Size([8192, 3072]) and shard spec {devices=[2,1]<=[2]}
input 16: device = xla:0, shape torch.Size([3072, 8192]) and shard spec {devices=[1,2]<=[2]}
input 17: device = xla:0, shape torch.Size([3072]) and shard spec {replicated}
CPU logit shape:  torch.Size([1, 7, 128256]) XLA logit shape:  torch.Size([1, 1, 128256])
 theGenerated tokens: [' the', ' the', ' the', ' the', ' the']
PCCs: [0.999358854626345, 0.5882825894473835, 0.48091930970027535, 0.46284896118395635, 0.44767506700476534]
PASSED

============================== 1 passed in 59.19s ==============================
