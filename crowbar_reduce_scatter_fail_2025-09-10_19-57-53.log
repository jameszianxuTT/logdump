WARNING:root:Defaulting to PJRT_DEVICE=CPU
2025-09-10 19:53:53.124 (   0.000s) [        F1708000]      dylib_platform.cc:47       1| DylibPlatform::SubclassInitialize
2025-09-10 19:53:53.126 (   0.002s) [        F1708000]     client_instance.cc:38       1| ClientInstance::ClientInstance
2025-09-10 19:53:53.126 (   0.002s) [        F1708000]              client.cc:18       1| TTClientInstance::TTClientInstance
2025-09-10 19:53:53.126 (   0.002s) [        F1708000]     client_instance.cc:59       1| ClientInstance::Initialize
2025-09-10 19:53:53.173 | info     |          Device | Opening user mode device driver (tt_cluster.cpp:190)
2025-09-10 19:53:53.287 | info     |   SiliconDriver | Harvesting mask for chip 0 is 0x300 (NOC0: 0x300, simulated harvesting mask: 0x0). (cluster.cpp:344)
2025-09-10 19:53:53.356 | info     |   SiliconDriver | Harvesting mask for chip 1 is 0x210 (NOC0: 0x210, simulated harvesting mask: 0x0). (cluster.cpp:344)
2025-09-10 19:54:03.735 | info     |   SiliconDriver | Opening local chip ids/pci ids: {0}/[3] and remote chip ids {1} (cluster.cpp:204)
2025-09-10 19:54:04.600 | info     |   SiliconDriver | All devices in cluster running firmware version: 18.2.0 (cluster.cpp:185)
2025-09-10 19:54:04.600 | info     |   SiliconDriver | IOMMU: disabled (cluster.cpp:159)
2025-09-10 19:54:04.600 | info     |   SiliconDriver | KMD version: 1.34.0 (cluster.cpp:162)
2025-09-10 19:54:04.600 | info     |   SiliconDriver | Software version 6.0.0, Ethernet FW version 6.14.0 (Device 0) (cluster.cpp:1003)
2025-09-10 19:54:06.906 | info     |   SiliconDriver | Software version 6.0.0, Ethernet FW version 6.14.0 (Device 1) (cluster.cpp:1003)
2025-09-10 19:54:06.912 | info     |   SiliconDriver | Pinning pages for Hugepage: virtual address 0x7fdfc0000000 and size 0x40000000 pinned to physical address 0x3c0000000 (pci_device.cpp:612)
2025-09-10 19:54:06.916 | info     |   SiliconDriver | Pinning pages for Hugepage: virtual address 0x7fdf80000000 and size 0x40000000 pinned to physical address 0x380000000 (pci_device.cpp:612)
2025-09-10 19:54:18.168 | info     |           Metal | While initializing device 0, active ethernet dispatch core (x=18,y=17) detected as still running, issuing exit signal. (metal_context.cpp:628)
2025-09-10 19:54:18.168 | info     |           Metal | While initializing device 0, active ethernet dispatch core (x=25,y=17) detected as still running, issuing exit signal. (metal_context.cpp:628)
2025-09-10 19:54:18.204 | info     |           Metal | While initializing device 1, active ethernet dispatch core (x=18,y=16) detected as still running, issuing exit signal. (metal_context.cpp:628)
2025-09-10 19:54:18.271 | info     |           Metal | While initializing device 1, active ethernet dispatch core (x=25,y=16) detected as still running, issuing exit signal. (metal_context.cpp:628)
2025-09-10 19:54:18.328 | info     |           Metal | Dispatch on FabricConfig::FABRIC_1D with 1 Command Queues
 (device_pool.cpp:328)
2025-09-10 19:54:18.334 | info     |           Metal | Initializing Fabric (device_pool.cpp:397)
2025-09-10 19:54:19.735 | info     |           Metal | Fabric initialized on Device 0 (device.cpp:420)
2025-09-10 19:54:19.735 | info     |           Metal | Fabric initialized on Device 1 (device.cpp:420)
2025-09-10 19:54:19.735 | info     |           Metal | Fabric Initialized with config FabricConfig::FABRIC_1D (device_pool.cpp:402)
2025-09-10 19:54:21.227 | info     |           Metal | Command Queue initialized on Device 1 (device_pool.cpp:482)
2025-09-10 19:54:21.233 (  28.109s) [        F1708000]              stubs.inc:112   WARN| STUB: PJRT_Client_TopologyDescription
2025-09-10 19:54:21.234 (  28.109s) [        F1708000]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-09-10 19:54:21.234 (  28.109s) [        F1708000]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-09-10 19:54:21.234 (  28.109s) [        F1708000]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-09-10 19:54:21.234 (  28.109s) [        F1708000]     client_instance.cc:340      1| ClientInstance::PJRT_Client_PlatformVersion
2025-09-10 19:54:21.234 (  28.109s) [        F1708000]     client_instance.cc:320      1| ClientInstance::PJRT_Client_PlatformName
2025-09-10 19:54:21.234 (  28.109s) [        F1708000]     client_instance.cc:352      1| ClientInstance::PJRT_Client_Devices
2025-09-10 19:54:21.234 (  28.109s) [        F1708000]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-09-10 19:54:21.234 (  28.109s) [        F1708000]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-09-10 19:54:21.234 (  28.109s) [        F1708000]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-09-10 19:54:21.234 (  28.109s) [        F1708000]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-09-10 19:54:21.234 (  28.109s) [        F1708000]     client_instance.cc:365      1| ClientInstance::PJRT_Client_AddressableDevices
2025-09-10 19:54:21.234 (  28.109s) [        F1708000]     client_instance.cc:415      1| ClientInstance::PJRT_Client_AddressableMemories
2025-09-10 19:54:21.234 (  28.110s) [        F1708000]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-09-10 19:54:21.234 (  28.110s) [        F1708000]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-09-10 19:54:21.234 (  28.110s) [        F1708000]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-09-10 19:54:21.234 (  28.110s) [        F1708000]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-09-10 19:54:21.234 (  28.110s) [        F1708000]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-09-10 19:54:21.234 (  28.110s) [        F1708000]        api_bindings.cc:76       1| PJRT_Plugin_Attributes
2025-09-10 19:54:21.234284: W torch_xla/csrc/runtime/profiler.cpp:88] Profiler API not found for PJRT plugin
2025-09-10 19:54:21.234 (  28.110s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:21.234 (  28.110s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:21.234 (  28.110s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:21.234 (  28.110s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:21.234 (  28.110s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:21.234 (  28.110s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:21.234 (  28.110s) [        F1708000]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-09-10 19:54:21.234 (  28.110s) [        F1708000]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-09-10 19:54:21.234 (  28.110s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:21.234 (  28.110s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:21.234 (  28.110s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:21.234 (  28.110s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
/localdev/jameszianxu/tt-xla/examples/pytorch/llama.py:68: DeprecationWarning: Use torch_xla.device instead
  device = xm.xla_device()
Using TT-Metal from the source tree: /localdev/jameszianxu/tt-xla/third_party/tt-mlir/src/tt-mlir/third_party/tt-metal/src/tt-metal
WARNING: TT plugin is setting XLA_STABLEHLO_COMPILE to 1. This is required for TT PJRT plugin to work correctly.
Setting up XLA environment...
XLA environment configured.
Created device mesh: (1, 2) with 2 devices
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 53.20it/s]
2025-09-10 19:54:22.700 (  29.576s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:22.700 (  29.576s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:22.700 (  29.576s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:22.700 (  29.576s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:22.700 (  29.576s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-09-10 19:54:22.701 (  29.577s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:22.701 (  29.577s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:22.701 (  29.577s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:22.701 (  29.577s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:22.701 (  29.577s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-09-10 19:54:22.701 (  29.577s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:22.701 (  29.577s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:22.702 (  29.578s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:22.702 (  29.578s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:22.702 (  29.578s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:22.702 (  29.578s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:22.702 (  29.578s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-09-10 19:54:22.702 (  29.578s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:22.702 (  29.578s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:22.702 (  29.578s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:22.702 (  29.578s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:22.702 (  29.578s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-09-10 19:54:22.702 (  29.578s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:22.702 (  29.578s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:22.702 (  29.578s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:22.702 (  29.578s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:22.702 (  29.578s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:22.702 (  29.578s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:22.702 (  29.578s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:22.702 (  29.578s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:22.703 (  29.578s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:22.703 (  29.578s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:22.703 (  29.578s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:22.703 (  29.578s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:22.703 (  29.578s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:22.703 (  29.578s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:22.703 (  29.579s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:22.703 (  29.579s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:22.703 (  29.579s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:22.703 (  29.579s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:22.703 (  29.579s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:22.703 (  29.579s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:22.703 (  29.579s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:22.703 (  29.579s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:22.703 (  29.579s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:22.703 (  29.579s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:22.703 (  29.579s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:22.703 (  29.579s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.351 (  30.227s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.351 (  30.227s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.359 (  30.235s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.359 (  30.235s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.359 (  30.235s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.359 (  30.235s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.359 (  30.235s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.359 (  30.235s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.359 (  30.235s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.359 (  30.235s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.360 (  30.235s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.360 (  30.235s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.364 (  30.240s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.364 (  30.240s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.372 (  30.248s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.372 (  30.248s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.372 (  30.248s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.373 (  30.249s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.373 (  30.249s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.373 (  30.249s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.373 (  30.249s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.373 (  30.249s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.373 (  30.249s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.373 (  30.249s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.377 (  30.253s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.377 (  30.253s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.389 (  30.265s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.389 (  30.265s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.389 (  30.265s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.389 (  30.265s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.389 (  30.265s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.389 (  30.265s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.389 (  30.265s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.389 (  30.265s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.389 (  30.265s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.389 (  30.265s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.394 (  30.270s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.394 (  30.270s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.395 (  30.271s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.395 (  30.271s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.395 (  30.271s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.395 (  30.271s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.395 (  30.271s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.395 (  30.271s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.396 (  30.271s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.396 (  30.271s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.396 (  30.271s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.396 (  30.272s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.396 (  30.272s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.396 (  30.272s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.397 (  30.273s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.397 (  30.273s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.397 (  30.273s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.397 (  30.273s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.397 (  30.273s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.397 (  30.273s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.397 (  30.273s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.397 (  30.273s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.397 (  30.273s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.397 (  30.273s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.398 (  30.273s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.398 (  30.273s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.399 (  30.275s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.399 (  30.275s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.399 (  30.275s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.399 (  30.275s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.399 (  30.275s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.399 (  30.275s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.399 (  30.275s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.399 (  30.275s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.399 (  30.275s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.399 (  30.275s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.399 (  30.275s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.399 (  30.275s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.401 (  30.276s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.401 (  30.276s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.401 (  30.276s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.401 (  30.277s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.401 (  30.277s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.401 (  30.277s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.401 (  30.277s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.401 (  30.277s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.401 (  30.277s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.401 (  30.277s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.401 (  30.277s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.401 (  30.277s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.409 (  30.284s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.409 (  30.284s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.409 (  30.284s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.409 (  30.285s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.409 (  30.285s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.409 (  30.285s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.409 (  30.285s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.409 (  30.285s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.409 (  30.285s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.409 (  30.285s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.414 (  30.289s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.414 (  30.289s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.421 (  30.297s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.421 (  30.297s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.421 (  30.297s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.422 (  30.297s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.422 (  30.297s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.422 (  30.297s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.422 (  30.297s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.422 (  30.297s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.422 (  30.298s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.422 (  30.298s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.426 (  30.302s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.426 (  30.302s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.434 (  30.310s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.434 (  30.310s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.434 (  30.310s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.434 (  30.310s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.434 (  30.310s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.434 (  30.310s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.434 (  30.310s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.434 (  30.310s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.434 (  30.310s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.434 (  30.310s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.439 (  30.315s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.439 (  30.315s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.440 (  30.316s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.440 (  30.316s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.440 (  30.316s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.441 (  30.316s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.441 (  30.317s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.441 (  30.317s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.441 (  30.317s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.441 (  30.317s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.441 (  30.317s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.441 (  30.317s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.441 (  30.317s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.441 (  30.317s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.442 (  30.317s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.442 (  30.317s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.442 (  30.317s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.442 (  30.318s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.442 (  30.318s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.442 (  30.318s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.442 (  30.318s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.442 (  30.318s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.442 (  30.318s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.442 (  30.318s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.442 (  30.318s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.442 (  30.318s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.442 (  30.318s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.442 (  30.318s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.442 (  30.318s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.442 (  30.318s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.442 (  30.318s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.442 (  30.318s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.442 (  30.318s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.442 (  30.318s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.443 (  30.318s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.443 (  30.318s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.443 (  30.319s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.443 (  30.319s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.444 (  30.320s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.444 (  30.320s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.444 (  30.320s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.444 (  30.320s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.444 (  30.320s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.444 (  30.320s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.444 (  30.320s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.444 (  30.320s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.444 (  30.320s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.445 (  30.320s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.445 (  30.320s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.445 (  30.320s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.451 (  30.327s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.451 (  30.327s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.451 (  30.327s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.451 (  30.327s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.451 (  30.327s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.451 (  30.327s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.451 (  30.327s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.451 (  30.327s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.451 (  30.327s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.451 (  30.327s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.456 (  30.332s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.456 (  30.332s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.463 (  30.339s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.463 (  30.339s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.463 (  30.339s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.463 (  30.339s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.463 (  30.339s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.463 (  30.339s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.463 (  30.339s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.463 (  30.339s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.463 (  30.339s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.463 (  30.339s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.468 (  30.343s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.468 (  30.343s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.475 (  30.351s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.475 (  30.351s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.475 (  30.351s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.475 (  30.351s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.475 (  30.351s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.475 (  30.351s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.475 (  30.351s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.475 (  30.351s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.475 (  30.351s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.475 (  30.351s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.480 (  30.355s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.480 (  30.355s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.481 (  30.356s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.481 (  30.356s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.481 (  30.356s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.481 (  30.357s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.481 (  30.357s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.481 (  30.357s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.481 (  30.357s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.481 (  30.357s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.481 (  30.357s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.481 (  30.357s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.481 (  30.357s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.481 (  30.357s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.482 (  30.357s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.482 (  30.357s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.482 (  30.357s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.482 (  30.358s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.482 (  30.358s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.482 (  30.358s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.482 (  30.358s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.482 (  30.358s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.482 (  30.358s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.482 (  30.358s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.482 (  30.358s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.482 (  30.358s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.483 (  30.359s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.483 (  30.359s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.483 (  30.359s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.483 (  30.359s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.483 (  30.359s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.483 (  30.359s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.483 (  30.359s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.483 (  30.359s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.483 (  30.359s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.483 (  30.359s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.483 (  30.359s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.483 (  30.359s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.485 (  30.360s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.485 (  30.360s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.485 (  30.360s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.485 (  30.361s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.485 (  30.361s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.485 (  30.361s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.485 (  30.361s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.485 (  30.361s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.485 (  30.361s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.485 (  30.361s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.485 (  30.361s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.485 (  30.361s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.491 (  30.367s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.491 (  30.367s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.491 (  30.367s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.491 (  30.367s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.491 (  30.367s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.491 (  30.367s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.491 (  30.367s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.491 (  30.367s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.491 (  30.367s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.491 (  30.367s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.496 (  30.372s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.496 (  30.372s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.503 (  30.379s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.503 (  30.379s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.503 (  30.379s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.503 (  30.379s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.503 (  30.379s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.503 (  30.379s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.503 (  30.379s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.503 (  30.379s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.503 (  30.379s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.503 (  30.379s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.508 (  30.384s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.508 (  30.384s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.515 (  30.391s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.515 (  30.391s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.515 (  30.391s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.515 (  30.391s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.515 (  30.391s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.515 (  30.391s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.515 (  30.391s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.515 (  30.391s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.515 (  30.391s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.515 (  30.391s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.520 (  30.395s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.520 (  30.395s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.521 (  30.397s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.521 (  30.397s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.521 (  30.397s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.521 (  30.397s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.521 (  30.397s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.521 (  30.397s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.521 (  30.397s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.521 (  30.397s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.521 (  30.397s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.521 (  30.397s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.521 (  30.397s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.521 (  30.397s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.522 (  30.398s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.522 (  30.398s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.522 (  30.398s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.522 (  30.398s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.522 (  30.398s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.522 (  30.398s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.522 (  30.398s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.522 (  30.398s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.522 (  30.398s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.522 (  30.398s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.522 (  30.398s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.522 (  30.398s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.523 (  30.399s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.523 (  30.399s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.523 (  30.399s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.523 (  30.399s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.523 (  30.399s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.523 (  30.399s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.523 (  30.399s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.523 (  30.399s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.523 (  30.399s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.523 (  30.399s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.523 (  30.399s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.523 (  30.399s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.525 (  30.401s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.525 (  30.401s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.525 (  30.401s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.525 (  30.401s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.525 (  30.401s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.525 (  30.401s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.525 (  30.401s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.525 (  30.401s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.525 (  30.401s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.525 (  30.401s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.525 (  30.401s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.525 (  30.401s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.531 (  30.407s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.531 (  30.407s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.531 (  30.407s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.532 (  30.407s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.532 (  30.407s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.532 (  30.407s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.532 (  30.407s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.532 (  30.407s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.532 (  30.408s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.532 (  30.408s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.536 (  30.412s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.536 (  30.412s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.543 (  30.419s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.543 (  30.419s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.543 (  30.419s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.543 (  30.419s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.543 (  30.419s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.544 (  30.419s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.544 (  30.419s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.544 (  30.419s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.544 (  30.419s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.544 (  30.419s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.548 (  30.424s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.548 (  30.424s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.557 (  30.433s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.557 (  30.433s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.557 (  30.433s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.557 (  30.433s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.557 (  30.433s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.557 (  30.433s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.557 (  30.433s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.557 (  30.433s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.557 (  30.433s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.557 (  30.433s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.561 (  30.437s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.561 (  30.437s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.563 (  30.439s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.563 (  30.439s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.563 (  30.439s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.563 (  30.439s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.563 (  30.439s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.563 (  30.439s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.563 (  30.439s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.563 (  30.439s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.563 (  30.439s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.563 (  30.439s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.563 (  30.439s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.563 (  30.439s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.564 (  30.440s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.564 (  30.440s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.564 (  30.440s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.564 (  30.440s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.564 (  30.440s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.564 (  30.440s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.564 (  30.440s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.564 (  30.440s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.564 (  30.440s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.564 (  30.440s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.564 (  30.440s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.564 (  30.440s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.565 (  30.441s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.565 (  30.441s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.565 (  30.441s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.565 (  30.441s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.565 (  30.441s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.565 (  30.441s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.565 (  30.441s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.565 (  30.441s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.565 (  30.441s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.565 (  30.441s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.566 (  30.441s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.566 (  30.441s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.567 (  30.443s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.567 (  30.443s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.567 (  30.443s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.567 (  30.443s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.567 (  30.443s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.567 (  30.443s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.567 (  30.443s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.567 (  30.443s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.567 (  30.443s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.567 (  30.443s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.567 (  30.443s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.567 (  30.443s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.574 (  30.450s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.574 (  30.450s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.574 (  30.450s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.574 (  30.450s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.574 (  30.450s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.574 (  30.450s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.574 (  30.450s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.574 (  30.450s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.574 (  30.450s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.574 (  30.450s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.578 (  30.454s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.578 (  30.454s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.585 (  30.461s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.585 (  30.461s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.585 (  30.461s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.585 (  30.461s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.585 (  30.461s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.585 (  30.461s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.585 (  30.461s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.585 (  30.461s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.585 (  30.461s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.585 (  30.461s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.589 (  30.465s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.589 (  30.465s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.596 (  30.472s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.596 (  30.472s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.596 (  30.472s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.596 (  30.472s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.596 (  30.472s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.596 (  30.472s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.596 (  30.472s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.596 (  30.472s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.597 (  30.472s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.597 (  30.472s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.601 (  30.476s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.601 (  30.476s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.602 (  30.478s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.602 (  30.478s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.602 (  30.478s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.602 (  30.478s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.602 (  30.478s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.602 (  30.478s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.602 (  30.478s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.602 (  30.478s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.602 (  30.478s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.602 (  30.478s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.602 (  30.478s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.602 (  30.478s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.603 (  30.479s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.603 (  30.479s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.603 (  30.479s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.603 (  30.479s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.603 (  30.479s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.603 (  30.479s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.603 (  30.479s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.603 (  30.479s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.603 (  30.479s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.603 (  30.479s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.603 (  30.479s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.603 (  30.479s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.604 (  30.480s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.604 (  30.480s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.604 (  30.480s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.604 (  30.480s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.604 (  30.480s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.604 (  30.480s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.604 (  30.480s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.604 (  30.480s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.604 (  30.480s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.604 (  30.480s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.604 (  30.480s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.604 (  30.480s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.605 (  30.481s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.605 (  30.481s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.605 (  30.481s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.606 (  30.481s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.606 (  30.481s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.606 (  30.481s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.606 (  30.482s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.606 (  30.482s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.606 (  30.482s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.606 (  30.482s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.606 (  30.482s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.606 (  30.482s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.612 (  30.488s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.612 (  30.488s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.612 (  30.488s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.612 (  30.488s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.612 (  30.488s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.612 (  30.488s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.612 (  30.488s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.612 (  30.488s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.612 (  30.488s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.612 (  30.488s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.616 (  30.492s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.616 (  30.492s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.623 (  30.499s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.623 (  30.499s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.623 (  30.499s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.623 (  30.499s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.623 (  30.499s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.623 (  30.499s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.623 (  30.499s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.623 (  30.499s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.623 (  30.499s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.624 (  30.499s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.627 (  30.503s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.627 (  30.503s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.635 (  30.510s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.635 (  30.510s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.635 (  30.511s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.635 (  30.511s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.635 (  30.511s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.635 (  30.511s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.635 (  30.511s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.635 (  30.511s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.635 (  30.511s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.635 (  30.511s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.639 (  30.515s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.639 (  30.515s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.640 (  30.516s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.640 (  30.516s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.640 (  30.516s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.640 (  30.516s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.640 (  30.516s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.640 (  30.516s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.640 (  30.516s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.640 (  30.516s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.640 (  30.516s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.640 (  30.516s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.641 (  30.516s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.641 (  30.516s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.641 (  30.517s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.641 (  30.517s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.641 (  30.517s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.641 (  30.517s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.641 (  30.517s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.641 (  30.517s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.641 (  30.517s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.641 (  30.517s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.641 (  30.517s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.641 (  30.517s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.641 (  30.517s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.641 (  30.517s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.642 (  30.518s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.642 (  30.518s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.642 (  30.518s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.642 (  30.518s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.642 (  30.518s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.642 (  30.518s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.642 (  30.518s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.642 (  30.518s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.642 (  30.518s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.642 (  30.518s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.642 (  30.518s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.642 (  30.518s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.644 (  30.520s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.644 (  30.520s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.644 (  30.520s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.644 (  30.520s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.644 (  30.520s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.644 (  30.520s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.644 (  30.520s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.644 (  30.520s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.644 (  30.520s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.644 (  30.520s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.644 (  30.520s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.644 (  30.520s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.649 (  30.525s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.649 (  30.525s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.649 (  30.525s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.649 (  30.525s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.649 (  30.525s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.649 (  30.525s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.649 (  30.525s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.649 (  30.525s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.649 (  30.525s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.649 (  30.525s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.654 (  30.529s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.654 (  30.529s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.661 (  30.536s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.661 (  30.536s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.661 (  30.536s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.661 (  30.537s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.661 (  30.537s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.661 (  30.537s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.661 (  30.537s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.661 (  30.537s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.661 (  30.537s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.661 (  30.537s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.665 (  30.540s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.665 (  30.540s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.673 (  30.549s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.673 (  30.549s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.673 (  30.549s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.673 (  30.549s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.673 (  30.549s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.673 (  30.549s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.673 (  30.549s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.673 (  30.549s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.673 (  30.549s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.673 (  30.549s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.677 (  30.553s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.677 (  30.553s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.678 (  30.554s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.678 (  30.554s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.678 (  30.554s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.679 (  30.554s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.679 (  30.554s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.679 (  30.554s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.679 (  30.554s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.679 (  30.554s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.679 (  30.554s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.679 (  30.554s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.679 (  30.555s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.679 (  30.555s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.679 (  30.555s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.679 (  30.555s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.679 (  30.555s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.679 (  30.555s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.679 (  30.555s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.679 (  30.555s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.679 (  30.555s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.679 (  30.555s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.680 (  30.555s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.680 (  30.555s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.680 (  30.555s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.680 (  30.555s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.680 (  30.556s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.680 (  30.556s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.680 (  30.556s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.680 (  30.556s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.680 (  30.556s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.680 (  30.556s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.680 (  30.556s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.680 (  30.556s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.680 (  30.556s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.680 (  30.556s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.680 (  30.556s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.680 (  30.556s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.682 (  30.558s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.682 (  30.558s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.682 (  30.558s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.683 (  30.558s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.683 (  30.558s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.683 (  30.558s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.683 (  30.558s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.683 (  30.559s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.683 (  30.559s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.683 (  30.559s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.683 (  30.559s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.683 (  30.559s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.688 (  30.564s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.688 (  30.564s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.688 (  30.564s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.688 (  30.564s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.688 (  30.564s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.688 (  30.564s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.688 (  30.564s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.688 (  30.564s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.688 (  30.564s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.688 (  30.564s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.692 (  30.568s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.692 (  30.568s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.700 (  30.575s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.700 (  30.575s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.700 (  30.575s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.700 (  30.576s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.700 (  30.576s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.700 (  30.576s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.700 (  30.576s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.700 (  30.576s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.700 (  30.576s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.700 (  30.576s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.704 (  30.579s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.704 (  30.579s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.711 (  30.587s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.711 (  30.587s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.711 (  30.587s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.711 (  30.587s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.711 (  30.587s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.711 (  30.587s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.711 (  30.587s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.711 (  30.587s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.711 (  30.587s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.711 (  30.587s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.715 (  30.591s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.715 (  30.591s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.716 (  30.592s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.716 (  30.592s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.716 (  30.592s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.716 (  30.592s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.716 (  30.592s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.716 (  30.592s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.716 (  30.592s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.716 (  30.592s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.716 (  30.592s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.716 (  30.592s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.717 (  30.592s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.717 (  30.592s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.717 (  30.593s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.717 (  30.593s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.717 (  30.593s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.717 (  30.593s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.717 (  30.593s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.717 (  30.593s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.717 (  30.593s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.717 (  30.593s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.717 (  30.593s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.717 (  30.593s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.717 (  30.593s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.717 (  30.593s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.718 (  30.594s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.718 (  30.594s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.718 (  30.594s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.718 (  30.594s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.718 (  30.594s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.718 (  30.594s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.718 (  30.594s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.718 (  30.594s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.718 (  30.594s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.718 (  30.594s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.718 (  30.594s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.718 (  30.594s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.720 (  30.596s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.720 (  30.596s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.720 (  30.596s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.720 (  30.596s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.720 (  30.596s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.720 (  30.596s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.720 (  30.596s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.720 (  30.596s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.720 (  30.596s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.720 (  30.596s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.720 (  30.596s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.720 (  30.596s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.725 (  30.601s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.725 (  30.601s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.725 (  30.601s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.725 (  30.601s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.725 (  30.601s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.725 (  30.601s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.725 (  30.601s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.725 (  30.601s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.725 (  30.601s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.725 (  30.601s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.730 (  30.605s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.730 (  30.605s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.737 (  30.612s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.737 (  30.612s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.737 (  30.612s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.737 (  30.613s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.737 (  30.613s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.737 (  30.613s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.737 (  30.613s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.737 (  30.613s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.737 (  30.613s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.737 (  30.613s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.741 (  30.617s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.741 (  30.617s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.748 (  30.624s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.748 (  30.624s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.748 (  30.624s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.748 (  30.624s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.748 (  30.624s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.748 (  30.624s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.748 (  30.624s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.748 (  30.624s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.748 (  30.624s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.748 (  30.624s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.752 (  30.628s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.752 (  30.628s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.753 (  30.629s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.753 (  30.629s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.753 (  30.629s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.753 (  30.629s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.753 (  30.629s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.753 (  30.629s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.753 (  30.629s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.753 (  30.629s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.754 (  30.629s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.754 (  30.629s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.754 (  30.629s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.754 (  30.630s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.754 (  30.630s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.754 (  30.630s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.754 (  30.630s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.754 (  30.630s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.754 (  30.630s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.754 (  30.630s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.754 (  30.630s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.754 (  30.630s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.754 (  30.630s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.754 (  30.630s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.754 (  30.630s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.754 (  30.630s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.755 (  30.631s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.755 (  30.631s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.755 (  30.631s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.755 (  30.631s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.755 (  30.631s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.755 (  30.631s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.755 (  30.631s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.755 (  30.631s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.755 (  30.631s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.755 (  30.631s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.755 (  30.631s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.755 (  30.631s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.757 (  30.633s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.757 (  30.633s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.757 (  30.633s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.757 (  30.633s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.757 (  30.633s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.757 (  30.633s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.757 (  30.633s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.757 (  30.633s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.757 (  30.633s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.757 (  30.633s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.757 (  30.633s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.757 (  30.633s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.762 (  30.638s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.762 (  30.638s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.762 (  30.638s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.762 (  30.638s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.762 (  30.638s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.762 (  30.638s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.762 (  30.638s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.762 (  30.638s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.762 (  30.638s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.762 (  30.638s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.767 (  30.642s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.767 (  30.642s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.774 (  30.649s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.774 (  30.649s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.774 (  30.649s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.774 (  30.650s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.774 (  30.650s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.774 (  30.650s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.774 (  30.650s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.774 (  30.650s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.774 (  30.650s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.774 (  30.650s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.778 (  30.654s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.778 (  30.654s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.786 (  30.662s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.786 (  30.662s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.786 (  30.662s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.786 (  30.662s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.786 (  30.662s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.786 (  30.662s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.786 (  30.662s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.786 (  30.662s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.786 (  30.662s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.786 (  30.662s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.790 (  30.666s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.790 (  30.666s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.792 (  30.668s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.792 (  30.668s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.792 (  30.668s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.792 (  30.668s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.792 (  30.668s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.792 (  30.668s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.792 (  30.668s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.792 (  30.668s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.792 (  30.668s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.792 (  30.668s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.792 (  30.668s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.792 (  30.668s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.793 (  30.668s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.793 (  30.668s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.793 (  30.668s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.793 (  30.669s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.793 (  30.669s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.793 (  30.669s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.793 (  30.669s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.793 (  30.669s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.793 (  30.669s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.793 (  30.669s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.793 (  30.669s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.793 (  30.669s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.793 (  30.669s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.794 (  30.669s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.794 (  30.669s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.794 (  30.669s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.794 (  30.669s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.794 (  30.670s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.794 (  30.670s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.794 (  30.670s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.794 (  30.670s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.794 (  30.670s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.794 (  30.670s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.794 (  30.670s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.796 (  30.672s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.796 (  30.672s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.796 (  30.672s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.796 (  30.672s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.796 (  30.672s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.796 (  30.672s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.796 (  30.672s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.796 (  30.672s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.796 (  30.672s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.796 (  30.672s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.796 (  30.672s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.796 (  30.672s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.801 (  30.677s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.801 (  30.677s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.801 (  30.677s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.801 (  30.677s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.801 (  30.677s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.801 (  30.677s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.801 (  30.677s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.801 (  30.677s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.801 (  30.677s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.801 (  30.677s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.805 (  30.681s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.806 (  30.681s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.813 (  30.688s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.813 (  30.688s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.813 (  30.688s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.813 (  30.689s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.813 (  30.689s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.813 (  30.689s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.813 (  30.689s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.813 (  30.689s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.813 (  30.689s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.813 (  30.689s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.817 (  30.692s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.817 (  30.692s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.824 (  30.700s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.824 (  30.700s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.824 (  30.700s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.824 (  30.700s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.824 (  30.700s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.824 (  30.700s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.824 (  30.700s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.824 (  30.700s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.824 (  30.700s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.824 (  30.700s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.828 (  30.704s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.828 (  30.704s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.829 (  30.705s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.829 (  30.705s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.829 (  30.705s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.829 (  30.705s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.829 (  30.705s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.829 (  30.705s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.829 (  30.705s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.829 (  30.705s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.829 (  30.705s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.829 (  30.705s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.829 (  30.705s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.829 (  30.705s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.830 (  30.706s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.830 (  30.706s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.830 (  30.706s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.830 (  30.706s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.830 (  30.706s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.830 (  30.706s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.830 (  30.706s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.830 (  30.706s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.830 (  30.706s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.830 (  30.706s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.830 (  30.706s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.830 (  30.706s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.831 (  30.707s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.831 (  30.707s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.831 (  30.707s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.831 (  30.707s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.831 (  30.707s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.831 (  30.707s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.831 (  30.707s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.831 (  30.707s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.831 (  30.707s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.831 (  30.707s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.831 (  30.707s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.831 (  30.707s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.833 (  30.708s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.833 (  30.708s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.833 (  30.708s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.833 (  30.708s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.833 (  30.709s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.833 (  30.709s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.833 (  30.709s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.833 (  30.709s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.833 (  30.709s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.833 (  30.709s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.833 (  30.709s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.833 (  30.709s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.838 (  30.714s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.838 (  30.714s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.838 (  30.714s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.838 (  30.714s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.838 (  30.714s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.838 (  30.714s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.838 (  30.714s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.838 (  30.714s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.838 (  30.714s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.838 (  30.714s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.842 (  30.718s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.842 (  30.718s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.849 (  30.725s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.849 (  30.725s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.849 (  30.725s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.849 (  30.725s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.849 (  30.725s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.849 (  30.725s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.849 (  30.725s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.849 (  30.725s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.850 (  30.725s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.850 (  30.725s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.853 (  30.729s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.853 (  30.729s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.861 (  30.737s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.861 (  30.737s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.861 (  30.737s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.861 (  30.737s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.861 (  30.737s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.861 (  30.737s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.861 (  30.737s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.861 (  30.737s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.861 (  30.737s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.861 (  30.737s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.865 (  30.741s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.865 (  30.741s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.866 (  30.742s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.866 (  30.742s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.866 (  30.742s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.866 (  30.742s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.866 (  30.742s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.866 (  30.742s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.866 (  30.742s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.866 (  30.742s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.866 (  30.742s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.866 (  30.742s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.866 (  30.742s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.866 (  30.742s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.867 (  30.743s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.867 (  30.743s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.867 (  30.743s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.867 (  30.743s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.867 (  30.743s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.867 (  30.743s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.867 (  30.743s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.867 (  30.743s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.867 (  30.743s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.867 (  30.743s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.867 (  30.743s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.867 (  30.743s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.868 (  30.744s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.868 (  30.744s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.868 (  30.744s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.868 (  30.744s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.868 (  30.744s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.868 (  30.744s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.868 (  30.744s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.868 (  30.744s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.868 (  30.744s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.868 (  30.744s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.868 (  30.744s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.868 (  30.744s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.870 (  30.745s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.870 (  30.746s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.870 (  30.746s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.870 (  30.746s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.870 (  30.746s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.870 (  30.746s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.870 (  30.746s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.870 (  30.746s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.870 (  30.746s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.870 (  30.746s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.870 (  30.746s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.870 (  30.746s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.875 (  30.751s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.875 (  30.751s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.875 (  30.751s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.875 (  30.751s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.875 (  30.751s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.875 (  30.751s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.875 (  30.751s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.875 (  30.751s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.875 (  30.751s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.875 (  30.751s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.879 (  30.755s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.879 (  30.755s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.886 (  30.762s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.886 (  30.762s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.886 (  30.762s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.887 (  30.762s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.887 (  30.762s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.887 (  30.762s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.887 (  30.762s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.887 (  30.762s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.887 (  30.762s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.887 (  30.762s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.890 (  30.766s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.890 (  30.766s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.898 (  30.774s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.898 (  30.774s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.898 (  30.774s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.899 (  30.774s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.899 (  30.774s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.899 (  30.774s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.899 (  30.774s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.899 (  30.774s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.899 (  30.775s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.899 (  30.775s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.903 (  30.779s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.903 (  30.779s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.905 (  30.781s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.905 (  30.781s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.905 (  30.781s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.905 (  30.781s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.905 (  30.781s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.905 (  30.781s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.905 (  30.781s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.905 (  30.781s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.905 (  30.781s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.905 (  30.781s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.905 (  30.781s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.905 (  30.781s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.906 (  30.782s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.906 (  30.782s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.906 (  30.782s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.906 (  30.782s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.906 (  30.782s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.906 (  30.782s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.906 (  30.782s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.906 (  30.782s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.906 (  30.782s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.906 (  30.782s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.906 (  30.782s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.906 (  30.782s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.907 (  30.783s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.907 (  30.783s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.907 (  30.783s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.907 (  30.783s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.907 (  30.783s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.907 (  30.783s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.907 (  30.783s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.907 (  30.783s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.907 (  30.783s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.907 (  30.783s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.907 (  30.783s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.907 (  30.783s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.909 (  30.784s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.909 (  30.784s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.909 (  30.784s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.909 (  30.785s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.909 (  30.785s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.909 (  30.785s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.909 (  30.785s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.909 (  30.785s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.909 (  30.785s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.909 (  30.785s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.909 (  30.785s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.909 (  30.785s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.914 (  30.790s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.914 (  30.790s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.914 (  30.790s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.914 (  30.790s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.914 (  30.790s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.914 (  30.790s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.914 (  30.790s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.914 (  30.790s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.914 (  30.790s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.914 (  30.790s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.918 (  30.794s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.918 (  30.794s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.925 (  30.801s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.925 (  30.801s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.925 (  30.801s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.926 (  30.801s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.926 (  30.801s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.926 (  30.801s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.926 (  30.801s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.926 (  30.801s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.926 (  30.802s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.926 (  30.802s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.930 (  30.805s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.930 (  30.805s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.937 (  30.813s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.937 (  30.813s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.937 (  30.813s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.937 (  30.813s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.937 (  30.813s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.937 (  30.813s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.937 (  30.813s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.937 (  30.813s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.937 (  30.813s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.937 (  30.813s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.941 (  30.817s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.941 (  30.817s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.942 (  30.818s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.942 (  30.818s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.942 (  30.818s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.942 (  30.818s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.942 (  30.818s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.942 (  30.818s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.942 (  30.818s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.942 (  30.818s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.942 (  30.818s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.942 (  30.818s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.942 (  30.818s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.942 (  30.818s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.943 (  30.819s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.943 (  30.819s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.943 (  30.819s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.943 (  30.819s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.943 (  30.819s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.943 (  30.819s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.943 (  30.819s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.943 (  30.819s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.943 (  30.819s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.943 (  30.819s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.943 (  30.819s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.943 (  30.819s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.944 (  30.820s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.944 (  30.820s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.944 (  30.820s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.944 (  30.820s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.944 (  30.820s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.944 (  30.820s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.944 (  30.820s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.944 (  30.820s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.944 (  30.820s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.944 (  30.820s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.944 (  30.820s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.944 (  30.820s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.946 (  30.821s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.946 (  30.821s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.946 (  30.821s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.946 (  30.822s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.946 (  30.822s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.946 (  30.822s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.946 (  30.822s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.946 (  30.822s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.946 (  30.822s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.946 (  30.822s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.946 (  30.822s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.946 (  30.822s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.951 (  30.827s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.951 (  30.827s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.951 (  30.827s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.951 (  30.827s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.951 (  30.827s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.951 (  30.827s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.951 (  30.827s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.951 (  30.827s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.951 (  30.827s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.951 (  30.827s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.955 (  30.831s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.955 (  30.831s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.962 (  30.838s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.962 (  30.838s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.962 (  30.838s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.962 (  30.838s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.962 (  30.838s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.962 (  30.838s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.962 (  30.838s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.962 (  30.838s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.963 (  30.838s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.963 (  30.838s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.966 (  30.842s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.966 (  30.842s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.974 (  30.849s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.974 (  30.849s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.974 (  30.849s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.974 (  30.850s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.974 (  30.850s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.974 (  30.850s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.974 (  30.850s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.974 (  30.850s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.974 (  30.850s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.974 (  30.850s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.978 (  30.854s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.978 (  30.854s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.979 (  30.855s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.979 (  30.855s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.979 (  30.855s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.979 (  30.855s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.979 (  30.855s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.979 (  30.855s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.979 (  30.855s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.979 (  30.855s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.979 (  30.855s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.979 (  30.855s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.979 (  30.855s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.979 (  30.855s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.980 (  30.856s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.980 (  30.856s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.980 (  30.856s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.980 (  30.856s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.980 (  30.856s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.980 (  30.856s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.980 (  30.856s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.980 (  30.856s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.980 (  30.856s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.980 (  30.856s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.980 (  30.856s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.980 (  30.856s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.981 (  30.856s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.981 (  30.856s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.981 (  30.856s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.981 (  30.857s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.981 (  30.857s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.981 (  30.857s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.981 (  30.857s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.981 (  30.857s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.981 (  30.857s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.981 (  30.857s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.981 (  30.857s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.981 (  30.857s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.982 (  30.858s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.982 (  30.858s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.982 (  30.858s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.982 (  30.858s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.982 (  30.858s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.983 (  30.858s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.983 (  30.858s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.983 (  30.858s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.983 (  30.858s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.983 (  30.858s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.983 (  30.859s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.983 (  30.859s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.988 (  30.864s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.988 (  30.864s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.988 (  30.864s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.988 (  30.864s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.988 (  30.864s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.988 (  30.864s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.988 (  30.864s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.988 (  30.864s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.988 (  30.864s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.988 (  30.864s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.992 (  30.868s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.992 (  30.868s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:23.999 (  30.875s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.999 (  30.875s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.999 (  30.875s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:23.999 (  30.875s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:23.999 (  30.875s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:23.999 (  30.875s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:23.999 (  30.875s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:23.999 (  30.875s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.000 (  30.875s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.000 (  30.875s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.003 (  30.879s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.003 (  30.879s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.011 (  30.887s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.011 (  30.887s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.011 (  30.887s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.011 (  30.887s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.011 (  30.887s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.011 (  30.887s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.011 (  30.887s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.011 (  30.887s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.011 (  30.887s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.011 (  30.887s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.016 (  30.891s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.016 (  30.891s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.017 (  30.893s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.017 (  30.893s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.017 (  30.893s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.018 (  30.893s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.018 (  30.893s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.018 (  30.893s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.018 (  30.893s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.018 (  30.893s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.018 (  30.894s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.018 (  30.894s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.018 (  30.894s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.018 (  30.894s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.018 (  30.894s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.018 (  30.894s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.018 (  30.894s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.018 (  30.894s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.018 (  30.894s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.018 (  30.894s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.018 (  30.894s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.018 (  30.894s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.018 (  30.894s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.018 (  30.894s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.019 (  30.894s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.019 (  30.894s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.019 (  30.895s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.019 (  30.895s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.019 (  30.895s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.019 (  30.895s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.019 (  30.895s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.019 (  30.895s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.019 (  30.895s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.019 (  30.895s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.020 (  30.895s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.020 (  30.895s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.020 (  30.896s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.020 (  30.896s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.021 (  30.897s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.021 (  30.897s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.021 (  30.897s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.021 (  30.897s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.021 (  30.897s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.021 (  30.897s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.021 (  30.897s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.021 (  30.897s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.021 (  30.897s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.021 (  30.897s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.021 (  30.897s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.021 (  30.897s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.026 (  30.902s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.026 (  30.902s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.026 (  30.902s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.027 (  30.902s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.027 (  30.902s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.027 (  30.902s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.027 (  30.902s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.027 (  30.902s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.027 (  30.903s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.027 (  30.903s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.031 (  30.907s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.031 (  30.907s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.038 (  30.914s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.038 (  30.914s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.038 (  30.914s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.038 (  30.914s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.038 (  30.914s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.038 (  30.914s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.038 (  30.914s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.038 (  30.914s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.038 (  30.914s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.038 (  30.914s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.042 (  30.918s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.042 (  30.918s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.049 (  30.925s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.049 (  30.925s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.049 (  30.925s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.049 (  30.925s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.049 (  30.925s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.049 (  30.925s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.049 (  30.925s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.049 (  30.925s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.049 (  30.925s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.049 (  30.925s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.053 (  30.929s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.053 (  30.929s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.055 (  30.930s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.055 (  30.931s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.055 (  30.931s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.055 (  30.931s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.055 (  30.931s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.055 (  30.931s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.055 (  30.931s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.055 (  30.931s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.055 (  30.931s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.055 (  30.931s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.055 (  30.931s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.055 (  30.931s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.055 (  30.931s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.055 (  30.931s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.055 (  30.931s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.056 (  30.931s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.056 (  30.931s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.056 (  30.931s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.056 (  30.931s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.056 (  30.931s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.056 (  30.932s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.056 (  30.932s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.056 (  30.932s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.056 (  30.932s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.056 (  30.932s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.056 (  30.932s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.056 (  30.932s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.056 (  30.932s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.056 (  30.932s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.056 (  30.932s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.056 (  30.932s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.056 (  30.932s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.056 (  30.932s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.057 (  30.932s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.057 (  30.932s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.057 (  30.932s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.058 (  30.934s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.058 (  30.934s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.058 (  30.934s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.058 (  30.934s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.058 (  30.934s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.058 (  30.934s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.058 (  30.934s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.058 (  30.934s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.058 (  30.934s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.058 (  30.934s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.059 (  30.934s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.059 (  30.934s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.064 (  30.939s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.064 (  30.939s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.064 (  30.939s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.064 (  30.940s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.064 (  30.940s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.064 (  30.940s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.064 (  30.940s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.064 (  30.940s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.064 (  30.940s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.064 (  30.940s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.068 (  30.944s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.068 (  30.944s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.075 (  30.951s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.075 (  30.951s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.075 (  30.951s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.075 (  30.951s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.075 (  30.951s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.075 (  30.951s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.075 (  30.951s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.075 (  30.951s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.075 (  30.951s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.075 (  30.951s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.079 (  30.955s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.079 (  30.955s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.086 (  30.962s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.086 (  30.962s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.086 (  30.962s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.087 (  30.962s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.087 (  30.962s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.087 (  30.962s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.087 (  30.962s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.087 (  30.962s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.087 (  30.963s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.087 (  30.963s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.091 (  30.966s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.091 (  30.966s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.092 (  30.968s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.092 (  30.968s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.092 (  30.968s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.092 (  30.968s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.092 (  30.968s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.092 (  30.968s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.092 (  30.968s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.092 (  30.968s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.092 (  30.968s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.092 (  30.968s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.092 (  30.968s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.092 (  30.968s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.093 (  30.968s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.093 (  30.968s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.093 (  30.968s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.093 (  30.969s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.093 (  30.969s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.093 (  30.969s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.093 (  30.969s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.093 (  30.969s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.093 (  30.969s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.093 (  30.969s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.093 (  30.969s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.093 (  30.969s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.093 (  30.969s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.093 (  30.969s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.093 (  30.969s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.093 (  30.969s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.093 (  30.969s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.093 (  30.969s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.093 (  30.969s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.093 (  30.969s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.094 (  30.969s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.094 (  30.969s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.094 (  30.970s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.094 (  30.970s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.095 (  30.971s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.095 (  30.971s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.095 (  30.971s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.095 (  30.971s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.095 (  30.971s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.095 (  30.971s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.095 (  30.971s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.095 (  30.971s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.095 (  30.971s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.095 (  30.971s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.096 (  30.971s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.096 (  30.971s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.101 (  30.977s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.101 (  30.977s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.101 (  30.977s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.101 (  30.977s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.101 (  30.977s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.101 (  30.977s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.101 (  30.977s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.101 (  30.977s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.101 (  30.977s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.101 (  30.977s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.105 (  30.981s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.105 (  30.981s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.112 (  30.988s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.112 (  30.988s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.112 (  30.988s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.112 (  30.988s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.112 (  30.988s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.112 (  30.988s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.112 (  30.988s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.112 (  30.988s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.112 (  30.988s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.112 (  30.988s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.116 (  30.992s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.116 (  30.992s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.124 (  31.000s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.124 (  31.000s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.124 (  31.000s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.125 (  31.000s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.125 (  31.000s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.125 (  31.000s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.125 (  31.000s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.125 (  31.000s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.125 (  31.000s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.125 (  31.000s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.129 (  31.004s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.129 (  31.004s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.130 (  31.006s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.130 (  31.006s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.130 (  31.006s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.130 (  31.006s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.130 (  31.006s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.130 (  31.006s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.130 (  31.006s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.130 (  31.006s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.130 (  31.006s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.130 (  31.006s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.130 (  31.006s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.130 (  31.006s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.131 (  31.007s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.131 (  31.007s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.131 (  31.007s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.131 (  31.007s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.131 (  31.007s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.131 (  31.007s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.131 (  31.007s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.131 (  31.007s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.131 (  31.007s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.131 (  31.007s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.131 (  31.007s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.131 (  31.007s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.132 (  31.008s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.132 (  31.008s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.132 (  31.008s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.132 (  31.008s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.132 (  31.008s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.132 (  31.008s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.132 (  31.008s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.132 (  31.008s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.132 (  31.008s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.132 (  31.008s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.132 (  31.008s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.132 (  31.008s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.134 (  31.010s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.134 (  31.010s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.134 (  31.010s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.134 (  31.010s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.134 (  31.010s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.134 (  31.010s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.134 (  31.010s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.134 (  31.010s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.134 (  31.010s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.134 (  31.010s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.134 (  31.010s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.134 (  31.010s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.140 (  31.016s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.140 (  31.016s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.140 (  31.016s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.140 (  31.016s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.140 (  31.016s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.140 (  31.016s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.140 (  31.016s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.140 (  31.016s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.140 (  31.016s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.140 (  31.016s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.144 (  31.020s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.144 (  31.020s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.151 (  31.027s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.151 (  31.027s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.151 (  31.027s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.151 (  31.027s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.151 (  31.027s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.151 (  31.027s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.151 (  31.027s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.151 (  31.027s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.151 (  31.027s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.151 (  31.027s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.155 (  31.031s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.155 (  31.031s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.162 (  31.038s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.162 (  31.038s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.162 (  31.038s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.162 (  31.038s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.162 (  31.038s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.162 (  31.038s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.162 (  31.038s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.162 (  31.038s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.163 (  31.038s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.163 (  31.038s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.166 (  31.042s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.166 (  31.042s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.168 (  31.044s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.168 (  31.044s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.168 (  31.044s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.168 (  31.044s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.168 (  31.044s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.168 (  31.044s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.168 (  31.044s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.168 (  31.044s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.168 (  31.044s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.168 (  31.044s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.168 (  31.044s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.168 (  31.044s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.168 (  31.044s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.168 (  31.044s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.168 (  31.044s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.169 (  31.044s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.169 (  31.044s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.169 (  31.044s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.169 (  31.044s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.169 (  31.044s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.169 (  31.045s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.169 (  31.045s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.169 (  31.045s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.169 (  31.045s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.169 (  31.045s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.169 (  31.045s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.169 (  31.045s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.169 (  31.045s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.169 (  31.045s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.169 (  31.045s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.169 (  31.045s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.169 (  31.045s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.170 (  31.045s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.170 (  31.045s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.170 (  31.045s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.170 (  31.045s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.171 (  31.047s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.171 (  31.047s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.171 (  31.047s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.171 (  31.047s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.171 (  31.047s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.171 (  31.047s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.171 (  31.047s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.171 (  31.047s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.171 (  31.047s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.171 (  31.047s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.171 (  31.047s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.171 (  31.047s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.177 (  31.053s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.177 (  31.053s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.177 (  31.053s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.177 (  31.053s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.177 (  31.053s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.177 (  31.053s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.177 (  31.053s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.177 (  31.053s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.177 (  31.053s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.177 (  31.053s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.181 (  31.057s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.181 (  31.057s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.188 (  31.064s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.188 (  31.064s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.188 (  31.064s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.188 (  31.064s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.188 (  31.064s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.188 (  31.064s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.188 (  31.064s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.188 (  31.064s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.188 (  31.064s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.188 (  31.064s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.192 (  31.068s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.192 (  31.068s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.199 (  31.075s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.199 (  31.075s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.199 (  31.075s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.199 (  31.075s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.199 (  31.075s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.199 (  31.075s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.199 (  31.075s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.199 (  31.075s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.199 (  31.075s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.199 (  31.075s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.203 (  31.079s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.203 (  31.079s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.205 (  31.081s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.205 (  31.081s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.205 (  31.081s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.205 (  31.081s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.205 (  31.081s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.205 (  31.081s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.205 (  31.081s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.205 (  31.081s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.205 (  31.081s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.205 (  31.081s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.205 (  31.081s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.205 (  31.081s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.206 (  31.081s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.206 (  31.081s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.206 (  31.081s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.206 (  31.082s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.206 (  31.082s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.206 (  31.082s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.206 (  31.082s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.206 (  31.082s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.206 (  31.082s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.206 (  31.082s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.206 (  31.082s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.206 (  31.082s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.206 (  31.082s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.206 (  31.082s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.206 (  31.082s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.206 (  31.082s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.206 (  31.082s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.206 (  31.082s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.206 (  31.082s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.206 (  31.082s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.206 (  31.082s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.207 (  31.082s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.207 (  31.082s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.207 (  31.082s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.208 (  31.084s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.208 (  31.084s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.208 (  31.084s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.208 (  31.084s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.208 (  31.084s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.208 (  31.084s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.208 (  31.084s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.208 (  31.084s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.208 (  31.084s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.208 (  31.084s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.208 (  31.084s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.208 (  31.084s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.214 (  31.090s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.214 (  31.090s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.214 (  31.090s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.214 (  31.090s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.214 (  31.090s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.214 (  31.090s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.214 (  31.090s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.214 (  31.090s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.214 (  31.090s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.214 (  31.090s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.218 (  31.094s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.218 (  31.094s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.225 (  31.101s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.225 (  31.101s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.225 (  31.101s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.226 (  31.101s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.226 (  31.101s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.226 (  31.101s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.226 (  31.101s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.226 (  31.101s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.226 (  31.101s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.226 (  31.101s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.229 (  31.105s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.229 (  31.105s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.238 (  31.113s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.238 (  31.113s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.238 (  31.114s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.238 (  31.114s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.238 (  31.114s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.238 (  31.114s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.238 (  31.114s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.238 (  31.114s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.238 (  31.114s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.238 (  31.114s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.242 (  31.118s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.242 (  31.118s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.243 (  31.119s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.243 (  31.119s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.243 (  31.119s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.244 (  31.119s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.244 (  31.119s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.244 (  31.119s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.244 (  31.119s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.244 (  31.119s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.244 (  31.120s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.244 (  31.120s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.244 (  31.120s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.244 (  31.120s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.244 (  31.120s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.244 (  31.120s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.244 (  31.120s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.244 (  31.120s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.244 (  31.120s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.244 (  31.120s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.244 (  31.120s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.244 (  31.120s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.244 (  31.120s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.244 (  31.120s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.245 (  31.120s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.245 (  31.120s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.245 (  31.121s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.245 (  31.121s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.245 (  31.121s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.245 (  31.121s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.245 (  31.121s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.245 (  31.121s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.245 (  31.121s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.245 (  31.121s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.245 (  31.121s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.245 (  31.121s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.245 (  31.121s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.245 (  31.121s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.248 (  31.123s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.248 (  31.123s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.248 (  31.123s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.248 (  31.124s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.248 (  31.124s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.248 (  31.124s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.248 (  31.124s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.248 (  31.124s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.248 (  31.124s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.248 (  31.124s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.248 (  31.124s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.248 (  31.124s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.253 (  31.129s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.253 (  31.129s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.253 (  31.129s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.253 (  31.129s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.253 (  31.129s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.253 (  31.129s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.253 (  31.129s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.253 (  31.129s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.253 (  31.129s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.253 (  31.129s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.257 (  31.133s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.257 (  31.133s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.265 (  31.140s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.265 (  31.140s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.265 (  31.140s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.265 (  31.141s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.265 (  31.141s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.265 (  31.141s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.265 (  31.141s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.265 (  31.141s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.265 (  31.141s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.265 (  31.141s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.269 (  31.144s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.269 (  31.145s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.276 (  31.152s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.276 (  31.152s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.276 (  31.152s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.276 (  31.152s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.276 (  31.152s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.276 (  31.152s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.276 (  31.152s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.276 (  31.152s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.276 (  31.152s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.276 (  31.152s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.280 (  31.156s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.280 (  31.156s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.282 (  31.157s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.282 (  31.157s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.282 (  31.157s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.282 (  31.158s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.282 (  31.158s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.282 (  31.158s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.282 (  31.158s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.282 (  31.158s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.282 (  31.158s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.282 (  31.158s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.282 (  31.158s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.282 (  31.158s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.282 (  31.158s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.282 (  31.158s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.282 (  31.158s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.283 (  31.158s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.283 (  31.158s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.283 (  31.158s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.283 (  31.158s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.283 (  31.158s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.283 (  31.159s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.283 (  31.159s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.283 (  31.159s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.283 (  31.159s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.283 (  31.159s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.283 (  31.159s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.283 (  31.159s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.283 (  31.159s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.283 (  31.159s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.283 (  31.159s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.283 (  31.159s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.283 (  31.159s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.283 (  31.159s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.283 (  31.159s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.284 (  31.159s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.284 (  31.159s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.285 (  31.161s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.285 (  31.161s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.285 (  31.161s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.285 (  31.161s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.285 (  31.161s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.285 (  31.161s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.285 (  31.161s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.285 (  31.161s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.285 (  31.161s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.285 (  31.161s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.285 (  31.161s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.285 (  31.161s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.291 (  31.167s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.291 (  31.167s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.291 (  31.167s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.291 (  31.167s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.291 (  31.167s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.291 (  31.167s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.291 (  31.167s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.291 (  31.167s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.291 (  31.167s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.291 (  31.167s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.296 (  31.171s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.296 (  31.171s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.303 (  31.179s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.303 (  31.179s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.303 (  31.179s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.303 (  31.179s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.303 (  31.179s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.303 (  31.179s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.303 (  31.179s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.303 (  31.179s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.303 (  31.179s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.303 (  31.179s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.307 (  31.183s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.307 (  31.183s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.314 (  31.190s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.314 (  31.190s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.314 (  31.190s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.314 (  31.190s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.314 (  31.190s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.314 (  31.190s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.314 (  31.190s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.314 (  31.190s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.314 (  31.190s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.314 (  31.190s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.319 (  31.194s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.319 (  31.194s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.320 (  31.196s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.320 (  31.196s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.320 (  31.196s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.320 (  31.196s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.320 (  31.196s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.320 (  31.196s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.320 (  31.196s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.320 (  31.196s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.320 (  31.196s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.320 (  31.196s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.320 (  31.196s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.320 (  31.196s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.321 (  31.196s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.321 (  31.196s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.321 (  31.196s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.321 (  31.197s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.321 (  31.197s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.321 (  31.197s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.321 (  31.197s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.321 (  31.197s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.321 (  31.197s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.321 (  31.197s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.321 (  31.197s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.321 (  31.197s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.321 (  31.197s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.321 (  31.197s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.321 (  31.197s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.322 (  31.197s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.322 (  31.197s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.322 (  31.197s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.322 (  31.197s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.322 (  31.197s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.322 (  31.198s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.322 (  31.198s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.322 (  31.198s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.322 (  31.198s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.324 (  31.199s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.324 (  31.199s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.324 (  31.199s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.324 (  31.200s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.324 (  31.200s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.324 (  31.200s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.324 (  31.200s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.324 (  31.200s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.324 (  31.200s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.324 (  31.200s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.324 (  31.200s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.324 (  31.200s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.329 (  31.205s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.329 (  31.205s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.329 (  31.205s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.329 (  31.205s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.329 (  31.205s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.329 (  31.205s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.329 (  31.205s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.329 (  31.205s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.329 (  31.205s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.329 (  31.205s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.333 (  31.209s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.333 (  31.209s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.341 (  31.216s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.341 (  31.216s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.341 (  31.216s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.341 (  31.217s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.341 (  31.217s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.341 (  31.217s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.341 (  31.217s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.341 (  31.217s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.341 (  31.217s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.341 (  31.217s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.345 (  31.221s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.345 (  31.221s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.353 (  31.229s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.353 (  31.229s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.353 (  31.229s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.353 (  31.229s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.353 (  31.229s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.353 (  31.229s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.353 (  31.229s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.353 (  31.229s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.353 (  31.229s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.353 (  31.229s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.357 (  31.233s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.357 (  31.233s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.359 (  31.235s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.359 (  31.235s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.359 (  31.235s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.359 (  31.235s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.359 (  31.235s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.359 (  31.235s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.359 (  31.235s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.359 (  31.235s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.359 (  31.235s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.359 (  31.235s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.359 (  31.235s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.359 (  31.235s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.360 (  31.236s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.360 (  31.236s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.360 (  31.236s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.360 (  31.236s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.360 (  31.236s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.360 (  31.236s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.360 (  31.236s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.360 (  31.236s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.360 (  31.236s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.360 (  31.236s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.360 (  31.236s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.360 (  31.236s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.361 (  31.237s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.361 (  31.237s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.361 (  31.237s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.361 (  31.237s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.361 (  31.237s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.361 (  31.237s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.361 (  31.237s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.361 (  31.237s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.361 (  31.237s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.361 (  31.237s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.361 (  31.237s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.361 (  31.237s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.363 (  31.239s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.363 (  31.239s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.363 (  31.239s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.363 (  31.239s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.363 (  31.239s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.363 (  31.239s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.363 (  31.239s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.363 (  31.239s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.363 (  31.239s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.363 (  31.239s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.363 (  31.239s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.363 (  31.239s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.368 (  31.244s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.368 (  31.244s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.368 (  31.244s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.368 (  31.244s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.368 (  31.244s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.368 (  31.244s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.368 (  31.244s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.368 (  31.244s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.369 (  31.244s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.369 (  31.244s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.373 (  31.249s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.373 (  31.249s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.380 (  31.256s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.380 (  31.256s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.380 (  31.256s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.380 (  31.256s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.380 (  31.256s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.380 (  31.256s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.380 (  31.256s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.380 (  31.256s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.380 (  31.256s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.380 (  31.256s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.384 (  31.260s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.384 (  31.260s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.391 (  31.267s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.391 (  31.267s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.391 (  31.267s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.392 (  31.268s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.392 (  31.268s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.392 (  31.268s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.392 (  31.268s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.392 (  31.268s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.392 (  31.268s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.392 (  31.268s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.396 (  31.271s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.396 (  31.272s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.397 (  31.273s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.397 (  31.273s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.397 (  31.273s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.397 (  31.273s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.397 (  31.273s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.397 (  31.273s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.397 (  31.273s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.397 (  31.273s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.397 (  31.273s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.397 (  31.273s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.397 (  31.273s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.397 (  31.273s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.398 (  31.274s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.398 (  31.274s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.398 (  31.274s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.398 (  31.274s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.398 (  31.274s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.398 (  31.274s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.398 (  31.274s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.398 (  31.274s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.398 (  31.274s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.398 (  31.274s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.398 (  31.274s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.398 (  31.274s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.399 (  31.274s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.399 (  31.274s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.399 (  31.274s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.399 (  31.274s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.399 (  31.274s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.399 (  31.274s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.399 (  31.275s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.399 (  31.275s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.399 (  31.275s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.399 (  31.275s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.399 (  31.275s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.399 (  31.275s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.400 (  31.276s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.400 (  31.276s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.400 (  31.276s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.401 (  31.276s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.401 (  31.276s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.401 (  31.276s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.401 (  31.276s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.401 (  31.276s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.401 (  31.276s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.401 (  31.276s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.401 (  31.277s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.401 (  31.277s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.406 (  31.282s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.406 (  31.282s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.406 (  31.282s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.406 (  31.282s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.406 (  31.282s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.406 (  31.282s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.406 (  31.282s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.406 (  31.282s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.406 (  31.282s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.406 (  31.282s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.410 (  31.286s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.410 (  31.286s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.417 (  31.293s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.417 (  31.293s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.417 (  31.293s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.417 (  31.293s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.417 (  31.293s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.418 (  31.293s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.418 (  31.293s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.418 (  31.293s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.418 (  31.293s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.418 (  31.293s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.422 (  31.297s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.422 (  31.297s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.429 (  31.305s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.429 (  31.305s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.429 (  31.305s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.429 (  31.305s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.429 (  31.305s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.429 (  31.305s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.430 (  31.305s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.430 (  31.305s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.430 (  31.305s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.430 (  31.305s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.433 (  31.309s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.433 (  31.309s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.435 (  31.311s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.435 (  31.311s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.435 (  31.311s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.435 (  31.311s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.435 (  31.311s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.435 (  31.311s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.435 (  31.311s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.435 (  31.311s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.435 (  31.311s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.435 (  31.311s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.435 (  31.311s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.435 (  31.311s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.436 (  31.311s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.436 (  31.311s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.436 (  31.311s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.436 (  31.311s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.436 (  31.311s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.436 (  31.311s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.436 (  31.311s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.436 (  31.312s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.436 (  31.312s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.436 (  31.312s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.436 (  31.312s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.436 (  31.312s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.436 (  31.312s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.436 (  31.312s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.436 (  31.312s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.436 (  31.312s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.436 (  31.312s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.436 (  31.312s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.436 (  31.312s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.436 (  31.312s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.436 (  31.312s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.436 (  31.312s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.437 (  31.312s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.437 (  31.312s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:24.438 (  31.314s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.438 (  31.314s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.438 (  31.314s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.438 (  31.314s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.438 (  31.314s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:24.438 (  31.314s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:24.438 (  31.314s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:24.438 (  31.314s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:24.438 (  31.314s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:24.438 (  31.314s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:25.627 (  32.503s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:25.627 (  32.503s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:25.627 (  32.503s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:25.627 (  32.503s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:25.627 (  32.503s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:25.628 (  32.504s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:25.628 (  32.504s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:25.628 (  32.504s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:25.628 (  32.504s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:25.628 (  32.504s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:25.628 (  32.504s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:25.628 (  32.504s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:25.628 (  32.504s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:25.628 (  32.504s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:25.628 (  32.504s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:25.628 (  32.504s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:25.628 (  32.504s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:25.628 (  32.504s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:25.628 (  32.504s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:25.628 (  32.504s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:25.628 (  32.504s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:25.628 (  32.504s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:25.628 (  32.504s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:25.628 (  32.504s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:25.628 (  32.504s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:25.628 (  32.504s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:25.628 (  32.504s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:25.628 (  32.504s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:25.628 (  32.504s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:25.629 (  32.504s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:25.629 (  32.504s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:25.629 (  32.504s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:25.629 (  32.504s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:25.629 (  32.504s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:25.629 (  32.505s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:25.629 (  32.505s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:25.629 (  32.505s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:25.629 (  32.505s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:25.751 (  32.627s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:25.751 (  32.627s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:25.751 (  32.627s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:25.751 (  32.627s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:25.751 (  32.627s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:25.751 (  32.627s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:25.751 (  32.627s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:25.751 (  32.627s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:25.751 (  32.627s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:25.751 (  32.627s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:25.797 (  32.673s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:25.797 (  32.673s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:25.923 (  32.799s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:25.923 (  32.799s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:25.923 (  32.799s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:25.923 (  32.799s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:25.923 (  32.799s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:25.923 (  32.799s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:25.923 (  32.799s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:25.923 (  32.799s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:25.923 (  32.799s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:25.923 (  32.799s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:25.967 (  32.842s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:25.967 (  32.842s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:25.967 (  32.843s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:25.967 (  32.843s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:25.967 (  32.843s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:25.967 (  32.843s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:25.967 (  32.843s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:25.967 (  32.843s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:25.967 (  32.843s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:25.967 (  32.843s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:25.967 (  32.843s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:25.967 (  32.843s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:25.968 (  32.844s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:25.968 (  32.844s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:25.968 (  32.844s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:25.968 (  32.844s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:25.968 (  32.844s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:25.968 (  32.844s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:25.968 (  32.844s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:25.968 (  32.844s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:25.968 (  32.844s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:25.968 (  32.844s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:25.968 (  32.844s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:25.968 (  32.844s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:25.971 (  32.847s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:25.971 (  32.847s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:25.971 (  32.847s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:25.971 (  32.847s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:25.971 (  32.847s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:25.971 (  32.847s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:25.971 (  32.847s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:25.971 (  32.847s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:25.971 (  32.847s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:25.971 (  32.847s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:25.971 (  32.847s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:25.971 (  32.847s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:25.973 (  32.848s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:25.973 (  32.848s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:25.973 (  32.848s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:25.973 (  32.848s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:25.973 (  32.848s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-09-10 19:54:25.973 (  32.849s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:25.973 (  32.849s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:25.973 (  32.849s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:25.973 (  32.849s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:25.973 (  32.849s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-09-10 19:54:25.973 (  32.849s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:25.973 (  32.849s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:25.974 (  32.850s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:25.974 (  32.850s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:25.974 (  32.850s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:25.974 (  32.850s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:25.974 (  32.850s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:25.974 (  32.850s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:25.974 (  32.850s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:25.974 (  32.850s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:54:25.974 (  32.850s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:54:25.974 (  32.850s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-10 19:54:25.974 (  32.850s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:54:25.974 (  32.850s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.977 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.978 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.978 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.978 (  32.853s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.978 (  32.854s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.978 (  32.854s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.978 (  32.854s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.978 (  32.854s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.978 (  32.854s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.978 (  32.854s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.978 (  32.854s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.978 (  32.854s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.979 (  32.855s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.979 (  32.855s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:54:25.979 (  32.855s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted

Compilation Analysis: ================================================================================
Compilation Analysis: Compilation Cause
Compilation Analysis:   most likely user code trying to access tensor value before torch_xla.sync
Compilation Analysis: Graph Info: 
Compilation Analysis:   Graph Hash: 85313521f7a99f71abd1320109a6355c
Compilation Analysis:   Number of Graph Inputs: 21
Compilation Analysis:   Number of Graph Outputs: 1
Compilation Analysis: Python Frame Triggered Execution: 
Compilation Analysis:   __call__ (/localdev/jameszianxu/tt-xla/tt_torch/backend/backend.py:99)
Compilation Analysis:   _fn (/localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:838)
Compilation Analysis:   wrapper (/localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/utils/generic.py:953)
Compilation Analysis:   _call_impl (/localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762)
Compilation Analysis:   _fn (/localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:655)
Compilation Analysis:   _wrapped_call_impl (/localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1749)
Compilation Analysis:   llama (/localdev/jameszianxu/tt-xla/examples/pytorch/llama.py:155)
Compilation Analysis:   <module> (/localdev/jameszianxu/tt-xla/examples/pytorch/llama.py:207)
Compilation Analysis: --------------------------------------------------------------------------------
Compilation Analysis: ================================================================================
2025-09-10 19:54:26.009 (  32.885s) [        F1708000]     client_instance.cc:428      1| ClientInstance::PJRT_Client_Compile
2025-09-10 19:54:26.009 (  32.885s) [        F1708000]      module_builder.cc:103      1| ModuleBuilder::buildModule
2025-09-10 19:54:26.012 (  32.888s) [        F1708000]      module_builder.cc:165      1| VHLO Module:
module @SyncTensorsGraph.448 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, %arg1: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg2: !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>, %arg4: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg5: !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>, %arg6: !vhlo.tensor_v1<1x7x!vhlo.i64_v1>, %arg7: !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, %arg8: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg9: !vhlo.tensor_v1<7x!vhlo.i64_v1>, %arg10: !vhlo.tensor_v1<!vhlo.i64_v1>, %arg11: !vhlo.tensor_v1<1x8x32x128x!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg13: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg14: !vhlo.tensor_v1<64x!vhlo.f32_v1>, %arg15: !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>, %arg16: !vhlo.tensor_v1<1x8x32x128x!vhlo.bf16_v1>, %arg17: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg18: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg19: !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>, %arg20: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi64>>}> : () -> !vhlo.tensor_v1<32x!vhlo.i64_v1>
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>>}> : () -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %3 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF800000> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %4 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %5 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<3.25520843E-4> : tensor<1x7xf32>>}> : () -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %6 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0> : tensor<7xi64>>}> : () -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %7 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1> : tensor<i64>>}> : () -> !vhlo.tensor_v1<!vhlo.i64_v1>
    %8 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %9 = "vhlo.broadcast_in_dim_v1"(%8) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x32x!vhlo.bf16_v1>
    %10 = "vhlo.broadcast_in_dim_v1"(%7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x32x!vhlo.i64_v1>
    %11 = "vhlo.broadcast_in_dim_v1"(%4) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %12 = "vhlo.reshape_v1"(%arg20) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %13 = "vhlo.custom_call_v1"(%12) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_norm_weight">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1>
    %15 = "vhlo.convert_v1"(%14) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %16 = "vhlo.broadcast_in_dim_v1"(%15) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %17 = "vhlo.reshape_v1"(%arg7) : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>
    %18 = "vhlo.custom_call_v1"(%17) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_embed_tokens_weight">}>} : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>
    %19 = "vhlo.reshape_v1"(%18) : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>
    %20 = "vhlo.reshape_v1"(%arg6) : (!vhlo.tensor_v1<1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>
    %21 = "vhlo.custom_call_v1"(%20) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_0">}>} : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>
    %22 = "vhlo.reshape_v1"(%21) : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.ui32_v1>
    %24 = "vhlo.gather_v2"(%19, %23) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 3072]> : tensor<2xi64>>, start_index_map = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x!vhlo.ui32_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %25 = "vhlo.reshape_v1"(%24) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %26 = "vhlo.reshape_v1"(%arg8) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %27 = "vhlo.custom_call_v1"(%26) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___input_layernorm_weight">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %28 = "vhlo.reshape_v1"(%27) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1>
    %29 = "vhlo.convert_v1"(%28) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %30 = "vhlo.broadcast_in_dim_v1"(%29) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %31 = "vhlo.convert_v1"(%25) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %32 = "vhlo.power_v1"(%31, %11) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %33 = "vhlo.reduce_v1"(%32, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %242 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%242) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %34 = "vhlo.multiply_v1"(%33, %5) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %35 = "vhlo.reshape_v1"(%34) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %36 = "vhlo.broadcast_in_dim_v1"(%arg1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %37 = "vhlo.add_v1"(%35, %36) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %38 = "vhlo.rsqrt_v2"(%37) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %39 = "vhlo.reshape_v1"(%38) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %40 = "vhlo.broadcast_in_dim_v1"(%39) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %41 = "vhlo.multiply_v1"(%31, %40) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %42 = "vhlo.convert_v1"(%41) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %43 = "vhlo.convert_v1"(%42) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %44 = "vhlo.multiply_v1"(%30, %43) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %45 = "vhlo.convert_v1"(%44) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %46 = "vhlo.reshape_v1"(%45) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %47 = "vhlo.reshape_v1"(%arg17) : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>
    %48 = "vhlo.custom_call_v1"(%47) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>
    %49 = "vhlo.reshape_v1"(%48) : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %50 = "vhlo.transpose_v1"(%49) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,3072]{0,1}">} : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %51 = "vhlo.dot_general_v2"(%46, %50) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %52 = "vhlo.reshape_v1"(%51) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>
    %53 = "vhlo.transpose_v1"(%52) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,24,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %54 = "vhlo.convert_v1"(%53) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,24,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %55 = "vhlo.reshape_v1"(%arg14) : (!vhlo.tensor_v1<64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>
    %56 = "vhlo.custom_call_v1"(%55) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_rotary_emb_inv_freq">}>} : (!vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>
    %57 = "vhlo.reshape_v1"(%56) : (!vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>
    %58 = "vhlo.reshape_v1"(%arg9) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>
    %59 = "vhlo.custom_call_v1"(%58) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_1">}>} : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>
    %60 = "vhlo.reshape_v1"(%59) : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %61 = "vhlo.convert_v1"(%59) : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.f32_v1>
    %62 = "vhlo.dot_general_v2"(%57, %61) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>
    %63 = "vhlo.transpose_v1"(%62) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1]> : tensor<3xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[1, 2, 0]> : tensor<3xindex>>, xla_shape = #vhlo.string_v1<"f32[1,7,64]{1,2,0}">} : (!vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>
    %64 = "vhlo.concatenate_v1"(%63, %63) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %65 = "vhlo.cosine_v2"(%64) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %66 = "vhlo.convert_v1"(%65) : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>
    %67 = "vhlo.reshape_v1"(%66) : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>
    %68 = "vhlo.convert_v1"(%67) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>
    %69 = "vhlo.reshape_v1"(%68) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %70 = "vhlo.broadcast_in_dim_v1"(%69) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %71 = "vhlo.multiply_v1"(%54, %70) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %72 = "vhlo.convert_v1"(%71) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %73 = "vhlo.slice_v1"(%53) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 7, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>
    %74 = "vhlo.negate_v1"(%73) : (!vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>
    %75 = "vhlo.slice_v1"(%53) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 7, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>
    %76 = "vhlo.concatenate_v1"(%74, %75) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %77 = "vhlo.convert_v1"(%76) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %78 = "vhlo.sine_v2"(%64) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %79 = "vhlo.convert_v1"(%78) : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>
    %80 = "vhlo.reshape_v1"(%79) : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>
    %81 = "vhlo.convert_v1"(%80) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>
    %82 = "vhlo.reshape_v1"(%81) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %83 = "vhlo.broadcast_in_dim_v1"(%82) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %84 = "vhlo.multiply_v1"(%77, %83) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %85 = "vhlo.convert_v1"(%84) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %86 = "vhlo.add_v1"(%72, %85) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %87 = "vhlo.reshape_v1"(%86) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>
    %88 = "vhlo.custom_call_v1"(%arg16) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_2">}>} : (!vhlo.tensor_v1<1x8x32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x32x128x!vhlo.bf16_v1>
    %89 = "vhlo.compare_v1"(%60, %6) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 LT>}> : (!vhlo.tensor_v1<7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.bool_v1>
    %90 = "vhlo.broadcast_in_dim_v1"(%arg10) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %91 = "vhlo.add_v1"(%60, %90) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %92 = "vhlo.select_v1"(%89, %91, %60) : (!vhlo.tensor_v1<7x!vhlo.bool_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %93 = "vhlo.reshape_v1"(%92) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x1x!vhlo.i64_v1>
    %94 = "vhlo.reshape_v1"(%arg15) : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>
    %95 = "vhlo.custom_call_v1"(%94) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>
    %96 = "vhlo.reshape_v1"(%95) : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>
    %97 = "vhlo.transpose_v1"(%96) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,1024]{0,1}">} : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>
    %98 = "vhlo.dot_general_v2"(%46, %97) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>
    %99 = "vhlo.reshape_v1"(%98) : (!vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>
    %100 = "vhlo.transpose_v1"(%99) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %101 = "vhlo.convert_v1"(%100) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %102 = "vhlo.broadcast_in_dim_v1"(%69) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %103 = "vhlo.multiply_v1"(%101, %102) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %104 = "vhlo.convert_v1"(%103) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %105 = "vhlo.slice_v1"(%100) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 7, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %106 = "vhlo.negate_v1"(%105) : (!vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %107 = "vhlo.slice_v1"(%100) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 7, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %108 = "vhlo.concatenate_v1"(%106, %107) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %109 = "vhlo.convert_v1"(%108) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %110 = "vhlo.broadcast_in_dim_v1"(%82) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %111 = "vhlo.multiply_v1"(%109, %110) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %112 = "vhlo.convert_v1"(%111) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %113 = "vhlo.add_v1"(%104, %112) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %114 = "vhlo.scatter_v2"(%88, %93, %113) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg22: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg22) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x32x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x32x128x!vhlo.bf16_v1>
    %115 = "vhlo.broadcast_in_dim_v1"(%114) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x32x128x!vhlo.bf16_v1>
    %116 = "vhlo.reshape_v1"(%115) : (!vhlo.tensor_v1<1x8x3x32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x32x128x!vhlo.bf16_v1>
    %117 = "vhlo.transpose_v1"(%116) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 3, 1, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,24,128,32]{2,3,1,0}">} : (!vhlo.tensor_v1<1x24x32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x128x32x!vhlo.bf16_v1>
    %118 = "vhlo.reshape_v1"(%117) : (!vhlo.tensor_v1<1x24x128x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x128x32x!vhlo.bf16_v1>
    %119 = "vhlo.dot_general_v2"(%87, %118) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x128x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x32x!vhlo.bf16_v1>
    %120 = "vhlo.reshape_v1"(%119) : (!vhlo.tensor_v1<24x7x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x32x!vhlo.bf16_v1>
    %121 = "vhlo.convert_v1"(%120) : (!vhlo.tensor_v1<1x24x7x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x32x!vhlo.f32_v1>
    %122 = "vhlo.broadcast_in_dim_v1"(%arg13) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x32x!vhlo.f32_v1>
    %123 = "vhlo.multiply_v1"(%121, %122) : (!vhlo.tensor_v1<1x24x7x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x32x!vhlo.f32_v1>
    %124 = "vhlo.convert_v1"(%123) : (!vhlo.tensor_v1<1x24x7x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x32x!vhlo.bf16_v1>
    %125 = "vhlo.broadcast_in_dim_v1"(%1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x32x!vhlo.i64_v1>
    %126 = "vhlo.broadcast_in_dim_v1"(%2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x32x!vhlo.i64_v1>
    %127 = "vhlo.subtract_v1"(%125, %126) : (!vhlo.tensor_v1<7x32x!vhlo.i64_v1>, !vhlo.tensor_v1<7x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x32x!vhlo.i64_v1>
    %128 = "vhlo.compare_v1"(%127, %10) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GE>}> : (!vhlo.tensor_v1<7x32x!vhlo.i64_v1>, !vhlo.tensor_v1<7x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x32x!vhlo.bool_v1>
    %129 = "vhlo.broadcast_in_dim_v1"(%arg12) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x32x!vhlo.bf16_v1>
    %130 = "vhlo.select_v1"(%128, %129, %9) : (!vhlo.tensor_v1<7x32x!vhlo.bool_v1>, !vhlo.tensor_v1<7x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x32x!vhlo.bf16_v1>
    %131 = "vhlo.convert_v1"(%130) : (!vhlo.tensor_v1<7x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x32x!vhlo.f32_v1>
    %132 = "vhlo.broadcast_in_dim_v1"(%60) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x32x!vhlo.i64_v1>
    %133 = "vhlo.compare_v1"(%125, %132) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<7x32x!vhlo.i64_v1>, !vhlo.tensor_v1<7x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x32x!vhlo.bool_v1>
    %134 = "vhlo.convert_v1"(%133) : (!vhlo.tensor_v1<7x32x!vhlo.bool_v1>) -> !vhlo.tensor_v1<7x32x!vhlo.f32_v1>
    %135 = "vhlo.multiply_v1"(%131, %134) : (!vhlo.tensor_v1<7x32x!vhlo.f32_v1>, !vhlo.tensor_v1<7x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<7x32x!vhlo.f32_v1>
    %136 = "vhlo.convert_v1"(%135) : (!vhlo.tensor_v1<7x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<7x32x!vhlo.bf16_v1>
    %137 = "vhlo.reshape_v1"(%136) : (!vhlo.tensor_v1<7x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x32x!vhlo.bf16_v1>
    %138 = "vhlo.broadcast_in_dim_v1"(%137) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x32x!vhlo.bf16_v1>
    %139 = "vhlo.add_v1"(%124, %138) : (!vhlo.tensor_v1<1x24x7x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x7x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x32x!vhlo.bf16_v1>
    %140 = "vhlo.convert_v1"(%139) : (!vhlo.tensor_v1<1x24x7x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x32x!vhlo.f32_v1>
    %141 = "vhlo.reduce_v1"(%140, %3) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %242 = "vhlo.maximum_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%242) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x7x32x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>
    %142 = "vhlo.broadcast_in_dim_v1"(%141) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x32x!vhlo.f32_v1>
    %143 = "vhlo.subtract_v1"(%140, %142) : (!vhlo.tensor_v1<1x24x7x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x32x!vhlo.f32_v1>
    %144 = "vhlo.exponential_v2"(%143) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x24x7x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x32x!vhlo.f32_v1>
    %145 = "vhlo.reduce_v1"(%144, %0) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %242 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%242) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x7x32x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>
    %146 = "vhlo.broadcast_in_dim_v1"(%145) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x32x!vhlo.f32_v1>
    %147 = "vhlo.divide_v1"(%144, %146) : (!vhlo.tensor_v1<1x24x7x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x32x!vhlo.f32_v1>
    %148 = "vhlo.convert_v1"(%147) : (!vhlo.tensor_v1<1x24x7x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x32x!vhlo.bf16_v1>
    %149 = "vhlo.reshape_v1"(%148) : (!vhlo.tensor_v1<1x24x7x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x32x!vhlo.bf16_v1>
    %150 = "vhlo.custom_call_v1"(%arg11) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_3">}>} : (!vhlo.tensor_v1<1x8x32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x32x128x!vhlo.bf16_v1>
    %151 = "vhlo.reshape_v1"(%arg5) : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>
    %152 = "vhlo.custom_call_v1"(%151) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>
    %153 = "vhlo.reshape_v1"(%152) : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>
    %154 = "vhlo.transpose_v1"(%153) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,1024]{0,1}">} : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>
    %155 = "vhlo.dot_general_v2"(%46, %154) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>
    %156 = "vhlo.reshape_v1"(%155) : (!vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>
    %157 = "vhlo.transpose_v1"(%156) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %158 = "vhlo.scatter_v2"(%150, %93, %157) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg22: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg22) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x32x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x32x128x!vhlo.bf16_v1>
    %159 = "vhlo.broadcast_in_dim_v1"(%158) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x32x128x!vhlo.bf16_v1>
    %160 = "vhlo.reshape_v1"(%159) : (!vhlo.tensor_v1<1x8x3x32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x32x128x!vhlo.bf16_v1>
    %161 = "vhlo.dot_general_v2"(%149, %160) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x7x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>
    %162 = "vhlo.reshape_v1"(%161) : (!vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %163 = "vhlo.transpose_v1"(%162) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,7,24,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>
    %164 = "vhlo.reshape_v1"(%163) : (!vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %165 = "vhlo.reshape_v1"(%arg4) : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>
    %166 = "vhlo.custom_call_v1"(%165) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_o_proj_weight">}>} : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>
    %167 = "vhlo.reshape_v1"(%166) : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %168 = "vhlo.transpose_v1"(%167) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,3072]{0,1}">} : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %169 = "vhlo.dot_general_v2"(%164, %168) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %170 = "vhlo.reshape_v1"(%169) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %171 = "vhlo.add_v1"(%25, %170) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %172 = "vhlo.reshape_v1"(%arg18) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %173 = "vhlo.custom_call_v1"(%172) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___post_attention_layernorm_weight">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %174 = "vhlo.reshape_v1"(%173) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1>
    %175 = "vhlo.convert_v1"(%174) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %176 = "vhlo.broadcast_in_dim_v1"(%175) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %177 = "vhlo.convert_v1"(%171) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %178 = "vhlo.power_v1"(%177, %11) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %179 = "vhlo.reduce_v1"(%178, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %242 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%242) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %180 = "vhlo.multiply_v1"(%179, %5) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %181 = "vhlo.reshape_v1"(%180) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %182 = "vhlo.add_v1"(%181, %36) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %183 = "vhlo.rsqrt_v2"(%182) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %184 = "vhlo.reshape_v1"(%183) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %185 = "vhlo.broadcast_in_dim_v1"(%184) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %186 = "vhlo.multiply_v1"(%177, %185) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %187 = "vhlo.convert_v1"(%186) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %188 = "vhlo.convert_v1"(%187) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %189 = "vhlo.multiply_v1"(%176, %188) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %190 = "vhlo.convert_v1"(%189) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %191 = "vhlo.reshape_v1"(%190) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %192 = "vhlo.reshape_v1"(%arg19) : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>
    %193 = "vhlo.custom_call_v1"(%192) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_gate_proj_weight">}>} : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>
    %194 = "vhlo.reshape_v1"(%193) : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>
    %195 = "vhlo.transpose_v1"(%194) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,8192]{0,1}">} : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>
    %196 = "vhlo.dot_general_v2"(%191, %195) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %197 = "vhlo.reshape_v1"(%196) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %198 = "vhlo.convert_v1"(%197) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %199 = "vhlo.logistic_v2"(%197) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %200 = "vhlo.convert_v1"(%199) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %201 = "vhlo.multiply_v1"(%198, %200) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %202 = "vhlo.convert_v1"(%201) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %203 = "vhlo.convert_v1"(%202) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %204 = "vhlo.reshape_v1"(%arg3) : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>
    %205 = "vhlo.custom_call_v1"(%204) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_up_proj_weight">}>} : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>
    %206 = "vhlo.reshape_v1"(%205) : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>
    %207 = "vhlo.transpose_v1"(%206) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,8192]{0,1}">} : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>
    %208 = "vhlo.dot_general_v2"(%191, %207) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %209 = "vhlo.reshape_v1"(%208) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %210 = "vhlo.convert_v1"(%209) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %211 = "vhlo.multiply_v1"(%203, %210) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %212 = "vhlo.convert_v1"(%211) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %213 = "vhlo.reshape_v1"(%212) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %214 = "vhlo.reshape_v1"(%arg2) : (!vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1>
    %215 = "vhlo.custom_call_v1"(%214) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_down_proj_weight">}>} : (!vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1>
    %216 = "vhlo.reshape_v1"(%215) : (!vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>
    %217 = "vhlo.transpose_v1"(%216) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,3072]{0,1}">} : (!vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>
    %218 = "vhlo.dot_general_v2"(%213, %217) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %219 = "vhlo.reshape_v1"(%218) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %220 = "vhlo.add_v1"(%171, %219) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %221 = "vhlo.convert_v1"(%220) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %222 = "vhlo.power_v1"(%221, %11) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %223 = "vhlo.reduce_v1"(%222, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %242 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%242) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %224 = "vhlo.multiply_v1"(%223, %5) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %225 = "vhlo.reshape_v1"(%224) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %226 = "vhlo.add_v1"(%225, %36) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %227 = "vhlo.rsqrt_v2"(%226) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %228 = "vhlo.reshape_v1"(%227) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %229 = "vhlo.broadcast_in_dim_v1"(%228) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %230 = "vhlo.multiply_v1"(%221, %229) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %231 = "vhlo.convert_v1"(%230) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %232 = "vhlo.convert_v1"(%231) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %233 = "vhlo.multiply_v1"(%16, %232) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %234 = "vhlo.convert_v1"(%233) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %235 = "vhlo.reshape_v1"(%234) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %236 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>
    %237 = "vhlo.custom_call_v1"(%236) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___lm_head_weight">}>} : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>
    %238 = "vhlo.reshape_v1"(%237) : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>
    %239 = "vhlo.transpose_v1"(%238) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,128256]{0,1}">} : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x128256x!vhlo.bf16_v1>
    %240 = "vhlo.dot_general_v2"(%235, %239) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>
    %241 = "vhlo.reshape_v1"(%240) : (!vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>
    "vhlo.return_v1"(%241) : (!vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}
2025-09-10 19:54:26.027 (  32.903s) [        F1708000]      module_builder.cc:185      1| Is using shardy? true
2025-09-10 19:54:26.031 (  32.907s) [        F1708000]      module_builder.cc:203      1| SHLO Module:
module @SyncTensorsGraph.448 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg1: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg2: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>}, %arg3: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg4: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>}, %arg5: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg6: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg7: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg8: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg9: tensor<7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg10: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg11: tensor<1x8x32x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}, %arg12: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg13: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg14: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg15: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg16: tensor<1x8x32x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}) -> tensor<1x7x128256xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_4 = stablehlo.constant dense<0> : tensor<7xi64>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x32xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x32xi64>
    %2 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %4 = stablehlo.custom_call @tt.mark_argument(%3) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_norm_weight"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %5 = stablehlo.convert %4 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %6 = stablehlo.reshape %5 : (tensor<1x1x3072xf32>) -> tensor<3072xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %8 = stablehlo.reshape %arg7 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %9 = stablehlo.custom_call @tt.mark_argument(%8) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_embed_tokens_weight"}} : (tensor<1x128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %10 = stablehlo.reshape %9 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %11 = stablehlo.reshape %arg6 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %12 = stablehlo.custom_call @tt.mark_argument(%11) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_0"}} : (tensor<1x1x7xi64>) -> tensor<1x1x7xi64>
    %13 = stablehlo.convert %12 : (tensor<1x1x7xi64>) -> tensor<1x1x7xui32>
    %14 = stablehlo.reshape %13 : (tensor<1x1x7xui32>) -> tensor<7xui32>
    %15 = "stablehlo.gather"(%10, %14) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %16 = stablehlo.reshape %15 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %17 = stablehlo.reshape %arg8 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %18 = stablehlo.custom_call @tt.mark_argument(%17) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %19 = stablehlo.convert %18 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x1x3072xf32>) -> tensor<3072xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.convert %16 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %23 = stablehlo.power %22, %2 : tensor<1x7x3072xf32>
    %24 = stablehlo.reduce(%23 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %25 = stablehlo.multiply %24, %cst_3 : tensor<1x7xf32>
    %26 = stablehlo.reshape %25 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %28 = stablehlo.add %26, %27 : tensor<1x7x1xf32>
    %29 = stablehlo.rsqrt %28 : tensor<1x7x1xf32>
    %30 = stablehlo.reshape %29 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %31 = stablehlo.broadcast_in_dim %30, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %32 = stablehlo.multiply %22, %31 : tensor<1x7x3072xf32>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %35 = stablehlo.multiply %21, %34 : tensor<1x7x3072xf32>
    %36 = stablehlo.convert %35 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %37 = stablehlo.reshape %36 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %38 = stablehlo.reshape %arg17 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %39 = stablehlo.custom_call @tt.mark_argument(%38) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}} : (tensor<1x3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %40 = stablehlo.reshape %39 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %41 = stablehlo.transpose %40, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %42 = stablehlo.dot_general %37, %41, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %43 = stablehlo.reshape %42 : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %44 = stablehlo.transpose %43, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %45 = stablehlo.convert %44 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %46 = stablehlo.reshape %arg14 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %47 = stablehlo.custom_call @tt.mark_argument(%46) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "l__self___model_rotary_emb_inv_freq"}} : (tensor<1x1x64xf32>) -> tensor<1x1x64xf32>
    %48 = stablehlo.reshape %47 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %49 = stablehlo.reshape %arg9 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %50 = stablehlo.custom_call @tt.mark_argument(%49) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_1"}} : (tensor<1x1x7xi64>) -> tensor<1x1x7xi64>
    %51 = stablehlo.reshape %50 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %52 = stablehlo.convert %50 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %53 = stablehlo.dot_general %48, %52, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %54 = stablehlo.transpose %53, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %55 = stablehlo.concatenate %54, %54, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %56 = stablehlo.cosine %55 : tensor<1x7x128xf32>
    %57 = stablehlo.convert %56 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %58 = stablehlo.convert %57 : (tensor<1x7x128xbf16>) -> tensor<1x7x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %60 = stablehlo.multiply %45, %59 : tensor<1x24x7x128xf32>
    %61 = stablehlo.convert %60 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %62 = stablehlo.slice %44 [0:1, 0:24, 0:7, 64:128] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %63 = stablehlo.negate %62 : tensor<1x24x7x64xbf16>
    %64 = stablehlo.slice %44 [0:1, 0:24, 0:7, 0:64] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %65 = stablehlo.concatenate %63, %64, dim = 3 : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %67 = stablehlo.sine %55 : tensor<1x7x128xf32>
    %68 = stablehlo.convert %67 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %69 = stablehlo.convert %68 : (tensor<1x7x128xbf16>) -> tensor<1x7x128xf32>
    %70 = stablehlo.broadcast_in_dim %69, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %71 = stablehlo.multiply %66, %70 : tensor<1x24x7x128xf32>
    %72 = stablehlo.convert %71 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %73 = stablehlo.add %61, %72 : tensor<1x24x7x128xbf16>
    %74 = stablehlo.reshape %73 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %75 = stablehlo.custom_call @tt.mark_argument(%arg16) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_2"}} : (tensor<1x8x32x128xbf16>) -> tensor<1x8x32x128xbf16>
    %76 = stablehlo.compare  LT, %51, %c_4 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %77 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %78 = stablehlo.add %51, %77 : tensor<7xi64>
    %79 = stablehlo.select %76, %78, %51 : tensor<7xi1>, tensor<7xi64>
    %80 = stablehlo.reshape %79 : (tensor<7xi64>) -> tensor<7x1xi64>
    %81 = stablehlo.reshape %arg15 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %82 = stablehlo.custom_call @tt.mark_argument(%81) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}} : (tensor<1x1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %84 = stablehlo.transpose %83, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %85 = stablehlo.dot_general %37, %84, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %86 = stablehlo.reshape %85 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %87 = stablehlo.transpose %86, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %88 = stablehlo.convert %87 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %89 = stablehlo.broadcast_in_dim %58, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %90 = stablehlo.multiply %88, %89 : tensor<1x8x7x128xf32>
    %91 = stablehlo.convert %90 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %92 = stablehlo.slice %87 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %93 = stablehlo.negate %92 : tensor<1x8x7x64xbf16>
    %94 = stablehlo.slice %87 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %95 = stablehlo.concatenate %93, %94, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %96 = stablehlo.convert %95 : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %97 = stablehlo.broadcast_in_dim %69, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %98 = stablehlo.multiply %96, %97 : tensor<1x8x7x128xf32>
    %99 = stablehlo.convert %98 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %100 = stablehlo.add %91, %99 : tensor<1x8x7x128xbf16>
    %101 = "stablehlo.scatter"(%75, %80, %100) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x32x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x32x128xbf16>
    %102 = stablehlo.broadcast_in_dim %101, dims = [0, 1, 3, 4] : (tensor<1x8x32x128xbf16>) -> tensor<1x8x3x32x128xbf16>
    %103 = stablehlo.reshape %102 : (tensor<1x8x3x32x128xbf16>) -> tensor<1x24x32x128xbf16>
    %104 = stablehlo.transpose %103, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,32]{2,3,1,0}"} : (tensor<1x24x32x128xbf16>) -> tensor<1x24x128x32xbf16>
    %105 = stablehlo.reshape %104 : (tensor<1x24x128x32xbf16>) -> tensor<24x128x32xbf16>
    %106 = stablehlo.dot_general %74, %105, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x32xbf16>) -> tensor<24x7x32xbf16>
    %107 = stablehlo.convert %106 : (tensor<24x7x32xbf16>) -> tensor<24x7x32xf32>
    %108 = stablehlo.reshape %107 : (tensor<24x7x32xf32>) -> tensor<1x24x7x32xf32>
    %109 = stablehlo.broadcast_in_dim %arg13, dims = [] : (tensor<f32>) -> tensor<1x24x7x32xf32>
    %110 = stablehlo.multiply %108, %109 : tensor<1x24x7x32xf32>
    %111 = stablehlo.convert %110 : (tensor<1x24x7x32xf32>) -> tensor<1x24x7x32xbf16>
    %112 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<32xi64>) -> tensor<7x32xi64>
    %113 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x32xi64>
    %114 = stablehlo.subtract %112, %113 : tensor<7x32xi64>
    %115 = stablehlo.compare  GE, %114, %1 : (tensor<7x32xi64>, tensor<7x32xi64>) -> tensor<7x32xi1>
    %116 = stablehlo.broadcast_in_dim %arg12, dims = [] : (tensor<bf16>) -> tensor<7x32xbf16>
    %117 = stablehlo.select %115, %116, %0 : tensor<7x32xi1>, tensor<7x32xbf16>
    %118 = stablehlo.convert %117 : (tensor<7x32xbf16>) -> tensor<7x32xf32>
    %119 = stablehlo.broadcast_in_dim %51, dims = [0] : (tensor<7xi64>) -> tensor<7x32xi64>
    %120 = stablehlo.compare  GT, %112, %119 : (tensor<7x32xi64>, tensor<7x32xi64>) -> tensor<7x32xi1>
    %121 = stablehlo.convert %120 : (tensor<7x32xi1>) -> tensor<7x32xf32>
    %122 = stablehlo.multiply %118, %121 : tensor<7x32xf32>
    %123 = stablehlo.convert %122 : (tensor<7x32xf32>) -> tensor<7x32xbf16>
    %124 = stablehlo.reshape %123 : (tensor<7x32xbf16>) -> tensor<1x7x32xbf16>
    %125 = stablehlo.broadcast_in_dim %124, dims = [0, 2, 3] : (tensor<1x7x32xbf16>) -> tensor<1x24x7x32xbf16>
    %126 = stablehlo.add %111, %125 : tensor<1x24x7x32xbf16>
    %127 = stablehlo.convert %126 : (tensor<1x24x7x32xbf16>) -> tensor<1x24x7x32xf32>
    %128 = stablehlo.reduce(%127 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x7x32xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x32xf32>
    %130 = stablehlo.subtract %127, %129 : tensor<1x24x7x32xf32>
    %131 = stablehlo.exponential %130 : tensor<1x24x7x32xf32>
    %132 = stablehlo.reduce(%131 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x7x32xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x32xf32>
    %134 = stablehlo.divide %131, %133 : tensor<1x24x7x32xf32>
    %135 = stablehlo.convert %134 : (tensor<1x24x7x32xf32>) -> tensor<1x24x7x32xbf16>
    %136 = stablehlo.reshape %135 : (tensor<1x24x7x32xbf16>) -> tensor<24x7x32xbf16>
    %137 = stablehlo.custom_call @tt.mark_argument(%arg11) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_3"}} : (tensor<1x8x32x128xbf16>) -> tensor<1x8x32x128xbf16>
    %138 = stablehlo.reshape %arg5 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %139 = stablehlo.custom_call @tt.mark_argument(%138) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}} : (tensor<1x1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %140 = stablehlo.reshape %139 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %141 = stablehlo.transpose %140, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %142 = stablehlo.dot_general %37, %141, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %143 = stablehlo.reshape %142 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %144 = stablehlo.transpose %143, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %145 = "stablehlo.scatter"(%137, %80, %144) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x32x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x32x128xbf16>
    %146 = stablehlo.broadcast_in_dim %145, dims = [0, 1, 3, 4] : (tensor<1x8x32x128xbf16>) -> tensor<1x8x3x32x128xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x8x3x32x128xbf16>) -> tensor<24x32x128xbf16>
    %148 = stablehlo.dot_general %136, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x32xbf16>, tensor<24x32x128xbf16>) -> tensor<24x7x128xbf16>
    %149 = stablehlo.reshape %148 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %152 = stablehlo.reshape %arg4 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %153 = stablehlo.custom_call @tt.mark_argument(%152) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}} : (tensor<1x3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %155 = stablehlo.transpose %154, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %156 = stablehlo.dot_general %151, %155, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %157 = stablehlo.reshape %156 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %158 = stablehlo.add %16, %157 : tensor<1x7x3072xbf16>
    %159 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %160 = stablehlo.custom_call @tt.mark_argument(%159) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %161 = stablehlo.convert %160 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %162 = stablehlo.reshape %161 : (tensor<1x1x3072xf32>) -> tensor<3072xf32>
    %163 = stablehlo.broadcast_in_dim %162, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %164 = stablehlo.convert %158 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %165 = stablehlo.power %164, %2 : tensor<1x7x3072xf32>
    %166 = stablehlo.reduce(%165 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %167 = stablehlo.multiply %166, %cst_3 : tensor<1x7xf32>
    %168 = stablehlo.reshape %167 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %169 = stablehlo.add %168, %27 : tensor<1x7x1xf32>
    %170 = stablehlo.rsqrt %169 : tensor<1x7x1xf32>
    %171 = stablehlo.reshape %170 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %172 = stablehlo.broadcast_in_dim %171, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %173 = stablehlo.multiply %164, %172 : tensor<1x7x3072xf32>
    %174 = stablehlo.convert %173 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %175 = stablehlo.convert %174 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %176 = stablehlo.multiply %163, %175 : tensor<1x7x3072xf32>
    %177 = stablehlo.convert %176 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %178 = stablehlo.reshape %177 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %179 = stablehlo.reshape %arg19 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %180 = stablehlo.custom_call @tt.mark_argument(%179) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}} : (tensor<1x8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %181 = stablehlo.reshape %180 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %182 = stablehlo.transpose %181, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %183 = stablehlo.dot_general %178, %182, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %184 = stablehlo.reshape %183 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %185 = stablehlo.convert %184 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %186 = stablehlo.logistic %184 : tensor<1x7x8192xbf16>
    %187 = stablehlo.convert %186 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %188 = stablehlo.multiply %185, %187 : tensor<1x7x8192xf32>
    %189 = stablehlo.convert %188 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %190 = stablehlo.convert %189 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %191 = stablehlo.reshape %arg3 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %192 = stablehlo.custom_call @tt.mark_argument(%191) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}} : (tensor<1x8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %193 = stablehlo.reshape %192 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %194 = stablehlo.transpose %193, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %195 = stablehlo.dot_general %178, %194, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %196 = stablehlo.convert %195 : (tensor<7x8192xbf16>) -> tensor<7x8192xf32>
    %197 = stablehlo.reshape %196 : (tensor<7x8192xf32>) -> tensor<1x7x8192xf32>
    %198 = stablehlo.multiply %190, %197 : tensor<1x7x8192xf32>
    %199 = stablehlo.convert %198 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %200 = stablehlo.reshape %199 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %201 = stablehlo.reshape %arg2 : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %202 = stablehlo.custom_call @tt.mark_argument(%201) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}} : (tensor<1x3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %203 = stablehlo.reshape %202 : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %204 = stablehlo.transpose %203, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %205 = stablehlo.dot_general %200, %204, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %206 = stablehlo.reshape %205 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %207 = stablehlo.add %158, %206 : tensor<1x7x3072xbf16>
    %208 = stablehlo.convert %207 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %209 = stablehlo.power %208, %2 : tensor<1x7x3072xf32>
    %210 = stablehlo.reduce(%209 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %211 = stablehlo.multiply %210, %cst_3 : tensor<1x7xf32>
    %212 = stablehlo.reshape %211 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %213 = stablehlo.add %212, %27 : tensor<1x7x1xf32>
    %214 = stablehlo.rsqrt %213 : tensor<1x7x1xf32>
    %215 = stablehlo.reshape %214 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %216 = stablehlo.broadcast_in_dim %215, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %217 = stablehlo.multiply %208, %216 : tensor<1x7x3072xf32>
    %218 = stablehlo.convert %217 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %219 = stablehlo.convert %218 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %220 = stablehlo.multiply %7, %219 : tensor<1x7x3072xf32>
    %221 = stablehlo.convert %220 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %222 = stablehlo.reshape %221 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %223 = stablehlo.reshape %arg0 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %224 = stablehlo.custom_call @tt.mark_argument(%223) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___lm_head_weight"}} : (tensor<1x128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %225 = stablehlo.reshape %224 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %226 = stablehlo.transpose %225, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %227 = stablehlo.dot_general %222, %226, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %228 = stablehlo.reshape %227 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %228 : tensor<1x7x128256xbf16>
  }
}
2025-09-10 19:54:26.038 (  32.914s) [        F1708000]      module_builder.cc:212      1| SHLO Module after frontend StableHLO pipeline:
module @SyncTensorsGraph.448 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___lm_head_weight"}, %arg1: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_0"}, %arg2: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg3: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg4: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg5: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg6: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"}, %arg7: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg8: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg9: tensor<7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_1"}, %arg10: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_1"}, %arg11: tensor<1x8x32x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_3"}, %arg12: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_2"}, %arg13: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_3"}, %arg14: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg15: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg16: tensor<1x8x32x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_2"}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_norm_weight"}) -> tensor<1x7x128256xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_4 = stablehlo.constant dense<0> : tensor<7xi64>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x32xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x32xi64>
    %2 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %4 = stablehlo.convert %3 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %5 = stablehlo.reshape %4 : (tensor<1x1x3072xf32>) -> tensor<3072xf32>
    %6 = stablehlo.broadcast_in_dim %5, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %7 = stablehlo.reshape %arg7 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %8 = stablehlo.reshape %7 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %9 = stablehlo.reshape %arg6 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %10 = stablehlo.convert %9 : (tensor<1x1x7xi64>) -> tensor<1x1x7xui32>
    %11 = stablehlo.reshape %10 : (tensor<1x1x7xui32>) -> tensor<7xui32>
    %12 = "stablehlo.gather"(%8, %11) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %13 = stablehlo.reshape %12 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %14 = stablehlo.reshape %arg8 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %15 = stablehlo.convert %14 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x1x3072xf32>) -> tensor<3072xf32>
    %17 = stablehlo.broadcast_in_dim %16, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %18 = stablehlo.convert %13 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %19 = stablehlo.power %18, %2 : tensor<1x7x3072xf32>
    %20 = stablehlo.reduce(%19 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %21 = stablehlo.multiply %20, %cst_3 : tensor<1x7xf32>
    %22 = stablehlo.reshape %21 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %23 = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %24 = stablehlo.add %22, %23 : tensor<1x7x1xf32>
    %25 = stablehlo.rsqrt %24 : tensor<1x7x1xf32>
    %26 = stablehlo.reshape %25 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %27 = stablehlo.broadcast_in_dim %26, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %28 = stablehlo.multiply %18, %27 : tensor<1x7x3072xf32>
    %29 = stablehlo.convert %28 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %30 = stablehlo.convert %29 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %17, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.reshape %32 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %34 = stablehlo.reshape %arg17 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %35 = stablehlo.reshape %34 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %36 = stablehlo.transpose %35, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %37 = stablehlo.dot_general %33, %36, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %38 = stablehlo.reshape %37 : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %39 = stablehlo.transpose %38, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %40 = stablehlo.convert %39 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %41 = stablehlo.reshape %arg14 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %42 = stablehlo.reshape %41 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %43 = stablehlo.reshape %arg9 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %44 = stablehlo.reshape %43 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %45 = stablehlo.convert %43 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %46 = stablehlo.dot_general %42, %45, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %47 = stablehlo.transpose %46, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %48 = stablehlo.concatenate %47, %47, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %49 = stablehlo.cosine %48 : tensor<1x7x128xf32>
    %50 = stablehlo.convert %49 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xbf16>) -> tensor<1x7x128xf32>
    %52 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %53 = stablehlo.multiply %40, %52 : tensor<1x24x7x128xf32>
    %54 = stablehlo.convert %53 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %55 = stablehlo.slice %39 [0:1, 0:24, 0:7, 64:128] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %56 = stablehlo.negate %55 : tensor<1x24x7x64xbf16>
    %57 = stablehlo.slice %39 [0:1, 0:24, 0:7, 0:64] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %58 = stablehlo.concatenate %56, %57, dim = 3 : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %59 = stablehlo.convert %58 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %60 = stablehlo.sine %48 : tensor<1x7x128xf32>
    %61 = stablehlo.convert %60 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %62 = stablehlo.convert %61 : (tensor<1x7x128xbf16>) -> tensor<1x7x128xf32>
    %63 = stablehlo.broadcast_in_dim %62, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %64 = stablehlo.multiply %59, %63 : tensor<1x24x7x128xf32>
    %65 = stablehlo.convert %64 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %66 = stablehlo.add %54, %65 : tensor<1x24x7x128xbf16>
    %67 = stablehlo.reshape %66 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %68 = stablehlo.compare  LT, %44, %c_4 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %69 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %70 = stablehlo.add %44, %69 : tensor<7xi64>
    %71 = stablehlo.select %68, %70, %44 : tensor<7xi1>, tensor<7xi64>
    %72 = stablehlo.reshape %71 : (tensor<7xi64>) -> tensor<7x1xi64>
    %73 = stablehlo.reshape %arg15 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %74 = stablehlo.reshape %73 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %75 = stablehlo.transpose %74, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %76 = stablehlo.dot_general %33, %75, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %77 = stablehlo.reshape %76 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %78 = stablehlo.transpose %77, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %79 = stablehlo.convert %78 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %80 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %81 = stablehlo.multiply %79, %80 : tensor<1x8x7x128xf32>
    %82 = stablehlo.convert %81 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %83 = stablehlo.slice %78 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %84 = stablehlo.negate %83 : tensor<1x8x7x64xbf16>
    %85 = stablehlo.slice %78 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %86 = stablehlo.concatenate %84, %85, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %87 = stablehlo.convert %86 : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %88 = stablehlo.broadcast_in_dim %62, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %89 = stablehlo.multiply %87, %88 : tensor<1x8x7x128xf32>
    %90 = stablehlo.convert %89 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %91 = stablehlo.add %82, %90 : tensor<1x8x7x128xbf16>
    %92 = "stablehlo.scatter"(%arg16, %72, %91) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x32x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x32x128xbf16>
    %93 = stablehlo.broadcast_in_dim %92, dims = [0, 1, 3, 4] : (tensor<1x8x32x128xbf16>) -> tensor<1x8x3x32x128xbf16>
    %94 = stablehlo.reshape %93 : (tensor<1x8x3x32x128xbf16>) -> tensor<1x24x32x128xbf16>
    %95 = stablehlo.transpose %94, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,32]{2,3,1,0}"} : (tensor<1x24x32x128xbf16>) -> tensor<1x24x128x32xbf16>
    %96 = stablehlo.reshape %95 : (tensor<1x24x128x32xbf16>) -> tensor<24x128x32xbf16>
    %97 = stablehlo.dot_general %67, %96, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x32xbf16>) -> tensor<24x7x32xbf16>
    %98 = stablehlo.convert %97 : (tensor<24x7x32xbf16>) -> tensor<24x7x32xf32>
    %99 = stablehlo.reshape %98 : (tensor<24x7x32xf32>) -> tensor<1x24x7x32xf32>
    %100 = stablehlo.broadcast_in_dim %arg13, dims = [] : (tensor<f32>) -> tensor<1x24x7x32xf32>
    %101 = stablehlo.multiply %99, %100 : tensor<1x24x7x32xf32>
    %102 = stablehlo.convert %101 : (tensor<1x24x7x32xf32>) -> tensor<1x24x7x32xbf16>
    %103 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<32xi64>) -> tensor<7x32xi64>
    %104 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x32xi64>
    %105 = stablehlo.subtract %103, %104 : tensor<7x32xi64>
    %106 = stablehlo.compare  GE, %105, %1 : (tensor<7x32xi64>, tensor<7x32xi64>) -> tensor<7x32xi1>
    %107 = stablehlo.broadcast_in_dim %arg12, dims = [] : (tensor<bf16>) -> tensor<7x32xbf16>
    %108 = stablehlo.select %106, %107, %0 : tensor<7x32xi1>, tensor<7x32xbf16>
    %109 = stablehlo.convert %108 : (tensor<7x32xbf16>) -> tensor<7x32xf32>
    %110 = stablehlo.broadcast_in_dim %44, dims = [0] : (tensor<7xi64>) -> tensor<7x32xi64>
    %111 = stablehlo.compare  GT, %103, %110 : (tensor<7x32xi64>, tensor<7x32xi64>) -> tensor<7x32xi1>
    %112 = stablehlo.convert %111 : (tensor<7x32xi1>) -> tensor<7x32xf32>
    %113 = stablehlo.multiply %109, %112 : tensor<7x32xf32>
    %114 = stablehlo.convert %113 : (tensor<7x32xf32>) -> tensor<7x32xbf16>
    %115 = stablehlo.reshape %114 : (tensor<7x32xbf16>) -> tensor<1x7x32xbf16>
    %116 = stablehlo.broadcast_in_dim %115, dims = [0, 2, 3] : (tensor<1x7x32xbf16>) -> tensor<1x24x7x32xbf16>
    %117 = stablehlo.add %102, %116 : tensor<1x24x7x32xbf16>
    %118 = stablehlo.convert %117 : (tensor<1x24x7x32xbf16>) -> tensor<1x24x7x32xf32>
    %119 = stablehlo.reduce(%118 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x7x32xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %120 = stablehlo.broadcast_in_dim %119, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x32xf32>
    %121 = stablehlo.subtract %118, %120 : tensor<1x24x7x32xf32>
    %122 = stablehlo.exponential %121 : tensor<1x24x7x32xf32>
    %123 = stablehlo.reduce(%122 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x7x32xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %124 = stablehlo.broadcast_in_dim %123, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x32xf32>
    %125 = stablehlo.divide %122, %124 : tensor<1x24x7x32xf32>
    %126 = stablehlo.convert %125 : (tensor<1x24x7x32xf32>) -> tensor<1x24x7x32xbf16>
    %127 = stablehlo.reshape %126 : (tensor<1x24x7x32xbf16>) -> tensor<24x7x32xbf16>
    %128 = stablehlo.reshape %arg5 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %129 = stablehlo.reshape %128 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %130 = stablehlo.transpose %129, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %131 = stablehlo.dot_general %33, %130, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %132 = stablehlo.reshape %131 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %133 = stablehlo.transpose %132, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %134 = "stablehlo.scatter"(%arg11, %72, %133) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x32x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x32x128xbf16>
    %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 3, 4] : (tensor<1x8x32x128xbf16>) -> tensor<1x8x3x32x128xbf16>
    %136 = stablehlo.reshape %135 : (tensor<1x8x3x32x128xbf16>) -> tensor<24x32x128xbf16>
    %137 = stablehlo.dot_general %127, %136, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x32xbf16>, tensor<24x32x128xbf16>) -> tensor<24x7x128xbf16>
    %138 = stablehlo.reshape %137 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %139 = stablehlo.transpose %138, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %140 = stablehlo.reshape %139 : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %141 = stablehlo.reshape %arg4 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %142 = stablehlo.reshape %141 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %143 = stablehlo.transpose %142, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %144 = stablehlo.dot_general %140, %143, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %145 = stablehlo.reshape %144 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %146 = stablehlo.add %13, %145 : tensor<1x7x3072xbf16>
    %147 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %148 = stablehlo.convert %147 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %149 = stablehlo.reshape %148 : (tensor<1x1x3072xf32>) -> tensor<3072xf32>
    %150 = stablehlo.broadcast_in_dim %149, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %151 = stablehlo.convert %146 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %152 = stablehlo.power %151, %2 : tensor<1x7x3072xf32>
    %153 = stablehlo.reduce(%152 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %154 = stablehlo.multiply %153, %cst_3 : tensor<1x7xf32>
    %155 = stablehlo.reshape %154 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %156 = stablehlo.add %155, %23 : tensor<1x7x1xf32>
    %157 = stablehlo.rsqrt %156 : tensor<1x7x1xf32>
    %158 = stablehlo.reshape %157 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %159 = stablehlo.broadcast_in_dim %158, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %160 = stablehlo.multiply %151, %159 : tensor<1x7x3072xf32>
    %161 = stablehlo.convert %160 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %162 = stablehlo.convert %161 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %163 = stablehlo.multiply %150, %162 : tensor<1x7x3072xf32>
    %164 = stablehlo.convert %163 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %165 = stablehlo.reshape %164 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %166 = stablehlo.reshape %arg19 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %167 = stablehlo.reshape %166 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %168 = stablehlo.transpose %167, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %169 = stablehlo.dot_general %165, %168, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %170 = stablehlo.reshape %169 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %171 = stablehlo.convert %170 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %172 = stablehlo.logistic %170 : tensor<1x7x8192xbf16>
    %173 = stablehlo.convert %172 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %174 = stablehlo.multiply %171, %173 : tensor<1x7x8192xf32>
    %175 = stablehlo.convert %174 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %176 = stablehlo.convert %175 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %177 = stablehlo.reshape %arg3 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %178 = stablehlo.reshape %177 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %179 = stablehlo.transpose %178, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %180 = stablehlo.dot_general %165, %179, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %181 = stablehlo.convert %180 : (tensor<7x8192xbf16>) -> tensor<7x8192xf32>
    %182 = stablehlo.reshape %181 : (tensor<7x8192xf32>) -> tensor<1x7x8192xf32>
    %183 = stablehlo.multiply %176, %182 : tensor<1x7x8192xf32>
    %184 = stablehlo.convert %183 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %185 = stablehlo.reshape %184 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %186 = stablehlo.reshape %arg2 : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %187 = stablehlo.reshape %186 : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %188 = stablehlo.transpose %187, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %189 = stablehlo.dot_general %185, %188, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %190 = stablehlo.reshape %189 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %191 = stablehlo.add %146, %190 : tensor<1x7x3072xbf16>
    %192 = stablehlo.convert %191 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %193 = stablehlo.power %192, %2 : tensor<1x7x3072xf32>
    %194 = stablehlo.reduce(%193 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %195 = stablehlo.multiply %194, %cst_3 : tensor<1x7xf32>
    %196 = stablehlo.reshape %195 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %197 = stablehlo.add %196, %23 : tensor<1x7x1xf32>
    %198 = stablehlo.rsqrt %197 : tensor<1x7x1xf32>
    %199 = stablehlo.reshape %198 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %200 = stablehlo.broadcast_in_dim %199, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %201 = stablehlo.multiply %192, %200 : tensor<1x7x3072xf32>
    %202 = stablehlo.convert %201 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %203 = stablehlo.convert %202 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %204 = stablehlo.multiply %6, %203 : tensor<1x7x3072xf32>
    %205 = stablehlo.convert %204 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %206 = stablehlo.reshape %205 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %207 = stablehlo.reshape %arg0 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %208 = stablehlo.reshape %207 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %209 = stablehlo.transpose %208, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %210 = stablehlo.dot_general %206, %209, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %211 = stablehlo.reshape %210 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %211 : tensor<1x7x128256xbf16>
  }
}
2025-09-10 19:54:26.063 (  32.939s) [        F1708000]      module_builder.cc:486      1| SHLO Module after compiler StableHLO pipeline:
module @SyncTensorsGraph.448 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<128256x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg1: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg2: tensor<3072x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg3: tensor<8192x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg4: tensor<3072x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg5: tensor<1024x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg6: tensor<1x7xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg7: tensor<128256x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg8: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg9: tensor<7xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg10: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg11: tensor<1x8x32x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg12: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg13: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg14: tensor<64xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg15: tensor<1024x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg16: tensor<1x8x32x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg17: tensor<3072x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %arg16, %arg17, %arg18, %arg19, %arg20) in_shardings=[<@mesh, [{}, {}]>, <@mesh, []>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, []>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, []>, <@mesh, []>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>] out_shardings=[<@mesh, [{}, {}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg21: tensor<128256x3072xbf16>, %arg22: tensor<f32>, %arg23: tensor<3072x4096xbf16>, %arg24: tensor<4096x3072xbf16>, %arg25: tensor<3072x1536xbf16>, %arg26: tensor<512x3072xbf16>, %arg27: tensor<1x7xi64>, %arg28: tensor<128256x3072xbf16>, %arg29: tensor<3072xbf16>, %arg30: tensor<7xi64>, %arg31: tensor<i64>, %arg32: tensor<1x4x32x128xbf16>, %arg33: tensor<bf16>, %arg34: tensor<f32>, %arg35: tensor<64xf32>, %arg36: tensor<512x3072xbf16>, %arg37: tensor<1x4x32x128xbf16>, %arg38: tensor<1536x3072xbf16>, %arg39: tensor<3072xbf16>, %arg40: tensor<4096x3072xbf16>, %arg41: tensor<3072xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
      %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi64>
      %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
      %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
      %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %cst_3 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
      %c_4 = stablehlo.constant dense<0> : tensor<7xi64>
      %c_5 = stablehlo.constant dense<1> : tensor<i64>
      %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x32xbf16>
      %2 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x32xi64>
      %3 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
      %4 = stablehlo.reshape %arg41 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
      %5 = stablehlo.convert %4 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %6 = stablehlo.reshape %5 : (tensor<1x1x3072xf32>) -> tensor<3072xf32>
      %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
      %8 = stablehlo.reshape %arg28 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
      %9 = stablehlo.reshape %8 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
      %10 = stablehlo.reshape %arg27 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
      %11 = stablehlo.convert %10 : (tensor<1x1x7xi64>) -> tensor<1x1x7xui32>
      %12 = stablehlo.reshape %11 : (tensor<1x1x7xui32>) -> tensor<7xui32>
      %13 = "stablehlo.gather"(%9, %12) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
      %14 = stablehlo.reshape %13 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
      %15 = stablehlo.reshape %arg29 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
      %16 = stablehlo.convert %15 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %17 = stablehlo.reshape %16 : (tensor<1x1x3072xf32>) -> tensor<3072xf32>
      %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
      %19 = stablehlo.convert %14 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
      %20 = stablehlo.power %19, %3 : tensor<1x7x3072xf32>
      %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
      %22 = stablehlo.multiply %21, %cst_3 : tensor<1x7xf32>
      %23 = stablehlo.reshape %22 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
      %24 = stablehlo.broadcast_in_dim %arg22, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
      %25 = stablehlo.add %23, %24 : tensor<1x7x1xf32>
      %26 = stablehlo.rsqrt %25 : tensor<1x7x1xf32>
      %27 = stablehlo.reshape %26 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
      %28 = stablehlo.broadcast_in_dim %27, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
      %29 = stablehlo.multiply %19, %28 : tensor<1x7x3072xf32>
      %30 = stablehlo.convert %29 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
      %31 = stablehlo.convert %30 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
      %32 = stablehlo.multiply %18, %31 : tensor<1x7x3072xf32>
      %33 = stablehlo.convert %32 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
      %34 = stablehlo.reshape %33 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
      %35 = stablehlo.reshape %arg38 : (tensor<1536x3072xbf16>) -> tensor<1x1536x3072xbf16>
      %36 = stablehlo.reshape %35 : (tensor<1x1536x3072xbf16>) -> tensor<1536x3072xbf16>
      %37 = stablehlo.transpose %36, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<1536x3072xbf16>) -> tensor<3072x1536xbf16>
      %38 = stablehlo.dot_general %34, %37, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1536xbf16>) -> tensor<7x1536xbf16>
      %39 = stablehlo.reshape %38 : (tensor<7x1536xbf16>) -> tensor<1x7x12x128xbf16>
      %40 = stablehlo.transpose %39, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x12x128xbf16>) -> tensor<1x12x7x128xbf16>
      %41 = stablehlo.convert %40 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x12x7x128xbf16>) -> tensor<1x12x7x128xf32>
      %42 = stablehlo.reshape %arg35 : (tensor<64xf32>) -> tensor<1x1x64xf32>
      %43 = stablehlo.reshape %42 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
      %44 = stablehlo.reshape %arg30 : (tensor<7xi64>) -> tensor<1x1x7xi64>
      %45 = stablehlo.reshape %44 : (tensor<1x1x7xi64>) -> tensor<7xi64>
      %46 = stablehlo.convert %44 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
      %47 = stablehlo.dot_general %43, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
      %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
      %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
      %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
      %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
      %52 = stablehlo.convert %51 : (tensor<1x7x128xbf16>) -> tensor<1x7x128xf32>
      %53 = stablehlo.broadcast_in_dim %52, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x12x7x128xf32>
      %54 = stablehlo.multiply %41, %53 : tensor<1x12x7x128xf32>
      %55 = stablehlo.convert %54 : (tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xbf16>
      %56 = stablehlo.slice %40 [0:1, 0:12, 0:7, 64:128] : (tensor<1x12x7x128xbf16>) -> tensor<1x12x7x64xbf16>
      %57 = stablehlo.negate %56 : tensor<1x12x7x64xbf16>
      %58 = stablehlo.slice %40 [0:1, 0:12, 0:7, 0:64] : (tensor<1x12x7x128xbf16>) -> tensor<1x12x7x64xbf16>
      %59 = stablehlo.concatenate %57, %58, dim = 3 : (tensor<1x12x7x64xbf16>, tensor<1x12x7x64xbf16>) -> tensor<1x12x7x128xbf16>
      %60 = stablehlo.convert %59 : (tensor<1x12x7x128xbf16>) -> tensor<1x12x7x128xf32>
      %61 = stablehlo.sine %49 : tensor<1x7x128xf32>
      %62 = stablehlo.convert %61 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
      %63 = stablehlo.convert %62 : (tensor<1x7x128xbf16>) -> tensor<1x7x128xf32>
      %64 = stablehlo.broadcast_in_dim %63, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x12x7x128xf32>
      %65 = stablehlo.multiply %60, %64 : tensor<1x12x7x128xf32>
      %66 = stablehlo.convert %65 : (tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xbf16>
      %67 = stablehlo.add %55, %66 : tensor<1x12x7x128xbf16>
      %68 = stablehlo.reshape %67 : (tensor<1x12x7x128xbf16>) -> tensor<12x7x128xbf16>
      %69 = stablehlo.compare  LT, %45, %c_4 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
      %70 = stablehlo.broadcast_in_dim %arg31, dims = [] : (tensor<i64>) -> tensor<7xi64>
      %71 = stablehlo.add %45, %70 : tensor<7xi64>
      %72 = stablehlo.select %69, %71, %45 : tensor<7xi1>, tensor<7xi64>
      %73 = stablehlo.reshape %72 : (tensor<7xi64>) -> tensor<7x1xi64>
      %74 = stablehlo.reshape %arg36 : (tensor<512x3072xbf16>) -> tensor<1x512x3072xbf16>
      %75 = stablehlo.reshape %74 : (tensor<1x512x3072xbf16>) -> tensor<512x3072xbf16>
      %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<512x3072xbf16>) -> tensor<3072x512xbf16>
      %77 = stablehlo.dot_general %34, %76, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x512xbf16>) -> tensor<7x512xbf16>
      %78 = stablehlo.reshape %77 : (tensor<7x512xbf16>) -> tensor<1x7x4x128xbf16>
      %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x4x128xbf16>) -> tensor<1x4x7x128xbf16>
      %80 = stablehlo.convert %79 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x4x7x128xbf16>) -> tensor<1x4x7x128xf32>
      %81 = stablehlo.broadcast_in_dim %52, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x4x7x128xf32>
      %82 = stablehlo.multiply %80, %81 : tensor<1x4x7x128xf32>
      %83 = stablehlo.convert %82 : (tensor<1x4x7x128xf32>) -> tensor<1x4x7x128xbf16>
      %84 = stablehlo.slice %79 [0:1, 0:4, 0:7, 64:128] : (tensor<1x4x7x128xbf16>) -> tensor<1x4x7x64xbf16>
      %85 = stablehlo.negate %84 : tensor<1x4x7x64xbf16>
      %86 = stablehlo.slice %79 [0:1, 0:4, 0:7, 0:64] : (tensor<1x4x7x128xbf16>) -> tensor<1x4x7x64xbf16>
      %87 = stablehlo.concatenate %85, %86, dim = 3 : (tensor<1x4x7x64xbf16>, tensor<1x4x7x64xbf16>) -> tensor<1x4x7x128xbf16>
      %88 = stablehlo.convert %87 : (tensor<1x4x7x128xbf16>) -> tensor<1x4x7x128xf32>
      %89 = stablehlo.broadcast_in_dim %63, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x4x7x128xf32>
      %90 = stablehlo.multiply %88, %89 : tensor<1x4x7x128xf32>
      %91 = stablehlo.convert %90 : (tensor<1x4x7x128xf32>) -> tensor<1x4x7x128xbf16>
      %92 = stablehlo.add %83, %91 : tensor<1x4x7x128xbf16>
      %93 = "stablehlo.scatter"(%arg37, %73, %92) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg42: tensor<bf16>, %arg43: tensor<bf16>):
        stablehlo.return %arg43 : tensor<bf16>
      }) : (tensor<1x4x32x128xbf16>, tensor<7x1xi64>, tensor<1x4x7x128xbf16>) -> tensor<1x4x32x128xbf16>
      %94 = stablehlo.broadcast_in_dim %93, dims = [0, 1, 3, 4] : (tensor<1x4x32x128xbf16>) -> tensor<1x4x3x32x128xbf16>
      %95 = stablehlo.reshape %94 : (tensor<1x4x3x32x128xbf16>) -> tensor<1x12x32x128xbf16>
      %96 = stablehlo.transpose %95, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,32]{2,3,1,0}"} : (tensor<1x12x32x128xbf16>) -> tensor<1x12x128x32xbf16>
      %97 = stablehlo.reshape %96 : (tensor<1x12x128x32xbf16>) -> tensor<12x128x32xbf16>
      %98 = stablehlo.dot_general %68, %97, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<12x7x128xbf16>, tensor<12x128x32xbf16>) -> tensor<12x7x32xbf16>
      %99 = stablehlo.convert %98 : (tensor<12x7x32xbf16>) -> tensor<12x7x32xf32>
      %100 = stablehlo.reshape %99 : (tensor<12x7x32xf32>) -> tensor<1x12x7x32xf32>
      %101 = stablehlo.broadcast_in_dim %arg34, dims = [] : (tensor<f32>) -> tensor<1x12x7x32xf32>
      %102 = stablehlo.multiply %100, %101 : tensor<1x12x7x32xf32>
      %103 = stablehlo.convert %102 : (tensor<1x12x7x32xf32>) -> tensor<1x12x7x32xbf16>
      %104 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<32xi64>) -> tensor<7x32xi64>
      %105 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x32xi64>
      %106 = stablehlo.subtract %104, %105 : tensor<7x32xi64>
      %107 = stablehlo.compare  GE, %106, %2 : (tensor<7x32xi64>, tensor<7x32xi64>) -> tensor<7x32xi1>
      %108 = stablehlo.broadcast_in_dim %arg33, dims = [] : (tensor<bf16>) -> tensor<7x32xbf16>
      %109 = stablehlo.select %107, %108, %1 : tensor<7x32xi1>, tensor<7x32xbf16>
      %110 = stablehlo.convert %109 : (tensor<7x32xbf16>) -> tensor<7x32xf32>
      %111 = stablehlo.broadcast_in_dim %45, dims = [0] : (tensor<7xi64>) -> tensor<7x32xi64>
      %112 = stablehlo.compare  GT, %104, %111 : (tensor<7x32xi64>, tensor<7x32xi64>) -> tensor<7x32xi1>
      %113 = stablehlo.convert %112 : (tensor<7x32xi1>) -> tensor<7x32xf32>
      %114 = stablehlo.multiply %110, %113 : tensor<7x32xf32>
      %115 = stablehlo.convert %114 : (tensor<7x32xf32>) -> tensor<7x32xbf16>
      %116 = stablehlo.reshape %115 : (tensor<7x32xbf16>) -> tensor<1x7x32xbf16>
      %117 = stablehlo.broadcast_in_dim %116, dims = [0, 2, 3] : (tensor<1x7x32xbf16>) -> tensor<1x12x7x32xbf16>
      %118 = stablehlo.add %103, %117 : tensor<1x12x7x32xbf16>
      %119 = stablehlo.convert %118 : (tensor<1x12x7x32xbf16>) -> tensor<1x12x7x32xf32>
      %120 = stablehlo.reduce(%119 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x12x7x32xf32>, tensor<f32>) -> tensor<1x12x7xf32>
      %121 = stablehlo.broadcast_in_dim %120, dims = [0, 1, 2] : (tensor<1x12x7xf32>) -> tensor<1x12x7x32xf32>
      %122 = stablehlo.subtract %119, %121 : tensor<1x12x7x32xf32>
      %123 = stablehlo.exponential %122 : tensor<1x12x7x32xf32>
      %124 = stablehlo.reduce(%123 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x12x7x32xf32>, tensor<f32>) -> tensor<1x12x7xf32>
      %125 = stablehlo.broadcast_in_dim %124, dims = [0, 1, 2] : (tensor<1x12x7xf32>) -> tensor<1x12x7x32xf32>
      %126 = stablehlo.divide %123, %125 : tensor<1x12x7x32xf32>
      %127 = stablehlo.convert %126 : (tensor<1x12x7x32xf32>) -> tensor<1x12x7x32xbf16>
      %128 = stablehlo.reshape %127 : (tensor<1x12x7x32xbf16>) -> tensor<12x7x32xbf16>
      %129 = stablehlo.reshape %arg26 : (tensor<512x3072xbf16>) -> tensor<1x512x3072xbf16>
      %130 = stablehlo.reshape %129 : (tensor<1x512x3072xbf16>) -> tensor<512x3072xbf16>
      %131 = stablehlo.transpose %130, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<512x3072xbf16>) -> tensor<3072x512xbf16>
      %132 = stablehlo.dot_general %34, %131, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x512xbf16>) -> tensor<7x512xbf16>
      %133 = stablehlo.reshape %132 : (tensor<7x512xbf16>) -> tensor<1x7x4x128xbf16>
      %134 = stablehlo.transpose %133, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x4x128xbf16>) -> tensor<1x4x7x128xbf16>
      %135 = "stablehlo.scatter"(%arg32, %73, %134) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg42: tensor<bf16>, %arg43: tensor<bf16>):
        stablehlo.return %arg43 : tensor<bf16>
      }) : (tensor<1x4x32x128xbf16>, tensor<7x1xi64>, tensor<1x4x7x128xbf16>) -> tensor<1x4x32x128xbf16>
      %136 = stablehlo.broadcast_in_dim %135, dims = [0, 1, 3, 4] : (tensor<1x4x32x128xbf16>) -> tensor<1x4x3x32x128xbf16>
      %137 = stablehlo.reshape %136 : (tensor<1x4x3x32x128xbf16>) -> tensor<12x32x128xbf16>
      %138 = stablehlo.dot_general %128, %137, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<12x7x32xbf16>, tensor<12x32x128xbf16>) -> tensor<12x7x128xbf16>
      %139 = stablehlo.reshape %138 : (tensor<12x7x128xbf16>) -> tensor<1x12x7x128xbf16>
      %140 = stablehlo.transpose %139, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x12x7x128xbf16>) -> tensor<1x7x12x128xbf16>
      %141 = stablehlo.reshape %140 : (tensor<1x7x12x128xbf16>) -> tensor<7x1536xbf16>
      %142 = stablehlo.reshape %arg25 : (tensor<3072x1536xbf16>) -> tensor<1x3072x1536xbf16>
      %143 = stablehlo.reshape %142 : (tensor<1x3072x1536xbf16>) -> tensor<3072x1536xbf16>
      %144 = stablehlo.transpose %143, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x1536xbf16>) -> tensor<1536x3072xbf16>
      %145 = stablehlo.dot_general %141, %144, contracting_dims = [1] x [0] : (tensor<7x1536xbf16>, tensor<1536x3072xbf16>) -> tensor<7x3072xbf16>
      %146 = "stablehlo.all_reduce"(%145) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> ({
      ^bb0(%arg42: tensor<bf16>, %arg43: tensor<bf16>):
        %215 = stablehlo.add %arg42, %arg43 : tensor<bf16>
        stablehlo.return %215 : tensor<bf16>
      }) : (tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
      %147 = stablehlo.reshape %146 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
      %148 = stablehlo.add %14, %147 : tensor<1x7x3072xbf16>
      %149 = stablehlo.reshape %arg39 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
      %150 = stablehlo.convert %149 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %151 = stablehlo.reshape %150 : (tensor<1x1x3072xf32>) -> tensor<3072xf32>
      %152 = stablehlo.broadcast_in_dim %151, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
      %153 = stablehlo.convert %148 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
      %154 = stablehlo.power %153, %3 : tensor<1x7x3072xf32>
      %155 = stablehlo.reduce(%154 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
      %156 = stablehlo.multiply %155, %cst_3 : tensor<1x7xf32>
      %157 = stablehlo.reshape %156 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
      %158 = stablehlo.add %157, %24 : tensor<1x7x1xf32>
      %159 = stablehlo.rsqrt %158 : tensor<1x7x1xf32>
      %160 = stablehlo.reshape %159 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
      %161 = stablehlo.broadcast_in_dim %160, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
      %162 = stablehlo.multiply %153, %161 : tensor<1x7x3072xf32>
      %163 = stablehlo.convert %162 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
      %164 = stablehlo.convert %163 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
      %165 = stablehlo.multiply %152, %164 : tensor<1x7x3072xf32>
      %166 = stablehlo.convert %165 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
      %167 = stablehlo.reshape %166 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
      %168 = stablehlo.reshape %arg40 : (tensor<4096x3072xbf16>) -> tensor<1x4096x3072xbf16>
      %169 = stablehlo.reshape %168 : (tensor<1x4096x3072xbf16>) -> tensor<4096x3072xbf16>
      %170 = stablehlo.transpose %169, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<4096x3072xbf16>) -> tensor<3072x4096xbf16>
      %171 = stablehlo.dot_general %167, %170, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<7x4096xbf16>
      %172 = stablehlo.reshape %171 : (tensor<7x4096xbf16>) -> tensor<1x7x4096xbf16>
      %173 = stablehlo.convert %172 : (tensor<1x7x4096xbf16>) -> tensor<1x7x4096xf32>
      %174 = stablehlo.logistic %172 : tensor<1x7x4096xbf16>
      %175 = stablehlo.convert %174 : (tensor<1x7x4096xbf16>) -> tensor<1x7x4096xf32>
      %176 = stablehlo.multiply %173, %175 : tensor<1x7x4096xf32>
      %177 = stablehlo.convert %176 : (tensor<1x7x4096xf32>) -> tensor<1x7x4096xbf16>
      %178 = stablehlo.convert %177 : (tensor<1x7x4096xbf16>) -> tensor<1x7x4096xf32>
      %179 = stablehlo.reshape %arg24 : (tensor<4096x3072xbf16>) -> tensor<1x4096x3072xbf16>
      %180 = stablehlo.reshape %179 : (tensor<1x4096x3072xbf16>) -> tensor<4096x3072xbf16>
      %181 = stablehlo.transpose %180, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<4096x3072xbf16>) -> tensor<3072x4096xbf16>
      %182 = stablehlo.dot_general %167, %181, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<7x4096xbf16>
      %183 = stablehlo.convert %182 : (tensor<7x4096xbf16>) -> tensor<7x4096xf32>
      %184 = stablehlo.reshape %183 : (tensor<7x4096xf32>) -> tensor<1x7x4096xf32>
      %185 = stablehlo.multiply %178, %184 : tensor<1x7x4096xf32>
      %186 = stablehlo.convert %185 : (tensor<1x7x4096xf32>) -> tensor<1x7x4096xbf16>
      %187 = stablehlo.reshape %186 : (tensor<1x7x4096xbf16>) -> tensor<7x4096xbf16>
      %188 = stablehlo.reshape %arg23 : (tensor<3072x4096xbf16>) -> tensor<1x3072x4096xbf16>
      %189 = stablehlo.reshape %188 : (tensor<1x3072x4096xbf16>) -> tensor<3072x4096xbf16>
      %190 = stablehlo.transpose %189, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x4096xbf16>) -> tensor<4096x3072xbf16>
      %191 = stablehlo.dot_general %187, %190, contracting_dims = [1] x [0] : (tensor<7x4096xbf16>, tensor<4096x3072xbf16>) -> tensor<7x3072xbf16>
      %192 = "stablehlo.all_reduce"(%191) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> ({
      ^bb0(%arg42: tensor<bf16>, %arg43: tensor<bf16>):
        %215 = stablehlo.add %arg42, %arg43 : tensor<bf16>
        stablehlo.return %215 : tensor<bf16>
      }) : (tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
      %193 = stablehlo.reshape %192 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
      %194 = stablehlo.add %148, %193 : tensor<1x7x3072xbf16>
      %195 = stablehlo.convert %194 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
      %196 = stablehlo.power %195, %3 : tensor<1x7x3072xf32>
      %197 = stablehlo.reduce(%196 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
      %198 = stablehlo.multiply %197, %cst_3 : tensor<1x7xf32>
      %199 = stablehlo.reshape %198 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
      %200 = stablehlo.add %199, %24 : tensor<1x7x1xf32>
      %201 = stablehlo.rsqrt %200 : tensor<1x7x1xf32>
      %202 = stablehlo.reshape %201 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
      %203 = stablehlo.broadcast_in_dim %202, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
      %204 = stablehlo.multiply %195, %203 : tensor<1x7x3072xf32>
      %205 = stablehlo.convert %204 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
      %206 = stablehlo.convert %205 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
      %207 = stablehlo.multiply %7, %206 : tensor<1x7x3072xf32>
      %208 = stablehlo.convert %207 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
      %209 = stablehlo.reshape %208 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
      %210 = stablehlo.reshape %arg21 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
      %211 = stablehlo.reshape %210 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
      %212 = stablehlo.transpose %211, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
      %213 = stablehlo.dot_general %209, %212, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
      %214 = stablehlo.reshape %213 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
      sdy.return %214 : tensor<1x7x128256xbf16>
    } : (tensor<128256x3072xbf16>, tensor<f32>, tensor<3072x8192xbf16>, tensor<8192x3072xbf16>, tensor<3072x3072xbf16>, tensor<1024x3072xbf16>, tensor<1x7xi64>, tensor<128256x3072xbf16>, tensor<3072xbf16>, tensor<7xi64>, tensor<i64>, tensor<1x8x32x128xbf16>, tensor<bf16>, tensor<f32>, tensor<64xf32>, tensor<1024x3072xbf16>, tensor<1x8x32x128xbf16>, tensor<3072x3072xbf16>, tensor<3072xbf16>, tensor<8192x3072xbf16>, tensor<3072xbf16>) -> tensor<1x7x128256xbf16>
    return %0 : tensor<1x7x128256xbf16>
  }
}
2025-09-10 19:54:26.075 (  32.951s) [        F1708000]      module_builder.cc:510      1| TTIR Module:
module @SyncTensorsGraph.448 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  func.func @main(%arg0: tensor<128256x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg1: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg2: tensor<3072x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg3: tensor<8192x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg4: tensor<3072x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg5: tensor<1024x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg6: tensor<1x7xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg7: tensor<128256x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg8: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg9: tensor<7xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg10: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg11: tensor<1x8x32x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg12: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg13: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg14: tensor<64xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg15: tensor<1024x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg16: tensor<1x8x32x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg17: tensor<3072x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = ttir.empty() : tensor<128256x3072xbf16>
    %1 = "ttir.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16>, tensor<128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %2 = ttir.empty() : tensor<f32>
    %3 = "ttir.mesh_shard"(%arg1, %2) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %4 = ttir.empty() : tensor<3072x4096xbf16>
    %5 = "ttir.mesh_shard"(%arg2, %4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x8192xbf16>, tensor<3072x4096xbf16>) -> tensor<3072x4096xbf16>
    %6 = ttir.empty() : tensor<4096x3072xbf16>
    %7 = "ttir.mesh_shard"(%arg3, %6) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16>, tensor<4096x3072xbf16>) -> tensor<4096x3072xbf16>
    %8 = ttir.empty() : tensor<3072x1536xbf16>
    %9 = "ttir.mesh_shard"(%arg4, %8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16>, tensor<3072x1536xbf16>) -> tensor<3072x1536xbf16>
    %10 = ttir.empty() : tensor<512x3072xbf16>
    %11 = "ttir.mesh_shard"(%arg5, %10) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16>, tensor<512x3072xbf16>) -> tensor<512x3072xbf16>
    %12 = ttir.empty() : tensor<1x7xi64>
    %13 = "ttir.mesh_shard"(%arg6, %12) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x7xi64>, tensor<1x7xi64>) -> tensor<1x7xi64>
    %14 = ttir.empty() : tensor<128256x3072xbf16>
    %15 = "ttir.mesh_shard"(%arg7, %14) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16>, tensor<128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = ttir.empty() : tensor<3072xbf16>
    %17 = "ttir.mesh_shard"(%arg8, %16) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16>, tensor<3072xbf16>) -> tensor<3072xbf16>
    %18 = ttir.empty() : tensor<7xi64>
    %19 = "ttir.mesh_shard"(%arg9, %18) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi64>
    %20 = ttir.empty() : tensor<i64>
    %21 = "ttir.mesh_shard"(%arg10, %20) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %22 = ttir.empty() : tensor<1x4x32x128xbf16>
    %23 = "ttir.mesh_shard"(%arg11, %22) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x32x128xbf16>, tensor<1x4x32x128xbf16>) -> tensor<1x4x32x128xbf16>
    %24 = ttir.empty() : tensor<bf16>
    %25 = "ttir.mesh_shard"(%arg12, %24) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
    %26 = ttir.empty() : tensor<f32>
    %27 = "ttir.mesh_shard"(%arg13, %26) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %28 = ttir.empty() : tensor<64xf32>
    %29 = "ttir.mesh_shard"(%arg14, %28) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32>, tensor<64xf32>) -> tensor<64xf32>
    %30 = ttir.empty() : tensor<512x3072xbf16>
    %31 = "ttir.mesh_shard"(%arg15, %30) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16>, tensor<512x3072xbf16>) -> tensor<512x3072xbf16>
    %32 = ttir.empty() : tensor<1x4x32x128xbf16>
    %33 = "ttir.mesh_shard"(%arg16, %32) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x32x128xbf16>, tensor<1x4x32x128xbf16>) -> tensor<1x4x32x128xbf16>
    %34 = ttir.empty() : tensor<1536x3072xbf16>
    %35 = "ttir.mesh_shard"(%arg17, %34) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16>, tensor<1536x3072xbf16>) -> tensor<1536x3072xbf16>
    %36 = ttir.empty() : tensor<3072xbf16>
    %37 = "ttir.mesh_shard"(%arg18, %36) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16>, tensor<3072xbf16>) -> tensor<3072xbf16>
    %38 = ttir.empty() : tensor<4096x3072xbf16>
    %39 = "ttir.mesh_shard"(%arg19, %38) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16>, tensor<4096x3072xbf16>) -> tensor<4096x3072xbf16>
    %40 = ttir.empty() : tensor<3072xbf16>
    %41 = "ttir.mesh_shard"(%arg20, %40) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16>, tensor<3072xbf16>) -> tensor<3072xbf16>
    %42 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<f32>}> : () -> tensor<f32>
    %43 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi64>}> : () -> tensor<32xi64>
    %44 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>}> : () -> tensor<7xi64>
    %45 = "ttir.constant"() <{value = dense<0xFF800000> : tensor<f32>}> : () -> tensor<f32>
    %46 = "ttir.constant"() <{value = dense<2.000000e+00> : tensor<f32>}> : () -> tensor<f32>
    %47 = "ttir.constant"() <{value = dense<3.25520843E-4> : tensor<1x7xf32>}> : () -> tensor<1x7xf32>
    %48 = "ttir.constant"() <{value = dense<0> : tensor<7xi64>}> : () -> tensor<7xi64>
    %49 = "ttir.constant"() <{value = dense<1> : tensor<i64>}> : () -> tensor<i64>
    %50 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<bf16>}> : () -> tensor<bf16>
    %51 = ttir.empty() : tensor<1x1xbf16>
    %52 = "ttir.reshape"(%50, %51) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %53 = ttir.empty() : tensor<7x32xbf16>
    %54 = "ttir.broadcast"(%52, %53) <{broadcast_dimensions = array<i64: 7, 32>}> : (tensor<1x1xbf16>, tensor<7x32xbf16>) -> tensor<7x32xbf16>
    %55 = ttir.empty() : tensor<1x1xi64>
    %56 = "ttir.reshape"(%49, %55) <{shape = [1 : i32, 1 : i32]}> : (tensor<i64>, tensor<1x1xi64>) -> tensor<1x1xi64>
    %57 = ttir.empty() : tensor<7x32xi64>
    %58 = "ttir.broadcast"(%56, %57) <{broadcast_dimensions = array<i64: 7, 32>}> : (tensor<1x1xi64>, tensor<7x32xi64>) -> tensor<7x32xi64>
    %59 = ttir.empty() : tensor<1x1x1xf32>
    %60 = "ttir.reshape"(%46, %59) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %61 = ttir.empty() : tensor<1x7x3072xf32>
    %62 = "ttir.broadcast"(%60, %61) <{broadcast_dimensions = array<i64: 1, 7, 3072>}> : (tensor<1x1x1xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %63 = ttir.empty() : tensor<1x1x3072xbf16>
    %64 = "ttir.reshape"(%41, %63) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %65 = ttir.empty() : tensor<1x1x3072xf32>
    %66 = "ttir.typecast"(%64, %65) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %67 = ttir.empty() : tensor<3072xf32>
    %68 = "ttir.reshape"(%66, %67) <{shape = [3072 : i32]}> : (tensor<1x1x3072xf32>, tensor<3072xf32>) -> tensor<3072xf32>
    %69 = ttir.empty() : tensor<1x1x3072xf32>
    %70 = "ttir.reshape"(%68, %69) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %71 = ttir.empty() : tensor<1x7x3072xf32>
    %72 = "ttir.broadcast"(%70, %71) <{broadcast_dimensions = array<i64: 1, 7, 1>}> : (tensor<1x1x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %73 = ttir.empty() : tensor<1x128256x3072xbf16>
    %74 = "ttir.reshape"(%15, %73) <{shape = [1 : i32, 128256 : i32, 3072 : i32]}> : (tensor<128256x3072xbf16>, tensor<1x128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %75 = ttir.empty() : tensor<128256x3072xbf16>
    %76 = "ttir.reshape"(%74, %75) <{shape = [128256 : i32, 3072 : i32]}> : (tensor<1x128256x3072xbf16>, tensor<128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %77 = ttir.empty() : tensor<1x1x7xi64>
    %78 = "ttir.reshape"(%13, %77) <{shape = [1 : i32, 1 : i32, 7 : i32]}> : (tensor<1x7xi64>, tensor<1x1x7xi64>) -> tensor<1x1x7xi64>
    %79 = ttir.empty() : tensor<1x1x7xui32>
    %80 = "ttir.typecast"(%78, %79) <{conservative_folding = false}> : (tensor<1x1x7xi64>, tensor<1x1x7xui32>) -> tensor<1x1x7xui32>
    %81 = ttir.empty() : tensor<7xui32>
    %82 = "ttir.reshape"(%80, %81) <{shape = [7 : i32]}> : (tensor<1x1x7xui32>, tensor<7xui32>) -> tensor<7xui32>
    %83 = ttir.empty() : tensor<7x3072xbf16>
    %84 = "ttir.gather"(%76, %82, %83) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 3072>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<128256x3072xbf16>, tensor<7xui32>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %85 = ttir.empty() : tensor<1x7x3072xbf16>
    %86 = "ttir.reshape"(%84, %85) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %87 = ttir.empty() : tensor<1x1x3072xbf16>
    %88 = "ttir.reshape"(%17, %87) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %89 = ttir.empty() : tensor<1x1x3072xf32>
    %90 = "ttir.typecast"(%88, %89) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %91 = ttir.empty() : tensor<3072xf32>
    %92 = "ttir.reshape"(%90, %91) <{shape = [3072 : i32]}> : (tensor<1x1x3072xf32>, tensor<3072xf32>) -> tensor<3072xf32>
    %93 = ttir.empty() : tensor<1x1x3072xf32>
    %94 = "ttir.reshape"(%92, %93) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %95 = ttir.empty() : tensor<1x7x3072xf32>
    %96 = "ttir.broadcast"(%94, %95) <{broadcast_dimensions = array<i64: 1, 7, 1>}> : (tensor<1x1x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %97 = ttir.empty() : tensor<1x7x3072xf32>
    %98 = "ttir.typecast"(%86, %97) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %99 = ttir.empty() : tensor<1x7x3072xf32>
    %100 = "ttir.pow"(%98, %62, %99) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %101 = ttir.empty() : tensor<1x7xf32>
    %102 = "ttir.sum"(%100, %101) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %103 = ttir.empty() : tensor<1x7xf32>
    %104 = "ttir.multiply"(%102, %47, %103) : (tensor<1x7xf32>, tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %105 = ttir.empty() : tensor<1x7x1xf32>
    %106 = "ttir.reshape"(%104, %105) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %107 = ttir.empty() : tensor<1x1x1xf32>
    %108 = "ttir.reshape"(%3, %107) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %109 = ttir.empty() : tensor<1x7x1xf32>
    %110 = "ttir.broadcast"(%108, %109) <{broadcast_dimensions = array<i64: 1, 7, 1>}> : (tensor<1x1x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %111 = ttir.empty() : tensor<1x7x1xf32>
    %112 = "ttir.add"(%106, %110, %111) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %113 = ttir.empty() : tensor<1x7x1xf32>
    %114 = "ttir.rsqrt"(%112, %113) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %115 = ttir.empty() : tensor<1x7xf32>
    %116 = "ttir.reshape"(%114, %115) <{shape = [1 : i32, 7 : i32]}> : (tensor<1x7x1xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %117 = ttir.empty() : tensor<1x7x1xf32>
    %118 = "ttir.reshape"(%116, %117) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %119 = ttir.empty() : tensor<1x7x3072xf32>
    %120 = "ttir.broadcast"(%118, %119) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x7x1xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %121 = ttir.empty() : tensor<1x7x3072xf32>
    %122 = "ttir.multiply"(%98, %120, %121) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %123 = ttir.empty() : tensor<1x7x3072xbf16>
    %124 = "ttir.typecast"(%122, %123) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %125 = ttir.empty() : tensor<1x7x3072xf32>
    %126 = "ttir.typecast"(%124, %125) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %127 = ttir.empty() : tensor<1x7x3072xf32>
    %128 = "ttir.multiply"(%96, %126, %127) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %129 = ttir.empty() : tensor<1x7x3072xbf16>
    %130 = "ttir.typecast"(%128, %129) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %131 = ttir.empty() : tensor<7x3072xbf16>
    %132 = "ttir.reshape"(%130, %131) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x7x3072xbf16>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %133 = ttir.empty() : tensor<1x1536x3072xbf16>
    %134 = "ttir.reshape"(%35, %133) <{shape = [1 : i32, 1536 : i32, 3072 : i32]}> : (tensor<1536x3072xbf16>, tensor<1x1536x3072xbf16>) -> tensor<1x1536x3072xbf16>
    %135 = ttir.empty() : tensor<1536x3072xbf16>
    %136 = "ttir.reshape"(%134, %135) <{shape = [1536 : i32, 3072 : i32]}> : (tensor<1x1536x3072xbf16>, tensor<1536x3072xbf16>) -> tensor<1536x3072xbf16>
    %137 = ttir.empty() : tensor<3072x1536xbf16>
    %138 = "ttir.permute"(%136, %137) <{permutation = array<i64: 1, 0>}> : (tensor<1536x3072xbf16>, tensor<3072x1536xbf16>) -> tensor<3072x1536xbf16>
    %139 = "ttir.dot_general"(%132, %138) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x1536xbf16>) -> tensor<7x1536xbf16>
    %140 = ttir.empty() : tensor<1x7x12x128xbf16>
    %141 = "ttir.reshape"(%139, %140) <{shape = [1 : i32, 7 : i32, 12 : i32, 128 : i32]}> : (tensor<7x1536xbf16>, tensor<1x7x12x128xbf16>) -> tensor<1x7x12x128xbf16>
    %142 = ttir.empty() : tensor<1x12x7x128xbf16>
    %143 = "ttir.permute"(%141, %142) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x12x128xbf16>, tensor<1x12x7x128xbf16>) -> tensor<1x12x7x128xbf16>
    %144 = ttir.empty() : tensor<1x12x7x128xf32>
    %145 = "ttir.typecast"(%143, %144) <{conservative_folding = false}> : (tensor<1x12x7x128xbf16>, tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xf32>
    %146 = ttir.empty() : tensor<1x1x64xf32>
    %147 = "ttir.reshape"(%29, %146) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xf32>, tensor<1x1x64xf32>) -> tensor<1x1x64xf32>
    %148 = ttir.empty() : tensor<1x64x1xf32>
    %149 = "ttir.reshape"(%147, %148) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<1x1x64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
    %150 = ttir.empty() : tensor<1x1x7xi64>
    %151 = "ttir.reshape"(%19, %150) <{shape = [1 : i32, 1 : i32, 7 : i32]}> : (tensor<7xi64>, tensor<1x1x7xi64>) -> tensor<1x1x7xi64>
    %152 = ttir.empty() : tensor<7xi64>
    %153 = "ttir.reshape"(%151, %152) <{shape = [7 : i32]}> : (tensor<1x1x7xi64>, tensor<7xi64>) -> tensor<7xi64>
    %154 = ttir.empty() : tensor<1x1x7xf32>
    %155 = "ttir.typecast"(%151, %154) <{conservative_folding = false}> : (tensor<1x1x7xi64>, tensor<1x1x7xf32>) -> tensor<1x1x7xf32>
    %156 = "ttir.dot_general"(%149, %155) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %157 = ttir.empty() : tensor<1x7x64xf32>
    %158 = "ttir.permute"(%156, %157) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x7xf32>, tensor<1x7x64xf32>) -> tensor<1x7x64xf32>
    %159 = ttir.empty() : tensor<1x7x128xf32>
    %160 = "ttir.concat"(%158, %158, %159) <{dim = 2 : si32}> : (tensor<1x7x64xf32>, tensor<1x7x64xf32>, tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
    %161 = ttir.empty() : tensor<1x7x128xf32>
    %162 = "ttir.cos"(%160, %161) : (tensor<1x7x128xf32>, tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
    %163 = ttir.empty() : tensor<1x7x128xbf16>
    %164 = "ttir.typecast"(%162, %163) <{conservative_folding = false}> : (tensor<1x7x128xf32>, tensor<1x7x128xbf16>) -> tensor<1x7x128xbf16>
    %165 = ttir.empty() : tensor<1x7x128xf32>
    %166 = "ttir.typecast"(%164, %165) <{conservative_folding = false}> : (tensor<1x7x128xbf16>, tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
    %167 = ttir.empty() : tensor<1x1x7x128xf32>
    %168 = "ttir.reshape"(%166, %167) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32>, tensor<1x1x7x128xf32>) -> tensor<1x1x7x128xf32>
    %169 = ttir.empty() : tensor<1x12x7x128xf32>
    %170 = "ttir.broadcast"(%168, %169) <{broadcast_dimensions = array<i64: 1, 12, 1, 1>}> : (tensor<1x1x7x128xf32>, tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xf32>
    %171 = ttir.empty() : tensor<1x12x7x128xf32>
    %172 = "ttir.multiply"(%145, %170, %171) : (tensor<1x12x7x128xf32>, tensor<1x12x7x128xf32>, tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xf32>
    %173 = ttir.empty() : tensor<1x12x7x128xbf16>
    %174 = "ttir.typecast"(%172, %173) <{conservative_folding = false}> : (tensor<1x12x7x128xf32>, tensor<1x12x7x128xbf16>) -> tensor<1x12x7x128xbf16>
    %175 = ttir.empty() : tensor<1x12x7x64xbf16>
    %176 = "ttir.slice_static"(%143, %175) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 12 : i32, 7 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x7x128xbf16>, tensor<1x12x7x64xbf16>) -> tensor<1x12x7x64xbf16>
    %177 = ttir.empty() : tensor<1x12x7x64xbf16>
    %178 = "ttir.neg"(%176, %177) : (tensor<1x12x7x64xbf16>, tensor<1x12x7x64xbf16>) -> tensor<1x12x7x64xbf16>
    %179 = ttir.empty() : tensor<1x12x7x64xbf16>
    %180 = "ttir.slice_static"(%143, %179) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 12 : i32, 7 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x7x128xbf16>, tensor<1x12x7x64xbf16>) -> tensor<1x12x7x64xbf16>
    %181 = ttir.empty() : tensor<1x12x7x128xbf16>
    %182 = "ttir.concat"(%178, %180, %181) <{dim = 3 : si32}> : (tensor<1x12x7x64xbf16>, tensor<1x12x7x64xbf16>, tensor<1x12x7x128xbf16>) -> tensor<1x12x7x128xbf16>
    %183 = ttir.empty() : tensor<1x12x7x128xf32>
    %184 = "ttir.typecast"(%182, %183) <{conservative_folding = false}> : (tensor<1x12x7x128xbf16>, tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xf32>
    %185 = ttir.empty() : tensor<1x7x128xf32>
    %186 = "ttir.sin"(%160, %185) : (tensor<1x7x128xf32>, tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
    %187 = ttir.empty() : tensor<1x7x128xbf16>
    %188 = "ttir.typecast"(%186, %187) <{conservative_folding = false}> : (tensor<1x7x128xf32>, tensor<1x7x128xbf16>) -> tensor<1x7x128xbf16>
    %189 = ttir.empty() : tensor<1x7x128xf32>
    %190 = "ttir.typecast"(%188, %189) <{conservative_folding = false}> : (tensor<1x7x128xbf16>, tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
    %191 = ttir.empty() : tensor<1x1x7x128xf32>
    %192 = "ttir.reshape"(%190, %191) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32>, tensor<1x1x7x128xf32>) -> tensor<1x1x7x128xf32>
    %193 = ttir.empty() : tensor<1x12x7x128xf32>
    %194 = "ttir.broadcast"(%192, %193) <{broadcast_dimensions = array<i64: 1, 12, 1, 1>}> : (tensor<1x1x7x128xf32>, tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xf32>
    %195 = ttir.empty() : tensor<1x12x7x128xf32>
    %196 = "ttir.multiply"(%184, %194, %195) : (tensor<1x12x7x128xf32>, tensor<1x12x7x128xf32>, tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xf32>
    %197 = ttir.empty() : tensor<1x12x7x128xbf16>
    %198 = "ttir.typecast"(%196, %197) <{conservative_folding = false}> : (tensor<1x12x7x128xf32>, tensor<1x12x7x128xbf16>) -> tensor<1x12x7x128xbf16>
    %199 = ttir.empty() : tensor<1x12x7x128xbf16>
    %200 = "ttir.add"(%174, %198, %199) : (tensor<1x12x7x128xbf16>, tensor<1x12x7x128xbf16>, tensor<1x12x7x128xbf16>) -> tensor<1x12x7x128xbf16>
    %201 = ttir.empty() : tensor<12x7x128xbf16>
    %202 = "ttir.reshape"(%200, %201) <{shape = [12 : i32, 7 : i32, 128 : i32]}> : (tensor<1x12x7x128xbf16>, tensor<12x7x128xbf16>) -> tensor<12x7x128xbf16>
    %203 = ttir.empty() : tensor<7xi1>
    %204 = "ttir.lt"(%153, %48, %203) : (tensor<7xi64>, tensor<7xi64>, tensor<7xi1>) -> tensor<7xi1>
    %205 = ttir.empty() : tensor<1xi64>
    %206 = "ttir.reshape"(%21, %205) <{shape = [1 : i32]}> : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %207 = ttir.empty() : tensor<7xi64>
    %208 = "ttir.broadcast"(%206, %207) <{broadcast_dimensions = array<i64: 7>}> : (tensor<1xi64>, tensor<7xi64>) -> tensor<7xi64>
    %209 = ttir.empty() : tensor<7xi64>
    %210 = "ttir.add"(%153, %208, %209) : (tensor<7xi64>, tensor<7xi64>, tensor<7xi64>) -> tensor<7xi64>
    %211 = ttir.empty() : tensor<7xi64>
    %212 = "ttir.where"(%204, %210, %153, %211) : (tensor<7xi1>, tensor<7xi64>, tensor<7xi64>, tensor<7xi64>) -> tensor<7xi64>
    %213 = ttir.empty() : tensor<7x1xi64>
    %214 = "ttir.reshape"(%212, %213) <{shape = [7 : i32, 1 : i32]}> : (tensor<7xi64>, tensor<7x1xi64>) -> tensor<7x1xi64>
    %215 = ttir.empty() : tensor<1x512x3072xbf16>
    %216 = "ttir.reshape"(%31, %215) <{shape = [1 : i32, 512 : i32, 3072 : i32]}> : (tensor<512x3072xbf16>, tensor<1x512x3072xbf16>) -> tensor<1x512x3072xbf16>
    %217 = ttir.empty() : tensor<512x3072xbf16>
    %218 = "ttir.reshape"(%216, %217) <{shape = [512 : i32, 3072 : i32]}> : (tensor<1x512x3072xbf16>, tensor<512x3072xbf16>) -> tensor<512x3072xbf16>
    %219 = ttir.empty() : tensor<3072x512xbf16>
    %220 = "ttir.permute"(%218, %219) <{permutation = array<i64: 1, 0>}> : (tensor<512x3072xbf16>, tensor<3072x512xbf16>) -> tensor<3072x512xbf16>
    %221 = "ttir.dot_general"(%132, %220) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x512xbf16>) -> tensor<7x512xbf16>
    %222 = ttir.empty() : tensor<1x7x4x128xbf16>
    %223 = "ttir.reshape"(%221, %222) <{shape = [1 : i32, 7 : i32, 4 : i32, 128 : i32]}> : (tensor<7x512xbf16>, tensor<1x7x4x128xbf16>) -> tensor<1x7x4x128xbf16>
    %224 = ttir.empty() : tensor<1x4x7x128xbf16>
    %225 = "ttir.permute"(%223, %224) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x4x128xbf16>, tensor<1x4x7x128xbf16>) -> tensor<1x4x7x128xbf16>
    %226 = ttir.empty() : tensor<1x4x7x128xf32>
    %227 = "ttir.typecast"(%225, %226) <{conservative_folding = false}> : (tensor<1x4x7x128xbf16>, tensor<1x4x7x128xf32>) -> tensor<1x4x7x128xf32>
    %228 = ttir.empty() : tensor<1x1x7x128xf32>
    %229 = "ttir.reshape"(%166, %228) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32>, tensor<1x1x7x128xf32>) -> tensor<1x1x7x128xf32>
    %230 = ttir.empty() : tensor<1x4x7x128xf32>
    %231 = "ttir.broadcast"(%229, %230) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x7x128xf32>, tensor<1x4x7x128xf32>) -> tensor<1x4x7x128xf32>
    %232 = ttir.empty() : tensor<1x4x7x128xf32>
    %233 = "ttir.multiply"(%227, %231, %232) : (tensor<1x4x7x128xf32>, tensor<1x4x7x128xf32>, tensor<1x4x7x128xf32>) -> tensor<1x4x7x128xf32>
    %234 = ttir.empty() : tensor<1x4x7x128xbf16>
    %235 = "ttir.typecast"(%233, %234) <{conservative_folding = false}> : (tensor<1x4x7x128xf32>, tensor<1x4x7x128xbf16>) -> tensor<1x4x7x128xbf16>
    %236 = ttir.empty() : tensor<1x4x7x64xbf16>
    %237 = "ttir.slice_static"(%225, %236) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 7 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x7x128xbf16>, tensor<1x4x7x64xbf16>) -> tensor<1x4x7x64xbf16>
    %238 = ttir.empty() : tensor<1x4x7x64xbf16>
    %239 = "ttir.neg"(%237, %238) : (tensor<1x4x7x64xbf16>, tensor<1x4x7x64xbf16>) -> tensor<1x4x7x64xbf16>
    %240 = ttir.empty() : tensor<1x4x7x64xbf16>
    %241 = "ttir.slice_static"(%225, %240) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 7 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x7x128xbf16>, tensor<1x4x7x64xbf16>) -> tensor<1x4x7x64xbf16>
    %242 = ttir.empty() : tensor<1x4x7x128xbf16>
    %243 = "ttir.concat"(%239, %241, %242) <{dim = 3 : si32}> : (tensor<1x4x7x64xbf16>, tensor<1x4x7x64xbf16>, tensor<1x4x7x128xbf16>) -> tensor<1x4x7x128xbf16>
    %244 = ttir.empty() : tensor<1x4x7x128xf32>
    %245 = "ttir.typecast"(%243, %244) <{conservative_folding = false}> : (tensor<1x4x7x128xbf16>, tensor<1x4x7x128xf32>) -> tensor<1x4x7x128xf32>
    %246 = ttir.empty() : tensor<1x1x7x128xf32>
    %247 = "ttir.reshape"(%190, %246) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32>, tensor<1x1x7x128xf32>) -> tensor<1x1x7x128xf32>
    %248 = ttir.empty() : tensor<1x4x7x128xf32>
    %249 = "ttir.broadcast"(%247, %248) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x7x128xf32>, tensor<1x4x7x128xf32>) -> tensor<1x4x7x128xf32>
    %250 = ttir.empty() : tensor<1x4x7x128xf32>
    %251 = "ttir.multiply"(%245, %249, %250) : (tensor<1x4x7x128xf32>, tensor<1x4x7x128xf32>, tensor<1x4x7x128xf32>) -> tensor<1x4x7x128xf32>
    %252 = ttir.empty() : tensor<1x4x7x128xbf16>
    %253 = "ttir.typecast"(%251, %252) <{conservative_folding = false}> : (tensor<1x4x7x128xf32>, tensor<1x4x7x128xbf16>) -> tensor<1x4x7x128xbf16>
    %254 = ttir.empty() : tensor<1x4x7x128xbf16>
    %255 = "ttir.add"(%235, %253, %254) : (tensor<1x4x7x128xbf16>, tensor<1x4x7x128xbf16>, tensor<1x4x7x128xbf16>) -> tensor<1x4x7x128xbf16>
    %256 = ttir.empty() : tensor<1x4x32x128xbf16>
    %257 = "ttir.scatter"(%33, %214, %255, %256) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x4x32x128xbf16>, tensor<7x1xi64>, tensor<1x4x7x128xbf16>, tensor<1x4x32x128xbf16>) -> tensor<1x4x32x128xbf16>
    %258 = ttir.empty() : tensor<1x4x1x32x128xbf16>
    %259 = "ttir.reshape"(%257, %258) <{shape = [1 : i32, 4 : i32, 1 : i32, 32 : i32, 128 : i32]}> : (tensor<1x4x32x128xbf16>, tensor<1x4x1x32x128xbf16>) -> tensor<1x4x1x32x128xbf16>
    %260 = ttir.empty() : tensor<1x4x3x32x128xbf16>
    %261 = "ttir.broadcast"(%259, %260) <{broadcast_dimensions = array<i64: 1, 1, 3, 1, 1>}> : (tensor<1x4x1x32x128xbf16>, tensor<1x4x3x32x128xbf16>) -> tensor<1x4x3x32x128xbf16>
    %262 = ttir.empty() : tensor<1x12x32x128xbf16>
    %263 = "ttir.reshape"(%261, %262) <{shape = [1 : i32, 12 : i32, 32 : i32, 128 : i32]}> : (tensor<1x4x3x32x128xbf16>, tensor<1x12x32x128xbf16>) -> tensor<1x12x32x128xbf16>
    %264 = ttir.empty() : tensor<1x12x128x32xbf16>
    %265 = "ttir.permute"(%263, %264) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x12x32x128xbf16>, tensor<1x12x128x32xbf16>) -> tensor<1x12x128x32xbf16>
    %266 = ttir.empty() : tensor<12x128x32xbf16>
    %267 = "ttir.reshape"(%265, %266) <{shape = [12 : i32, 128 : i32, 32 : i32]}> : (tensor<1x12x128x32xbf16>, tensor<12x128x32xbf16>) -> tensor<12x128x32xbf16>
    %268 = "ttir.dot_general"(%202, %267) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<12x7x128xbf16>, tensor<12x128x32xbf16>) -> tensor<12x7x32xbf16>
    %269 = ttir.empty() : tensor<12x7x32xf32>
    %270 = "ttir.typecast"(%268, %269) <{conservative_folding = false}> : (tensor<12x7x32xbf16>, tensor<12x7x32xf32>) -> tensor<12x7x32xf32>
    %271 = ttir.empty() : tensor<1x12x7x32xf32>
    %272 = "ttir.reshape"(%270, %271) <{shape = [1 : i32, 12 : i32, 7 : i32, 32 : i32]}> : (tensor<12x7x32xf32>, tensor<1x12x7x32xf32>) -> tensor<1x12x7x32xf32>
    %273 = ttir.empty() : tensor<1x1x1x1xf32>
    %274 = "ttir.reshape"(%27, %273) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %275 = ttir.empty() : tensor<1x12x7x32xf32>
    %276 = "ttir.broadcast"(%274, %275) <{broadcast_dimensions = array<i64: 1, 12, 7, 32>}> : (tensor<1x1x1x1xf32>, tensor<1x12x7x32xf32>) -> tensor<1x12x7x32xf32>
    %277 = ttir.empty() : tensor<1x12x7x32xf32>
    %278 = "ttir.multiply"(%272, %276, %277) : (tensor<1x12x7x32xf32>, tensor<1x12x7x32xf32>, tensor<1x12x7x32xf32>) -> tensor<1x12x7x32xf32>
    %279 = ttir.empty() : tensor<1x12x7x32xbf16>
    %280 = "ttir.typecast"(%278, %279) <{conservative_folding = false}> : (tensor<1x12x7x32xf32>, tensor<1x12x7x32xbf16>) -> tensor<1x12x7x32xbf16>
    %281 = ttir.empty() : tensor<1x32xi64>
    %282 = "ttir.reshape"(%43, %281) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xi64>, tensor<1x32xi64>) -> tensor<1x32xi64>
    %283 = ttir.empty() : tensor<7x32xi64>
    %284 = "ttir.broadcast"(%282, %283) <{broadcast_dimensions = array<i64: 7, 1>}> : (tensor<1x32xi64>, tensor<7x32xi64>) -> tensor<7x32xi64>
    %285 = ttir.empty() : tensor<7x1xi64>
    %286 = "ttir.reshape"(%44, %285) <{shape = [7 : i32, 1 : i32]}> : (tensor<7xi64>, tensor<7x1xi64>) -> tensor<7x1xi64>
    %287 = ttir.empty() : tensor<7x32xi64>
    %288 = "ttir.broadcast"(%286, %287) <{broadcast_dimensions = array<i64: 1, 32>}> : (tensor<7x1xi64>, tensor<7x32xi64>) -> tensor<7x32xi64>
    %289 = ttir.empty() : tensor<7x32xi64>
    %290 = "ttir.subtract"(%284, %288, %289) : (tensor<7x32xi64>, tensor<7x32xi64>, tensor<7x32xi64>) -> tensor<7x32xi64>
    %291 = ttir.empty() : tensor<7x32xi1>
    %292 = "ttir.ge"(%290, %58, %291) : (tensor<7x32xi64>, tensor<7x32xi64>, tensor<7x32xi1>) -> tensor<7x32xi1>
    %293 = ttir.empty() : tensor<1x1xbf16>
    %294 = "ttir.reshape"(%25, %293) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %295 = ttir.empty() : tensor<7x32xbf16>
    %296 = "ttir.broadcast"(%294, %295) <{broadcast_dimensions = array<i64: 7, 32>}> : (tensor<1x1xbf16>, tensor<7x32xbf16>) -> tensor<7x32xbf16>
    %297 = ttir.empty() : tensor<7x32xbf16>
    %298 = "ttir.where"(%292, %296, %54, %297) : (tensor<7x32xi1>, tensor<7x32xbf16>, tensor<7x32xbf16>, tensor<7x32xbf16>) -> tensor<7x32xbf16>
    %299 = ttir.empty() : tensor<7x32xf32>
    %300 = "ttir.typecast"(%298, %299) <{conservative_folding = false}> : (tensor<7x32xbf16>, tensor<7x32xf32>) -> tensor<7x32xf32>
    %301 = ttir.empty() : tensor<7x1xi64>
    %302 = "ttir.reshape"(%153, %301) <{shape = [7 : i32, 1 : i32]}> : (tensor<7xi64>, tensor<7x1xi64>) -> tensor<7x1xi64>
    %303 = ttir.empty() : tensor<7x32xi64>
    %304 = "ttir.broadcast"(%302, %303) <{broadcast_dimensions = array<i64: 1, 32>}> : (tensor<7x1xi64>, tensor<7x32xi64>) -> tensor<7x32xi64>
    %305 = ttir.empty() : tensor<7x32xi1>
    %306 = "ttir.gt"(%284, %304, %305) : (tensor<7x32xi64>, tensor<7x32xi64>, tensor<7x32xi1>) -> tensor<7x32xi1>
    %307 = ttir.empty() : tensor<7x32xf32>
    %308 = "ttir.typecast"(%306, %307) <{conservative_folding = false}> : (tensor<7x32xi1>, tensor<7x32xf32>) -> tensor<7x32xf32>
    %309 = ttir.empty() : tensor<7x32xf32>
    %310 = "ttir.multiply"(%300, %308, %309) : (tensor<7x32xf32>, tensor<7x32xf32>, tensor<7x32xf32>) -> tensor<7x32xf32>
    %311 = ttir.empty() : tensor<7x32xbf16>
    %312 = "ttir.typecast"(%310, %311) <{conservative_folding = false}> : (tensor<7x32xf32>, tensor<7x32xbf16>) -> tensor<7x32xbf16>
    %313 = ttir.empty() : tensor<1x7x32xbf16>
    %314 = "ttir.reshape"(%312, %313) <{shape = [1 : i32, 7 : i32, 32 : i32]}> : (tensor<7x32xbf16>, tensor<1x7x32xbf16>) -> tensor<1x7x32xbf16>
    %315 = ttir.empty() : tensor<1x1x7x32xbf16>
    %316 = "ttir.reshape"(%314, %315) <{shape = [1 : i32, 1 : i32, 7 : i32, 32 : i32]}> : (tensor<1x7x32xbf16>, tensor<1x1x7x32xbf16>) -> tensor<1x1x7x32xbf16>
    %317 = ttir.empty() : tensor<1x12x7x32xbf16>
    %318 = "ttir.broadcast"(%316, %317) <{broadcast_dimensions = array<i64: 1, 12, 1, 1>}> : (tensor<1x1x7x32xbf16>, tensor<1x12x7x32xbf16>) -> tensor<1x12x7x32xbf16>
    %319 = ttir.empty() : tensor<1x12x7x32xbf16>
    %320 = "ttir.add"(%280, %318, %319) : (tensor<1x12x7x32xbf16>, tensor<1x12x7x32xbf16>, tensor<1x12x7x32xbf16>) -> tensor<1x12x7x32xbf16>
    %321 = ttir.empty() : tensor<1x12x7x32xf32>
    %322 = "ttir.typecast"(%320, %321) <{conservative_folding = false}> : (tensor<1x12x7x32xbf16>, tensor<1x12x7x32xf32>) -> tensor<1x12x7x32xf32>
    %323 = ttir.empty() : tensor<1x12x7xf32>
    %324 = "ttir.max"(%322, %323) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x12x7x32xf32>, tensor<1x12x7xf32>) -> tensor<1x12x7xf32>
    %325 = ttir.empty() : tensor<1x12x7x1xf32>
    %326 = "ttir.reshape"(%324, %325) <{shape = [1 : i32, 12 : i32, 7 : i32, 1 : i32]}> : (tensor<1x12x7xf32>, tensor<1x12x7x1xf32>) -> tensor<1x12x7x1xf32>
    %327 = ttir.empty() : tensor<1x12x7x32xf32>
    %328 = "ttir.broadcast"(%326, %327) <{broadcast_dimensions = array<i64: 1, 1, 1, 32>}> : (tensor<1x12x7x1xf32>, tensor<1x12x7x32xf32>) -> tensor<1x12x7x32xf32>
    %329 = ttir.empty() : tensor<1x12x7x32xf32>
    %330 = "ttir.subtract"(%322, %328, %329) : (tensor<1x12x7x32xf32>, tensor<1x12x7x32xf32>, tensor<1x12x7x32xf32>) -> tensor<1x12x7x32xf32>
    %331 = ttir.empty() : tensor<1x12x7x32xf32>
    %332 = "ttir.exp"(%330, %331) : (tensor<1x12x7x32xf32>, tensor<1x12x7x32xf32>) -> tensor<1x12x7x32xf32>
    %333 = ttir.empty() : tensor<1x12x7xf32>
    %334 = "ttir.sum"(%332, %333) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x12x7x32xf32>, tensor<1x12x7xf32>) -> tensor<1x12x7xf32>
    %335 = ttir.empty() : tensor<1x12x7x1xf32>
    %336 = "ttir.reshape"(%334, %335) <{shape = [1 : i32, 12 : i32, 7 : i32, 1 : i32]}> : (tensor<1x12x7xf32>, tensor<1x12x7x1xf32>) -> tensor<1x12x7x1xf32>
    %337 = ttir.empty() : tensor<1x12x7x32xf32>
    %338 = "ttir.broadcast"(%336, %337) <{broadcast_dimensions = array<i64: 1, 1, 1, 32>}> : (tensor<1x12x7x1xf32>, tensor<1x12x7x32xf32>) -> tensor<1x12x7x32xf32>
    %339 = ttir.empty() : tensor<1x12x7x32xf32>
    %340 = "ttir.div"(%332, %338, %339) : (tensor<1x12x7x32xf32>, tensor<1x12x7x32xf32>, tensor<1x12x7x32xf32>) -> tensor<1x12x7x32xf32>
    %341 = ttir.empty() : tensor<1x12x7x32xbf16>
    %342 = "ttir.typecast"(%340, %341) <{conservative_folding = false}> : (tensor<1x12x7x32xf32>, tensor<1x12x7x32xbf16>) -> tensor<1x12x7x32xbf16>
    %343 = ttir.empty() : tensor<12x7x32xbf16>
    %344 = "ttir.reshape"(%342, %343) <{shape = [12 : i32, 7 : i32, 32 : i32]}> : (tensor<1x12x7x32xbf16>, tensor<12x7x32xbf16>) -> tensor<12x7x32xbf16>
    %345 = ttir.empty() : tensor<1x512x3072xbf16>
    %346 = "ttir.reshape"(%11, %345) <{shape = [1 : i32, 512 : i32, 3072 : i32]}> : (tensor<512x3072xbf16>, tensor<1x512x3072xbf16>) -> tensor<1x512x3072xbf16>
    %347 = ttir.empty() : tensor<512x3072xbf16>
    %348 = "ttir.reshape"(%346, %347) <{shape = [512 : i32, 3072 : i32]}> : (tensor<1x512x3072xbf16>, tensor<512x3072xbf16>) -> tensor<512x3072xbf16>
    %349 = ttir.empty() : tensor<3072x512xbf16>
    %350 = "ttir.permute"(%348, %349) <{permutation = array<i64: 1, 0>}> : (tensor<512x3072xbf16>, tensor<3072x512xbf16>) -> tensor<3072x512xbf16>
    %351 = "ttir.dot_general"(%132, %350) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x512xbf16>) -> tensor<7x512xbf16>
    %352 = ttir.empty() : tensor<1x7x4x128xbf16>
    %353 = "ttir.reshape"(%351, %352) <{shape = [1 : i32, 7 : i32, 4 : i32, 128 : i32]}> : (tensor<7x512xbf16>, tensor<1x7x4x128xbf16>) -> tensor<1x7x4x128xbf16>
    %354 = ttir.empty() : tensor<1x4x7x128xbf16>
    %355 = "ttir.permute"(%353, %354) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x4x128xbf16>, tensor<1x4x7x128xbf16>) -> tensor<1x4x7x128xbf16>
    %356 = ttir.empty() : tensor<1x4x32x128xbf16>
    %357 = "ttir.scatter"(%23, %214, %355, %356) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x4x32x128xbf16>, tensor<7x1xi64>, tensor<1x4x7x128xbf16>, tensor<1x4x32x128xbf16>) -> tensor<1x4x32x128xbf16>
    %358 = ttir.empty() : tensor<1x4x1x32x128xbf16>
    %359 = "ttir.reshape"(%357, %358) <{shape = [1 : i32, 4 : i32, 1 : i32, 32 : i32, 128 : i32]}> : (tensor<1x4x32x128xbf16>, tensor<1x4x1x32x128xbf16>) -> tensor<1x4x1x32x128xbf16>
    %360 = ttir.empty() : tensor<1x4x3x32x128xbf16>
    %361 = "ttir.broadcast"(%359, %360) <{broadcast_dimensions = array<i64: 1, 1, 3, 1, 1>}> : (tensor<1x4x1x32x128xbf16>, tensor<1x4x3x32x128xbf16>) -> tensor<1x4x3x32x128xbf16>
    %362 = ttir.empty() : tensor<12x32x128xbf16>
    %363 = "ttir.reshape"(%361, %362) <{shape = [12 : i32, 32 : i32, 128 : i32]}> : (tensor<1x4x3x32x128xbf16>, tensor<12x32x128xbf16>) -> tensor<12x32x128xbf16>
    %364 = "ttir.dot_general"(%344, %363) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<12x7x32xbf16>, tensor<12x32x128xbf16>) -> tensor<12x7x128xbf16>
    %365 = ttir.empty() : tensor<1x12x7x128xbf16>
    %366 = "ttir.reshape"(%364, %365) <{shape = [1 : i32, 12 : i32, 7 : i32, 128 : i32]}> : (tensor<12x7x128xbf16>, tensor<1x12x7x128xbf16>) -> tensor<1x12x7x128xbf16>
    %367 = ttir.empty() : tensor<1x7x12x128xbf16>
    %368 = "ttir.permute"(%366, %367) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x12x7x128xbf16>, tensor<1x7x12x128xbf16>) -> tensor<1x7x12x128xbf16>
    %369 = ttir.empty() : tensor<7x1536xbf16>
    %370 = "ttir.reshape"(%368, %369) <{shape = [7 : i32, 1536 : i32]}> : (tensor<1x7x12x128xbf16>, tensor<7x1536xbf16>) -> tensor<7x1536xbf16>
    %371 = ttir.empty() : tensor<1x3072x1536xbf16>
    %372 = "ttir.reshape"(%9, %371) <{shape = [1 : i32, 3072 : i32, 1536 : i32]}> : (tensor<3072x1536xbf16>, tensor<1x3072x1536xbf16>) -> tensor<1x3072x1536xbf16>
    %373 = ttir.empty() : tensor<3072x1536xbf16>
    %374 = "ttir.reshape"(%372, %373) <{shape = [3072 : i32, 1536 : i32]}> : (tensor<1x3072x1536xbf16>, tensor<3072x1536xbf16>) -> tensor<3072x1536xbf16>
    %375 = ttir.empty() : tensor<1536x3072xbf16>
    %376 = "ttir.permute"(%374, %375) <{permutation = array<i64: 1, 0>}> : (tensor<3072x1536xbf16>, tensor<1536x3072xbf16>) -> tensor<1536x3072xbf16>
    %377 = "ttir.dot_general"(%370, %376) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x1536xbf16>, tensor<1536x3072xbf16>) -> tensor<7x3072xbf16>
    %378 = ttir.empty() : tensor<7x3072xbf16>
    %379 = "ttir.all_reduce"(%377, %378) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<7x3072xbf16>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %380 = ttir.empty() : tensor<1x7x3072xbf16>
    %381 = "ttir.reshape"(%379, %380) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %382 = ttir.empty() : tensor<1x7x3072xbf16>
    %383 = "ttir.add"(%86, %381, %382) : (tensor<1x7x3072xbf16>, tensor<1x7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %384 = ttir.empty() : tensor<1x1x3072xbf16>
    %385 = "ttir.reshape"(%37, %384) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %386 = ttir.empty() : tensor<1x1x3072xf32>
    %387 = "ttir.typecast"(%385, %386) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %388 = ttir.empty() : tensor<3072xf32>
    %389 = "ttir.reshape"(%387, %388) <{shape = [3072 : i32]}> : (tensor<1x1x3072xf32>, tensor<3072xf32>) -> tensor<3072xf32>
    %390 = ttir.empty() : tensor<1x1x3072xf32>
    %391 = "ttir.reshape"(%389, %390) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %392 = ttir.empty() : tensor<1x7x3072xf32>
    %393 = "ttir.broadcast"(%391, %392) <{broadcast_dimensions = array<i64: 1, 7, 1>}> : (tensor<1x1x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %394 = ttir.empty() : tensor<1x7x3072xf32>
    %395 = "ttir.typecast"(%383, %394) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %396 = ttir.empty() : tensor<1x7x3072xf32>
    %397 = "ttir.pow"(%395, %62, %396) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %398 = ttir.empty() : tensor<1x7xf32>
    %399 = "ttir.sum"(%397, %398) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %400 = ttir.empty() : tensor<1x7xf32>
    %401 = "ttir.multiply"(%399, %47, %400) : (tensor<1x7xf32>, tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %402 = ttir.empty() : tensor<1x7x1xf32>
    %403 = "ttir.reshape"(%401, %402) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %404 = ttir.empty() : tensor<1x7x1xf32>
    %405 = "ttir.add"(%403, %110, %404) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %406 = ttir.empty() : tensor<1x7x1xf32>
    %407 = "ttir.rsqrt"(%405, %406) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %408 = ttir.empty() : tensor<1x7xf32>
    %409 = "ttir.reshape"(%407, %408) <{shape = [1 : i32, 7 : i32]}> : (tensor<1x7x1xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %410 = ttir.empty() : tensor<1x7x1xf32>
    %411 = "ttir.reshape"(%409, %410) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %412 = ttir.empty() : tensor<1x7x3072xf32>
    %413 = "ttir.broadcast"(%411, %412) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x7x1xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %414 = ttir.empty() : tensor<1x7x3072xf32>
    %415 = "ttir.multiply"(%395, %413, %414) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %416 = ttir.empty() : tensor<1x7x3072xbf16>
    %417 = "ttir.typecast"(%415, %416) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %418 = ttir.empty() : tensor<1x7x3072xf32>
    %419 = "ttir.typecast"(%417, %418) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %420 = ttir.empty() : tensor<1x7x3072xf32>
    %421 = "ttir.multiply"(%393, %419, %420) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %422 = ttir.empty() : tensor<1x7x3072xbf16>
    %423 = "ttir.typecast"(%421, %422) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %424 = ttir.empty() : tensor<7x3072xbf16>
    %425 = "ttir.reshape"(%423, %424) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x7x3072xbf16>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %426 = ttir.empty() : tensor<1x4096x3072xbf16>
    %427 = "ttir.reshape"(%39, %426) <{shape = [1 : i32, 4096 : i32, 3072 : i32]}> : (tensor<4096x3072xbf16>, tensor<1x4096x3072xbf16>) -> tensor<1x4096x3072xbf16>
    %428 = ttir.empty() : tensor<4096x3072xbf16>
    %429 = "ttir.reshape"(%427, %428) <{shape = [4096 : i32, 3072 : i32]}> : (tensor<1x4096x3072xbf16>, tensor<4096x3072xbf16>) -> tensor<4096x3072xbf16>
    %430 = ttir.empty() : tensor<3072x4096xbf16>
    %431 = "ttir.permute"(%429, %430) <{permutation = array<i64: 1, 0>}> : (tensor<4096x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<3072x4096xbf16>
    %432 = "ttir.dot_general"(%425, %431) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<7x4096xbf16>
    %433 = ttir.empty() : tensor<1x7x4096xbf16>
    %434 = "ttir.reshape"(%432, %433) <{shape = [1 : i32, 7 : i32, 4096 : i32]}> : (tensor<7x4096xbf16>, tensor<1x7x4096xbf16>) -> tensor<1x7x4096xbf16>
    %435 = ttir.empty() : tensor<1x7x4096xf32>
    %436 = "ttir.typecast"(%434, %435) <{conservative_folding = false}> : (tensor<1x7x4096xbf16>, tensor<1x7x4096xf32>) -> tensor<1x7x4096xf32>
    %437 = ttir.empty() : tensor<1x7x4096xbf16>
    %438 = "ttir.sigmoid"(%434, %437) : (tensor<1x7x4096xbf16>, tensor<1x7x4096xbf16>) -> tensor<1x7x4096xbf16>
    %439 = ttir.empty() : tensor<1x7x4096xf32>
    %440 = "ttir.typecast"(%438, %439) <{conservative_folding = false}> : (tensor<1x7x4096xbf16>, tensor<1x7x4096xf32>) -> tensor<1x7x4096xf32>
    %441 = ttir.empty() : tensor<1x7x4096xf32>
    %442 = "ttir.multiply"(%436, %440, %441) : (tensor<1x7x4096xf32>, tensor<1x7x4096xf32>, tensor<1x7x4096xf32>) -> tensor<1x7x4096xf32>
    %443 = ttir.empty() : tensor<1x7x4096xbf16>
    %444 = "ttir.typecast"(%442, %443) <{conservative_folding = false}> : (tensor<1x7x4096xf32>, tensor<1x7x4096xbf16>) -> tensor<1x7x4096xbf16>
    %445 = ttir.empty() : tensor<1x7x4096xf32>
    %446 = "ttir.typecast"(%444, %445) <{conservative_folding = false}> : (tensor<1x7x4096xbf16>, tensor<1x7x4096xf32>) -> tensor<1x7x4096xf32>
    %447 = ttir.empty() : tensor<1x4096x3072xbf16>
    %448 = "ttir.reshape"(%7, %447) <{shape = [1 : i32, 4096 : i32, 3072 : i32]}> : (tensor<4096x3072xbf16>, tensor<1x4096x3072xbf16>) -> tensor<1x4096x3072xbf16>
    %449 = ttir.empty() : tensor<4096x3072xbf16>
    %450 = "ttir.reshape"(%448, %449) <{shape = [4096 : i32, 3072 : i32]}> : (tensor<1x4096x3072xbf16>, tensor<4096x3072xbf16>) -> tensor<4096x3072xbf16>
    %451 = ttir.empty() : tensor<3072x4096xbf16>
    %452 = "ttir.permute"(%450, %451) <{permutation = array<i64: 1, 0>}> : (tensor<4096x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<3072x4096xbf16>
    %453 = "ttir.dot_general"(%425, %452) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<7x4096xbf16>
    %454 = ttir.empty() : tensor<7x4096xf32>
    %455 = "ttir.typecast"(%453, %454) <{conservative_folding = false}> : (tensor<7x4096xbf16>, tensor<7x4096xf32>) -> tensor<7x4096xf32>
    %456 = ttir.empty() : tensor<1x7x4096xf32>
    %457 = "ttir.reshape"(%455, %456) <{shape = [1 : i32, 7 : i32, 4096 : i32]}> : (tensor<7x4096xf32>, tensor<1x7x4096xf32>) -> tensor<1x7x4096xf32>
    %458 = ttir.empty() : tensor<1x7x4096xf32>
    %459 = "ttir.multiply"(%446, %457, %458) : (tensor<1x7x4096xf32>, tensor<1x7x4096xf32>, tensor<1x7x4096xf32>) -> tensor<1x7x4096xf32>
    %460 = ttir.empty() : tensor<1x7x4096xbf16>
    %461 = "ttir.typecast"(%459, %460) <{conservative_folding = false}> : (tensor<1x7x4096xf32>, tensor<1x7x4096xbf16>) -> tensor<1x7x4096xbf16>
    %462 = ttir.empty() : tensor<7x4096xbf16>
    %463 = "ttir.reshape"(%461, %462) <{shape = [7 : i32, 4096 : i32]}> : (tensor<1x7x4096xbf16>, tensor<7x4096xbf16>) -> tensor<7x4096xbf16>
    %464 = ttir.empty() : tensor<1x3072x4096xbf16>
    %465 = "ttir.reshape"(%5, %464) <{shape = [1 : i32, 3072 : i32, 4096 : i32]}> : (tensor<3072x4096xbf16>, tensor<1x3072x4096xbf16>) -> tensor<1x3072x4096xbf16>
    %466 = ttir.empty() : tensor<3072x4096xbf16>
    %467 = "ttir.reshape"(%465, %466) <{shape = [3072 : i32, 4096 : i32]}> : (tensor<1x3072x4096xbf16>, tensor<3072x4096xbf16>) -> tensor<3072x4096xbf16>
    %468 = ttir.empty() : tensor<4096x3072xbf16>
    %469 = "ttir.permute"(%467, %468) <{permutation = array<i64: 1, 0>}> : (tensor<3072x4096xbf16>, tensor<4096x3072xbf16>) -> tensor<4096x3072xbf16>
    %470 = "ttir.dot_general"(%463, %469) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x4096xbf16>, tensor<4096x3072xbf16>) -> tensor<7x3072xbf16>
    %471 = ttir.empty() : tensor<7x3072xbf16>
    %472 = "ttir.all_reduce"(%470, %471) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<7x3072xbf16>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %473 = ttir.empty() : tensor<1x7x3072xbf16>
    %474 = "ttir.reshape"(%472, %473) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %475 = ttir.empty() : tensor<1x7x3072xbf16>
    %476 = "ttir.add"(%383, %474, %475) : (tensor<1x7x3072xbf16>, tensor<1x7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %477 = ttir.empty() : tensor<1x7x3072xf32>
    %478 = "ttir.typecast"(%476, %477) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %479 = ttir.empty() : tensor<1x7x3072xf32>
    %480 = "ttir.pow"(%478, %62, %479) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %481 = ttir.empty() : tensor<1x7xf32>
    %482 = "ttir.sum"(%480, %481) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %483 = ttir.empty() : tensor<1x7xf32>
    %484 = "ttir.multiply"(%482, %47, %483) : (tensor<1x7xf32>, tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %485 = ttir.empty() : tensor<1x7x1xf32>
    %486 = "ttir.reshape"(%484, %485) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %487 = ttir.empty() : tensor<1x7x1xf32>
    %488 = "ttir.add"(%486, %110, %487) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %489 = ttir.empty() : tensor<1x7x1xf32>
    %490 = "ttir.rsqrt"(%488, %489) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %491 = ttir.empty() : tensor<1x7xf32>
    %492 = "ttir.reshape"(%490, %491) <{shape = [1 : i32, 7 : i32]}> : (tensor<1x7x1xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %493 = ttir.empty() : tensor<1x7x1xf32>
    %494 = "ttir.reshape"(%492, %493) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %495 = ttir.empty() : tensor<1x7x3072xf32>
    %496 = "ttir.broadcast"(%494, %495) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x7x1xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %497 = ttir.empty() : tensor<1x7x3072xf32>
    %498 = "ttir.multiply"(%478, %496, %497) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %499 = ttir.empty() : tensor<1x7x3072xbf16>
    %500 = "ttir.typecast"(%498, %499) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %501 = ttir.empty() : tensor<1x7x3072xf32>
    %502 = "ttir.typecast"(%500, %501) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %503 = ttir.empty() : tensor<1x7x3072xf32>
    %504 = "ttir.multiply"(%72, %502, %503) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %505 = ttir.empty() : tensor<1x7x3072xbf16>
    %506 = "ttir.typecast"(%504, %505) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %507 = ttir.empty() : tensor<7x3072xbf16>
    %508 = "ttir.reshape"(%506, %507) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x7x3072xbf16>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %509 = ttir.empty() : tensor<1x128256x3072xbf16>
    %510 = "ttir.reshape"(%1, %509) <{shape = [1 : i32, 128256 : i32, 3072 : i32]}> : (tensor<128256x3072xbf16>, tensor<1x128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %511 = ttir.empty() : tensor<128256x3072xbf16>
    %512 = "ttir.reshape"(%510, %511) <{shape = [128256 : i32, 3072 : i32]}> : (tensor<1x128256x3072xbf16>, tensor<128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %513 = ttir.empty() : tensor<3072x128256xbf16>
    %514 = "ttir.permute"(%512, %513) <{permutation = array<i64: 1, 0>}> : (tensor<128256x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<3072x128256xbf16>
    %515 = "ttir.dot_general"(%508, %514) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %516 = ttir.empty() : tensor<1x7x128256xbf16>
    %517 = "ttir.reshape"(%515, %516) <{shape = [1 : i32, 7 : i32, 128256 : i32]}> : (tensor<7x128256xbf16>, tensor<1x7x128256xbf16>) -> tensor<1x7x128256xbf16>
    %518 = ttir.empty() : tensor<1x7x128256xbf16>
    %519 = "ttir.mesh_shard"(%517, %518) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x7x128256xbf16>, tensor<1x7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %519 : tensor<1x7x128256xbf16>
  }
}
2025-09-10 19:54:26.086 (  32.962s) [        F1708000]      module_builder.cc:561   WARN| `mhlo.num_partitions` attribute not found, assuming default number of partitions: 1
2025-09-10 19:54:26.086 (  32.962s) [        F1708000]      module_builder.cc:575   WARN| `mhlo.num_replicas` attribute not found, assuming default number of replicas: 1
2025-09-10 19:54:26.086 (  32.962s) [        F1708000]      module_builder.cc:585   WARN| Num replicas and num partitions are not set, inferring the number of devices from mesh shape
2025-09-10 19:54:26.171 (  33.047s) [        F1708000]      module_builder.cc:630      1| TTNN Module:
module @SyncTensorsGraph.448 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.448 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073185088, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073193408, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x16, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 2)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 2, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main_const_eval_0(%arg0: tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_1(%arg0: tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_2(%arg0: tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_3(%arg0: tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> (tensor<1x1x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.constant"(%0) <{dtype = #ttcore.supportedDataTypes<si32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <interleaved>>, value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xsi32>}> : (!ttnn.device) -> tensor<32xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.constant"(%0) <{dtype = #ttcore.supportedDataTypes<si32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <interleaved>>, value = dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xsi32>}> : (!ttnn.device) -> tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %4 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<si32>, fill_value = 1 : i32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %5 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %6 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %7 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32]}> : (tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %8 = "ttnn.reshape"(%1) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<32xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %9 = "ttnn.repeat"(%8) <{repeat_dims = #ttnn.shape<7x1>}> : (tensor<1x32xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x32xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %10 = "ttnn.reshape"(%2) <{shape = [7 : i32, 1 : i32]}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %11 = "ttnn.neg"(%10) : (tensor<7x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<7x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %12 = "ttnn.add"(%8, %11) <{dtype = #ttcore.supportedDataTypes<si32>}> : (tensor<1x32xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x32xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<7x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x32xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %13 = "ttnn.typecast"(%12) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x32xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x32xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<7x32xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %14 = "ttnn.typecast"(%7) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %15 = "ttnn.ge"(%13, %14) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x32xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x32xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<7x32xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %16 = "ttnn.typecast"(%15) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x32xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x32xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<7x32xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %17 = "ttnn.reshape"(%5) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %18 = "ttnn.repeat"(%17) <{repeat_dims = #ttnn.shape<7x32>}> : (tensor<1x1xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x32xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<1x1xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %19 = "ttnn.repeat"(%6) <{repeat_dims = #ttnn.shape<7x32>}> : (tensor<1x1xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x32xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x1xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %20 = "ttnn.where"(%16, %18, %19) : (tensor<7x32xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x32xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x32xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x32xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<7x32xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<7x32xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<7x32xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %21 = "ttnn.reshape"(%20) <{shape = [1 : i32, 1 : i32, 7 : i32, 32 : i32]}> : (tensor<7x32xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<7x32xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %22 = "ttnn.typecast"(%21) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x7x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%21) <{force = false}> : (tensor<1x1x7x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %23 = "ttnn.reshape"(%9) <{shape = [1 : i32, 1 : i32, 7 : i32, 32 : i32]}> : (tensor<7x32xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x32xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<7x32xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %24 = "ttnn.typecast"(%23) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x7x32xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%23) <{force = false}> : (tensor<1x1x7x32xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %22, %24 : tensor<1x1x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_4(%arg0: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_5(%arg0: tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_6(%arg0: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.repeat"(%2) <{repeat_dims = #ttnn.shape<7x1>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 7 : i32]}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %5 = "ttnn.reshape"(%3) <{shape = [1 : i32, 7 : i32]}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %6 = "ttnn.reshape"(%3) <{shape = [1 : i32, 7 : i32]}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %4, %5, %6 : tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_7(%arg0: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %3 : tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_8(%arg0: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %3 : tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_9(%arg0: tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_10(%arg0: tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_11(%arg0: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_12(%arg0: tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_13(%arg0: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.repeat"(%2) <{repeat_dims = #ttnn.shape<1x12x7x32>}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %4 = "ttnn.reshape"(%3) <{shape = [12 : i32, 7 : i32, 32 : i32]}> : (tensor<1x12x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x12x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %4 : tensor<12x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_14(%arg0: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %3 : tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_15() -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %2 : tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main(%arg0: tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg1: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg2: tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg3: tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg4: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg5: tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg6: tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg7: tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg8: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg9: tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg10: tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg11: tensor<1x8x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg12: tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg13: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg14: tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg15: tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg16: tensor<1x8x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg17: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, [%arg0]) : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg10) <{force = false}> : (tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %1 = ttcore.load_cached(@main_const_eval_1, [%arg7]) : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg7) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %2 = ttcore.load_cached(@main_const_eval_2, [%arg15]) : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg15) <{force = false}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3:2 = ttcore.load_cached(@main_const_eval_3, [%arg12]) : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> (tensor<1x1x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>)
        "ttnn.deallocate"(%arg12) <{force = false}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %4 = ttcore.load_cached(@main_const_eval_4, [%arg17]) : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg17) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %5 = ttcore.load_cached(@main_const_eval_5, [%arg5]) : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg5) <{force = false}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %6:3 = ttcore.load_cached(@main_const_eval_6, [%arg1]) : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>)
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %7 = ttcore.load_cached(@main_const_eval_7, [%arg8]) : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg8) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %8 = ttcore.load_cached(@main_const_eval_8, [%arg18]) : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg18) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %9 = ttcore.load_cached(@main_const_eval_9, [%arg19]) : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg19) <{force = false}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %10 = ttcore.load_cached(@main_const_eval_10, [%arg2]) : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %11 = ttcore.load_cached(@main_const_eval_11, [%arg4]) : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %12 = ttcore.load_cached(@main_const_eval_12, [%arg3]) : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %13 = ttcore.load_cached(@main_const_eval_13, [%arg13]) : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg13) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %14 = ttcore.load_cached(@main_const_eval_14, [%arg20]) : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg20) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %15 = ttcore.load_cached(@main_const_eval_15, []) : () -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %16 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %17 = "ttnn.full"(%16) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 3.25520843E-4 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x7>}> : (!ttnn.device) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %18 = "ttnn.mesh_shard"(%arg6, %16) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg6) <{force = false}> : (tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %19 = "ttnn.mesh_shard"(%arg9, %16) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg9) <{force = false}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %20 = "ttnn.mesh_shard"(%arg11, %16) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg11) <{force = false}> : (tensor<1x8x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %21 = "ttnn.mesh_shard"(%arg14, %16) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg14) <{force = false}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %22 = "ttnn.mesh_shard"(%arg16, %16) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg16) <{force = false}> : (tensor<1x8x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %23 = "ttnn.typecast"(%18) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %24 = "ttnn.reshape"(%23) <{shape = [7 : i32]}> : (tensor<1x7xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%23) <{force = false}> : (tensor<1x7xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %25 = "ttnn.from_device"(%24) : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%24) <{force = false}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %26 = "ttnn.to_layout"(%25) <{layout = #ttnn.layout<row_major>}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%25) <{force = false}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> ()
        %27 = "ttnn.to_device"(%26, %16) <{memory_config = #ttnn.memory_config<<dram>, <interleaved>>}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%26) <{force = false}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<system_memory>>>>) -> ()
        %28 = "ttnn.embedding"(%27, %1) : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%27) <{force = false}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %29 = "ttnn.typecast"(%28) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %30 = "ttnn.reshape"(%29) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %31 = "ttnn.pow"(%30, %15) : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%30) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %32 = "ttnn.sum"(%31) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%31) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %33 = "ttnn.multiply"(%32, %17) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%32) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %34 = "ttnn.add"(%33, %6#0) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%33) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%6#0) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %35 = "ttnn.rsqrt"(%34) : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%34) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %36 = "ttnn.reshape"(%35) <{shape = [7 : i32, 1 : i32]}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%35) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %37 = "ttnn.multiply"(%29, %36) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%36) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%29) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %38 = "ttnn.multiply"(%7, %37) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%37) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %39 = "ttnn.typecast"(%38) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%38) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %40 = "ttnn.matmul"(%39, %4) <{transpose_a = false, transpose_b = true}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %41 = "ttnn.reshape"(%40) <{shape = [1 : i32, 7 : i32, 12 : i32, 128 : i32]}> : (tensor<7x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x12x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%40) <{force = false}> : (tensor<7x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %42 = "ttnn.permute"(%41) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x12x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%41) <{force = false}> : (tensor<1x7x12x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %43 = "ttnn.typecast"(%42) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %44 = "ttnn.reshape"(%43) <{shape = [12 : i32, 7 : i32, 128 : i32]}> : (tensor<1x12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%43) <{force = false}> : (tensor<1x12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %45 = "ttnn.reshape"(%21) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%21) <{force = false}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %46 = "ttnn.typecast"(%19) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %47 = "ttnn.reshape"(%46) <{shape = [1 : i32, 1 : i32, 7 : i32]}> : (tensor<7xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%46) <{force = false}> : (tensor<7xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %48 = "ttnn.matmul"(%45, %47) <{transpose_a = false, transpose_b = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%47) <{force = false}> : (tensor<1x1x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%45) <{force = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %49 = "ttnn.permute"(%48) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%48) <{force = false}> : (tensor<1x64x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %50 = "ttnn.concat"(%49, %49) <{dim = 2 : si32}> : (tensor<1x7x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%49) <{force = false}> : (tensor<1x7x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %51 = "ttnn.cos"(%50) : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %52 = "ttnn.multiply"(%44, %51) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%44) <{force = false}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %53 = "ttnn.typecast"(%52) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%52) <{force = false}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %54 = "ttnn.slice_static"(%42) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 12 : i32, 7 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %55 = "ttnn.neg"(%54) : (tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%54) <{force = false}> : (tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %56 = "ttnn.reshape"(%55) <{shape = [12 : i32, 7 : i32, 64 : i32]}> : (tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%55) <{force = false}> : (tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %57 = "ttnn.slice_static"(%42) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 12 : i32, 7 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%42) <{force = false}> : (tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %58 = "ttnn.reshape"(%57) <{shape = [12 : i32, 7 : i32, 64 : i32]}> : (tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%57) <{force = false}> : (tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %59 = "ttnn.concat"(%56, %58) <{dim = 2 : si32}> : (tensor<12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%58) <{force = false}> : (tensor<12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%56) <{force = false}> : (tensor<12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %60 = "ttnn.typecast"(%59) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%59) <{force = false}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %61 = "ttnn.sin"(%50) : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%50) <{force = false}> : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %62 = "ttnn.multiply"(%60, %61) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%60) <{force = false}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %63 = "ttnn.typecast"(%62) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%62) <{force = false}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %64 = "ttnn.add"(%53, %63) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%63) <{force = false}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%53) <{force = false}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %65 = "ttnn.matmul"(%39, %2) <{transpose_a = false, transpose_b = true}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %66 = "ttnn.reshape"(%65) <{shape = [1 : i32, 7 : i32, 4 : i32, 128 : i32]}> : (tensor<7x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x4x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%65) <{force = false}> : (tensor<7x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %67 = "ttnn.permute"(%66) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x4x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%66) <{force = false}> : (tensor<1x7x4x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %68 = "ttnn.typecast"(%67) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %69 = "ttnn.reshape"(%51) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%51) <{force = false}> : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %70 = "ttnn.multiply"(%68, %69) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%69) <{force = false}> : (tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%68) <{force = false}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %71 = "ttnn.typecast"(%70) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%70) <{force = false}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %72 = "ttnn.slice_static"(%67) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 7 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %73 = "ttnn.neg"(%72) : (tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%72) <{force = false}> : (tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %74 = "ttnn.slice_static"(%67) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 7 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%67) <{force = false}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %75 = "ttnn.concat"(%73, %74) <{dim = 3 : si32}> : (tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%74) <{force = false}> : (tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%73) <{force = false}> : (tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %76 = "ttnn.typecast"(%75) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%75) <{force = false}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %77 = "ttnn.reshape"(%61) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%61) <{force = false}> : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %78 = "ttnn.multiply"(%76, %77) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%77) <{force = false}> : (tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%76) <{force = false}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %79 = "ttnn.typecast"(%78) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%78) <{force = false}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %80 = "ttnn.add"(%71, %79) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%79) <{force = false}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%71) <{force = false}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.fill_cache"(%22, %80) <{batch_offset = 0 : i32}> : (tensor<1x4x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%80) <{force = false}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %81 = "ttnn.reshape"(%22) <{shape = [1 : i32, 4 : i32, 1 : i32, 32 : i32, 128 : i32]}> : (tensor<1x4x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 128 + d1 * 32 + d2 * 32 + d3, d4), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%22) <{force = false}> : (tensor<1x4x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %82 = "ttnn.repeat"(%81) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x4x1x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 128 + d1 * 32 + d2 * 32 + d3, d4), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x3x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 384 + d1 * 96 + d2 * 32 + d3, d4), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%81) <{force = false}> : (tensor<1x4x1x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 128 + d1 * 32 + d2 * 32 + d3, d4), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %83 = "ttnn.reshape"(%82) <{shape = [1 : i32, 12 : i32, 32 : i32, 128 : i32]}> : (tensor<1x4x3x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 384 + d1 * 96 + d2 * 32 + d3, d4), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%82) <{force = false}> : (tensor<1x4x3x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 384 + d1 * 96 + d2 * 32 + d3, d4), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %84 = "ttnn.permute"(%83) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x12x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x128x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%83) <{force = false}> : (tensor<1x12x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %85 = "ttnn.reshape"(%84) <{shape = [12 : i32, 128 : i32, 32 : i32]}> : (tensor<1x12x128x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x128x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%84) <{force = false}> : (tensor<1x12x128x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %86 = "ttnn.matmul"(%64, %85) <{transpose_a = false, transpose_b = false}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x128x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%85) <{force = false}> : (tensor<12x128x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%64) <{force = false}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %87 = "ttnn.typecast"(%86) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x7x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%86) <{force = false}> : (tensor<12x7x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %88 = "ttnn.multiply"(%87, %13) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%87) <{force = false}> : (tensor<12x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<12x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %89 = "ttnn.typecast"(%88) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%88) <{force = false}> : (tensor<12x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %90 = "ttnn.reshape"(%89) <{shape = [1 : i32, 12 : i32, 7 : i32, 32 : i32]}> : (tensor<12x7x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%89) <{force = false}> : (tensor<12x7x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %91 = "ttnn.reshape"(%19) <{shape = [1 : i32, 1 : i32, 7 : i32, 1 : i32]}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %92 = "ttnn.typecast"(%91) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x7x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%91) <{force = false}> : (tensor<1x1x7x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %93 = "ttnn.gt"(%3#1, %92) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%92) <{force = false}> : (tensor<1x1x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%3#1) <{force = false}> : (tensor<1x1x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %94 = "ttnn.typecast"(%93) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%93) <{force = false}> : (tensor<1x1x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %95 = "ttnn.typecast"(%94) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x7x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%94) <{force = false}> : (tensor<1x1x7x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %96 = "ttnn.multiply"(%3#0, %95) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%95) <{force = false}> : (tensor<1x1x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%3#0) <{force = false}> : (tensor<1x1x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %97 = "ttnn.typecast"(%96) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%96) <{force = false}> : (tensor<1x1x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %98 = "ttnn.add"(%90, %97) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x12x7x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%97) <{force = false}> : (tensor<1x1x7x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%90) <{force = false}> : (tensor<1x12x7x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %99 = "ttnn.typecast"(%98) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x12x7x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%98) <{force = false}> : (tensor<1x12x7x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %100 = "ttnn.softmax"(%99) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x12x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%99) <{force = false}> : (tensor<1x12x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %101 = "ttnn.typecast"(%100) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x12x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%100) <{force = false}> : (tensor<1x12x7x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %102 = "ttnn.reshape"(%101) <{shape = [12 : i32, 7 : i32, 32 : i32]}> : (tensor<1x12x7x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%101) <{force = false}> : (tensor<1x12x7x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %103 = "ttnn.matmul"(%39, %5) <{transpose_a = false, transpose_b = true}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%39) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %104 = "ttnn.reshape"(%103) <{shape = [1 : i32, 7 : i32, 4 : i32, 128 : i32]}> : (tensor<7x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x4x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%103) <{force = false}> : (tensor<7x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %105 = "ttnn.permute"(%104) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x4x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%104) <{force = false}> : (tensor<1x7x4x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.fill_cache"(%20, %105) <{batch_offset = 0 : i32}> : (tensor<1x4x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%105) <{force = false}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %106 = "ttnn.reshape"(%20) <{shape = [1 : i32, 4 : i32, 1 : i32, 32 : i32, 128 : i32]}> : (tensor<1x4x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 128 + d1 * 32 + d2 * 32 + d3, d4), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x4x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %107 = "ttnn.repeat"(%106) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x4x1x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 128 + d1 * 32 + d2 * 32 + d3, d4), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x3x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 384 + d1 * 96 + d2 * 32 + d3, d4), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%106) <{force = false}> : (tensor<1x4x1x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 128 + d1 * 32 + d2 * 32 + d3, d4), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %108 = "ttnn.reshape"(%107) <{shape = [12 : i32, 32 : i32, 128 : i32]}> : (tensor<1x4x3x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 384 + d1 * 96 + d2 * 32 + d3, d4), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%107) <{force = false}> : (tensor<1x4x3x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 384 + d1 * 96 + d2 * 32 + d3, d4), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %109 = "ttnn.matmul"(%102, %108) <{transpose_a = false, transpose_b = false}> : (tensor<12x7x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%108) <{force = false}> : (tensor<12x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%102) <{force = false}> : (tensor<12x7x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %110 = "ttnn.reshape"(%109) <{shape = [1 : i32, 12 : i32, 7 : i32, 128 : i32]}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%109) <{force = false}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %111 = "ttnn.concatenate_heads"(%110) : (tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%110) <{force = false}> : (tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %112 = "ttnn.reshape"(%111) <{shape = [7 : i32, 1536 : i32]}> : (tensor<1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%111) <{force = false}> : (tensor<1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %113 = "ttnn.matmul"(%112, %11) <{transpose_a = false, transpose_b = true}> : (tensor<7x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%112) <{force = false}> : (tensor<7x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %114 = "ttnn.reshape"(%113) <{shape = [1 : i32, 1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%113) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %115 = "ttnn.reduce_scatter"(%114, %16) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 3 : si32}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%114) <{force = false}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %116 = "ttnn.all_gather"(%115, %16) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%115) <{force = false}> : (tensor<1x1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %117 = "ttnn.reshape"(%116) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%116) <{force = false}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %118 = "ttnn.add"(%28, %117) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%117) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%28) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %119 = "ttnn.typecast"(%118) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %120 = "ttnn.reshape"(%119) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %121 = "ttnn.pow"(%120, %15) : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%120) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %122 = "ttnn.sum"(%121) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%121) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %123 = "ttnn.multiply"(%122, %17) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%122) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %124 = "ttnn.add"(%123, %6#1) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%123) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%6#1) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %125 = "ttnn.rsqrt"(%124) : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%124) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %126 = "ttnn.reshape"(%125) <{shape = [7 : i32, 1 : i32]}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%125) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %127 = "ttnn.multiply"(%119, %126) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%126) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%119) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %128 = "ttnn.multiply"(%8, %127) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%127) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %129 = "ttnn.typecast"(%128) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%128) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %130 = "ttnn.matmul"(%129, %9) <{transpose_a = false, transpose_b = true}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %131 = "ttnn.typecast"(%130) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %132 = "ttnn.sigmoid"(%130) : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%130) <{force = false}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %133 = "ttnn.typecast"(%132) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%132) <{force = false}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %134 = "ttnn.multiply"(%131, %133) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%133) <{force = false}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%131) <{force = false}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %135 = "ttnn.matmul"(%129, %12) <{transpose_a = false, transpose_b = true}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%129) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %136 = "ttnn.typecast"(%135) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%135) <{force = false}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %137 = "ttnn.multiply"(%134, %136) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%136) <{force = false}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%134) <{force = false}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %138 = "ttnn.typecast"(%137) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%137) <{force = false}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %139 = "ttnn.matmul"(%138, %10) <{transpose_a = false, transpose_b = true}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%138) <{force = false}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %140 = "ttnn.reshape"(%139) <{shape = [1 : i32, 1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%139) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %141 = "ttnn.reduce_scatter"(%140, %16) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 3 : si32}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%140) <{force = false}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %142 = "ttnn.all_gather"(%141, %16) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%141) <{force = false}> : (tensor<1x1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %143 = "ttnn.reshape"(%142) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%142) <{force = false}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %144 = "ttnn.add"(%118, %143) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%143) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%118) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %145 = "ttnn.typecast"(%144) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%144) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %146 = "ttnn.reshape"(%145) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %147 = "ttnn.pow"(%146, %15) : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%146) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %148 = "ttnn.sum"(%147) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%147) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %149 = "ttnn.multiply"(%148, %17) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%148) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %150 = "ttnn.add"(%149, %6#2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%149) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%6#2) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %151 = "ttnn.rsqrt"(%150) : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%150) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %152 = "ttnn.reshape"(%151) <{shape = [7 : i32, 1 : i32]}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%151) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %153 = "ttnn.multiply"(%145, %152) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%152) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%145) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %154 = "ttnn.multiply"(%14, %153) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%153) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %155 = "ttnn.typecast"(%154) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%154) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %156 = "ttnn.matmul"(%155, %0) <{transpose_a = false, transpose_b = true}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%155) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %157 = "ttnn.reshape"(%156) <{shape = [1 : i32, 7 : i32, 128256 : i32]}> : (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%156) <{force = false}> : (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %158 = "ttnn.to_layout"(%157) <{layout = #ttnn.layout<row_major>}> : (tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%157) <{force = false}> : (tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %159 = "ttnn.from_device"(%158) : (tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%158) <{force = false}> : (tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %160 = "ttnn.mesh_shard"(%159, %16) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%159) <{force = false}> : (tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>) -> ()
        return %160 : tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>
      }
    }
  }
}
2025-09-10 19:54:26.209 (  33.085s) [        F1708000]loaded_executable_insta:471      1| LoadedExecutableInstance::PJRT_LoadedExecutable_GetExecutable
2025-09-10 19:54:26.209 (  33.085s) [        F1708000]loaded_executable_insta:490      1| LoadedExecutableInstance::PJRT_LoadedExecutable_AddressableDevices
2025-09-10 19:54:26.210 (  33.085s) [        F1708000]              stubs.inc:76    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2025-09-10 19:54:26.210 (  33.085s) [        F1708000]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-09-10 19:54:26.210 (  33.085s) [        F1708000]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-09-10 19:54:26.210 (  33.085s) [        F1708000]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-09-10 19:54:26.210 (  33.085s) [        F1708000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-10 19:54:26.210 (  33.086s) [        F1708000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-10 19:54:26.222 (  33.098s) [        F1708000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-10 19:54:26.222 (  33.098s) [        F1708000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-10 19:54:26.232 (  33.108s) [        F1708000]              stubs.inc:76    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2025-09-10 19:54:26.232 (  33.108s) [        F1708000]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-09-10 19:54:26.232 (  33.108s) [        F1708000]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-09-10 19:54:26.232 (  33.108s) [        F1708000]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy

Computation hash: 464566451d5ed6403330c8e40428274b

Post Compilation Analysis: ================================================================================
Post Compilation Analysis: Graph input size: Unknown  GB
Post Compilation Analysis: Graph output size: Unknown  GB
Post Compilation Analysis: Aliased Input size: Unknown  GB
Post Compilation Analysis: Intermediate tensor size: Unknown  GB
Post Compilation Analysis: Compiled program size: Unknown  GB
Post Compilation Analysis: --------------------------------------------------------------------------------
Post Compilation Analysis: ================================================================================

Execution Analysis: ================================================================================
Execution Analysis: Execution Cause
Execution Analysis:   most likely user code trying to access tensor value before torch_xla.sync
Execution Analysis: Graph Info: 
Execution Analysis:   Graph Hash: 85313521f7a99f71abd1320109a6355c
Execution Analysis:   Number of Graph Inputs: 21
Execution Analysis:   Number of Graph Outputs: 1
Execution Analysis: Python Frame Triggered Execution: 
Execution Analysis:   __call__ (/localdev/jameszianxu/tt-xla/tt_torch/backend/backend.py:99)
Execution Analysis:   _fn (/localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:838)
Execution Analysis:   wrapper (/localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/utils/generic.py:953)
Execution Analysis:   _call_impl (/localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762)
Execution Analysis:   _fn (/localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:655)
Execution Analysis:   _wrapped_call_impl (/localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1749)
Execution Analysis:   llama (/localdev/jameszianxu/tt-xla/examples/pytorch/llama.py:155)
Execution Analysis:   <module> (/localdev/jameszianxu/tt-xla/examples/pytorch/llama.py:207)
Execution Analysis: --------------------------------------------------------------------------------
Execution Analysis: ================================================================================
2025-09-10 19:54:26.240 (  33.116s) [        8F7FE640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:26.240 (  33.116s) [        8F7FE640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:54:26.240 (  33.116s) [        8F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.240 (  33.116s) [        8F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.240 (  33.116s) [        8F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.240 (  33.116s) [        8F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.240 (  33.116s) [        8F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.240 (  33.116s) [        8F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.240 (  33.116s) [        8F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.240 (  33.116s) [        8F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.240 (  33.116s) [        A0FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.240 (  33.116s) [        8F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.240 (  33.116s) [        8F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.240 (  33.116s) [        A0FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.240 (  33.116s) [        8F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.240 (  33.116s) [        A27FC640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.240 (  33.116s) [        8F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.240 (  33.116s) [        A27FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.240 (  33.116s) [        8F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.240 (  33.116s) [        A27FC640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.240 (  33.116s) [        A1FFB640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.240 (  33.116s) [        8F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.240 (  33.116s) [        A1FFB640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.240 (  33.116s) [        A0FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.240 (  33.116s) [        A17FA640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.240 (  33.116s) [        8F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.240 (  33.116s) [        A17FA640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.240 (  33.116s) [        A0FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.240 (  33.116s) [        A27FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.240 (  33.116s) [        8F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.240 (  33.116s) [        A27FC640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.240 (  33.116s) [        A1FFB640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.240 (  33.116s) [        A27FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.240 (  33.116s) [        A17FA640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.240 (  33.116s) [        8F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.240 (  33.116s) [        A17FA640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.240 (  33.116s) [        A1FFB640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.240 (  33.116s) [        A17FA640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.240 (  33.116s) [        A27FC640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.240 (  33.116s) [        A17FA640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.240 (  33.116s) [        A0FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.241 (  33.116s) [        A27FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.240 (  33.116s) [        8F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.241 (  33.116s) [        8F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.241 (  33.116s) [        8F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.241 (  33.116s) [        A0FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.241 (  33.116s) [        A0FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.241 (  33.116s) [        A0FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.241 (  33.116s) [        A0FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.241 (  33.116s) [        A0FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.241 (  33.116s) [        A0FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.241 (  33.116s) [        A0FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.241 (  33.116s) [        A0FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.241 (  33.116s) [        A0FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.241 (  33.116s) [        A0FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.241 (  33.116s) [        A0FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.241 (  33.116s) [        A0FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.241 (  33.116s) [        A0FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.241 (  33.116s) [        A0FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.241 (  33.116s) [        A0FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.241 (  33.116s) [        A17FA640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.241 (  33.116s) [        A17FA640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.241 (  33.116s) [        A27FC640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.241 (  33.116s) [        A17FA640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.241 (  33.116s) [        A27FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.241 (  33.117s) [        A17FA640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.241 (  33.117s) [        A27FC640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.241 (  33.117s) [        A27FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.241 (  33.117s) [        A17FA640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.241 (  33.117s) [        A17FA640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.241 (  33.117s) [        A17FA640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.241 (  33.117s) [        A17FA640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.241 (  33.117s) [        A17FA640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.241 (  33.117s) [        A17FA640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.241 (  33.117s) [        A27FC640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.241 (  33.117s) [        A17FA640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.241 (  33.117s) [        A27FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.241 (  33.117s) [        A17FA640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.241 (  33.117s) [        A27FC640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.241 (  33.117s) [        A17FA640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.241 (  33.117s) [        A27FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.241 (  33.117s) [        A17FA640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.241 (  33.117s) [        A27FC640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.241 (  33.117s) [        A27FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.241 (  33.117s) [        A27FC640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:54:26.241 (  33.117s) [        A27FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:54:26.241 (  33.117s) [        8F7FE640] executable_instance.cc:138      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-09-10 19:54:26.241 (  33.117s) [        8F7FE640]loaded_executable_insta:527      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-09-10 19:54:26.241 (  33.117s) [        8F7FE640]loaded_executable_insta:96       1| LoadedExecutableInstance::Execute
2025-09-10 19:54:26.241 (  33.117s) [        8F7FE640]loaded_executable_insta:119      1| [DEVICE] Runtime device not opened, opening devices...
2025-09-10 19:54:26.241 | warning  |          Always | Closing and re-initializing MetalContext with new parameters. (metal_context.cpp:76)
2025-09-10 19:54:26.312 | warning  |           Metal | Got num_routing_planes: 1, which is less than current value: 255, ignoring the override (metal_context.cpp:478)
2025-09-10 19:54:26.312 | info     |           Metal | Dispatch on FabricConfig::FABRIC_1D with 1 Command Queues
 (device_pool.cpp:328)
2025-09-10 19:54:26.317 | info     |           Metal | Initializing Fabric (device_pool.cpp:397)
2025-09-10 19:54:27.742 | info     |           Metal | Fabric initialized on Device 0 (device.cpp:420)
2025-09-10 19:54:27.745 | info     |           Metal | Fabric initialized on Device 1 (device.cpp:420)
2025-09-10 19:54:27.745 | info     |           Metal | Fabric Initialized with config FabricConfig::FABRIC_1D (device_pool.cpp:402)
2025-09-10 19:54:29.168 | info     |           Metal | Command Queue initialized on Device 1 (device_pool.cpp:482)
2025-09-10 19:54:29.168 | info     |           Metal | Disabling and clearing program cache on MeshDevice 1 (mesh_device.cpp:635)
2025-09-10 19:54:29.169 (  36.044s) [        8F7FE640]loaded_executable_insta:127      1| [DEVICE] Successfully opened runtime device
2025-09-10 19:55:14.640 | warning  |              Op | get_forward_backward_line_unicast_configuration called: (ccl_common.cpp:1535)
2025-09-10 19:55:14.640 | warning  |              Op |   topology: 1 (ccl_common.cpp:1536)
2025-09-10 19:55:14.640 | warning  |              Op |   src_device: 0 (ccl_common.cpp:1537)
2025-09-10 19:55:14.640 | warning  |              Op |   forward_device: 1 (ccl_common.cpp:1538)
2025-09-10 19:55:14.640 | warning  |              Op |   backward_device: -1 (ccl_common.cpp:1539)
2025-09-10 19:55:14.640 | warning  |              Op |   fabric_config: 1 (ccl_common.cpp:1540)
2025-09-10 19:55:14.640 | warning  |              Op | Using 1D fabric config path (ccl_common.cpp:1560)
2025-09-10 19:55:14.640 | warning  |              Op |   forward_args: [0, 1] (ccl_common.cpp:1564)
2025-09-10 19:55:14.641 | warning  |              Op | get_forward_backward_line_unicast_configuration called: (ccl_common.cpp:1535)
2025-09-10 19:55:14.641 | warning  |              Op |   topology: 1 (ccl_common.cpp:1536)
2025-09-10 19:55:14.641 | warning  |              Op |   src_device: 1 (ccl_common.cpp:1537)
2025-09-10 19:55:14.641 | warning  |              Op |   forward_device: -1 (ccl_common.cpp:1538)
2025-09-10 19:55:14.641 | warning  |              Op |   backward_device: 0 (ccl_common.cpp:1539)
2025-09-10 19:55:14.641 | warning  |              Op |   fabric_config: 1 (ccl_common.cpp:1540)
2025-09-10 19:55:14.641 | warning  |              Op | Using 1D fabric config path (ccl_common.cpp:1560)
2025-09-10 19:55:14.641 | warning  |              Op |   backward_args: [0, 1] (ccl_common.cpp:1569)
2025-09-10 19:55:15.869 | warning  |              Op | get_forward_backward_line_unicast_configuration called: (ccl_common.cpp:1535)
2025-09-10 19:55:15.869 | warning  |              Op |   topology: 1 (ccl_common.cpp:1536)
2025-09-10 19:55:15.869 | warning  |              Op |   src_device: 0 (ccl_common.cpp:1537)
2025-09-10 19:55:15.869 | warning  |              Op |   forward_device: 1 (ccl_common.cpp:1538)
2025-09-10 19:55:15.869 | warning  |              Op |   backward_device: -1 (ccl_common.cpp:1539)
2025-09-10 19:55:15.869 | warning  |              Op |   fabric_config: 1 (ccl_common.cpp:1540)
2025-09-10 19:55:15.869 | warning  |              Op | Using 1D fabric config path (ccl_common.cpp:1560)
2025-09-10 19:55:15.869 | warning  |              Op |   forward_args: [0, 1] (ccl_common.cpp:1564)
2025-09-10 19:55:15.869 | warning  |              Op | get_forward_backward_line_unicast_configuration called: (ccl_common.cpp:1535)
2025-09-10 19:55:15.869 | warning  |              Op |   topology: 1 (ccl_common.cpp:1536)
2025-09-10 19:55:15.869 | warning  |              Op |   src_device: 1 (ccl_common.cpp:1537)
2025-09-10 19:55:15.869 | warning  |              Op |   forward_device: -1 (ccl_common.cpp:1538)
2025-09-10 19:55:15.869 | warning  |              Op |   backward_device: 0 (ccl_common.cpp:1539)
2025-09-10 19:55:15.869 | warning  |              Op |   fabric_config: 1 (ccl_common.cpp:1540)
2025-09-10 19:55:15.869 | warning  |              Op | Using 1D fabric config path (ccl_common.cpp:1560)
2025-09-10 19:55:15.869 | warning  |              Op |   backward_args: [0, 1] (ccl_common.cpp:1569)
2025-09-10 19:55:19.202 | warning  |              Op | get_forward_backward_line_unicast_configuration called: (ccl_common.cpp:1535)
2025-09-10 19:55:19.202 | warning  |              Op |   topology: 1 (ccl_common.cpp:1536)
2025-09-10 19:55:19.202 | warning  |              Op |   src_device: 0 (ccl_common.cpp:1537)
2025-09-10 19:55:19.202 | warning  |              Op |   forward_device: 1 (ccl_common.cpp:1538)
2025-09-10 19:55:19.202 | warning  |              Op |   backward_device: -1 (ccl_common.cpp:1539)
2025-09-10 19:55:19.202 | warning  |              Op |   fabric_config: 1 (ccl_common.cpp:1540)
2025-09-10 19:55:19.202 | warning  |              Op | Using 1D fabric config path (ccl_common.cpp:1560)
2025-09-10 19:55:19.202 | warning  |              Op |   forward_args: [0, 1] (ccl_common.cpp:1564)
2025-09-10 19:55:19.202 | warning  |              Op | get_forward_backward_line_unicast_configuration called: (ccl_common.cpp:1535)
2025-09-10 19:55:19.202 | warning  |              Op |   topology: 1 (ccl_common.cpp:1536)
2025-09-10 19:55:19.202 | warning  |              Op |   src_device: 1 (ccl_common.cpp:1537)
2025-09-10 19:55:19.202 | warning  |              Op |   forward_device: -1 (ccl_common.cpp:1538)
2025-09-10 19:55:19.202 | warning  |              Op |   backward_device: 0 (ccl_common.cpp:1539)
2025-09-10 19:55:19.202 | warning  |              Op |   fabric_config: 1 (ccl_common.cpp:1540)
2025-09-10 19:55:19.202 | warning  |              Op | Using 1D fabric config path (ccl_common.cpp:1560)
2025-09-10 19:55:19.202 | warning  |              Op |   backward_args: [0, 1] (ccl_common.cpp:1569)
2025-09-10 19:55:19.204 | warning  |              Op | get_forward_backward_line_unicast_configuration called: (ccl_common.cpp:1535)
2025-09-10 19:55:19.204 | warning  |              Op |   topology: 1 (ccl_common.cpp:1536)
2025-09-10 19:55:19.204 | warning  |              Op |   src_device: 0 (ccl_common.cpp:1537)
2025-09-10 19:55:19.204 | warning  |              Op |   forward_device: 1 (ccl_common.cpp:1538)
2025-09-10 19:55:19.204 | warning  |              Op |   backward_device: -1 (ccl_common.cpp:1539)
2025-09-10 19:55:19.204 | warning  |              Op |   fabric_config: 1 (ccl_common.cpp:1540)
2025-09-10 19:55:19.204 | warning  |              Op | Using 1D fabric config path (ccl_common.cpp:1560)
2025-09-10 19:55:19.204 | warning  |              Op |   forward_args: [0, 1] (ccl_common.cpp:1564)
2025-09-10 19:55:19.204 | warning  |              Op | get_forward_backward_line_unicast_configuration called: (ccl_common.cpp:1535)
2025-09-10 19:55:19.204 | warning  |              Op |   topology: 1 (ccl_common.cpp:1536)
2025-09-10 19:55:19.204 | warning  |              Op |   src_device: 1 (ccl_common.cpp:1537)
2025-09-10 19:55:19.204 | warning  |              Op |   forward_device: -1 (ccl_common.cpp:1538)
2025-09-10 19:55:19.204 | warning  |              Op |   backward_device: 0 (ccl_common.cpp:1539)
2025-09-10 19:55:19.204 | warning  |              Op |   fabric_config: 1 (ccl_common.cpp:1540)
2025-09-10 19:55:19.204 | warning  |              Op | Using 1D fabric config path (ccl_common.cpp:1560)
2025-09-10 19:55:19.204 | warning  |              Op |   backward_args: [0, 1] (ccl_common.cpp:1569)
2025-09-10 19:55:20.579 (  87.455s) [        8F7FE640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:55:20.579 (  87.455s) [        8F7FE640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:55:20.579 (  87.455s) [        8F7FE640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:55:20.579 (  87.455s) [        8F7FE640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:55:20.579 (  87.455s) [        8F7FE640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-10 19:55:20.579 (  87.455s) [        8F7FE640]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-10 19:55:20.579 (  87.455s) [        8F7FE640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-10 19:55:20.579 (  87.455s) [        8F7FE640]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-10 19:55:20.579 (  87.455s) [        8F7FE640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-10 19:55:20.579 (  87.455s) [        8F7FE640]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-10 19:55:20.579 (  87.455s) [        8F7FE640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-10 19:55:20.579 (  87.455s) [        8F7FE640]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-10 19:55:20.579 (  87.455s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:20.579 (  87.455s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:20.579 (  87.455s) [        F1708000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-10 19:55:20.579 (  87.455s) [        F1708000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-10 19:55:20.579 (  87.455s) [        F1708000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-10 19:55:20.580 (  87.455s) [        F1708000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-10 19:55:20.580 (  87.456s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:55:20.580 (  87.456s) [        3FFFF640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:55:20.583 (  87.459s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:20.583 (  87.459s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:20.583 (  87.459s) [        F1708000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-10 19:55:20.583 (  87.459s) [        F1708000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-10 19:55:20.583 (  87.459s) [        F1708000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-10 19:55:20.583 (  87.459s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:55:20.584 (  87.460s) [        667FC640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:55:20.586 (  87.462s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:20.586 (  87.462s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:20.586 (  87.462s) [        F1708000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-10 19:55:20.586 (  87.462s) [        F1708000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-10 19:55:20.586 (  87.462s) [        F1708000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-10 19:55:20.586 (  87.462s) [        F1708000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-10 19:55:20.587 (  87.462s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User is requesting to copy the data from a runtime tensor with data type: Int32 into buffer with expected data type: Int64, the values will be casted, this may impact the throughput and the integrity of the data.
2025-09-10 19:55:20.587 (  87.463s) [        667FC640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:55:21.688 (  88.564s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:55:21.688 (  88.564s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:55:21.688 (  88.564s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:55:21.688 (  88.564s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:55:21.688 (  88.564s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-09-10 19:55:21.689 (  88.565s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:55:21.689 (  88.565s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:55:21.689 (  88.565s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:55:21.689 (  88.565s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:55:21.689 (  88.565s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-09-10 19:55:21.689 (  88.565s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:55:21.689 (  88.565s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:55:21.689 (  88.565s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:55:21.689 (  88.565s) [        F1708000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:55:21.689 (  88.565s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:55:21.689 (  88.565s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:55:21.689 (  88.565s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-09-10 19:55:21.689 (  88.565s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:55:21.689 (  88.565s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:55:21.689 (  88.565s) [        F1708000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-10 19:55:21.689 (  88.565s) [        F1708000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-10 19:55:21.689 (  88.565s) [        F1708000]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-09-10 19:55:21.689 (  88.565s) [        F1708000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-10 19:55:21.689 (  88.565s) [        F1708000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.697 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.698 (  88.573s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.698 (  88.574s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.698 (  88.574s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.698 (  88.574s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.698 (  88.574s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.698 (  88.574s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.698 (  88.574s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.698 (  88.574s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.698 (  88.574s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.699 (  88.574s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-10 19:55:21.699 (  88.575s) [        F1708000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted

Compilation Analysis: ================================================================================
Compilation Analysis: Compilation Cause
Compilation Analysis:   most likely user code trying to access tensor value before torch_xla.sync
Compilation Analysis: Graph Info: 
Compilation Analysis:   Graph Hash: 5bb096aa11b17acc7688267ab7990745
Compilation Analysis:   Number of Graph Inputs: 21
Compilation Analysis:   Number of Graph Outputs: 1
Compilation Analysis: Python Frame Triggered Execution: 
Compilation Analysis:   __call__ (/localdev/jameszianxu/tt-xla/tt_torch/backend/backend.py:99)
Compilation Analysis:   _fn (/localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:838)
Compilation Analysis:   wrapper (/localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/utils/generic.py:953)
Compilation Analysis:   _call_impl (/localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762)
Compilation Analysis:   _fn (/localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:655)
Compilation Analysis:   _wrapped_call_impl (/localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1749)
Compilation Analysis:   llama (/localdev/jameszianxu/tt-xla/examples/pytorch/llama.py:155)
Compilation Analysis:   <module> (/localdev/jameszianxu/tt-xla/examples/pytorch/llama.py:207)
Compilation Analysis: --------------------------------------------------------------------------------
Compilation Analysis: ================================================================================
2025-09-10 19:55:21.727 (  88.603s) [        F1708000]     client_instance.cc:428      1| ClientInstance::PJRT_Client_Compile
2025-09-10 19:55:21.727 (  88.603s) [        F1708000]      module_builder.cc:103      1| ModuleBuilder::buildModule
2025-09-10 19:55:21.728 (  88.604s) [        F1708000]      module_builder.cc:165      1| VHLO Module:
module @SyncTensorsGraph.420 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, %arg1: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg2: !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>, %arg4: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg5: !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>, %arg6: !vhlo.tensor_v1<1x1x!vhlo.i64_v1>, %arg7: !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, %arg8: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg9: !vhlo.tensor_v1<1x!vhlo.i64_v1>, %arg10: !vhlo.tensor_v1<!vhlo.i64_v1>, %arg11: !vhlo.tensor_v1<1x8x32x128x!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg13: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg14: !vhlo.tensor_v1<64x!vhlo.f32_v1>, %arg15: !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>, %arg16: !vhlo.tensor_v1<1x8x32x128x!vhlo.bf16_v1>, %arg17: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg18: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg19: !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>, %arg20: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<1x1x128256x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF800000> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]]> : tensor<1x32xi64>>}> : () -> !vhlo.tensor_v1<1x32x!vhlo.i64_v1>
    %3 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : () -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %4 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<3.25520843E-4> : tensor<1x1xf32>>}> : () -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %5 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %7 = "vhlo.reshape_v1"(%arg20) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %8 = "vhlo.custom_call_v1"(%7) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_norm_weight">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %9 = "vhlo.reshape_v1"(%8) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1>
    %10 = "vhlo.convert_v1"(%9) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %12 = "vhlo.reshape_v1"(%arg7) : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>
    %13 = "vhlo.custom_call_v1"(%12) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_embed_tokens_weight">}>} : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>
    %15 = "vhlo.reshape_v1"(%arg6) : (!vhlo.tensor_v1<1x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.i64_v1>
    %16 = "vhlo.custom_call_v1"(%15) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_0">}>} : (!vhlo.tensor_v1<1x1x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.i64_v1>
    %17 = "vhlo.reshape_v1"(%16) : (!vhlo.tensor_v1<1x1x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %18 = "vhlo.convert_v1"(%17) : (!vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.ui32_v1>
    %19 = "vhlo.gather_v2"(%14, %18) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 3072]> : tensor<2xi64>>, start_index_map = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x!vhlo.ui32_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %20 = "vhlo.reshape_v1"(%19) : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %21 = "vhlo.reshape_v1"(%arg8) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %22 = "vhlo.custom_call_v1"(%21) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___input_layernorm_weight">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %23 = "vhlo.reshape_v1"(%22) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1>
    %24 = "vhlo.convert_v1"(%23) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %25 = "vhlo.reshape_v1"(%24) : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %26 = "vhlo.convert_v1"(%20) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %27 = "vhlo.power_v1"(%26, %6) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %28 = "vhlo.reduce_v1"(%27, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %228 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%228) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %29 = "vhlo.multiply_v1"(%28, %4) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %30 = "vhlo.reshape_v1"(%29) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %31 = "vhlo.reshape_v1"(%arg1) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %32 = "vhlo.add_v1"(%30, %31) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %33 = "vhlo.rsqrt_v2"(%32) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %34 = "vhlo.reshape_v1"(%33) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %35 = "vhlo.broadcast_in_dim_v1"(%34) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %36 = "vhlo.multiply_v1"(%26, %35) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %37 = "vhlo.convert_v1"(%36) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %38 = "vhlo.convert_v1"(%37) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %39 = "vhlo.multiply_v1"(%25, %38) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %40 = "vhlo.convert_v1"(%39) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %41 = "vhlo.reshape_v1"(%40) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %42 = "vhlo.reshape_v1"(%arg17) : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>
    %43 = "vhlo.custom_call_v1"(%42) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>
    %44 = "vhlo.reshape_v1"(%43) : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %45 = "vhlo.transpose_v1"(%44) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,3072]{0,1}">} : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %46 = "vhlo.dot_general_v2"(%41, %45) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %47 = "vhlo.reshape_v1"(%46) : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %48 = "vhlo.convert_v1"(%47) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,24,1,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %49 = "vhlo.reshape_v1"(%arg14) : (!vhlo.tensor_v1<64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>
    %50 = "vhlo.custom_call_v1"(%49) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_rotary_emb_inv_freq">}>} : (!vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>
    %51 = "vhlo.reshape_v1"(%50) : (!vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>
    %52 = "vhlo.reshape_v1"(%arg9) : (!vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.i64_v1>
    %53 = "vhlo.custom_call_v1"(%52) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_1">}>} : (!vhlo.tensor_v1<1x1x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.i64_v1>
    %54 = "vhlo.reshape_v1"(%53) : (!vhlo.tensor_v1<1x1x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %55 = "vhlo.convert_v1"(%53) : (!vhlo.tensor_v1<1x1x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %56 = "vhlo.dot_general_v2"(%51, %55) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>
    %57 = "vhlo.reshape_v1"(%56) : (!vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>
    %58 = "vhlo.concatenate_v1"(%57, %57) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>
    %59 = "vhlo.cosine_v2"(%58) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>
    %60 = "vhlo.convert_v1"(%59) : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>
    %61 = "vhlo.reshape_v1"(%60) : (!vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1x128x!vhlo.bf16_v1>
    %62 = "vhlo.convert_v1"(%61) : (!vhlo.tensor_v1<1x1x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1x128x!vhlo.f32_v1>
    %63 = "vhlo.reshape_v1"(%62) : (!vhlo.tensor_v1<1x1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>
    %64 = "vhlo.broadcast_in_dim_v1"(%63) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %65 = "vhlo.multiply_v1"(%48, %64) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %66 = "vhlo.convert_v1"(%65) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %67 = "vhlo.slice_v1"(%47) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 1, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>
    %68 = "vhlo.negate_v1"(%67) : (!vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>
    %69 = "vhlo.slice_v1"(%47) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 1, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>
    %70 = "vhlo.concatenate_v1"(%68, %69) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %71 = "vhlo.convert_v1"(%70) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %72 = "vhlo.sine_v2"(%58) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>
    %73 = "vhlo.convert_v1"(%72) : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>
    %74 = "vhlo.reshape_v1"(%73) : (!vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1x128x!vhlo.bf16_v1>
    %75 = "vhlo.convert_v1"(%74) : (!vhlo.tensor_v1<1x1x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1x128x!vhlo.f32_v1>
    %76 = "vhlo.reshape_v1"(%75) : (!vhlo.tensor_v1<1x1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>
    %77 = "vhlo.broadcast_in_dim_v1"(%76) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %78 = "vhlo.multiply_v1"(%71, %77) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %79 = "vhlo.convert_v1"(%78) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %80 = "vhlo.add_v1"(%66, %79) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %81 = "vhlo.reshape_v1"(%80) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>
    %82 = "vhlo.custom_call_v1"(%arg16) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_2">}>} : (!vhlo.tensor_v1<1x8x32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x32x128x!vhlo.bf16_v1>
    %83 = "vhlo.compare_v1"(%54, %3) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 LT>}> : (!vhlo.tensor_v1<1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.bool_v1>
    %84 = "vhlo.reshape_v1"(%arg10) : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %85 = "vhlo.add_v1"(%54, %84) : (!vhlo.tensor_v1<1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %86 = "vhlo.select_v1"(%83, %85, %54) : (!vhlo.tensor_v1<1x!vhlo.bool_v1>, !vhlo.tensor_v1<1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %87 = "vhlo.reshape_v1"(%86) : (!vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.i64_v1>
    %88 = "vhlo.reshape_v1"(%arg15) : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>
    %89 = "vhlo.custom_call_v1"(%88) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>
    %90 = "vhlo.reshape_v1"(%89) : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>
    %91 = "vhlo.transpose_v1"(%90) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,1024]{0,1}">} : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>
    %92 = "vhlo.dot_general_v2"(%41, %91) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>
    %93 = "vhlo.reshape_v1"(%92) : (!vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %94 = "vhlo.convert_v1"(%93) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,8,1,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %95 = "vhlo.broadcast_in_dim_v1"(%63) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %96 = "vhlo.multiply_v1"(%94, %95) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %97 = "vhlo.convert_v1"(%96) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %98 = "vhlo.slice_v1"(%93) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 1, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>
    %99 = "vhlo.negate_v1"(%98) : (!vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>
    %100 = "vhlo.slice_v1"(%93) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 1, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>
    %101 = "vhlo.concatenate_v1"(%99, %100) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %102 = "vhlo.convert_v1"(%101) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %103 = "vhlo.broadcast_in_dim_v1"(%76) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %104 = "vhlo.multiply_v1"(%102, %103) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %105 = "vhlo.convert_v1"(%104) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %106 = "vhlo.add_v1"(%97, %105) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %107 = "vhlo.scatter_v2"(%82, %87, %106) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg22: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg22) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x32x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x32x128x!vhlo.bf16_v1>
    %108 = "vhlo.broadcast_in_dim_v1"(%107) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x32x128x!vhlo.bf16_v1>
    %109 = "vhlo.reshape_v1"(%108) : (!vhlo.tensor_v1<1x8x3x32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x32x128x!vhlo.bf16_v1>
    %110 = "vhlo.transpose_v1"(%109) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 3, 1, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,24,128,32]{2,3,1,0}">} : (!vhlo.tensor_v1<1x24x32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x128x32x!vhlo.bf16_v1>
    %111 = "vhlo.reshape_v1"(%110) : (!vhlo.tensor_v1<1x24x128x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x128x32x!vhlo.bf16_v1>
    %112 = "vhlo.dot_general_v2"(%81, %111) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x128x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x1x32x!vhlo.bf16_v1>
    %113 = "vhlo.reshape_v1"(%112) : (!vhlo.tensor_v1<24x1x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x32x!vhlo.bf16_v1>
    %114 = "vhlo.convert_v1"(%113) : (!vhlo.tensor_v1<1x24x1x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x32x!vhlo.f32_v1>
    %115 = "vhlo.broadcast_in_dim_v1"(%arg13) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x32x!vhlo.f32_v1>
    %116 = "vhlo.multiply_v1"(%114, %115) : (!vhlo.tensor_v1<1x24x1x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x32x!vhlo.f32_v1>
    %117 = "vhlo.convert_v1"(%116) : (!vhlo.tensor_v1<1x24x1x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x32x!vhlo.bf16_v1>
    %118 = "vhlo.reshape_v1"(%arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x!vhlo.bf16_v1>
    %119 = "vhlo.broadcast_in_dim_v1"(%118) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x32x!vhlo.bf16_v1>
    %120 = "vhlo.convert_v1"(%119) : (!vhlo.tensor_v1<1x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x32x!vhlo.f32_v1>
    %121 = "vhlo.broadcast_in_dim_v1"(%54) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x32x!vhlo.i64_v1>
    %122 = "vhlo.compare_v1"(%2, %121) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<1x32x!vhlo.i64_v1>, !vhlo.tensor_v1<1x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x32x!vhlo.bool_v1>
    %123 = "vhlo.convert_v1"(%122) : (!vhlo.tensor_v1<1x32x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x32x!vhlo.f32_v1>
    %124 = "vhlo.multiply_v1"(%120, %123) : (!vhlo.tensor_v1<1x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x!vhlo.f32_v1>
    %125 = "vhlo.convert_v1"(%124) : (!vhlo.tensor_v1<1x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x!vhlo.bf16_v1>
    %126 = "vhlo.reshape_v1"(%125) : (!vhlo.tensor_v1<1x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x32x!vhlo.bf16_v1>
    %127 = "vhlo.broadcast_in_dim_v1"(%126) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x32x!vhlo.bf16_v1>
    %128 = "vhlo.add_v1"(%117, %127) : (!vhlo.tensor_v1<1x24x1x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x1x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x32x!vhlo.bf16_v1>
    %129 = "vhlo.convert_v1"(%128) : (!vhlo.tensor_v1<1x24x1x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x32x!vhlo.f32_v1>
    %130 = "vhlo.reduce_v1"(%129, %1) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %228 = "vhlo.maximum_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%228) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x1x32x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x!vhlo.f32_v1>
    %131 = "vhlo.broadcast_in_dim_v1"(%130) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x32x!vhlo.f32_v1>
    %132 = "vhlo.subtract_v1"(%129, %131) : (!vhlo.tensor_v1<1x24x1x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x32x!vhlo.f32_v1>
    %133 = "vhlo.exponential_v2"(%132) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x24x1x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x32x!vhlo.f32_v1>
    %134 = "vhlo.reduce_v1"(%133, %0) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %228 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%228) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x1x32x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x!vhlo.f32_v1>
    %135 = "vhlo.broadcast_in_dim_v1"(%134) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x32x!vhlo.f32_v1>
    %136 = "vhlo.divide_v1"(%133, %135) : (!vhlo.tensor_v1<1x24x1x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x32x!vhlo.f32_v1>
    %137 = "vhlo.convert_v1"(%136) : (!vhlo.tensor_v1<1x24x1x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x32x!vhlo.bf16_v1>
    %138 = "vhlo.reshape_v1"(%137) : (!vhlo.tensor_v1<1x24x1x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x1x32x!vhlo.bf16_v1>
    %139 = "vhlo.custom_call_v1"(%arg11) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_3">}>} : (!vhlo.tensor_v1<1x8x32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x32x128x!vhlo.bf16_v1>
    %140 = "vhlo.reshape_v1"(%arg5) : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>
    %141 = "vhlo.custom_call_v1"(%140) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>
    %142 = "vhlo.reshape_v1"(%141) : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>
    %143 = "vhlo.transpose_v1"(%142) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,1024]{0,1}">} : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>
    %144 = "vhlo.dot_general_v2"(%41, %143) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>
    %145 = "vhlo.reshape_v1"(%144) : (!vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %146 = "vhlo.scatter_v2"(%139, %87, %145) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg22: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg22) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x32x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x32x128x!vhlo.bf16_v1>
    %147 = "vhlo.broadcast_in_dim_v1"(%146) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x32x128x!vhlo.bf16_v1>
    %148 = "vhlo.reshape_v1"(%147) : (!vhlo.tensor_v1<1x8x3x32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x32x128x!vhlo.bf16_v1>
    %149 = "vhlo.dot_general_v2"(%138, %148) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x1x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>
    %150 = "vhlo.reshape_v1"(%149) : (!vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %151 = "vhlo.reshape_v1"(%arg4) : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>
    %152 = "vhlo.custom_call_v1"(%151) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_o_proj_weight">}>} : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>
    %153 = "vhlo.reshape_v1"(%152) : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %154 = "vhlo.transpose_v1"(%153) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,3072]{0,1}">} : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %155 = "vhlo.dot_general_v2"(%150, %154) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %156 = "vhlo.reshape_v1"(%155) : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %157 = "vhlo.add_v1"(%20, %156) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %158 = "vhlo.reshape_v1"(%arg18) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %159 = "vhlo.custom_call_v1"(%158) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___post_attention_layernorm_weight">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %160 = "vhlo.reshape_v1"(%159) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1>
    %161 = "vhlo.convert_v1"(%160) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %162 = "vhlo.reshape_v1"(%161) : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %163 = "vhlo.convert_v1"(%157) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %164 = "vhlo.power_v1"(%163, %6) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %165 = "vhlo.reduce_v1"(%164, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %228 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%228) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %166 = "vhlo.multiply_v1"(%165, %4) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %167 = "vhlo.reshape_v1"(%166) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %168 = "vhlo.add_v1"(%167, %31) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %169 = "vhlo.rsqrt_v2"(%168) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %170 = "vhlo.reshape_v1"(%169) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %171 = "vhlo.broadcast_in_dim_v1"(%170) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %172 = "vhlo.multiply_v1"(%163, %171) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %173 = "vhlo.convert_v1"(%172) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %174 = "vhlo.convert_v1"(%173) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %175 = "vhlo.multiply_v1"(%162, %174) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %176 = "vhlo.convert_v1"(%175) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %177 = "vhlo.reshape_v1"(%176) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %178 = "vhlo.reshape_v1"(%arg19) : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>
    %179 = "vhlo.custom_call_v1"(%178) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_gate_proj_weight">}>} : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>
    %180 = "vhlo.reshape_v1"(%179) : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>
    %181 = "vhlo.transpose_v1"(%180) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,8192]{0,1}">} : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>
    %182 = "vhlo.dot_general_v2"(%177, %181) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>
    %183 = "vhlo.reshape_v1"(%182) : (!vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %184 = "vhlo.convert_v1"(%183) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %185 = "vhlo.logistic_v2"(%183) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %186 = "vhlo.convert_v1"(%185) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %187 = "vhlo.multiply_v1"(%184, %186) : (!vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %188 = "vhlo.convert_v1"(%187) : (!vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %189 = "vhlo.convert_v1"(%188) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %190 = "vhlo.reshape_v1"(%arg3) : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>
    %191 = "vhlo.custom_call_v1"(%190) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_up_proj_weight">}>} : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>
    %192 = "vhlo.reshape_v1"(%191) : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>
    %193 = "vhlo.transpose_v1"(%192) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,8192]{0,1}">} : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>
    %194 = "vhlo.dot_general_v2"(%177, %193) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>
    %195 = "vhlo.reshape_v1"(%194) : (!vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %196 = "vhlo.convert_v1"(%195) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %197 = "vhlo.multiply_v1"(%189, %196) : (!vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %198 = "vhlo.convert_v1"(%197) : (!vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %199 = "vhlo.reshape_v1"(%198) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>
    %200 = "vhlo.reshape_v1"(%arg2) : (!vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1>
    %201 = "vhlo.custom_call_v1"(%200) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_down_proj_weight">}>} : (!vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1>
    %202 = "vhlo.reshape_v1"(%201) : (!vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>
    %203 = "vhlo.transpose_v1"(%202) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,3072]{0,1}">} : (!vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>
    %204 = "vhlo.dot_general_v2"(%199, %203) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %205 = "vhlo.reshape_v1"(%204) : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %206 = "vhlo.add_v1"(%157, %205) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %207 = "vhlo.convert_v1"(%206) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %208 = "vhlo.power_v1"(%207, %6) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %209 = "vhlo.reduce_v1"(%208, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %228 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%228) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %210 = "vhlo.multiply_v1"(%209, %4) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %211 = "vhlo.reshape_v1"(%210) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %212 = "vhlo.add_v1"(%211, %31) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %213 = "vhlo.rsqrt_v2"(%212) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %214 = "vhlo.reshape_v1"(%213) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %215 = "vhlo.broadcast_in_dim_v1"(%214) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %216 = "vhlo.multiply_v1"(%207, %215) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %217 = "vhlo.convert_v1"(%216) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %218 = "vhlo.convert_v1"(%217) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %219 = "vhlo.multiply_v1"(%11, %218) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %220 = "vhlo.convert_v1"(%219) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %221 = "vhlo.reshape_v1"(%220) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %222 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>
    %223 = "vhlo.custom_call_v1"(%222) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___lm_head_weight">}>} : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>
    %224 = "vhlo.reshape_v1"(%223) : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>
    %225 = "vhlo.transpose_v1"(%224) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,128256]{0,1}">} : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x128256x!vhlo.bf16_v1>
    %226 = "vhlo.dot_general_v2"(%221, %225) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x!vhlo.bf16_v1>
    %227 = "vhlo.reshape_v1"(%226) : (!vhlo.tensor_v1<1x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x128256x!vhlo.bf16_v1>
    "vhlo.return_v1"(%227) : (!vhlo.tensor_v1<1x1x128256x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}
2025-09-10 19:55:21.738 (  88.613s) [        F1708000]      module_builder.cc:185      1| Is using shardy? true
2025-09-10 19:55:21.742 (  88.618s) [        F1708000]      module_builder.cc:203      1| SHLO Module:
module @SyncTensorsGraph.420 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg1: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg2: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>}, %arg3: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg4: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>}, %arg5: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg6: tensor<1x1xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg7: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg8: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg9: tensor<1xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg10: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg11: tensor<1x8x32x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}, %arg12: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg13: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg14: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg15: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg16: tensor<1x8x32x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}) -> tensor<1x1x128256xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c = stablehlo.constant dense<[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]]> : tensor<1x32xi64>
    %c_1 = stablehlo.constant dense<0> : tensor<1xi64>
    %cst_2 = stablehlo.constant dense<3.25520843E-4> : tensor<1x1xf32>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %0 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x1x3072xf32>
    %1 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %2 = stablehlo.custom_call @tt.mark_argument(%1) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_norm_weight"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %3 = stablehlo.convert %2 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %4 = stablehlo.reshape %arg7 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %5 = stablehlo.custom_call @tt.mark_argument(%4) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_embed_tokens_weight"}} : (tensor<1x128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %6 = stablehlo.reshape %5 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %7 = stablehlo.reshape %arg6 : (tensor<1x1xi64>) -> tensor<1x1x1xi64>
    %8 = stablehlo.custom_call @tt.mark_argument(%7) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_0"}} : (tensor<1x1x1xi64>) -> tensor<1x1x1xi64>
    %9 = stablehlo.convert %8 : (tensor<1x1x1xi64>) -> tensor<1x1x1xui32>
    %10 = stablehlo.reshape %9 : (tensor<1x1x1xui32>) -> tensor<1xui32>
    %11 = "stablehlo.gather"(%6, %10) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<1xui32>) -> tensor<1x3072xbf16>
    %12 = stablehlo.reshape %11 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %13 = stablehlo.reshape %arg8 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %14 = stablehlo.custom_call @tt.mark_argument(%13) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %15 = stablehlo.convert %14 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %16 = stablehlo.convert %12 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %17 = stablehlo.power %16, %0 : tensor<1x1x3072xf32>
    %18 = stablehlo.reduce(%17 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %19 = stablehlo.multiply %18, %cst_2 : tensor<1x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %21 = stablehlo.reshape %arg1 : (tensor<f32>) -> tensor<1x1x1xf32>
    %22 = stablehlo.add %20, %21 : tensor<1x1x1xf32>
    %23 = stablehlo.rsqrt %22 : tensor<1x1x1xf32>
    %24 = stablehlo.reshape %23 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %25 = stablehlo.broadcast_in_dim %24, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %26 = stablehlo.multiply %16, %25 : tensor<1x1x3072xf32>
    %27 = stablehlo.convert %26 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %28 = stablehlo.convert %27 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %29 = stablehlo.multiply %15, %28 : tensor<1x1x3072xf32>
    %30 = stablehlo.convert %29 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %32 = stablehlo.reshape %arg17 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %33 = stablehlo.custom_call @tt.mark_argument(%32) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}} : (tensor<1x3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %34 = stablehlo.reshape %33 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %35 = stablehlo.transpose %34, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %36 = stablehlo.dot_general %31, %35, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %37 = stablehlo.reshape %36 : (tensor<1x3072xbf16>) -> tensor<1x24x1x128xbf16>
    %38 = stablehlo.convert %37 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,1,128]{3,1,2,0}"} : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %39 = stablehlo.reshape %arg14 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %40 = stablehlo.custom_call @tt.mark_argument(%39) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "l__self___model_rotary_emb_inv_freq"}} : (tensor<1x1x64xf32>) -> tensor<1x1x64xf32>
    %41 = stablehlo.reshape %40 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %42 = stablehlo.reshape %arg9 : (tensor<1xi64>) -> tensor<1x1x1xi64>
    %43 = stablehlo.custom_call @tt.mark_argument(%42) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_1"}} : (tensor<1x1x1xi64>) -> tensor<1x1x1xi64>
    %44 = stablehlo.reshape %43 : (tensor<1x1x1xi64>) -> tensor<1xi64>
    %45 = stablehlo.convert %43 : (tensor<1x1x1xi64>) -> tensor<1x1x1xf32>
    %46 = stablehlo.dot_general %41, %45, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x1xf32>) -> tensor<1x64x1xf32>
    %47 = stablehlo.reshape %46 : (tensor<1x64x1xf32>) -> tensor<1x1x64xf32>
    %48 = stablehlo.concatenate %47, %47, dim = 2 : (tensor<1x1x64xf32>, tensor<1x1x64xf32>) -> tensor<1x1x128xf32>
    %49 = stablehlo.cosine %48 : tensor<1x1x128xf32>
    %50 = stablehlo.convert %49 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
    %51 = stablehlo.convert %50 : (tensor<1x1x128xbf16>) -> tensor<1x1x128xf32>
    %52 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x24x1x128xf32>
    %53 = stablehlo.multiply %38, %52 : tensor<1x24x1x128xf32>
    %54 = stablehlo.convert %53 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %55 = stablehlo.slice %37 [0:1, 0:24, 0:1, 64:128] : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x64xbf16>
    %56 = stablehlo.negate %55 : tensor<1x24x1x64xbf16>
    %57 = stablehlo.slice %37 [0:1, 0:24, 0:1, 0:64] : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x64xbf16>
    %58 = stablehlo.concatenate %56, %57, dim = 3 : (tensor<1x24x1x64xbf16>, tensor<1x24x1x64xbf16>) -> tensor<1x24x1x128xbf16>
    %59 = stablehlo.convert %58 : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %60 = stablehlo.sine %48 : tensor<1x1x128xf32>
    %61 = stablehlo.convert %60 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
    %62 = stablehlo.convert %61 : (tensor<1x1x128xbf16>) -> tensor<1x1x128xf32>
    %63 = stablehlo.broadcast_in_dim %62, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x24x1x128xf32>
    %64 = stablehlo.multiply %59, %63 : tensor<1x24x1x128xf32>
    %65 = stablehlo.convert %64 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %66 = stablehlo.add %54, %65 : tensor<1x24x1x128xbf16>
    %67 = stablehlo.reshape %66 : (tensor<1x24x1x128xbf16>) -> tensor<24x1x128xbf16>
    %68 = stablehlo.custom_call @tt.mark_argument(%arg16) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_2"}} : (tensor<1x8x32x128xbf16>) -> tensor<1x8x32x128xbf16>
    %69 = stablehlo.compare  LT, %44, %c_1 : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
    %70 = stablehlo.reshape %arg10 : (tensor<i64>) -> tensor<1xi64>
    %71 = stablehlo.add %44, %70 : tensor<1xi64>
    %72 = stablehlo.select %69, %71, %44 : tensor<1xi1>, tensor<1xi64>
    %73 = stablehlo.reshape %72 : (tensor<1xi64>) -> tensor<1x1xi64>
    %74 = stablehlo.reshape %arg15 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.custom_call @tt.mark_argument(%74) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}} : (tensor<1x1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %76 = stablehlo.reshape %75 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %77 = stablehlo.transpose %76, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %78 = stablehlo.dot_general %31, %77, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %79 = stablehlo.reshape %78 : (tensor<1x1024xbf16>) -> tensor<1x8x1x128xbf16>
    %80 = stablehlo.convert %79 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,1,128]{3,1,2,0}"} : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xf32>
    %81 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x8x1x128xf32>
    %82 = stablehlo.multiply %80, %81 : tensor<1x8x1x128xf32>
    %83 = stablehlo.convert %82 : (tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xbf16>
    %84 = stablehlo.slice %79 [0:1, 0:8, 0:1, 64:128] : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x64xbf16>
    %85 = stablehlo.negate %84 : tensor<1x8x1x64xbf16>
    %86 = stablehlo.slice %79 [0:1, 0:8, 0:1, 0:64] : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x64xbf16>
    %87 = stablehlo.concatenate %85, %86, dim = 3 : (tensor<1x8x1x64xbf16>, tensor<1x8x1x64xbf16>) -> tensor<1x8x1x128xbf16>
    %88 = stablehlo.convert %87 : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xf32>
    %89 = stablehlo.broadcast_in_dim %62, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x8x1x128xf32>
    %90 = stablehlo.multiply %88, %89 : tensor<1x8x1x128xf32>
    %91 = stablehlo.convert %90 : (tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xbf16>
    %92 = stablehlo.add %83, %91 : tensor<1x8x1x128xbf16>
    %93 = "stablehlo.scatter"(%68, %73, %92) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x32x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>) -> tensor<1x8x32x128xbf16>
    %94 = stablehlo.broadcast_in_dim %93, dims = [0, 1, 3, 4] : (tensor<1x8x32x128xbf16>) -> tensor<1x8x3x32x128xbf16>
    %95 = stablehlo.reshape %94 : (tensor<1x8x3x32x128xbf16>) -> tensor<1x24x32x128xbf16>
    %96 = stablehlo.transpose %95, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,32]{2,3,1,0}"} : (tensor<1x24x32x128xbf16>) -> tensor<1x24x128x32xbf16>
    %97 = stablehlo.reshape %96 : (tensor<1x24x128x32xbf16>) -> tensor<24x128x32xbf16>
    %98 = stablehlo.dot_general %67, %97, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x1x128xbf16>, tensor<24x128x32xbf16>) -> tensor<24x1x32xbf16>
    %99 = stablehlo.convert %98 : (tensor<24x1x32xbf16>) -> tensor<24x1x32xf32>
    %100 = stablehlo.reshape %99 : (tensor<24x1x32xf32>) -> tensor<1x24x1x32xf32>
    %101 = stablehlo.broadcast_in_dim %arg13, dims = [] : (tensor<f32>) -> tensor<1x24x1x32xf32>
    %102 = stablehlo.multiply %100, %101 : tensor<1x24x1x32xf32>
    %103 = stablehlo.convert %102 : (tensor<1x24x1x32xf32>) -> tensor<1x24x1x32xbf16>
    %104 = stablehlo.reshape %arg12 : (tensor<bf16>) -> tensor<1xbf16>
    %105 = stablehlo.broadcast_in_dim %104, dims = [0] : (tensor<1xbf16>) -> tensor<1x32xbf16>
    %106 = stablehlo.convert %105 : (tensor<1x32xbf16>) -> tensor<1x32xf32>
    %107 = stablehlo.broadcast_in_dim %44, dims = [0] : (tensor<1xi64>) -> tensor<1x32xi64>
    %108 = stablehlo.compare  GT, %c, %107 : (tensor<1x32xi64>, tensor<1x32xi64>) -> tensor<1x32xi1>
    %109 = stablehlo.convert %108 : (tensor<1x32xi1>) -> tensor<1x32xf32>
    %110 = stablehlo.multiply %106, %109 : tensor<1x32xf32>
    %111 = stablehlo.convert %110 : (tensor<1x32xf32>) -> tensor<1x32xbf16>
    %112 = stablehlo.reshape %111 : (tensor<1x32xbf16>) -> tensor<1x1x32xbf16>
    %113 = stablehlo.broadcast_in_dim %112, dims = [0, 2, 3] : (tensor<1x1x32xbf16>) -> tensor<1x24x1x32xbf16>
    %114 = stablehlo.add %103, %113 : tensor<1x24x1x32xbf16>
    %115 = stablehlo.convert %114 : (tensor<1x24x1x32xbf16>) -> tensor<1x24x1x32xf32>
    %116 = stablehlo.reduce(%115 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x1x32xf32>, tensor<f32>) -> tensor<1x24x1xf32>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1, 2] : (tensor<1x24x1xf32>) -> tensor<1x24x1x32xf32>
    %118 = stablehlo.subtract %115, %117 : tensor<1x24x1x32xf32>
    %119 = stablehlo.exponential %118 : tensor<1x24x1x32xf32>
    %120 = stablehlo.reduce(%119 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x1x32xf32>, tensor<f32>) -> tensor<1x24x1xf32>
    %121 = stablehlo.broadcast_in_dim %120, dims = [0, 1, 2] : (tensor<1x24x1xf32>) -> tensor<1x24x1x32xf32>
    %122 = stablehlo.divide %119, %121 : tensor<1x24x1x32xf32>
    %123 = stablehlo.convert %122 : (tensor<1x24x1x32xf32>) -> tensor<1x24x1x32xbf16>
    %124 = stablehlo.reshape %123 : (tensor<1x24x1x32xbf16>) -> tensor<24x1x32xbf16>
    %125 = stablehlo.custom_call @tt.mark_argument(%arg11) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_3"}} : (tensor<1x8x32x128xbf16>) -> tensor<1x8x32x128xbf16>
    %126 = stablehlo.reshape %arg5 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %127 = stablehlo.custom_call @tt.mark_argument(%126) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}} : (tensor<1x1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %128 = stablehlo.reshape %127 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %129 = stablehlo.transpose %128, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %130 = stablehlo.dot_general %31, %129, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %131 = stablehlo.reshape %130 : (tensor<1x1024xbf16>) -> tensor<1x8x1x128xbf16>
    %132 = "stablehlo.scatter"(%125, %73, %131) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x32x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>) -> tensor<1x8x32x128xbf16>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 3, 4] : (tensor<1x8x32x128xbf16>) -> tensor<1x8x3x32x128xbf16>
    %134 = stablehlo.reshape %133 : (tensor<1x8x3x32x128xbf16>) -> tensor<24x32x128xbf16>
    %135 = stablehlo.dot_general %124, %134, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x1x32xbf16>, tensor<24x32x128xbf16>) -> tensor<24x1x128xbf16>
    %136 = stablehlo.reshape %135 : (tensor<24x1x128xbf16>) -> tensor<1x3072xbf16>
    %137 = stablehlo.reshape %arg4 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %138 = stablehlo.custom_call @tt.mark_argument(%137) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}} : (tensor<1x3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %139 = stablehlo.reshape %138 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %140 = stablehlo.transpose %139, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %141 = stablehlo.dot_general %136, %140, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %142 = stablehlo.reshape %141 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %143 = stablehlo.add %12, %142 : tensor<1x1x3072xbf16>
    %144 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %145 = stablehlo.custom_call @tt.mark_argument(%144) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %146 = stablehlo.convert %145 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %147 = stablehlo.convert %143 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %148 = stablehlo.power %147, %0 : tensor<1x1x3072xf32>
    %149 = stablehlo.reduce(%148 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %150 = stablehlo.multiply %149, %cst_2 : tensor<1x1xf32>
    %151 = stablehlo.reshape %150 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %152 = stablehlo.add %151, %21 : tensor<1x1x1xf32>
    %153 = stablehlo.rsqrt %152 : tensor<1x1x1xf32>
    %154 = stablehlo.reshape %153 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %155 = stablehlo.broadcast_in_dim %154, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %156 = stablehlo.multiply %147, %155 : tensor<1x1x3072xf32>
    %157 = stablehlo.convert %156 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %158 = stablehlo.convert %157 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %159 = stablehlo.multiply %146, %158 : tensor<1x1x3072xf32>
    %160 = stablehlo.convert %159 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %161 = stablehlo.reshape %160 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %162 = stablehlo.reshape %arg19 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %163 = stablehlo.custom_call @tt.mark_argument(%162) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}} : (tensor<1x8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %164 = stablehlo.reshape %163 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %165 = stablehlo.transpose %164, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %166 = stablehlo.dot_general %161, %165, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %167 = stablehlo.reshape %166 : (tensor<1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %168 = stablehlo.convert %167 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %169 = stablehlo.logistic %167 : tensor<1x1x8192xbf16>
    %170 = stablehlo.convert %169 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %171 = stablehlo.multiply %168, %170 : tensor<1x1x8192xf32>
    %172 = stablehlo.convert %171 : (tensor<1x1x8192xf32>) -> tensor<1x1x8192xbf16>
    %173 = stablehlo.convert %172 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %174 = stablehlo.reshape %arg3 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %175 = stablehlo.custom_call @tt.mark_argument(%174) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}} : (tensor<1x8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %176 = stablehlo.reshape %175 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %177 = stablehlo.transpose %176, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %178 = stablehlo.dot_general %161, %177, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %179 = stablehlo.convert %178 : (tensor<1x8192xbf16>) -> tensor<1x8192xf32>
    %180 = stablehlo.reshape %179 : (tensor<1x8192xf32>) -> tensor<1x1x8192xf32>
    %181 = stablehlo.multiply %173, %180 : tensor<1x1x8192xf32>
    %182 = stablehlo.convert %181 : (tensor<1x1x8192xf32>) -> tensor<1x1x8192xbf16>
    %183 = stablehlo.reshape %182 : (tensor<1x1x8192xbf16>) -> tensor<1x8192xbf16>
    %184 = stablehlo.reshape %arg2 : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %185 = stablehlo.custom_call @tt.mark_argument(%184) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}} : (tensor<1x3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %186 = stablehlo.reshape %185 : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %187 = stablehlo.transpose %186, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %188 = stablehlo.dot_general %183, %187, contracting_dims = [1] x [0] : (tensor<1x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<1x3072xbf16>
    %189 = stablehlo.reshape %188 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %190 = stablehlo.add %143, %189 : tensor<1x1x3072xbf16>
    %191 = stablehlo.convert %190 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %192 = stablehlo.power %191, %0 : tensor<1x1x3072xf32>
    %193 = stablehlo.reduce(%192 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %194 = stablehlo.multiply %193, %cst_2 : tensor<1x1xf32>
    %195 = stablehlo.reshape %194 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %196 = stablehlo.add %195, %21 : tensor<1x1x1xf32>
    %197 = stablehlo.rsqrt %196 : tensor<1x1x1xf32>
    %198 = stablehlo.reshape %197 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %199 = stablehlo.broadcast_in_dim %198, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %200 = stablehlo.multiply %191, %199 : tensor<1x1x3072xf32>
    %201 = stablehlo.convert %200 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %202 = stablehlo.convert %201 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %203 = stablehlo.multiply %3, %202 : tensor<1x1x3072xf32>
    %204 = stablehlo.convert %203 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %205 = stablehlo.reshape %204 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %206 = stablehlo.reshape %arg0 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %207 = stablehlo.custom_call @tt.mark_argument(%206) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___lm_head_weight"}} : (tensor<1x128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %208 = stablehlo.reshape %207 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %209 = stablehlo.transpose %208, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %210 = stablehlo.dot_general %205, %209, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<1x128256xbf16>
    %211 = stablehlo.reshape %210 : (tensor<1x128256xbf16>) -> tensor<1x1x128256xbf16>
    return %211 : tensor<1x1x128256xbf16>
  }
}
2025-09-10 19:55:21.748 (  88.624s) [        F1708000]      module_builder.cc:212      1| SHLO Module after frontend StableHLO pipeline:
module @SyncTensorsGraph.420 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___lm_head_weight"}, %arg1: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_0"}, %arg2: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg3: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg4: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg5: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg6: tensor<1x1xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"}, %arg7: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg8: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg9: tensor<1xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_1"}, %arg10: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_1"}, %arg11: tensor<1x8x32x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_3"}, %arg12: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_2"}, %arg13: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_3"}, %arg14: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg15: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg16: tensor<1x8x32x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_2"}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_norm_weight"}) -> tensor<1x1x128256xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c = stablehlo.constant dense<[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]]> : tensor<1x32xi64>
    %c_1 = stablehlo.constant dense<0> : tensor<1xi64>
    %cst_2 = stablehlo.constant dense<3.25520843E-4> : tensor<1x1xf32>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %0 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x1x3072xf32>
    %1 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %2 = stablehlo.convert %1 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %3 = stablehlo.reshape %arg7 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %4 = stablehlo.reshape %3 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %5 = stablehlo.reshape %arg6 : (tensor<1x1xi64>) -> tensor<1x1x1xi64>
    %6 = stablehlo.convert %5 : (tensor<1x1x1xi64>) -> tensor<1x1x1xui32>
    %7 = stablehlo.reshape %6 : (tensor<1x1x1xui32>) -> tensor<1xui32>
    %8 = "stablehlo.gather"(%4, %7) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<1xui32>) -> tensor<1x3072xbf16>
    %9 = stablehlo.reshape %8 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %10 = stablehlo.reshape %arg8 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.convert %10 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %12 = stablehlo.convert %9 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %13 = stablehlo.power %12, %0 : tensor<1x1x3072xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %15 = stablehlo.multiply %14, %cst_2 : tensor<1x1xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %17 = stablehlo.reshape %arg1 : (tensor<f32>) -> tensor<1x1x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x1x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x1x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x1x3072xf32>
    %23 = stablehlo.convert %22 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %24 = stablehlo.convert %23 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %25 = stablehlo.multiply %11, %24 : tensor<1x1x3072xf32>
    %26 = stablehlo.convert %25 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %27 = stablehlo.reshape %26 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %28 = stablehlo.reshape %arg17 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %29 = stablehlo.reshape %28 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %30 = stablehlo.transpose %29, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %31 = stablehlo.dot_general %27, %30, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %32 = stablehlo.reshape %31 : (tensor<1x3072xbf16>) -> tensor<1x24x1x128xbf16>
    %33 = stablehlo.convert %32 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,1,128]{3,1,2,0}"} : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %34 = stablehlo.reshape %arg14 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %35 = stablehlo.reshape %34 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %36 = stablehlo.reshape %arg9 : (tensor<1xi64>) -> tensor<1x1x1xi64>
    %37 = stablehlo.reshape %36 : (tensor<1x1x1xi64>) -> tensor<1xi64>
    %38 = stablehlo.convert %36 : (tensor<1x1x1xi64>) -> tensor<1x1x1xf32>
    %39 = stablehlo.dot_general %35, %38, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x1xf32>) -> tensor<1x64x1xf32>
    %40 = stablehlo.reshape %39 : (tensor<1x64x1xf32>) -> tensor<1x1x64xf32>
    %41 = stablehlo.concatenate %40, %40, dim = 2 : (tensor<1x1x64xf32>, tensor<1x1x64xf32>) -> tensor<1x1x128xf32>
    %42 = stablehlo.cosine %41 : tensor<1x1x128xf32>
    %43 = stablehlo.convert %42 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
    %44 = stablehlo.convert %43 : (tensor<1x1x128xbf16>) -> tensor<1x1x128xf32>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x24x1x128xf32>
    %46 = stablehlo.multiply %33, %45 : tensor<1x24x1x128xf32>
    %47 = stablehlo.convert %46 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %48 = stablehlo.slice %32 [0:1, 0:24, 0:1, 64:128] : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x64xbf16>
    %49 = stablehlo.negate %48 : tensor<1x24x1x64xbf16>
    %50 = stablehlo.slice %32 [0:1, 0:24, 0:1, 0:64] : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x64xbf16>
    %51 = stablehlo.concatenate %49, %50, dim = 3 : (tensor<1x24x1x64xbf16>, tensor<1x24x1x64xbf16>) -> tensor<1x24x1x128xbf16>
    %52 = stablehlo.convert %51 : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %53 = stablehlo.sine %41 : tensor<1x1x128xf32>
    %54 = stablehlo.convert %53 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
    %55 = stablehlo.convert %54 : (tensor<1x1x128xbf16>) -> tensor<1x1x128xf32>
    %56 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x24x1x128xf32>
    %57 = stablehlo.multiply %52, %56 : tensor<1x24x1x128xf32>
    %58 = stablehlo.convert %57 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %59 = stablehlo.add %47, %58 : tensor<1x24x1x128xbf16>
    %60 = stablehlo.reshape %59 : (tensor<1x24x1x128xbf16>) -> tensor<24x1x128xbf16>
    %61 = stablehlo.compare  LT, %37, %c_1 : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
    %62 = stablehlo.reshape %arg10 : (tensor<i64>) -> tensor<1xi64>
    %63 = stablehlo.add %37, %62 : tensor<1xi64>
    %64 = stablehlo.select %61, %63, %37 : tensor<1xi1>, tensor<1xi64>
    %65 = stablehlo.reshape %64 : (tensor<1xi64>) -> tensor<1x1xi64>
    %66 = stablehlo.reshape %arg15 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %67 = stablehlo.reshape %66 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %68 = stablehlo.transpose %67, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %69 = stablehlo.dot_general %27, %68, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %70 = stablehlo.reshape %69 : (tensor<1x1024xbf16>) -> tensor<1x8x1x128xbf16>
    %71 = stablehlo.convert %70 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,1,128]{3,1,2,0}"} : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xf32>
    %72 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x8x1x128xf32>
    %73 = stablehlo.multiply %71, %72 : tensor<1x8x1x128xf32>
    %74 = stablehlo.convert %73 : (tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xbf16>
    %75 = stablehlo.slice %70 [0:1, 0:8, 0:1, 64:128] : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x64xbf16>
    %76 = stablehlo.negate %75 : tensor<1x8x1x64xbf16>
    %77 = stablehlo.slice %70 [0:1, 0:8, 0:1, 0:64] : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x64xbf16>
    %78 = stablehlo.concatenate %76, %77, dim = 3 : (tensor<1x8x1x64xbf16>, tensor<1x8x1x64xbf16>) -> tensor<1x8x1x128xbf16>
    %79 = stablehlo.convert %78 : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xf32>
    %80 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x8x1x128xf32>
    %81 = stablehlo.multiply %79, %80 : tensor<1x8x1x128xf32>
    %82 = stablehlo.convert %81 : (tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xbf16>
    %83 = stablehlo.add %74, %82 : tensor<1x8x1x128xbf16>
    %84 = "stablehlo.scatter"(%arg16, %65, %83) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x32x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>) -> tensor<1x8x32x128xbf16>
    %85 = stablehlo.broadcast_in_dim %84, dims = [0, 1, 3, 4] : (tensor<1x8x32x128xbf16>) -> tensor<1x8x3x32x128xbf16>
    %86 = stablehlo.reshape %85 : (tensor<1x8x3x32x128xbf16>) -> tensor<1x24x32x128xbf16>
    %87 = stablehlo.transpose %86, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,32]{2,3,1,0}"} : (tensor<1x24x32x128xbf16>) -> tensor<1x24x128x32xbf16>
    %88 = stablehlo.reshape %87 : (tensor<1x24x128x32xbf16>) -> tensor<24x128x32xbf16>
    %89 = stablehlo.dot_general %60, %88, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x1x128xbf16>, tensor<24x128x32xbf16>) -> tensor<24x1x32xbf16>
    %90 = stablehlo.convert %89 : (tensor<24x1x32xbf16>) -> tensor<24x1x32xf32>
    %91 = stablehlo.reshape %90 : (tensor<24x1x32xf32>) -> tensor<1x24x1x32xf32>
    %92 = stablehlo.broadcast_in_dim %arg13, dims = [] : (tensor<f32>) -> tensor<1x24x1x32xf32>
    %93 = stablehlo.multiply %91, %92 : tensor<1x24x1x32xf32>
    %94 = stablehlo.convert %93 : (tensor<1x24x1x32xf32>) -> tensor<1x24x1x32xbf16>
    %95 = stablehlo.reshape %arg12 : (tensor<bf16>) -> tensor<1xbf16>
    %96 = stablehlo.broadcast_in_dim %95, dims = [0] : (tensor<1xbf16>) -> tensor<1x32xbf16>
    %97 = stablehlo.convert %96 : (tensor<1x32xbf16>) -> tensor<1x32xf32>
    %98 = stablehlo.broadcast_in_dim %37, dims = [0] : (tensor<1xi64>) -> tensor<1x32xi64>
    %99 = stablehlo.compare  GT, %c, %98 : (tensor<1x32xi64>, tensor<1x32xi64>) -> tensor<1x32xi1>
    %100 = stablehlo.convert %99 : (tensor<1x32xi1>) -> tensor<1x32xf32>
    %101 = stablehlo.multiply %97, %100 : tensor<1x32xf32>
    %102 = stablehlo.convert %101 : (tensor<1x32xf32>) -> tensor<1x32xbf16>
    %103 = stablehlo.reshape %102 : (tensor<1x32xbf16>) -> tensor<1x1x32xbf16>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0, 2, 3] : (tensor<1x1x32xbf16>) -> tensor<1x24x1x32xbf16>
    %105 = stablehlo.add %94, %104 : tensor<1x24x1x32xbf16>
    %106 = stablehlo.convert %105 : (tensor<1x24x1x32xbf16>) -> tensor<1x24x1x32xf32>
    %107 = stablehlo.reduce(%106 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x1x32xf32>, tensor<f32>) -> tensor<1x24x1xf32>
    %108 = stablehlo.broadcast_in_dim %107, dims = [0, 1, 2] : (tensor<1x24x1xf32>) -> tensor<1x24x1x32xf32>
    %109 = stablehlo.subtract %106, %108 : tensor<1x24x1x32xf32>
    %110 = stablehlo.exponential %109 : tensor<1x24x1x32xf32>
    %111 = stablehlo.reduce(%110 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x1x32xf32>, tensor<f32>) -> tensor<1x24x1xf32>
    %112 = stablehlo.broadcast_in_dim %111, dims = [0, 1, 2] : (tensor<1x24x1xf32>) -> tensor<1x24x1x32xf32>
    %113 = stablehlo.divide %110, %112 : tensor<1x24x1x32xf32>
    %114 = stablehlo.convert %113 : (tensor<1x24x1x32xf32>) -> tensor<1x24x1x32xbf16>
    %115 = stablehlo.reshape %114 : (tensor<1x24x1x32xbf16>) -> tensor<24x1x32xbf16>
    %116 = stablehlo.reshape %arg5 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %117 = stablehlo.reshape %116 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %118 = stablehlo.transpose %117, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %119 = stablehlo.dot_general %27, %118, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %120 = stablehlo.reshape %119 : (tensor<1x1024xbf16>) -> tensor<1x8x1x128xbf16>
    %121 = "stablehlo.scatter"(%arg11, %65, %120) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x32x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>) -> tensor<1x8x32x128xbf16>
    %122 = stablehlo.broadcast_in_dim %121, dims = [0, 1, 3, 4] : (tensor<1x8x32x128xbf16>) -> tensor<1x8x3x32x128xbf16>
    %123 = stablehlo.reshape %122 : (tensor<1x8x3x32x128xbf16>) -> tensor<24x32x128xbf16>
    %124 = stablehlo.dot_general %115, %123, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x1x32xbf16>, tensor<24x32x128xbf16>) -> tensor<24x1x128xbf16>
    %125 = stablehlo.reshape %124 : (tensor<24x1x128xbf16>) -> tensor<1x3072xbf16>
    %126 = stablehlo.reshape %arg4 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %127 = stablehlo.reshape %126 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %128 = stablehlo.transpose %127, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %129 = stablehlo.dot_general %125, %128, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %131 = stablehlo.add %9, %130 : tensor<1x1x3072xbf16>
    %132 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %133 = stablehlo.convert %132 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %134 = stablehlo.convert %131 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %135 = stablehlo.power %134, %0 : tensor<1x1x3072xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %137 = stablehlo.multiply %136, %cst_2 : tensor<1x1xf32>
    %138 = stablehlo.reshape %137 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %139 = stablehlo.add %138, %17 : tensor<1x1x1xf32>
    %140 = stablehlo.rsqrt %139 : tensor<1x1x1xf32>
    %141 = stablehlo.reshape %140 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %143 = stablehlo.multiply %134, %142 : tensor<1x1x3072xf32>
    %144 = stablehlo.convert %143 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %145 = stablehlo.convert %144 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %146 = stablehlo.multiply %133, %145 : tensor<1x1x3072xf32>
    %147 = stablehlo.convert %146 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %148 = stablehlo.reshape %147 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %149 = stablehlo.reshape %arg19 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %150 = stablehlo.reshape %149 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %151 = stablehlo.transpose %150, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %152 = stablehlo.dot_general %148, %151, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %153 = stablehlo.reshape %152 : (tensor<1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %154 = stablehlo.convert %153 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %155 = stablehlo.logistic %153 : tensor<1x1x8192xbf16>
    %156 = stablehlo.convert %155 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %157 = stablehlo.multiply %154, %156 : tensor<1x1x8192xf32>
    %158 = stablehlo.convert %157 : (tensor<1x1x8192xf32>) -> tensor<1x1x8192xbf16>
    %159 = stablehlo.convert %158 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %160 = stablehlo.reshape %arg3 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %161 = stablehlo.reshape %160 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %162 = stablehlo.transpose %161, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %163 = stablehlo.dot_general %148, %162, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %164 = stablehlo.convert %163 : (tensor<1x8192xbf16>) -> tensor<1x8192xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x8192xf32>) -> tensor<1x1x8192xf32>
    %166 = stablehlo.multiply %159, %165 : tensor<1x1x8192xf32>
    %167 = stablehlo.convert %166 : (tensor<1x1x8192xf32>) -> tensor<1x1x8192xbf16>
    %168 = stablehlo.reshape %167 : (tensor<1x1x8192xbf16>) -> tensor<1x8192xbf16>
    %169 = stablehlo.reshape %arg2 : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %170 = stablehlo.reshape %169 : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %171 = stablehlo.transpose %170, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %172 = stablehlo.dot_general %168, %171, contracting_dims = [1] x [0] : (tensor<1x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<1x3072xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %174 = stablehlo.add %131, %173 : tensor<1x1x3072xbf16>
    %175 = stablehlo.convert %174 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %176 = stablehlo.power %175, %0 : tensor<1x1x3072xf32>
    %177 = stablehlo.reduce(%176 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %178 = stablehlo.multiply %177, %cst_2 : tensor<1x1xf32>
    %179 = stablehlo.reshape %178 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %180 = stablehlo.add %179, %17 : tensor<1x1x1xf32>
    %181 = stablehlo.rsqrt %180 : tensor<1x1x1xf32>
    %182 = stablehlo.reshape %181 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %183 = stablehlo.broadcast_in_dim %182, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %184 = stablehlo.multiply %175, %183 : tensor<1x1x3072xf32>
    %185 = stablehlo.convert %184 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %186 = stablehlo.convert %185 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %187 = stablehlo.multiply %2, %186 : tensor<1x1x3072xf32>
    %188 = stablehlo.convert %187 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %189 = stablehlo.reshape %188 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %190 = stablehlo.reshape %arg0 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %191 = stablehlo.reshape %190 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %192 = stablehlo.transpose %191, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %193 = stablehlo.dot_general %189, %192, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<1x128256xbf16>
    %194 = stablehlo.reshape %193 : (tensor<1x128256xbf16>) -> tensor<1x1x128256xbf16>
    return %194 : tensor<1x1x128256xbf16>
  }
}
2025-09-10 19:55:21.768 (  88.644s) [        F1708000]      module_builder.cc:486      1| SHLO Module after compiler StableHLO pipeline:
module @SyncTensorsGraph.420 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<128256x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg1: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg2: tensor<3072x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg3: tensor<8192x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg4: tensor<3072x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg5: tensor<1024x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg6: tensor<1x1xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg7: tensor<128256x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg8: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg9: tensor<1xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg10: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg11: tensor<1x8x32x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg12: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg13: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg14: tensor<64xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg15: tensor<1024x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg16: tensor<1x8x32x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg17: tensor<3072x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x1x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %arg16, %arg17, %arg18, %arg19, %arg20) in_shardings=[<@mesh, [{}, {}]>, <@mesh, []>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, []>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, []>, <@mesh, []>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>] out_shardings=[<@mesh, [{}, {}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg21: tensor<128256x3072xbf16>, %arg22: tensor<f32>, %arg23: tensor<3072x4096xbf16>, %arg24: tensor<4096x3072xbf16>, %arg25: tensor<3072x1536xbf16>, %arg26: tensor<512x3072xbf16>, %arg27: tensor<1x1xi64>, %arg28: tensor<128256x3072xbf16>, %arg29: tensor<3072xbf16>, %arg30: tensor<1xi64>, %arg31: tensor<i64>, %arg32: tensor<1x4x32x128xbf16>, %arg33: tensor<bf16>, %arg34: tensor<f32>, %arg35: tensor<64xf32>, %arg36: tensor<512x3072xbf16>, %arg37: tensor<1x4x32x128xbf16>, %arg38: tensor<1536x3072xbf16>, %arg39: tensor<3072xbf16>, %arg40: tensor<4096x3072xbf16>, %arg41: tensor<3072xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
      %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
      %c = stablehlo.constant dense<[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]]> : tensor<1x32xi64>
      %c_1 = stablehlo.constant dense<0> : tensor<1xi64>
      %cst_2 = stablehlo.constant dense<3.25520843E-4> : tensor<1x1xf32>
      %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %1 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x1x3072xf32>
      %2 = stablehlo.reshape %arg41 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
      %3 = stablehlo.convert %2 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %4 = stablehlo.reshape %arg28 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
      %5 = stablehlo.reshape %4 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
      %6 = stablehlo.reshape %arg27 : (tensor<1x1xi64>) -> tensor<1x1x1xi64>
      %7 = stablehlo.convert %6 : (tensor<1x1x1xi64>) -> tensor<1x1x1xui32>
      %8 = stablehlo.reshape %7 : (tensor<1x1x1xui32>) -> tensor<1xui32>
      %9 = "stablehlo.gather"(%5, %8) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<1xui32>) -> tensor<1x3072xbf16>
      %10 = stablehlo.reshape %9 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
      %11 = stablehlo.reshape %arg29 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
      %12 = stablehlo.convert %11 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %13 = stablehlo.convert %10 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %14 = stablehlo.power %13, %1 : tensor<1x1x3072xf32>
      %15 = stablehlo.reduce(%14 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
      %16 = stablehlo.multiply %15, %cst_2 : tensor<1x1xf32>
      %17 = stablehlo.reshape %16 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
      %18 = stablehlo.reshape %arg22 : (tensor<f32>) -> tensor<1x1x1xf32>
      %19 = stablehlo.add %17, %18 : tensor<1x1x1xf32>
      %20 = stablehlo.rsqrt %19 : tensor<1x1x1xf32>
      %21 = stablehlo.reshape %20 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
      %22 = stablehlo.broadcast_in_dim %21, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
      %23 = stablehlo.multiply %13, %22 : tensor<1x1x3072xf32>
      %24 = stablehlo.convert %23 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
      %25 = stablehlo.convert %24 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %26 = stablehlo.multiply %12, %25 : tensor<1x1x3072xf32>
      %27 = stablehlo.convert %26 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
      %28 = stablehlo.reshape %27 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
      %29 = stablehlo.reshape %arg38 : (tensor<1536x3072xbf16>) -> tensor<1x1536x3072xbf16>
      %30 = stablehlo.reshape %29 : (tensor<1x1536x3072xbf16>) -> tensor<1536x3072xbf16>
      %31 = stablehlo.transpose %30, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<1536x3072xbf16>) -> tensor<3072x1536xbf16>
      %32 = stablehlo.dot_general %28, %31, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1536xbf16>) -> tensor<1x1536xbf16>
      %33 = stablehlo.reshape %32 : (tensor<1x1536xbf16>) -> tensor<1x12x1x128xbf16>
      %34 = stablehlo.convert %33 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,1,128]{3,1,2,0}"} : (tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xf32>
      %35 = stablehlo.reshape %arg35 : (tensor<64xf32>) -> tensor<1x1x64xf32>
      %36 = stablehlo.reshape %35 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
      %37 = stablehlo.reshape %arg30 : (tensor<1xi64>) -> tensor<1x1x1xi64>
      %38 = stablehlo.reshape %37 : (tensor<1x1x1xi64>) -> tensor<1xi64>
      %39 = stablehlo.convert %37 : (tensor<1x1x1xi64>) -> tensor<1x1x1xf32>
      %40 = stablehlo.dot_general %36, %39, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x1xf32>) -> tensor<1x64x1xf32>
      %41 = stablehlo.reshape %40 : (tensor<1x64x1xf32>) -> tensor<1x1x64xf32>
      %42 = stablehlo.concatenate %41, %41, dim = 2 : (tensor<1x1x64xf32>, tensor<1x1x64xf32>) -> tensor<1x1x128xf32>
      %43 = stablehlo.cosine %42 : tensor<1x1x128xf32>
      %44 = stablehlo.convert %43 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
      %45 = stablehlo.convert %44 : (tensor<1x1x128xbf16>) -> tensor<1x1x128xf32>
      %46 = stablehlo.broadcast_in_dim %45, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x12x1x128xf32>
      %47 = stablehlo.multiply %34, %46 : tensor<1x12x1x128xf32>
      %48 = stablehlo.convert %47 : (tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xbf16>
      %49 = stablehlo.slice %33 [0:1, 0:12, 0:1, 64:128] : (tensor<1x12x1x128xbf16>) -> tensor<1x12x1x64xbf16>
      %50 = stablehlo.negate %49 : tensor<1x12x1x64xbf16>
      %51 = stablehlo.slice %33 [0:1, 0:12, 0:1, 0:64] : (tensor<1x12x1x128xbf16>) -> tensor<1x12x1x64xbf16>
      %52 = stablehlo.concatenate %50, %51, dim = 3 : (tensor<1x12x1x64xbf16>, tensor<1x12x1x64xbf16>) -> tensor<1x12x1x128xbf16>
      %53 = stablehlo.convert %52 : (tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xf32>
      %54 = stablehlo.sine %42 : tensor<1x1x128xf32>
      %55 = stablehlo.convert %54 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
      %56 = stablehlo.convert %55 : (tensor<1x1x128xbf16>) -> tensor<1x1x128xf32>
      %57 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x12x1x128xf32>
      %58 = stablehlo.multiply %53, %57 : tensor<1x12x1x128xf32>
      %59 = stablehlo.convert %58 : (tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xbf16>
      %60 = stablehlo.add %48, %59 : tensor<1x12x1x128xbf16>
      %61 = stablehlo.reshape %60 : (tensor<1x12x1x128xbf16>) -> tensor<12x1x128xbf16>
      %62 = stablehlo.compare  LT, %38, %c_1 : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
      %63 = stablehlo.reshape %arg31 : (tensor<i64>) -> tensor<1xi64>
      %64 = stablehlo.add %38, %63 : tensor<1xi64>
      %65 = stablehlo.select %62, %64, %38 : tensor<1xi1>, tensor<1xi64>
      %66 = stablehlo.reshape %65 : (tensor<1xi64>) -> tensor<1x1xi64>
      %67 = stablehlo.reshape %arg36 : (tensor<512x3072xbf16>) -> tensor<1x512x3072xbf16>
      %68 = stablehlo.reshape %67 : (tensor<1x512x3072xbf16>) -> tensor<512x3072xbf16>
      %69 = stablehlo.transpose %68, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<512x3072xbf16>) -> tensor<3072x512xbf16>
      %70 = stablehlo.dot_general %28, %69, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x512xbf16>) -> tensor<1x512xbf16>
      %71 = stablehlo.reshape %70 : (tensor<1x512xbf16>) -> tensor<1x4x1x128xbf16>
      %72 = stablehlo.convert %71 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,1,128]{3,1,2,0}"} : (tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xf32>
      %73 = stablehlo.broadcast_in_dim %45, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x4x1x128xf32>
      %74 = stablehlo.multiply %72, %73 : tensor<1x4x1x128xf32>
      %75 = stablehlo.convert %74 : (tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xbf16>
      %76 = stablehlo.slice %71 [0:1, 0:4, 0:1, 64:128] : (tensor<1x4x1x128xbf16>) -> tensor<1x4x1x64xbf16>
      %77 = stablehlo.negate %76 : tensor<1x4x1x64xbf16>
      %78 = stablehlo.slice %71 [0:1, 0:4, 0:1, 0:64] : (tensor<1x4x1x128xbf16>) -> tensor<1x4x1x64xbf16>
      %79 = stablehlo.concatenate %77, %78, dim = 3 : (tensor<1x4x1x64xbf16>, tensor<1x4x1x64xbf16>) -> tensor<1x4x1x128xbf16>
      %80 = stablehlo.convert %79 : (tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xf32>
      %81 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x4x1x128xf32>
      %82 = stablehlo.multiply %80, %81 : tensor<1x4x1x128xf32>
      %83 = stablehlo.convert %82 : (tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xbf16>
      %84 = stablehlo.add %75, %83 : tensor<1x4x1x128xbf16>
      %85 = "stablehlo.scatter"(%arg37, %66, %84) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg42: tensor<bf16>, %arg43: tensor<bf16>):
        stablehlo.return %arg43 : tensor<bf16>
      }) : (tensor<1x4x32x128xbf16>, tensor<1x1xi64>, tensor<1x4x1x128xbf16>) -> tensor<1x4x32x128xbf16>
      %86 = stablehlo.broadcast_in_dim %85, dims = [0, 1, 3, 4] : (tensor<1x4x32x128xbf16>) -> tensor<1x4x3x32x128xbf16>
      %87 = stablehlo.reshape %86 : (tensor<1x4x3x32x128xbf16>) -> tensor<1x12x32x128xbf16>
      %88 = stablehlo.transpose %87, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,32]{2,3,1,0}"} : (tensor<1x12x32x128xbf16>) -> tensor<1x12x128x32xbf16>
      %89 = stablehlo.reshape %88 : (tensor<1x12x128x32xbf16>) -> tensor<12x128x32xbf16>
      %90 = stablehlo.dot_general %61, %89, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<12x1x128xbf16>, tensor<12x128x32xbf16>) -> tensor<12x1x32xbf16>
      %91 = stablehlo.convert %90 : (tensor<12x1x32xbf16>) -> tensor<12x1x32xf32>
      %92 = stablehlo.reshape %91 : (tensor<12x1x32xf32>) -> tensor<1x12x1x32xf32>
      %93 = stablehlo.broadcast_in_dim %arg34, dims = [] : (tensor<f32>) -> tensor<1x12x1x32xf32>
      %94 = stablehlo.multiply %92, %93 : tensor<1x12x1x32xf32>
      %95 = stablehlo.convert %94 : (tensor<1x12x1x32xf32>) -> tensor<1x12x1x32xbf16>
      %96 = stablehlo.reshape %arg33 : (tensor<bf16>) -> tensor<1xbf16>
      %97 = stablehlo.broadcast_in_dim %96, dims = [0] : (tensor<1xbf16>) -> tensor<1x32xbf16>
      %98 = stablehlo.convert %97 : (tensor<1x32xbf16>) -> tensor<1x32xf32>
      %99 = stablehlo.broadcast_in_dim %38, dims = [0] : (tensor<1xi64>) -> tensor<1x32xi64>
      %100 = stablehlo.compare  GT, %c, %99 : (tensor<1x32xi64>, tensor<1x32xi64>) -> tensor<1x32xi1>
      %101 = stablehlo.convert %100 : (tensor<1x32xi1>) -> tensor<1x32xf32>
      %102 = stablehlo.multiply %98, %101 : tensor<1x32xf32>
      %103 = stablehlo.convert %102 : (tensor<1x32xf32>) -> tensor<1x32xbf16>
      %104 = stablehlo.reshape %103 : (tensor<1x32xbf16>) -> tensor<1x1x32xbf16>
      %105 = stablehlo.broadcast_in_dim %104, dims = [0, 2, 3] : (tensor<1x1x32xbf16>) -> tensor<1x12x1x32xbf16>
      %106 = stablehlo.add %95, %105 : tensor<1x12x1x32xbf16>
      %107 = stablehlo.convert %106 : (tensor<1x12x1x32xbf16>) -> tensor<1x12x1x32xf32>
      %108 = stablehlo.reduce(%107 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x12x1x32xf32>, tensor<f32>) -> tensor<1x12x1xf32>
      %109 = stablehlo.broadcast_in_dim %108, dims = [0, 1, 2] : (tensor<1x12x1xf32>) -> tensor<1x12x1x32xf32>
      %110 = stablehlo.subtract %107, %109 : tensor<1x12x1x32xf32>
      %111 = stablehlo.exponential %110 : tensor<1x12x1x32xf32>
      %112 = stablehlo.reduce(%111 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x12x1x32xf32>, tensor<f32>) -> tensor<1x12x1xf32>
      %113 = stablehlo.broadcast_in_dim %112, dims = [0, 1, 2] : (tensor<1x12x1xf32>) -> tensor<1x12x1x32xf32>
      %114 = stablehlo.divide %111, %113 : tensor<1x12x1x32xf32>
      %115 = stablehlo.convert %114 : (tensor<1x12x1x32xf32>) -> tensor<1x12x1x32xbf16>
      %116 = stablehlo.reshape %115 : (tensor<1x12x1x32xbf16>) -> tensor<12x1x32xbf16>
      %117 = stablehlo.reshape %arg26 : (tensor<512x3072xbf16>) -> tensor<1x512x3072xbf16>
      %118 = stablehlo.reshape %117 : (tensor<1x512x3072xbf16>) -> tensor<512x3072xbf16>
      %119 = stablehlo.transpose %118, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<512x3072xbf16>) -> tensor<3072x512xbf16>
      %120 = stablehlo.dot_general %28, %119, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x512xbf16>) -> tensor<1x512xbf16>
      %121 = stablehlo.reshape %120 : (tensor<1x512xbf16>) -> tensor<1x4x1x128xbf16>
      %122 = "stablehlo.scatter"(%arg32, %66, %121) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg42: tensor<bf16>, %arg43: tensor<bf16>):
        stablehlo.return %arg43 : tensor<bf16>
      }) : (tensor<1x4x32x128xbf16>, tensor<1x1xi64>, tensor<1x4x1x128xbf16>) -> tensor<1x4x32x128xbf16>
      %123 = stablehlo.broadcast_in_dim %122, dims = [0, 1, 3, 4] : (tensor<1x4x32x128xbf16>) -> tensor<1x4x3x32x128xbf16>
      %124 = stablehlo.reshape %123 : (tensor<1x4x3x32x128xbf16>) -> tensor<12x32x128xbf16>
      %125 = stablehlo.dot_general %116, %124, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<12x1x32xbf16>, tensor<12x32x128xbf16>) -> tensor<12x1x128xbf16>
      %126 = stablehlo.reshape %125 : (tensor<12x1x128xbf16>) -> tensor<1x1536xbf16>
      %127 = stablehlo.reshape %arg25 : (tensor<3072x1536xbf16>) -> tensor<1x3072x1536xbf16>
      %128 = stablehlo.reshape %127 : (tensor<1x3072x1536xbf16>) -> tensor<3072x1536xbf16>
      %129 = stablehlo.transpose %128, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x1536xbf16>) -> tensor<1536x3072xbf16>
      %130 = stablehlo.dot_general %126, %129, contracting_dims = [1] x [0] : (tensor<1x1536xbf16>, tensor<1536x3072xbf16>) -> tensor<1x3072xbf16>
      %131 = "stablehlo.all_reduce"(%130) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> ({
      ^bb0(%arg42: tensor<bf16>, %arg43: tensor<bf16>):
        %198 = stablehlo.add %arg42, %arg43 : tensor<bf16>
        stablehlo.return %198 : tensor<bf16>
      }) : (tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
      %132 = stablehlo.reshape %131 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
      %133 = stablehlo.add %10, %132 : tensor<1x1x3072xbf16>
      %134 = stablehlo.reshape %arg39 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
      %135 = stablehlo.convert %134 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %136 = stablehlo.convert %133 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %137 = stablehlo.power %136, %1 : tensor<1x1x3072xf32>
      %138 = stablehlo.reduce(%137 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
      %139 = stablehlo.multiply %138, %cst_2 : tensor<1x1xf32>
      %140 = stablehlo.reshape %139 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
      %141 = stablehlo.add %140, %18 : tensor<1x1x1xf32>
      %142 = stablehlo.rsqrt %141 : tensor<1x1x1xf32>
      %143 = stablehlo.reshape %142 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
      %144 = stablehlo.broadcast_in_dim %143, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
      %145 = stablehlo.multiply %136, %144 : tensor<1x1x3072xf32>
      %146 = stablehlo.convert %145 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
      %147 = stablehlo.convert %146 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %148 = stablehlo.multiply %135, %147 : tensor<1x1x3072xf32>
      %149 = stablehlo.convert %148 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
      %150 = stablehlo.reshape %149 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
      %151 = stablehlo.reshape %arg40 : (tensor<4096x3072xbf16>) -> tensor<1x4096x3072xbf16>
      %152 = stablehlo.reshape %151 : (tensor<1x4096x3072xbf16>) -> tensor<4096x3072xbf16>
      %153 = stablehlo.transpose %152, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<4096x3072xbf16>) -> tensor<3072x4096xbf16>
      %154 = stablehlo.dot_general %150, %153, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<1x4096xbf16>
      %155 = stablehlo.reshape %154 : (tensor<1x4096xbf16>) -> tensor<1x1x4096xbf16>
      %156 = stablehlo.convert %155 : (tensor<1x1x4096xbf16>) -> tensor<1x1x4096xf32>
      %157 = stablehlo.logistic %155 : tensor<1x1x4096xbf16>
      %158 = stablehlo.convert %157 : (tensor<1x1x4096xbf16>) -> tensor<1x1x4096xf32>
      %159 = stablehlo.multiply %156, %158 : tensor<1x1x4096xf32>
      %160 = stablehlo.convert %159 : (tensor<1x1x4096xf32>) -> tensor<1x1x4096xbf16>
      %161 = stablehlo.convert %160 : (tensor<1x1x4096xbf16>) -> tensor<1x1x4096xf32>
      %162 = stablehlo.reshape %arg24 : (tensor<4096x3072xbf16>) -> tensor<1x4096x3072xbf16>
      %163 = stablehlo.reshape %162 : (tensor<1x4096x3072xbf16>) -> tensor<4096x3072xbf16>
      %164 = stablehlo.transpose %163, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<4096x3072xbf16>) -> tensor<3072x4096xbf16>
      %165 = stablehlo.dot_general %150, %164, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<1x4096xbf16>
      %166 = stablehlo.convert %165 : (tensor<1x4096xbf16>) -> tensor<1x4096xf32>
      %167 = stablehlo.reshape %166 : (tensor<1x4096xf32>) -> tensor<1x1x4096xf32>
      %168 = stablehlo.multiply %161, %167 : tensor<1x1x4096xf32>
      %169 = stablehlo.convert %168 : (tensor<1x1x4096xf32>) -> tensor<1x1x4096xbf16>
      %170 = stablehlo.reshape %169 : (tensor<1x1x4096xbf16>) -> tensor<1x4096xbf16>
      %171 = stablehlo.reshape %arg23 : (tensor<3072x4096xbf16>) -> tensor<1x3072x4096xbf16>
      %172 = stablehlo.reshape %171 : (tensor<1x3072x4096xbf16>) -> tensor<3072x4096xbf16>
      %173 = stablehlo.transpose %172, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x4096xbf16>) -> tensor<4096x3072xbf16>
      %174 = stablehlo.dot_general %170, %173, contracting_dims = [1] x [0] : (tensor<1x4096xbf16>, tensor<4096x3072xbf16>) -> tensor<1x3072xbf16>
      %175 = "stablehlo.all_reduce"(%174) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> ({
      ^bb0(%arg42: tensor<bf16>, %arg43: tensor<bf16>):
        %198 = stablehlo.add %arg42, %arg43 : tensor<bf16>
        stablehlo.return %198 : tensor<bf16>
      }) : (tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
      %176 = stablehlo.reshape %175 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
      %177 = stablehlo.add %133, %176 : tensor<1x1x3072xbf16>
      %178 = stablehlo.convert %177 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %179 = stablehlo.power %178, %1 : tensor<1x1x3072xf32>
      %180 = stablehlo.reduce(%179 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
      %181 = stablehlo.multiply %180, %cst_2 : tensor<1x1xf32>
      %182 = stablehlo.reshape %181 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
      %183 = stablehlo.add %182, %18 : tensor<1x1x1xf32>
      %184 = stablehlo.rsqrt %183 : tensor<1x1x1xf32>
      %185 = stablehlo.reshape %184 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
      %186 = stablehlo.broadcast_in_dim %185, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
      %187 = stablehlo.multiply %178, %186 : tensor<1x1x3072xf32>
      %188 = stablehlo.convert %187 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
      %189 = stablehlo.convert %188 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %190 = stablehlo.multiply %3, %189 : tensor<1x1x3072xf32>
      %191 = stablehlo.convert %190 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
      %192 = stablehlo.reshape %191 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
      %193 = stablehlo.reshape %arg21 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
      %194 = stablehlo.reshape %193 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
      %195 = stablehlo.transpose %194, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
      %196 = stablehlo.dot_general %192, %195, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<1x128256xbf16>
      %197 = stablehlo.reshape %196 : (tensor<1x128256xbf16>) -> tensor<1x1x128256xbf16>
      sdy.return %197 : tensor<1x1x128256xbf16>
    } : (tensor<128256x3072xbf16>, tensor<f32>, tensor<3072x8192xbf16>, tensor<8192x3072xbf16>, tensor<3072x3072xbf16>, tensor<1024x3072xbf16>, tensor<1x1xi64>, tensor<128256x3072xbf16>, tensor<3072xbf16>, tensor<1xi64>, tensor<i64>, tensor<1x8x32x128xbf16>, tensor<bf16>, tensor<f32>, tensor<64xf32>, tensor<1024x3072xbf16>, tensor<1x8x32x128xbf16>, tensor<3072x3072xbf16>, tensor<3072xbf16>, tensor<8192x3072xbf16>, tensor<3072xbf16>) -> tensor<1x1x128256xbf16>
    return %0 : tensor<1x1x128256xbf16>
  }
}
2025-09-10 19:55:21.778 (  88.654s) [        F1708000]      module_builder.cc:510      1| TTIR Module:
module @SyncTensorsGraph.420 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  func.func @main(%arg0: tensor<128256x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg1: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg2: tensor<3072x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg3: tensor<8192x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg4: tensor<3072x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg5: tensor<1024x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg6: tensor<1x1xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg7: tensor<128256x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg8: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg9: tensor<1xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg10: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg11: tensor<1x8x32x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg12: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg13: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg14: tensor<64xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg15: tensor<1024x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg16: tensor<1x8x32x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg17: tensor<3072x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x1x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = ttir.empty() : tensor<128256x3072xbf16>
    %1 = "ttir.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16>, tensor<128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %2 = ttir.empty() : tensor<f32>
    %3 = "ttir.mesh_shard"(%arg1, %2) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %4 = ttir.empty() : tensor<3072x4096xbf16>
    %5 = "ttir.mesh_shard"(%arg2, %4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x8192xbf16>, tensor<3072x4096xbf16>) -> tensor<3072x4096xbf16>
    %6 = ttir.empty() : tensor<4096x3072xbf16>
    %7 = "ttir.mesh_shard"(%arg3, %6) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16>, tensor<4096x3072xbf16>) -> tensor<4096x3072xbf16>
    %8 = ttir.empty() : tensor<3072x1536xbf16>
    %9 = "ttir.mesh_shard"(%arg4, %8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16>, tensor<3072x1536xbf16>) -> tensor<3072x1536xbf16>
    %10 = ttir.empty() : tensor<512x3072xbf16>
    %11 = "ttir.mesh_shard"(%arg5, %10) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16>, tensor<512x3072xbf16>) -> tensor<512x3072xbf16>
    %12 = ttir.empty() : tensor<1x1xi64>
    %13 = "ttir.mesh_shard"(%arg6, %12) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x1xi64>, tensor<1x1xi64>) -> tensor<1x1xi64>
    %14 = ttir.empty() : tensor<128256x3072xbf16>
    %15 = "ttir.mesh_shard"(%arg7, %14) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16>, tensor<128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = ttir.empty() : tensor<3072xbf16>
    %17 = "ttir.mesh_shard"(%arg8, %16) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16>, tensor<3072xbf16>) -> tensor<3072xbf16>
    %18 = ttir.empty() : tensor<1xi64>
    %19 = "ttir.mesh_shard"(%arg9, %18) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %20 = ttir.empty() : tensor<i64>
    %21 = "ttir.mesh_shard"(%arg10, %20) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %22 = ttir.empty() : tensor<1x4x32x128xbf16>
    %23 = "ttir.mesh_shard"(%arg11, %22) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x32x128xbf16>, tensor<1x4x32x128xbf16>) -> tensor<1x4x32x128xbf16>
    %24 = ttir.empty() : tensor<bf16>
    %25 = "ttir.mesh_shard"(%arg12, %24) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
    %26 = ttir.empty() : tensor<f32>
    %27 = "ttir.mesh_shard"(%arg13, %26) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %28 = ttir.empty() : tensor<64xf32>
    %29 = "ttir.mesh_shard"(%arg14, %28) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32>, tensor<64xf32>) -> tensor<64xf32>
    %30 = ttir.empty() : tensor<512x3072xbf16>
    %31 = "ttir.mesh_shard"(%arg15, %30) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16>, tensor<512x3072xbf16>) -> tensor<512x3072xbf16>
    %32 = ttir.empty() : tensor<1x4x32x128xbf16>
    %33 = "ttir.mesh_shard"(%arg16, %32) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x32x128xbf16>, tensor<1x4x32x128xbf16>) -> tensor<1x4x32x128xbf16>
    %34 = ttir.empty() : tensor<1536x3072xbf16>
    %35 = "ttir.mesh_shard"(%arg17, %34) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16>, tensor<1536x3072xbf16>) -> tensor<1536x3072xbf16>
    %36 = ttir.empty() : tensor<3072xbf16>
    %37 = "ttir.mesh_shard"(%arg18, %36) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16>, tensor<3072xbf16>) -> tensor<3072xbf16>
    %38 = ttir.empty() : tensor<4096x3072xbf16>
    %39 = "ttir.mesh_shard"(%arg19, %38) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16>, tensor<4096x3072xbf16>) -> tensor<4096x3072xbf16>
    %40 = ttir.empty() : tensor<3072xbf16>
    %41 = "ttir.mesh_shard"(%arg20, %40) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16>, tensor<3072xbf16>) -> tensor<3072xbf16>
    %42 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<f32>}> : () -> tensor<f32>
    %43 = "ttir.constant"() <{value = dense<0xFF800000> : tensor<f32>}> : () -> tensor<f32>
    %44 = "ttir.constant"() <{value = dense<[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]]> : tensor<1x32xi64>}> : () -> tensor<1x32xi64>
    %45 = "ttir.constant"() <{value = dense<0> : tensor<1xi64>}> : () -> tensor<1xi64>
    %46 = "ttir.constant"() <{value = dense<3.25520843E-4> : tensor<1x1xf32>}> : () -> tensor<1x1xf32>
    %47 = "ttir.constant"() <{value = dense<2.000000e+00> : tensor<f32>}> : () -> tensor<f32>
    %48 = ttir.empty() : tensor<1x1x1xf32>
    %49 = "ttir.reshape"(%47, %48) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %50 = ttir.empty() : tensor<1x1x3072xf32>
    %51 = "ttir.broadcast"(%49, %50) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x1x1xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %52 = ttir.empty() : tensor<1x1x3072xbf16>
    %53 = "ttir.reshape"(%41, %52) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %54 = ttir.empty() : tensor<1x1x3072xf32>
    %55 = "ttir.typecast"(%53, %54) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %56 = ttir.empty() : tensor<1x128256x3072xbf16>
    %57 = "ttir.reshape"(%15, %56) <{shape = [1 : i32, 128256 : i32, 3072 : i32]}> : (tensor<128256x3072xbf16>, tensor<1x128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %58 = ttir.empty() : tensor<128256x3072xbf16>
    %59 = "ttir.reshape"(%57, %58) <{shape = [128256 : i32, 3072 : i32]}> : (tensor<1x128256x3072xbf16>, tensor<128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %60 = ttir.empty() : tensor<1x1x1xi64>
    %61 = "ttir.reshape"(%13, %60) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xi64>, tensor<1x1x1xi64>) -> tensor<1x1x1xi64>
    %62 = ttir.empty() : tensor<1x1x1xui32>
    %63 = "ttir.typecast"(%61, %62) <{conservative_folding = false}> : (tensor<1x1x1xi64>, tensor<1x1x1xui32>) -> tensor<1x1x1xui32>
    %64 = ttir.empty() : tensor<1xui32>
    %65 = "ttir.reshape"(%63, %64) <{shape = [1 : i32]}> : (tensor<1x1x1xui32>, tensor<1xui32>) -> tensor<1xui32>
    %66 = ttir.empty() : tensor<1x3072xbf16>
    %67 = "ttir.gather"(%59, %65, %66) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 3072>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<128256x3072xbf16>, tensor<1xui32>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %68 = ttir.empty() : tensor<1x1x3072xbf16>
    %69 = "ttir.reshape"(%67, %68) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %70 = ttir.empty() : tensor<1x1x3072xbf16>
    %71 = "ttir.reshape"(%17, %70) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %72 = ttir.empty() : tensor<1x1x3072xf32>
    %73 = "ttir.typecast"(%71, %72) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %74 = ttir.empty() : tensor<1x1x3072xf32>
    %75 = "ttir.typecast"(%69, %74) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %76 = ttir.empty() : tensor<1x1x3072xf32>
    %77 = "ttir.pow"(%75, %51, %76) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %78 = ttir.empty() : tensor<1x1xf32>
    %79 = "ttir.sum"(%77, %78) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %80 = ttir.empty() : tensor<1x1xf32>
    %81 = "ttir.multiply"(%79, %46, %80) : (tensor<1x1xf32>, tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %82 = ttir.empty() : tensor<1x1x1xf32>
    %83 = "ttir.reshape"(%81, %82) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %84 = ttir.empty() : tensor<1x1x1xf32>
    %85 = "ttir.reshape"(%3, %84) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %86 = ttir.empty() : tensor<1x1x1xf32>
    %87 = "ttir.add"(%83, %85, %86) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %88 = ttir.empty() : tensor<1x1x1xf32>
    %89 = "ttir.rsqrt"(%87, %88) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %90 = ttir.empty() : tensor<1x1xf32>
    %91 = "ttir.reshape"(%89, %90) <{shape = [1 : i32, 1 : i32]}> : (tensor<1x1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %92 = ttir.empty() : tensor<1x1x1xf32>
    %93 = "ttir.reshape"(%91, %92) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %94 = ttir.empty() : tensor<1x1x3072xf32>
    %95 = "ttir.broadcast"(%93, %94) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x1x1xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %96 = ttir.empty() : tensor<1x1x3072xf32>
    %97 = "ttir.multiply"(%75, %95, %96) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %98 = ttir.empty() : tensor<1x1x3072xbf16>
    %99 = "ttir.typecast"(%97, %98) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %100 = ttir.empty() : tensor<1x1x3072xf32>
    %101 = "ttir.typecast"(%99, %100) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %102 = ttir.empty() : tensor<1x1x3072xf32>
    %103 = "ttir.multiply"(%73, %101, %102) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %104 = ttir.empty() : tensor<1x1x3072xbf16>
    %105 = "ttir.typecast"(%103, %104) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %106 = ttir.empty() : tensor<1x3072xbf16>
    %107 = "ttir.reshape"(%105, %106) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x3072xbf16>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %108 = ttir.empty() : tensor<1x1536x3072xbf16>
    %109 = "ttir.reshape"(%35, %108) <{shape = [1 : i32, 1536 : i32, 3072 : i32]}> : (tensor<1536x3072xbf16>, tensor<1x1536x3072xbf16>) -> tensor<1x1536x3072xbf16>
    %110 = ttir.empty() : tensor<1536x3072xbf16>
    %111 = "ttir.reshape"(%109, %110) <{shape = [1536 : i32, 3072 : i32]}> : (tensor<1x1536x3072xbf16>, tensor<1536x3072xbf16>) -> tensor<1536x3072xbf16>
    %112 = ttir.empty() : tensor<3072x1536xbf16>
    %113 = "ttir.permute"(%111, %112) <{permutation = array<i64: 1, 0>}> : (tensor<1536x3072xbf16>, tensor<3072x1536xbf16>) -> tensor<3072x1536xbf16>
    %114 = "ttir.dot_general"(%107, %113) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x1536xbf16>) -> tensor<1x1536xbf16>
    %115 = ttir.empty() : tensor<1x12x1x128xbf16>
    %116 = "ttir.reshape"(%114, %115) <{shape = [1 : i32, 12 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1536xbf16>, tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xbf16>
    %117 = ttir.empty() : tensor<1x12x1x128xf32>
    %118 = "ttir.typecast"(%116, %117) <{conservative_folding = false}> : (tensor<1x12x1x128xbf16>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %119 = ttir.empty() : tensor<1x1x64xf32>
    %120 = "ttir.reshape"(%29, %119) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xf32>, tensor<1x1x64xf32>) -> tensor<1x1x64xf32>
    %121 = ttir.empty() : tensor<1x64x1xf32>
    %122 = "ttir.reshape"(%120, %121) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<1x1x64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
    %123 = ttir.empty() : tensor<1x1x1xi64>
    %124 = "ttir.reshape"(%19, %123) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xi64>, tensor<1x1x1xi64>) -> tensor<1x1x1xi64>
    %125 = ttir.empty() : tensor<1xi64>
    %126 = "ttir.reshape"(%124, %125) <{shape = [1 : i32]}> : (tensor<1x1x1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %127 = ttir.empty() : tensor<1x1x1xf32>
    %128 = "ttir.typecast"(%124, %127) <{conservative_folding = false}> : (tensor<1x1x1xi64>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %129 = "ttir.dot_general"(%122, %128) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x64x1xf32>, tensor<1x1x1xf32>) -> tensor<1x64x1xf32>
    %130 = ttir.empty() : tensor<1x1x64xf32>
    %131 = "ttir.reshape"(%129, %130) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<1x64x1xf32>, tensor<1x1x64xf32>) -> tensor<1x1x64xf32>
    %132 = ttir.empty() : tensor<1x1x128xf32>
    %133 = "ttir.concat"(%131, %131, %132) <{dim = 2 : si32}> : (tensor<1x1x64xf32>, tensor<1x1x64xf32>, tensor<1x1x128xf32>) -> tensor<1x1x128xf32>
    %134 = ttir.empty() : tensor<1x1x128xf32>
    %135 = "ttir.cos"(%133, %134) : (tensor<1x1x128xf32>, tensor<1x1x128xf32>) -> tensor<1x1x128xf32>
    %136 = ttir.empty() : tensor<1x1x128xbf16>
    %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<1x1x128xf32>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16>
    %138 = ttir.empty() : tensor<1x1x128xf32>
    %139 = "ttir.typecast"(%137, %138) <{conservative_folding = false}> : (tensor<1x1x128xbf16>, tensor<1x1x128xf32>) -> tensor<1x1x128xf32>
    %140 = ttir.empty() : tensor<1x1x1x128xf32>
    %141 = "ttir.reshape"(%139, %140) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32>, tensor<1x1x1x128xf32>) -> tensor<1x1x1x128xf32>
    %142 = ttir.empty() : tensor<1x12x1x128xf32>
    %143 = "ttir.broadcast"(%141, %142) <{broadcast_dimensions = array<i64: 1, 12, 1, 1>}> : (tensor<1x1x1x128xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %144 = ttir.empty() : tensor<1x12x1x128xf32>
    %145 = "ttir.multiply"(%118, %143, %144) : (tensor<1x12x1x128xf32>, tensor<1x12x1x128xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %146 = ttir.empty() : tensor<1x12x1x128xbf16>
    %147 = "ttir.typecast"(%145, %146) <{conservative_folding = false}> : (tensor<1x12x1x128xf32>, tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xbf16>
    %148 = ttir.empty() : tensor<1x12x1x64xbf16>
    %149 = "ttir.slice_static"(%116, %148) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 12 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x1x128xbf16>, tensor<1x12x1x64xbf16>) -> tensor<1x12x1x64xbf16>
    %150 = ttir.empty() : tensor<1x12x1x64xbf16>
    %151 = "ttir.neg"(%149, %150) : (tensor<1x12x1x64xbf16>, tensor<1x12x1x64xbf16>) -> tensor<1x12x1x64xbf16>
    %152 = ttir.empty() : tensor<1x12x1x64xbf16>
    %153 = "ttir.slice_static"(%116, %152) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 12 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x1x128xbf16>, tensor<1x12x1x64xbf16>) -> tensor<1x12x1x64xbf16>
    %154 = ttir.empty() : tensor<1x12x1x128xbf16>
    %155 = "ttir.concat"(%151, %153, %154) <{dim = 3 : si32}> : (tensor<1x12x1x64xbf16>, tensor<1x12x1x64xbf16>, tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xbf16>
    %156 = ttir.empty() : tensor<1x12x1x128xf32>
    %157 = "ttir.typecast"(%155, %156) <{conservative_folding = false}> : (tensor<1x12x1x128xbf16>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %158 = ttir.empty() : tensor<1x1x128xf32>
    %159 = "ttir.sin"(%133, %158) : (tensor<1x1x128xf32>, tensor<1x1x128xf32>) -> tensor<1x1x128xf32>
    %160 = ttir.empty() : tensor<1x1x128xbf16>
    %161 = "ttir.typecast"(%159, %160) <{conservative_folding = false}> : (tensor<1x1x128xf32>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16>
    %162 = ttir.empty() : tensor<1x1x128xf32>
    %163 = "ttir.typecast"(%161, %162) <{conservative_folding = false}> : (tensor<1x1x128xbf16>, tensor<1x1x128xf32>) -> tensor<1x1x128xf32>
    %164 = ttir.empty() : tensor<1x1x1x128xf32>
    %165 = "ttir.reshape"(%163, %164) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32>, tensor<1x1x1x128xf32>) -> tensor<1x1x1x128xf32>
    %166 = ttir.empty() : tensor<1x12x1x128xf32>
    %167 = "ttir.broadcast"(%165, %166) <{broadcast_dimensions = array<i64: 1, 12, 1, 1>}> : (tensor<1x1x1x128xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %168 = ttir.empty() : tensor<1x12x1x128xf32>
    %169 = "ttir.multiply"(%157, %167, %168) : (tensor<1x12x1x128xf32>, tensor<1x12x1x128xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %170 = ttir.empty() : tensor<1x12x1x128xbf16>
    %171 = "ttir.typecast"(%169, %170) <{conservative_folding = false}> : (tensor<1x12x1x128xf32>, tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xbf16>
    %172 = ttir.empty() : tensor<1x12x1x128xbf16>
    %173 = "ttir.add"(%147, %171, %172) : (tensor<1x12x1x128xbf16>, tensor<1x12x1x128xbf16>, tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xbf16>
    %174 = ttir.empty() : tensor<12x1x128xbf16>
    %175 = "ttir.reshape"(%173, %174) <{shape = [12 : i32, 1 : i32, 128 : i32]}> : (tensor<1x12x1x128xbf16>, tensor<12x1x128xbf16>) -> tensor<12x1x128xbf16>
    %176 = ttir.empty() : tensor<1xi1>
    %177 = "ttir.lt"(%126, %45, %176) : (tensor<1xi64>, tensor<1xi64>, tensor<1xi1>) -> tensor<1xi1>
    %178 = ttir.empty() : tensor<1xi64>
    %179 = "ttir.reshape"(%21, %178) <{shape = [1 : i32]}> : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %180 = ttir.empty() : tensor<1xi64>
    %181 = "ttir.add"(%126, %179, %180) : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %182 = ttir.empty() : tensor<1xi64>
    %183 = "ttir.where"(%177, %181, %126, %182) : (tensor<1xi1>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %184 = ttir.empty() : tensor<1x1xi64>
    %185 = "ttir.reshape"(%183, %184) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xi64>, tensor<1x1xi64>) -> tensor<1x1xi64>
    %186 = ttir.empty() : tensor<1x512x3072xbf16>
    %187 = "ttir.reshape"(%31, %186) <{shape = [1 : i32, 512 : i32, 3072 : i32]}> : (tensor<512x3072xbf16>, tensor<1x512x3072xbf16>) -> tensor<1x512x3072xbf16>
    %188 = ttir.empty() : tensor<512x3072xbf16>
    %189 = "ttir.reshape"(%187, %188) <{shape = [512 : i32, 3072 : i32]}> : (tensor<1x512x3072xbf16>, tensor<512x3072xbf16>) -> tensor<512x3072xbf16>
    %190 = ttir.empty() : tensor<3072x512xbf16>
    %191 = "ttir.permute"(%189, %190) <{permutation = array<i64: 1, 0>}> : (tensor<512x3072xbf16>, tensor<3072x512xbf16>) -> tensor<3072x512xbf16>
    %192 = "ttir.dot_general"(%107, %191) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x512xbf16>) -> tensor<1x512xbf16>
    %193 = ttir.empty() : tensor<1x4x1x128xbf16>
    %194 = "ttir.reshape"(%192, %193) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32]}> : (tensor<1x512xbf16>, tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xbf16>
    %195 = ttir.empty() : tensor<1x4x1x128xf32>
    %196 = "ttir.typecast"(%194, %195) <{conservative_folding = false}> : (tensor<1x4x1x128xbf16>, tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xf32>
    %197 = ttir.empty() : tensor<1x1x1x128xf32>
    %198 = "ttir.reshape"(%139, %197) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32>, tensor<1x1x1x128xf32>) -> tensor<1x1x1x128xf32>
    %199 = ttir.empty() : tensor<1x4x1x128xf32>
    %200 = "ttir.broadcast"(%198, %199) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x1x128xf32>, tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xf32>
    %201 = ttir.empty() : tensor<1x4x1x128xf32>
    %202 = "ttir.multiply"(%196, %200, %201) : (tensor<1x4x1x128xf32>, tensor<1x4x1x128xf32>, tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xf32>
    %203 = ttir.empty() : tensor<1x4x1x128xbf16>
    %204 = "ttir.typecast"(%202, %203) <{conservative_folding = false}> : (tensor<1x4x1x128xf32>, tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xbf16>
    %205 = ttir.empty() : tensor<1x4x1x64xbf16>
    %206 = "ttir.slice_static"(%194, %205) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x1x128xbf16>, tensor<1x4x1x64xbf16>) -> tensor<1x4x1x64xbf16>
    %207 = ttir.empty() : tensor<1x4x1x64xbf16>
    %208 = "ttir.neg"(%206, %207) : (tensor<1x4x1x64xbf16>, tensor<1x4x1x64xbf16>) -> tensor<1x4x1x64xbf16>
    %209 = ttir.empty() : tensor<1x4x1x64xbf16>
    %210 = "ttir.slice_static"(%194, %209) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x1x128xbf16>, tensor<1x4x1x64xbf16>) -> tensor<1x4x1x64xbf16>
    %211 = ttir.empty() : tensor<1x4x1x128xbf16>
    %212 = "ttir.concat"(%208, %210, %211) <{dim = 3 : si32}> : (tensor<1x4x1x64xbf16>, tensor<1x4x1x64xbf16>, tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xbf16>
    %213 = ttir.empty() : tensor<1x4x1x128xf32>
    %214 = "ttir.typecast"(%212, %213) <{conservative_folding = false}> : (tensor<1x4x1x128xbf16>, tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xf32>
    %215 = ttir.empty() : tensor<1x1x1x128xf32>
    %216 = "ttir.reshape"(%163, %215) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32>, tensor<1x1x1x128xf32>) -> tensor<1x1x1x128xf32>
    %217 = ttir.empty() : tensor<1x4x1x128xf32>
    %218 = "ttir.broadcast"(%216, %217) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x1x128xf32>, tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xf32>
    %219 = ttir.empty() : tensor<1x4x1x128xf32>
    %220 = "ttir.multiply"(%214, %218, %219) : (tensor<1x4x1x128xf32>, tensor<1x4x1x128xf32>, tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xf32>
    %221 = ttir.empty() : tensor<1x4x1x128xbf16>
    %222 = "ttir.typecast"(%220, %221) <{conservative_folding = false}> : (tensor<1x4x1x128xf32>, tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xbf16>
    %223 = ttir.empty() : tensor<1x4x1x128xbf16>
    %224 = "ttir.add"(%204, %222, %223) : (tensor<1x4x1x128xbf16>, tensor<1x4x1x128xbf16>, tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xbf16>
    %225 = ttir.empty() : tensor<1x4x32x128xbf16>
    %226 = "ttir.scatter"(%33, %185, %224, %225) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x4x32x128xbf16>, tensor<1x1xi64>, tensor<1x4x1x128xbf16>, tensor<1x4x32x128xbf16>) -> tensor<1x4x32x128xbf16>
    %227 = ttir.empty() : tensor<1x4x1x32x128xbf16>
    %228 = "ttir.reshape"(%226, %227) <{shape = [1 : i32, 4 : i32, 1 : i32, 32 : i32, 128 : i32]}> : (tensor<1x4x32x128xbf16>, tensor<1x4x1x32x128xbf16>) -> tensor<1x4x1x32x128xbf16>
    %229 = ttir.empty() : tensor<1x4x3x32x128xbf16>
    %230 = "ttir.broadcast"(%228, %229) <{broadcast_dimensions = array<i64: 1, 1, 3, 1, 1>}> : (tensor<1x4x1x32x128xbf16>, tensor<1x4x3x32x128xbf16>) -> tensor<1x4x3x32x128xbf16>
    %231 = ttir.empty() : tensor<1x12x32x128xbf16>
    %232 = "ttir.reshape"(%230, %231) <{shape = [1 : i32, 12 : i32, 32 : i32, 128 : i32]}> : (tensor<1x4x3x32x128xbf16>, tensor<1x12x32x128xbf16>) -> tensor<1x12x32x128xbf16>
    %233 = ttir.empty() : tensor<1x12x128x32xbf16>
    %234 = "ttir.permute"(%232, %233) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x12x32x128xbf16>, tensor<1x12x128x32xbf16>) -> tensor<1x12x128x32xbf16>
    %235 = ttir.empty() : tensor<12x128x32xbf16>
    %236 = "ttir.reshape"(%234, %235) <{shape = [12 : i32, 128 : i32, 32 : i32]}> : (tensor<1x12x128x32xbf16>, tensor<12x128x32xbf16>) -> tensor<12x128x32xbf16>
    %237 = "ttir.dot_general"(%175, %236) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<12x1x128xbf16>, tensor<12x128x32xbf16>) -> tensor<12x1x32xbf16>
    %238 = ttir.empty() : tensor<12x1x32xf32>
    %239 = "ttir.typecast"(%237, %238) <{conservative_folding = false}> : (tensor<12x1x32xbf16>, tensor<12x1x32xf32>) -> tensor<12x1x32xf32>
    %240 = ttir.empty() : tensor<1x12x1x32xf32>
    %241 = "ttir.reshape"(%239, %240) <{shape = [1 : i32, 12 : i32, 1 : i32, 32 : i32]}> : (tensor<12x1x32xf32>, tensor<1x12x1x32xf32>) -> tensor<1x12x1x32xf32>
    %242 = ttir.empty() : tensor<1x1x1x1xf32>
    %243 = "ttir.reshape"(%27, %242) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %244 = ttir.empty() : tensor<1x12x1x32xf32>
    %245 = "ttir.broadcast"(%243, %244) <{broadcast_dimensions = array<i64: 1, 12, 1, 32>}> : (tensor<1x1x1x1xf32>, tensor<1x12x1x32xf32>) -> tensor<1x12x1x32xf32>
    %246 = ttir.empty() : tensor<1x12x1x32xf32>
    %247 = "ttir.multiply"(%241, %245, %246) : (tensor<1x12x1x32xf32>, tensor<1x12x1x32xf32>, tensor<1x12x1x32xf32>) -> tensor<1x12x1x32xf32>
    %248 = ttir.empty() : tensor<1x12x1x32xbf16>
    %249 = "ttir.typecast"(%247, %248) <{conservative_folding = false}> : (tensor<1x12x1x32xf32>, tensor<1x12x1x32xbf16>) -> tensor<1x12x1x32xbf16>
    %250 = ttir.empty() : tensor<1xbf16>
    %251 = "ttir.reshape"(%25, %250) <{shape = [1 : i32]}> : (tensor<bf16>, tensor<1xbf16>) -> tensor<1xbf16>
    %252 = ttir.empty() : tensor<1x1xbf16>
    %253 = "ttir.reshape"(%251, %252) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xbf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %254 = ttir.empty() : tensor<1x32xbf16>
    %255 = "ttir.broadcast"(%253, %254) <{broadcast_dimensions = array<i64: 1, 32>}> : (tensor<1x1xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
    %256 = ttir.empty() : tensor<1x32xf32>
    %257 = "ttir.typecast"(%255, %256) <{conservative_folding = false}> : (tensor<1x32xbf16>, tensor<1x32xf32>) -> tensor<1x32xf32>
    %258 = ttir.empty() : tensor<1x1xi64>
    %259 = "ttir.reshape"(%126, %258) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xi64>, tensor<1x1xi64>) -> tensor<1x1xi64>
    %260 = ttir.empty() : tensor<1x32xi64>
    %261 = "ttir.broadcast"(%259, %260) <{broadcast_dimensions = array<i64: 1, 32>}> : (tensor<1x1xi64>, tensor<1x32xi64>) -> tensor<1x32xi64>
    %262 = ttir.empty() : tensor<1x32xi1>
    %263 = "ttir.gt"(%44, %261, %262) : (tensor<1x32xi64>, tensor<1x32xi64>, tensor<1x32xi1>) -> tensor<1x32xi1>
    %264 = ttir.empty() : tensor<1x32xf32>
    %265 = "ttir.typecast"(%263, %264) <{conservative_folding = false}> : (tensor<1x32xi1>, tensor<1x32xf32>) -> tensor<1x32xf32>
    %266 = ttir.empty() : tensor<1x32xf32>
    %267 = "ttir.multiply"(%257, %265, %266) : (tensor<1x32xf32>, tensor<1x32xf32>, tensor<1x32xf32>) -> tensor<1x32xf32>
    %268 = ttir.empty() : tensor<1x32xbf16>
    %269 = "ttir.typecast"(%267, %268) <{conservative_folding = false}> : (tensor<1x32xf32>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
    %270 = ttir.empty() : tensor<1x1x32xbf16>
    %271 = "ttir.reshape"(%269, %270) <{shape = [1 : i32, 1 : i32, 32 : i32]}> : (tensor<1x32xbf16>, tensor<1x1x32xbf16>) -> tensor<1x1x32xbf16>
    %272 = ttir.empty() : tensor<1x1x1x32xbf16>
    %273 = "ttir.reshape"(%271, %272) <{shape = [1 : i32, 1 : i32, 1 : i32, 32 : i32]}> : (tensor<1x1x32xbf16>, tensor<1x1x1x32xbf16>) -> tensor<1x1x1x32xbf16>
    %274 = ttir.empty() : tensor<1x12x1x32xbf16>
    %275 = "ttir.broadcast"(%273, %274) <{broadcast_dimensions = array<i64: 1, 12, 1, 1>}> : (tensor<1x1x1x32xbf16>, tensor<1x12x1x32xbf16>) -> tensor<1x12x1x32xbf16>
    %276 = ttir.empty() : tensor<1x12x1x32xbf16>
    %277 = "ttir.add"(%249, %275, %276) : (tensor<1x12x1x32xbf16>, tensor<1x12x1x32xbf16>, tensor<1x12x1x32xbf16>) -> tensor<1x12x1x32xbf16>
    %278 = ttir.empty() : tensor<1x12x1x32xf32>
    %279 = "ttir.typecast"(%277, %278) <{conservative_folding = false}> : (tensor<1x12x1x32xbf16>, tensor<1x12x1x32xf32>) -> tensor<1x12x1x32xf32>
    %280 = ttir.empty() : tensor<1x12x1xf32>
    %281 = "ttir.max"(%279, %280) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x12x1x32xf32>, tensor<1x12x1xf32>) -> tensor<1x12x1xf32>
    %282 = ttir.empty() : tensor<1x12x1x1xf32>
    %283 = "ttir.reshape"(%281, %282) <{shape = [1 : i32, 12 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x1xf32>, tensor<1x12x1x1xf32>) -> tensor<1x12x1x1xf32>
    %284 = ttir.empty() : tensor<1x12x1x32xf32>
    %285 = "ttir.broadcast"(%283, %284) <{broadcast_dimensions = array<i64: 1, 1, 1, 32>}> : (tensor<1x12x1x1xf32>, tensor<1x12x1x32xf32>) -> tensor<1x12x1x32xf32>
    %286 = ttir.empty() : tensor<1x12x1x32xf32>
    %287 = "ttir.subtract"(%279, %285, %286) : (tensor<1x12x1x32xf32>, tensor<1x12x1x32xf32>, tensor<1x12x1x32xf32>) -> tensor<1x12x1x32xf32>
    %288 = ttir.empty() : tensor<1x12x1x32xf32>
    %289 = "ttir.exp"(%287, %288) : (tensor<1x12x1x32xf32>, tensor<1x12x1x32xf32>) -> tensor<1x12x1x32xf32>
    %290 = ttir.empty() : tensor<1x12x1xf32>
    %291 = "ttir.sum"(%289, %290) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x12x1x32xf32>, tensor<1x12x1xf32>) -> tensor<1x12x1xf32>
    %292 = ttir.empty() : tensor<1x12x1x1xf32>
    %293 = "ttir.reshape"(%291, %292) <{shape = [1 : i32, 12 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x1xf32>, tensor<1x12x1x1xf32>) -> tensor<1x12x1x1xf32>
    %294 = ttir.empty() : tensor<1x12x1x32xf32>
    %295 = "ttir.broadcast"(%293, %294) <{broadcast_dimensions = array<i64: 1, 1, 1, 32>}> : (tensor<1x12x1x1xf32>, tensor<1x12x1x32xf32>) -> tensor<1x12x1x32xf32>
    %296 = ttir.empty() : tensor<1x12x1x32xf32>
    %297 = "ttir.div"(%289, %295, %296) : (tensor<1x12x1x32xf32>, tensor<1x12x1x32xf32>, tensor<1x12x1x32xf32>) -> tensor<1x12x1x32xf32>
    %298 = ttir.empty() : tensor<1x12x1x32xbf16>
    %299 = "ttir.typecast"(%297, %298) <{conservative_folding = false}> : (tensor<1x12x1x32xf32>, tensor<1x12x1x32xbf16>) -> tensor<1x12x1x32xbf16>
    %300 = ttir.empty() : tensor<12x1x32xbf16>
    %301 = "ttir.reshape"(%299, %300) <{shape = [12 : i32, 1 : i32, 32 : i32]}> : (tensor<1x12x1x32xbf16>, tensor<12x1x32xbf16>) -> tensor<12x1x32xbf16>
    %302 = ttir.empty() : tensor<1x512x3072xbf16>
    %303 = "ttir.reshape"(%11, %302) <{shape = [1 : i32, 512 : i32, 3072 : i32]}> : (tensor<512x3072xbf16>, tensor<1x512x3072xbf16>) -> tensor<1x512x3072xbf16>
    %304 = ttir.empty() : tensor<512x3072xbf16>
    %305 = "ttir.reshape"(%303, %304) <{shape = [512 : i32, 3072 : i32]}> : (tensor<1x512x3072xbf16>, tensor<512x3072xbf16>) -> tensor<512x3072xbf16>
    %306 = ttir.empty() : tensor<3072x512xbf16>
    %307 = "ttir.permute"(%305, %306) <{permutation = array<i64: 1, 0>}> : (tensor<512x3072xbf16>, tensor<3072x512xbf16>) -> tensor<3072x512xbf16>
    %308 = "ttir.dot_general"(%107, %307) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x512xbf16>) -> tensor<1x512xbf16>
    %309 = ttir.empty() : tensor<1x4x1x128xbf16>
    %310 = "ttir.reshape"(%308, %309) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32]}> : (tensor<1x512xbf16>, tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xbf16>
    %311 = ttir.empty() : tensor<1x4x32x128xbf16>
    %312 = "ttir.scatter"(%23, %185, %310, %311) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x4x32x128xbf16>, tensor<1x1xi64>, tensor<1x4x1x128xbf16>, tensor<1x4x32x128xbf16>) -> tensor<1x4x32x128xbf16>
    %313 = ttir.empty() : tensor<1x4x1x32x128xbf16>
    %314 = "ttir.reshape"(%312, %313) <{shape = [1 : i32, 4 : i32, 1 : i32, 32 : i32, 128 : i32]}> : (tensor<1x4x32x128xbf16>, tensor<1x4x1x32x128xbf16>) -> tensor<1x4x1x32x128xbf16>
    %315 = ttir.empty() : tensor<1x4x3x32x128xbf16>
    %316 = "ttir.broadcast"(%314, %315) <{broadcast_dimensions = array<i64: 1, 1, 3, 1, 1>}> : (tensor<1x4x1x32x128xbf16>, tensor<1x4x3x32x128xbf16>) -> tensor<1x4x3x32x128xbf16>
    %317 = ttir.empty() : tensor<12x32x128xbf16>
    %318 = "ttir.reshape"(%316, %317) <{shape = [12 : i32, 32 : i32, 128 : i32]}> : (tensor<1x4x3x32x128xbf16>, tensor<12x32x128xbf16>) -> tensor<12x32x128xbf16>
    %319 = "ttir.dot_general"(%301, %318) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<12x1x32xbf16>, tensor<12x32x128xbf16>) -> tensor<12x1x128xbf16>
    %320 = ttir.empty() : tensor<1x1536xbf16>
    %321 = "ttir.reshape"(%319, %320) <{shape = [1 : i32, 1536 : i32]}> : (tensor<12x1x128xbf16>, tensor<1x1536xbf16>) -> tensor<1x1536xbf16>
    %322 = ttir.empty() : tensor<1x3072x1536xbf16>
    %323 = "ttir.reshape"(%9, %322) <{shape = [1 : i32, 3072 : i32, 1536 : i32]}> : (tensor<3072x1536xbf16>, tensor<1x3072x1536xbf16>) -> tensor<1x3072x1536xbf16>
    %324 = ttir.empty() : tensor<3072x1536xbf16>
    %325 = "ttir.reshape"(%323, %324) <{shape = [3072 : i32, 1536 : i32]}> : (tensor<1x3072x1536xbf16>, tensor<3072x1536xbf16>) -> tensor<3072x1536xbf16>
    %326 = ttir.empty() : tensor<1536x3072xbf16>
    %327 = "ttir.permute"(%325, %326) <{permutation = array<i64: 1, 0>}> : (tensor<3072x1536xbf16>, tensor<1536x3072xbf16>) -> tensor<1536x3072xbf16>
    %328 = "ttir.dot_general"(%321, %327) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x1536xbf16>, tensor<1536x3072xbf16>) -> tensor<1x3072xbf16>
    %329 = ttir.empty() : tensor<1x3072xbf16>
    %330 = "ttir.all_reduce"(%328, %329) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<1x3072xbf16>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %331 = ttir.empty() : tensor<1x1x3072xbf16>
    %332 = "ttir.reshape"(%330, %331) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %333 = ttir.empty() : tensor<1x1x3072xbf16>
    %334 = "ttir.add"(%69, %332, %333) : (tensor<1x1x3072xbf16>, tensor<1x1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %335 = ttir.empty() : tensor<1x1x3072xbf16>
    %336 = "ttir.reshape"(%37, %335) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %337 = ttir.empty() : tensor<1x1x3072xf32>
    %338 = "ttir.typecast"(%336, %337) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %339 = ttir.empty() : tensor<1x1x3072xf32>
    %340 = "ttir.typecast"(%334, %339) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %341 = ttir.empty() : tensor<1x1x3072xf32>
    %342 = "ttir.pow"(%340, %51, %341) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %343 = ttir.empty() : tensor<1x1xf32>
    %344 = "ttir.sum"(%342, %343) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %345 = ttir.empty() : tensor<1x1xf32>
    %346 = "ttir.multiply"(%344, %46, %345) : (tensor<1x1xf32>, tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %347 = ttir.empty() : tensor<1x1x1xf32>
    %348 = "ttir.reshape"(%346, %347) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %349 = ttir.empty() : tensor<1x1x1xf32>
    %350 = "ttir.add"(%348, %85, %349) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %351 = ttir.empty() : tensor<1x1x1xf32>
    %352 = "ttir.rsqrt"(%350, %351) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %353 = ttir.empty() : tensor<1x1xf32>
    %354 = "ttir.reshape"(%352, %353) <{shape = [1 : i32, 1 : i32]}> : (tensor<1x1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %355 = ttir.empty() : tensor<1x1x1xf32>
    %356 = "ttir.reshape"(%354, %355) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %357 = ttir.empty() : tensor<1x1x3072xf32>
    %358 = "ttir.broadcast"(%356, %357) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x1x1xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %359 = ttir.empty() : tensor<1x1x3072xf32>
    %360 = "ttir.multiply"(%340, %358, %359) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %361 = ttir.empty() : tensor<1x1x3072xbf16>
    %362 = "ttir.typecast"(%360, %361) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %363 = ttir.empty() : tensor<1x1x3072xf32>
    %364 = "ttir.typecast"(%362, %363) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %365 = ttir.empty() : tensor<1x1x3072xf32>
    %366 = "ttir.multiply"(%338, %364, %365) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %367 = ttir.empty() : tensor<1x1x3072xbf16>
    %368 = "ttir.typecast"(%366, %367) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %369 = ttir.empty() : tensor<1x3072xbf16>
    %370 = "ttir.reshape"(%368, %369) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x3072xbf16>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %371 = ttir.empty() : tensor<1x4096x3072xbf16>
    %372 = "ttir.reshape"(%39, %371) <{shape = [1 : i32, 4096 : i32, 3072 : i32]}> : (tensor<4096x3072xbf16>, tensor<1x4096x3072xbf16>) -> tensor<1x4096x3072xbf16>
    %373 = ttir.empty() : tensor<4096x3072xbf16>
    %374 = "ttir.reshape"(%372, %373) <{shape = [4096 : i32, 3072 : i32]}> : (tensor<1x4096x3072xbf16>, tensor<4096x3072xbf16>) -> tensor<4096x3072xbf16>
    %375 = ttir.empty() : tensor<3072x4096xbf16>
    %376 = "ttir.permute"(%374, %375) <{permutation = array<i64: 1, 0>}> : (tensor<4096x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<3072x4096xbf16>
    %377 = "ttir.dot_general"(%370, %376) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<1x4096xbf16>
    %378 = ttir.empty() : tensor<1x1x4096xbf16>
    %379 = "ttir.reshape"(%377, %378) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<1x4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
    %380 = ttir.empty() : tensor<1x1x4096xf32>
    %381 = "ttir.typecast"(%379, %380) <{conservative_folding = false}> : (tensor<1x1x4096xbf16>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %382 = ttir.empty() : tensor<1x1x4096xbf16>
    %383 = "ttir.sigmoid"(%379, %382) : (tensor<1x1x4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
    %384 = ttir.empty() : tensor<1x1x4096xf32>
    %385 = "ttir.typecast"(%383, %384) <{conservative_folding = false}> : (tensor<1x1x4096xbf16>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %386 = ttir.empty() : tensor<1x1x4096xf32>
    %387 = "ttir.multiply"(%381, %385, %386) : (tensor<1x1x4096xf32>, tensor<1x1x4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %388 = ttir.empty() : tensor<1x1x4096xbf16>
    %389 = "ttir.typecast"(%387, %388) <{conservative_folding = false}> : (tensor<1x1x4096xf32>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
    %390 = ttir.empty() : tensor<1x1x4096xf32>
    %391 = "ttir.typecast"(%389, %390) <{conservative_folding = false}> : (tensor<1x1x4096xbf16>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %392 = ttir.empty() : tensor<1x4096x3072xbf16>
    %393 = "ttir.reshape"(%7, %392) <{shape = [1 : i32, 4096 : i32, 3072 : i32]}> : (tensor<4096x3072xbf16>, tensor<1x4096x3072xbf16>) -> tensor<1x4096x3072xbf16>
    %394 = ttir.empty() : tensor<4096x3072xbf16>
    %395 = "ttir.reshape"(%393, %394) <{shape = [4096 : i32, 3072 : i32]}> : (tensor<1x4096x3072xbf16>, tensor<4096x3072xbf16>) -> tensor<4096x3072xbf16>
    %396 = ttir.empty() : tensor<3072x4096xbf16>
    %397 = "ttir.permute"(%395, %396) <{permutation = array<i64: 1, 0>}> : (tensor<4096x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<3072x4096xbf16>
    %398 = "ttir.dot_general"(%370, %397) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<1x4096xbf16>
    %399 = ttir.empty() : tensor<1x4096xf32>
    %400 = "ttir.typecast"(%398, %399) <{conservative_folding = false}> : (tensor<1x4096xbf16>, tensor<1x4096xf32>) -> tensor<1x4096xf32>
    %401 = ttir.empty() : tensor<1x1x4096xf32>
    %402 = "ttir.reshape"(%400, %401) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<1x4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %403 = ttir.empty() : tensor<1x1x4096xf32>
    %404 = "ttir.multiply"(%391, %402, %403) : (tensor<1x1x4096xf32>, tensor<1x1x4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %405 = ttir.empty() : tensor<1x1x4096xbf16>
    %406 = "ttir.typecast"(%404, %405) <{conservative_folding = false}> : (tensor<1x1x4096xf32>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
    %407 = ttir.empty() : tensor<1x4096xbf16>
    %408 = "ttir.reshape"(%406, %407) <{shape = [1 : i32, 4096 : i32]}> : (tensor<1x1x4096xbf16>, tensor<1x4096xbf16>) -> tensor<1x4096xbf16>
    %409 = ttir.empty() : tensor<1x3072x4096xbf16>
    %410 = "ttir.reshape"(%5, %409) <{shape = [1 : i32, 3072 : i32, 4096 : i32]}> : (tensor<3072x4096xbf16>, tensor<1x3072x4096xbf16>) -> tensor<1x3072x4096xbf16>
    %411 = ttir.empty() : tensor<3072x4096xbf16>
    %412 = "ttir.reshape"(%410, %411) <{shape = [3072 : i32, 4096 : i32]}> : (tensor<1x3072x4096xbf16>, tensor<3072x4096xbf16>) -> tensor<3072x4096xbf16>
    %413 = ttir.empty() : tensor<4096x3072xbf16>
    %414 = "ttir.permute"(%412, %413) <{permutation = array<i64: 1, 0>}> : (tensor<3072x4096xbf16>, tensor<4096x3072xbf16>) -> tensor<4096x3072xbf16>
    %415 = "ttir.dot_general"(%408, %414) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x4096xbf16>, tensor<4096x3072xbf16>) -> tensor<1x3072xbf16>
    %416 = ttir.empty() : tensor<1x3072xbf16>
    %417 = "ttir.all_reduce"(%415, %416) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<1x3072xbf16>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %418 = ttir.empty() : tensor<1x1x3072xbf16>
    %419 = "ttir.reshape"(%417, %418) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %420 = ttir.empty() : tensor<1x1x3072xbf16>
    %421 = "ttir.add"(%334, %419, %420) : (tensor<1x1x3072xbf16>, tensor<1x1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %422 = ttir.empty() : tensor<1x1x3072xf32>
    %423 = "ttir.typecast"(%421, %422) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %424 = ttir.empty() : tensor<1x1x3072xf32>
    %425 = "ttir.pow"(%423, %51, %424) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %426 = ttir.empty() : tensor<1x1xf32>
    %427 = "ttir.sum"(%425, %426) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %428 = ttir.empty() : tensor<1x1xf32>
    %429 = "ttir.multiply"(%427, %46, %428) : (tensor<1x1xf32>, tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %430 = ttir.empty() : tensor<1x1x1xf32>
    %431 = "ttir.reshape"(%429, %430) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %432 = ttir.empty() : tensor<1x1x1xf32>
    %433 = "ttir.add"(%431, %85, %432) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %434 = ttir.empty() : tensor<1x1x1xf32>
    %435 = "ttir.rsqrt"(%433, %434) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %436 = ttir.empty() : tensor<1x1xf32>
    %437 = "ttir.reshape"(%435, %436) <{shape = [1 : i32, 1 : i32]}> : (tensor<1x1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %438 = ttir.empty() : tensor<1x1x1xf32>
    %439 = "ttir.reshape"(%437, %438) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %440 = ttir.empty() : tensor<1x1x3072xf32>
    %441 = "ttir.broadcast"(%439, %440) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x1x1xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %442 = ttir.empty() : tensor<1x1x3072xf32>
    %443 = "ttir.multiply"(%423, %441, %442) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %444 = ttir.empty() : tensor<1x1x3072xbf16>
    %445 = "ttir.typecast"(%443, %444) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %446 = ttir.empty() : tensor<1x1x3072xf32>
    %447 = "ttir.typecast"(%445, %446) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %448 = ttir.empty() : tensor<1x1x3072xf32>
    %449 = "ttir.multiply"(%55, %447, %448) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %450 = ttir.empty() : tensor<1x1x3072xbf16>
    %451 = "ttir.typecast"(%449, %450) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %452 = ttir.empty() : tensor<1x3072xbf16>
    %453 = "ttir.reshape"(%451, %452) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x3072xbf16>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %454 = ttir.empty() : tensor<1x128256x3072xbf16>
    %455 = "ttir.reshape"(%1, %454) <{shape = [1 : i32, 128256 : i32, 3072 : i32]}> : (tensor<128256x3072xbf16>, tensor<1x128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %456 = ttir.empty() : tensor<128256x3072xbf16>
    %457 = "ttir.reshape"(%455, %456) <{shape = [128256 : i32, 3072 : i32]}> : (tensor<1x128256x3072xbf16>, tensor<128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %458 = ttir.empty() : tensor<3072x128256xbf16>
    %459 = "ttir.permute"(%457, %458) <{permutation = array<i64: 1, 0>}> : (tensor<128256x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<3072x128256xbf16>
    %460 = "ttir.dot_general"(%453, %459) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<1x128256xbf16>
    %461 = ttir.empty() : tensor<1x1x128256xbf16>
    %462 = "ttir.reshape"(%460, %461) <{shape = [1 : i32, 1 : i32, 128256 : i32]}> : (tensor<1x128256xbf16>, tensor<1x1x128256xbf16>) -> tensor<1x1x128256xbf16>
    %463 = ttir.empty() : tensor<1x1x128256xbf16>
    %464 = "ttir.mesh_shard"(%462, %463) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x1x128256xbf16>, tensor<1x1x128256xbf16>) -> tensor<1x1x128256xbf16>
    return %464 : tensor<1x1x128256xbf16>
  }
}
2025-09-10 19:55:21.787 (  88.663s) [        F1708000]      module_builder.cc:561   WARN| `mhlo.num_partitions` attribute not found, assuming default number of partitions: 1
2025-09-10 19:55:21.787 (  88.663s) [        F1708000]      module_builder.cc:575   WARN| `mhlo.num_replicas` attribute not found, assuming default number of replicas: 1
2025-09-10 19:55:21.787 (  88.663s) [        F1708000]      module_builder.cc:585   WARN| Num replicas and num partitions are not set, inferring the number of devices from mesh shape
2025-09-10 19:55:21.856 (  88.732s) [        F1708000]      module_builder.cc:630      1| TTNN Module:
module @SyncTensorsGraph.420 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.420 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073185088, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073193408, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x16, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 2)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 2, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main_const_eval_0(%arg0: tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_1(%arg0: tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_2(%arg0: tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %3 : tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_3(%arg0: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_4(%arg0: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.repeat"(%2) <{repeat_dims = #ttnn.shape<1x12x1x32>}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %4 = "ttnn.reshape"(%3) <{shape = [12 : i32, 1 : i32, 32 : i32]}> : (tensor<1x12x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x12x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %4 : tensor<12x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_5(%arg0: tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_6(%arg0: tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_7(%arg0: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %3 : tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_8(%arg0: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %3 : tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_9(%arg0: tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_10(%arg0: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_11(%arg0: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %3 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %4 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %2, %3, %4 : tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_12(%arg0: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %3 : tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_13() -> tensor<1x1x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.constant"(%0) <{dtype = #ttcore.supportedDataTypes<si32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <interleaved>>, value = dense<[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]]> : tensor<1x32xsi32>}> : (!ttnn.device) -> tensor<1x32xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32, 32 : i32]}> : (tensor<1x32xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x32xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x32xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x32xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1x1x32xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %3 : tensor<1x1x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_14(%arg0: tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_15() -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %2 : tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_16(%arg0: tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main(%arg0: tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg1: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg2: tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg3: tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg4: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg5: tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg6: tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg7: tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg8: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg9: tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg10: tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg11: tensor<1x8x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg12: tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg13: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg14: tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg15: tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg16: tensor<1x8x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg17: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, [%arg0]) : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg10) <{force = false}> : (tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %1 = ttcore.load_cached(@main_const_eval_1, [%arg7]) : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg7) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %2 = ttcore.load_cached(@main_const_eval_2, [%arg12]) : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg12) <{force = false}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = ttcore.load_cached(@main_const_eval_3, [%arg17]) : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg17) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %4 = ttcore.load_cached(@main_const_eval_4, [%arg13]) : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg13) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %5 = ttcore.load_cached(@main_const_eval_5, [%arg19]) : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg19) <{force = false}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %6 = ttcore.load_cached(@main_const_eval_6, [%arg5]) : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg5) <{force = false}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %7 = ttcore.load_cached(@main_const_eval_7, [%arg18]) : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg18) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %8 = ttcore.load_cached(@main_const_eval_8, [%arg8]) : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg8) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %9 = ttcore.load_cached(@main_const_eval_9, [%arg2]) : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %10 = ttcore.load_cached(@main_const_eval_10, [%arg4]) : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %11:3 = ttcore.load_cached(@main_const_eval_11, [%arg1]) : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>)
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %12 = ttcore.load_cached(@main_const_eval_12, [%arg20]) : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg20) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %13 = ttcore.load_cached(@main_const_eval_13, []) : () -> tensor<1x1x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %14 = ttcore.load_cached(@main_const_eval_14, [%arg15]) : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg15) <{force = false}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %15 = ttcore.load_cached(@main_const_eval_15, []) : () -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %16 = ttcore.load_cached(@main_const_eval_16, [%arg3]) : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %17 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %18 = "ttnn.full"(%17) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 3.25520843E-4 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %19 = "ttnn.mesh_shard"(%arg6, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg6) <{force = false}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %20 = "ttnn.mesh_shard"(%arg9, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %21 = "ttnn.mesh_shard"(%arg11, %17) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg11) <{force = false}> : (tensor<1x8x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %22 = "ttnn.mesh_shard"(%arg14, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg14) <{force = false}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %23 = "ttnn.mesh_shard"(%arg16, %17) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg16) <{force = false}> : (tensor<1x8x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %24 = "ttnn.typecast"(%19) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %25 = "ttnn.reshape"(%24) <{shape = [1 : i32]}> : (tensor<1x1xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%24) <{force = false}> : (tensor<1x1xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %26 = "ttnn.from_device"(%25) : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%25) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %27 = "ttnn.to_layout"(%26) <{layout = #ttnn.layout<row_major>}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%26) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> ()
        %28 = "ttnn.to_device"(%27, %17) <{memory_config = #ttnn.memory_config<<dram>, <interleaved>>}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%27) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<system_memory>>>>) -> ()
        %29 = "ttnn.embedding"(%28, %1) : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%28) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %30 = "ttnn.typecast"(%29) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %31 = "ttnn.reshape"(%30) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %32 = "ttnn.pow"(%31, %15) : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%31) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %33 = "ttnn.sum"(%32) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%32) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %34 = "ttnn.multiply"(%33, %18) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%33) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %35 = "ttnn.add"(%34, %11#0) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%34) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%11#0) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %36 = "ttnn.rsqrt"(%35) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%35) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %37 = "ttnn.multiply"(%30, %36) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%36) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%30) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %38 = "ttnn.multiply"(%8, %37) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%37) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %39 = "ttnn.typecast"(%38) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%38) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %40 = "ttnn.matmul"(%39, %3) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %41 = "ttnn.reshape"(%40) <{shape = [1 : i32, 12 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %42 = "ttnn.typecast"(%40) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1536xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%40) <{force = false}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %43 = "ttnn.reshape"(%42) <{shape = [12 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1536xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%42) <{force = false}> : (tensor<1x1536xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %44 = "ttnn.reshape"(%22) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%22) <{force = false}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %45 = "ttnn.typecast"(%20) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %46 = "ttnn.reshape"(%45) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%45) <{force = false}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %47 = "ttnn.matmul"(%44, %46) <{transpose_a = false, transpose_b = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%46) <{force = false}> : (tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%44) <{force = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %48 = "ttnn.reshape"(%47) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%47) <{force = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %49 = "ttnn.concat"(%48, %48) <{dim = 2 : si32}> : (tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%48) <{force = false}> : (tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %50 = "ttnn.cos"(%49) : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %51 = "ttnn.multiply"(%43, %50) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%43) <{force = false}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %52 = "ttnn.typecast"(%51) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%51) <{force = false}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %53 = "ttnn.slice_static"(%41) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 12 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %54 = "ttnn.neg"(%53) : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%53) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %55 = "ttnn.reshape"(%54) <{shape = [12 : i32, 1 : i32, 64 : i32]}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%54) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %56 = "ttnn.slice_static"(%41) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 12 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%41) <{force = false}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %57 = "ttnn.reshape"(%56) <{shape = [12 : i32, 1 : i32, 64 : i32]}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%56) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %58 = "ttnn.concat"(%55, %57) <{dim = 2 : si32}> : (tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%57) <{force = false}> : (tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%55) <{force = false}> : (tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %59 = "ttnn.typecast"(%58) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%58) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %60 = "ttnn.sin"(%49) : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%49) <{force = false}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %61 = "ttnn.multiply"(%59, %60) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%59) <{force = false}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %62 = "ttnn.typecast"(%61) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%61) <{force = false}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %63 = "ttnn.add"(%52, %62) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%62) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%52) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %64 = "ttnn.matmul"(%39, %14) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %65 = "ttnn.reshape"(%64) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32]}> : (tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%64) <{force = false}> : (tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %66 = "ttnn.typecast"(%65) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %67 = "ttnn.reshape"(%50) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%50) <{force = false}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %68 = "ttnn.multiply"(%66, %67) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%67) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%66) <{force = false}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %69 = "ttnn.typecast"(%68) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%68) <{force = false}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %70 = "ttnn.slice_static"(%65) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %71 = "ttnn.neg"(%70) : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%70) <{force = false}> : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %72 = "ttnn.slice_static"(%65) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%65) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %73 = "ttnn.concat"(%71, %72) <{dim = 3 : si32}> : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%72) <{force = false}> : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%71) <{force = false}> : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %74 = "ttnn.typecast"(%73) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%73) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %75 = "ttnn.reshape"(%60) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%60) <{force = false}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %76 = "ttnn.multiply"(%74, %75) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%75) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%74) <{force = false}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %77 = "ttnn.typecast"(%76) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%76) <{force = false}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %78 = "ttnn.add"(%69, %77) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%77) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%69) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %79 = "ttnn.typecast"(%arg9) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.update_cache"(%23, %78, %79) <{batch_offset = 0 : i32}> : (tensor<1x4x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%79) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%78) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %80 = "ttnn.reshape"(%23) <{shape = [1 : i32, 4 : i32, 1 : i32, 32 : i32, 128 : i32]}> : (tensor<1x4x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 128 + d1 * 32 + d2 * 32 + d3, d4), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%23) <{force = false}> : (tensor<1x4x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %81 = "ttnn.repeat"(%80) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x4x1x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 128 + d1 * 32 + d2 * 32 + d3, d4), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x3x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 384 + d1 * 96 + d2 * 32 + d3, d4), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%80) <{force = false}> : (tensor<1x4x1x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 128 + d1 * 32 + d2 * 32 + d3, d4), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %82 = "ttnn.reshape"(%81) <{shape = [1 : i32, 12 : i32, 32 : i32, 128 : i32]}> : (tensor<1x4x3x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 384 + d1 * 96 + d2 * 32 + d3, d4), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%81) <{force = false}> : (tensor<1x4x3x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 384 + d1 * 96 + d2 * 32 + d3, d4), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %83 = "ttnn.permute"(%82) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x12x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x128x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%82) <{force = false}> : (tensor<1x12x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %84 = "ttnn.reshape"(%83) <{shape = [12 : i32, 128 : i32, 32 : i32]}> : (tensor<1x12x128x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x128x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%83) <{force = false}> : (tensor<1x12x128x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %85 = "ttnn.matmul"(%63, %84) <{transpose_a = false, transpose_b = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x128x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%84) <{force = false}> : (tensor<12x128x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%63) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %86 = "ttnn.typecast"(%85) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%85) <{force = false}> : (tensor<12x1x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %87 = "ttnn.multiply"(%86, %4) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%86) <{force = false}> : (tensor<12x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<12x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %88 = "ttnn.typecast"(%87) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%87) <{force = false}> : (tensor<12x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %89 = "ttnn.reshape"(%88) <{shape = [1 : i32, 12 : i32, 1 : i32, 32 : i32]}> : (tensor<12x1x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%88) <{force = false}> : (tensor<12x1x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %90 = "ttnn.reshape"(%20) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %91 = "ttnn.typecast"(%90) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%90) <{force = false}> : (tensor<1x1x1x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %92 = "ttnn.gt"(%13, %91) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%91) <{force = false}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %93 = "ttnn.typecast"(%92) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%92) <{force = false}> : (tensor<1x1x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %94 = "ttnn.typecast"(%93) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%93) <{force = false}> : (tensor<1x1x1x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %95 = "ttnn.multiply"(%2, %94) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%94) <{force = false}> : (tensor<1x1x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %96 = "ttnn.typecast"(%95) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%95) <{force = false}> : (tensor<1x1x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %97 = "ttnn.add"(%89, %96) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x12x1x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%96) <{force = false}> : (tensor<1x1x1x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%89) <{force = false}> : (tensor<1x12x1x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %98 = "ttnn.typecast"(%97) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x12x1x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%97) <{force = false}> : (tensor<1x12x1x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %99 = "ttnn.softmax"(%98) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x12x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%98) <{force = false}> : (tensor<1x12x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %100 = "ttnn.typecast"(%99) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x12x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%99) <{force = false}> : (tensor<1x12x1x32xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %101 = "ttnn.reshape"(%100) <{shape = [12 : i32, 1 : i32, 32 : i32]}> : (tensor<1x12x1x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%100) <{force = false}> : (tensor<1x12x1x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %102 = "ttnn.matmul"(%39, %6) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%39) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %103 = "ttnn.reshape"(%102) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32]}> : (tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%102) <{force = false}> : (tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %104 = "ttnn.typecast"(%arg9) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg9) <{force = false}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.update_cache"(%21, %103, %104) <{batch_offset = 0 : i32}> : (tensor<1x4x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%104) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%103) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %105 = "ttnn.reshape"(%21) <{shape = [1 : i32, 4 : i32, 1 : i32, 32 : i32, 128 : i32]}> : (tensor<1x4x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 128 + d1 * 32 + d2 * 32 + d3, d4), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%21) <{force = false}> : (tensor<1x4x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %106 = "ttnn.repeat"(%105) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x4x1x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 128 + d1 * 32 + d2 * 32 + d3, d4), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x3x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 384 + d1 * 96 + d2 * 32 + d3, d4), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%105) <{force = false}> : (tensor<1x4x1x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 128 + d1 * 32 + d2 * 32 + d3, d4), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %107 = "ttnn.reshape"(%106) <{shape = [12 : i32, 32 : i32, 128 : i32]}> : (tensor<1x4x3x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 384 + d1 * 96 + d2 * 32 + d3, d4), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%106) <{force = false}> : (tensor<1x4x3x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 384 + d1 * 96 + d2 * 32 + d3, d4), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %108 = "ttnn.matmul"(%101, %107) <{transpose_a = false, transpose_b = false}> : (tensor<12x1x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%107) <{force = false}> : (tensor<12x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%101) <{force = false}> : (tensor<12x1x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %109 = "ttnn.reshape"(%108) <{shape = [1 : i32, 1536 : i32]}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%108) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %110 = "ttnn.matmul"(%109, %10) <{transpose_a = false, transpose_b = true}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%109) <{force = false}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %111 = "ttnn.reshape"(%110) <{shape = [1 : i32, 1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%110) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %112 = "ttnn.reduce_scatter"(%111, %17) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 3 : si32}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%111) <{force = false}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %113 = "ttnn.all_gather"(%112, %17) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%112) <{force = false}> : (tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %114 = "ttnn.reshape"(%113) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%113) <{force = false}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %115 = "ttnn.add"(%29, %114) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%114) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%29) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %116 = "ttnn.typecast"(%115) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %117 = "ttnn.reshape"(%116) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %118 = "ttnn.pow"(%117, %15) : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%117) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %119 = "ttnn.sum"(%118) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%118) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %120 = "ttnn.multiply"(%119, %18) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%119) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %121 = "ttnn.add"(%120, %11#1) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%120) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%11#1) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %122 = "ttnn.rsqrt"(%121) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%121) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %123 = "ttnn.multiply"(%116, %122) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%122) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%116) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %124 = "ttnn.multiply"(%7, %123) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%123) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %125 = "ttnn.typecast"(%124) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%124) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %126 = "ttnn.matmul"(%125, %5) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %127 = "ttnn.typecast"(%126) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %128 = "ttnn.sigmoid"(%126) : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%126) <{force = false}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %129 = "ttnn.typecast"(%128) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%128) <{force = false}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %130 = "ttnn.multiply"(%127, %129) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%129) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%127) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %131 = "ttnn.matmul"(%125, %16) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%125) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %132 = "ttnn.typecast"(%131) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%131) <{force = false}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %133 = "ttnn.multiply"(%130, %132) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%132) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%130) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %134 = "ttnn.typecast"(%133) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%133) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %135 = "ttnn.matmul"(%134, %9) <{transpose_a = false, transpose_b = true}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%134) <{force = false}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %136 = "ttnn.reshape"(%135) <{shape = [1 : i32, 1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%135) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %137 = "ttnn.reduce_scatter"(%136, %17) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 3 : si32}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%136) <{force = false}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %138 = "ttnn.all_gather"(%137, %17) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%137) <{force = false}> : (tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %139 = "ttnn.reshape"(%138) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%138) <{force = false}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %140 = "ttnn.add"(%115, %139) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%139) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%115) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %141 = "ttnn.typecast"(%140) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%140) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %142 = "ttnn.reshape"(%141) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %143 = "ttnn.pow"(%142, %15) : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%142) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %144 = "ttnn.sum"(%143) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%143) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %145 = "ttnn.multiply"(%144, %18) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%144) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %146 = "ttnn.add"(%145, %11#2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%145) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%11#2) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %147 = "ttnn.rsqrt"(%146) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%146) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %148 = "ttnn.multiply"(%141, %147) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%147) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%141) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %149 = "ttnn.multiply"(%12, %148) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%148) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %150 = "ttnn.typecast"(%149) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%149) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %151 = "ttnn.matmul"(%150, %0) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%150) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %152 = "ttnn.reshape"(%151) <{shape = [1 : i32, 1 : i32, 128256 : i32]}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%151) <{force = false}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %153 = "ttnn.to_layout"(%152) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%152) <{force = false}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %154 = "ttnn.from_device"(%153) : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%153) <{force = false}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %155 = "ttnn.mesh_shard"(%154, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%154) <{force = false}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>) -> ()
        return %155 : tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>
      }
    }
  }
}
2025-09-10 19:55:21.894 (  88.770s) [        F1708000]loaded_executable_insta:471      1| LoadedExecutableInstance::PJRT_LoadedExecutable_GetExecutable
2025-09-10 19:55:21.894 (  88.770s) [        F1708000]loaded_executable_insta:490      1| LoadedExecutableInstance::PJRT_LoadedExecutable_AddressableDevices
2025-09-10 19:55:21.894 (  88.770s) [        F1708000]              stubs.inc:76    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2025-09-10 19:55:21.894 (  88.770s) [        F1708000]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-09-10 19:55:21.894 (  88.770s) [        F1708000]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-09-10 19:55:21.894 (  88.770s) [        F1708000]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-09-10 19:55:21.894 (  88.770s) [        F1708000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-10 19:55:21.894 (  88.770s) [        F1708000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-10 19:55:21.905 (  88.781s) [        F1708000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-10 19:55:21.905 (  88.781s) [        F1708000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-10 19:55:21.915 (  88.791s) [        F1708000]              stubs.inc:76    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2025-09-10 19:55:21.915 (  88.791s) [        F1708000]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-09-10 19:55:21.915 (  88.791s) [        F1708000]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-09-10 19:55:21.915 (  88.791s) [        F1708000]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy

Computation hash: c4e28cde2628c627962d30cfb85089f

Post Compilation Analysis: ================================================================================
Post Compilation Analysis: Graph input size: Unknown  GB
Post Compilation Analysis: Graph output size: Unknown  GB
Post Compilation Analysis: Aliased Input size: Unknown  GB
Post Compilation Analysis: Intermediate tensor size: Unknown  GB
Post Compilation Analysis: Compiled program size: Unknown  GB
Post Compilation Analysis: --------------------------------------------------------------------------------
Post Compilation Analysis: ================================================================================

Execution Analysis: ================================================================================
Execution Analysis: Execution Cause
Execution Analysis:   most likely user code trying to access tensor value before torch_xla.sync
Execution Analysis: Graph Info: 
Execution Analysis:   Graph Hash: 5bb096aa11b17acc7688267ab7990745
Execution Analysis:   Number of Graph Inputs: 21
Execution Analysis:   Number of Graph Outputs: 1
Execution Analysis: Python Frame Triggered Execution: 
Execution Analysis:   __call__ (/localdev/jameszianxu/tt-xla/tt_torch/backend/backend.py:99)
Execution Analysis:   _fn (/localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:838)
Execution Analysis:   wrapper (/localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/utils/generic.py:953)
Execution Analysis:   _call_impl (/localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762)
Execution Analysis:   _fn (/localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:655)
Execution Analysis:   _wrapped_call_impl (/localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1749)
Execution Analysis:   llama (/localdev/jameszianxu/tt-xla/examples/pytorch/llama.py:155)
Execution Analysis:   <module> (/localdev/jameszianxu/tt-xla/examples/pytorch/llama.py:207)
Execution Analysis: --------------------------------------------------------------------------------
Execution Analysis: ================================================================================
2025-09-10 19:55:21.915 (  88.791s) [        8F7FE640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:55:21.915 (  88.791s) [        8F7FE640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-10 19:55:21.915 (  88.791s) [        8F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.915 (  88.791s) [        8F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.915 (  88.791s) [        8F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.915 (  88.791s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:21.915 (  88.791s) [        8F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.915 (  88.791s) [        8F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.915 (  88.791s) [        8F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.915 (  88.791s) [        8F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.915 (  88.791s) [        8F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.915 (  88.791s) [        8F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.915 (  88.791s) [        8F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.915 (  88.791s) [        8F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.915 (  88.791s) [        8F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.915 (  88.791s) [        8F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.915 (  88.791s) [        8F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.915 (  88.791s) [        8F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.915 (  88.791s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:21.915 (  88.791s) [        8F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.915 (  88.791s) [        8F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.915 (  88.791s) [        A17FA640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.915 (  88.791s) [        8F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.915 (  88.791s) [        A17FA640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.915 (  88.791s) [        8F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.915 (  88.791s) [        A0FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.915 (  88.791s) [        A1FFB640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.915 (  88.791s) [        8F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.916 (  88.791s) [        A1FFB640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.915 (  88.791s) [        A17FA640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.916 (  88.791s) [        A1FFB640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.916 (  88.791s) [        A17FA640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.915 (  88.791s) [        A0FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.916 (  88.791s) [        A17FA640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.916 (  88.791s) [        A1FFB640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.916 (  88.791s) [        A17FA640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.915 (  88.791s) [        A27FC640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.916 (  88.791s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:21.916 (  88.791s) [        A0FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.916 (  88.791s) [        A27FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.916 (  88.791s) [        A17FA640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.916 (  88.791s) [        A27FC640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.916 (  88.792s) [        A17FA640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.916 (  88.791s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:21.916 (  88.792s) [        A17FA640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.916 (  88.791s) [        A0FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.916 (  88.792s) [        A27FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.916 (  88.792s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:21.916 (  88.792s) [        A17FA640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.916 (  88.792s) [        A27FC640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.916 (  88.792s) [        A0FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.916 (  88.792s) [        A17FA640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.916 (  88.792s) [        A0FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.916 (  88.792s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:21.916 (  88.792s) [        A27FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.916 (  88.792s) [        A17FA640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.916 (  88.792s) [        A0FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.916 (  88.792s) [        A17FA640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.916 (  88.792s) [        A27FC640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.916 (  88.792s) [        A17FA640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.916 (  88.792s) [        A0FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.916 (  88.792s) [        A17FA640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.916 (  88.792s) [        A27FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.916 (  88.792s) [        A0FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.916 (  88.792s) [        A17FA640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.916 (  88.792s) [        A27FC640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.916 (  88.792s) [        A17FA640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.916 (  88.792s) [        A0FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.916 (  88.792s) [        A17FA640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.916 (  88.792s) [        A27FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.916 (  88.792s) [        A0FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.916 (  88.792s) [        A17FA640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.916 (  88.792s) [        A0FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.916 (  88.792s) [        A27FC640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.916 (  88.792s) [        A17FA640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.916 (  88.792s) [        A0FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.916 (  88.792s) [        A0FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.916 (  88.792s) [        A27FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.916 (  88.792s) [        A0FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.916 (  88.792s) [        A27FC640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.916 (  88.792s) [        A0FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.916 (  88.792s) [        A27FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.916 (  88.792s) [        A0FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.916 (  88.792s) [        A27FC640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.916 (  88.792s) [        A0FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.916 (  88.792s) [        A27FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.916 (  88.792s) [        A0FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.916 (  88.792s) [        A27FC640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.916 (  88.792s) [        A0FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.916 (  88.792s) [        A27FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.916 (  88.792s) [        A27FC640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-10 19:55:21.916 (  88.792s) [        A27FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-10 19:55:21.916 (  88.792s) [        8F7FE640] executable_instance.cc:138      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-09-10 19:55:21.916 (  88.792s) [        8F7FE640]loaded_executable_insta:527      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-09-10 19:55:21.916 (  88.792s) [        8F7FE640]loaded_executable_insta:96       1| LoadedExecutableInstance::Execute
2025-09-10 19:55:21.916 (  88.792s) [        8F7FE640]loaded_executable_insta:131      1| [DEVICE] Runtime device already opened, reusing existing device
2025-09-10 19:55:26.067 | warning  |              Op | get_forward_backward_line_unicast_configuration called: (ccl_common.cpp:1535)
2025-09-10 19:55:26.067 | warning  |              Op |   topology: 1 (ccl_common.cpp:1536)
2025-09-10 19:55:26.067 | warning  |              Op |   src_device: 0 (ccl_common.cpp:1537)
2025-09-10 19:55:26.067 | warning  |              Op |   forward_device: 1 (ccl_common.cpp:1538)
2025-09-10 19:55:26.067 | warning  |              Op |   backward_device: -1 (ccl_common.cpp:1539)
2025-09-10 19:55:26.067 | warning  |              Op |   fabric_config: 0 (ccl_common.cpp:1540)
2025-09-10 19:55:26.067 | warning  |              Op | Unsupported fabric config: 0 (ccl_common.cpp:1572)
2025-09-10 19:55:26.067 | critical |          Always | Unsupported fabric config (assert.hpp:107)
CausalLMOutputWithPast(loss=None, logits=tensor([[[-0.8633, -2.3438, -0.5430,  ...,  0.8477,  0.8477,  0.8477],
         [-0.1650,  0.2227,  0.8438,  ..., -0.8867, -0.8867, -0.8867],
         [ 1.8047, -1.4062, -0.8906,  ..., -1.4062, -1.4062, -1.4062],
         ...,
         [ 0.0461, -0.8633, -1.4531,  ..., -0.6562, -0.6484, -0.6484],
         [ 0.0527, -1.0859, -1.1094,  ..., -2.4062, -2.4062, -2.4062],
         [-1.1250,  0.0242, -0.7617,  ..., -1.5078, -1.5156, -1.5078]]],
       device='xla:0', dtype=torch.bfloat16), past_key_values=<transformers.cache_utils.StaticCache object at 0x7fe180246f10>, hidden_states=None, attentions=None)

Generated token @ step 0:  the
Traceback (most recent call last):
  File "/localdev/jameszianxu/tt-xla/examples/pytorch/llama.py", line 207, in <module>
    llama()
  File "/localdev/jameszianxu/tt-xla/examples/pytorch/llama.py", line 156, in llama
    print(output)
  File "/usr/lib/python3.11/dataclasses.py", line 240, in wrapper
    result = user_function(self)
             ^^^^^^^^^^^^^^^^^^^
  File "<string>", line 3, in __repr__
  File "/localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/torch/_tensor.py", line 590, in __repr__
    return torch._tensor_str._str(self, tensor_contents=tensor_contents)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/torch/_tensor_str.py", line 710, in _str
    return _str_intern(self, tensor_contents=tensor_contents)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/torch/_tensor_str.py", line 446, in _str_intern
    self = self.to("cpu")
           ^^^^^^^^^^^^^^
RuntimeError: TT_THROW @ /localdev/jameszianxu/tt-xla/third_party/tt-mlir/src/tt-mlir/third_party/tt-metal/src/tt-metal/ttnn/cpp/ttnn/operations/ccl/ccl_common.cpp:1573: tt::exception
info:
Unsupported fabric config
backtrace:
 --- /localdev/jameszianxu/tt-xla/third_party/tt-mlir/install/lib/libTTMLIRCompiler.so(+0xa1b2618) [0x7fe05a23e618]
 --- ttnn::ccl::get_forward_backward_line_unicast_configuration(tt::tt_fabric::Topology, tt::tt_metal::IDevice*, std::optional<tt::tt_metal::IDevice*>, std::optional<tt::tt_metal::IDevice*>)
 --- /localdev/jameszianxu/tt-xla/third_party/tt-mlir/install/lib/_ttnncpp.so(_ZN4ttnn40line_reduce_scatter_minimal_async_helperERN2tt8tt_metal7ProgramERKNS1_6TensorERS4_PNS1_7IDeviceESt8optionalIS9_ESB_S7_jjjjNS0_9tt_fabric8TopologyERKSt6vectorINS1_15GlobalSemaphoreESaISF_EERKSA_ISF_EbRKSA_IN4ttsl10StrongTypeIhNS1_14SubDeviceIdTagEEEERSA_INS_12experimental3ccl28ReduceScatterFusedOpSignalerEESA_IjESZ_SZ_NS0_3umd7xy_pairE+0xc8) [0x7fe063a39878]
 --- ttnn::reduce_scatter_minimal_async(tt::tt_metal::Tensor const&, tt::tt_metal::Tensor&, tt::tt_metal::IDevice*, std::optional<tt::tt_metal::IDevice*>, std::optional<tt::tt_metal::IDevice*>, tt::tt_metal::Tensor&, unsigned int, unsigned int, unsigned int, unsigned int, tt::tt_fabric::Topology, std::vector<tt::tt_metal::GlobalSemaphore, std::allocator<tt::tt_metal::GlobalSemaphore> > const&, std::optional<tt::tt_metal::GlobalSemaphore> const&, bool, std::optional<ttsl::StrongType<unsigned char, tt::tt_metal::SubDeviceIdTag> > const&, std::optional<unsigned int>, std::optional<unsigned int>, std::optional<unsigned int>)
 --- ttnn::ReduceScatterMinimalAsync::create_program_at(tt::tt_metal::distributed::MeshCoordinate const&, std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > const&, std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> >&) const
 --- /localdev/jameszianxu/tt-xla/third_party/tt-mlir/install/lib/_ttnncpp.so(+0xbc7317) [0x7fe063a46317]
 --- ttnn::ccl::create_mesh_workload_from_programs(tt::tt_metal::distributed::MeshCoordinateRangeSet const&, std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > const&, std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> >&, std::function<tt::tt_metal::operation::CacheableProgram<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > > (tt::tt_metal::distributed::MeshCoordinate const&)> const&)
 --- /localdev/jameszianxu/tt-xla/third_party/tt-mlir/install/lib/_ttnncpp.so(+0xbc848c) [0x7fe063a4748c]
 --- /localdev/jameszianxu/tt-xla/third_party/tt-mlir/install/lib/_ttnncpp.so(+0xbc8405) [0x7fe063a47405]
 --- tt::tt_metal::operation::OldInfraDeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > >::MeshWorkloadFactory::create_mesh_workload(tt::tt_metal::operation::DeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > > const&, tt::tt_metal::distributed::MeshCoordinateRangeSet const&, tt::tt_metal::operation::OldInfraDeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > >::tensor_args_t const&, std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> >&)
 --- /localdev/jameszianxu/tt-xla/third_party/tt-mlir/install/lib/_ttnncpp.so(+0x69cb59) [0x7fe06351bb59]
 --- /localdev/jameszianxu/tt-xla/third_party/tt-mlir/install/lib/_ttnncpp.so(_ZN4ttnn16device_operation6detail29launch_operation_with_adapterINS0_26MeshDeviceOperationAdapterIN2tt8tt_metal9operation23OldInfraDeviceOperationISt6vectorINS5_6TensorESaIS9_EEEEEEEEvN4ttsl10StrongTypeIhNS_10QueueIdTagEEERKNT_22operation_attributes_tERKNSI_13tensor_args_tERNSI_21tensor_return_value_tEPNS5_11distributed10MeshDeviceE+0x311) [0x7fe063518621]
 --- tt::tt_metal::operation::OldInfraDeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > >::tensor_return_value_t ttnn::device_operation::detail::launch_on_device<tt::tt_metal::operation::OldInfraDeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > > >(ttsl::StrongType<unsigned char, ttnn::QueueIdTag>, tt::tt_metal::operation::OldInfraDeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > >::operation_attributes_t const&, tt::tt_metal::operation::OldInfraDeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > >::tensor_args_t const&)
 --- tt::tt_metal::operation::OldInfraDeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > >::tensor_return_value_t ttnn::device_operation::detail::invoke<tt::tt_metal::operation::OldInfraDeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > > >(ttsl::StrongType<unsigned char, ttnn::QueueIdTag>, tt::tt_metal::operation::OldInfraDeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > >::operation_attributes_t const&, tt::tt_metal::operation::OldInfraDeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > >::tensor_args_t const&)
 --- /localdev/jameszianxu/tt-xla/third_party/tt-mlir/install/lib/_ttnncpp.so(+0x6984ce) [0x7fe0635174ce]
 --- std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > tt::tt_metal::operation::run<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > >(tt::tt_metal::operation::DeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > >&&, std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > const&, std::vector<std::optional<tt::tt_metal::Tensor const>, std::allocator<std::optional<tt::tt_metal::Tensor const> > > const&, std::vector<std::optional<tt::tt_metal::Tensor>, std::allocator<std::optional<tt::tt_metal::Tensor> > > const&, ttsl::StrongType<unsigned char, ttnn::QueueIdTag>)
 --- /localdev/jameszianxu/tt-xla/third_party/tt-mlir/install/lib/_ttnncpp.so(_ZN4ttnn10operations12experimental3ccl28reduce_scatter_minimal_asyncERKN2tt8tt_metal6TensorERKSt8optionalISt6vectorIS5_SaIS5_EEEjRKS9_INS4_15GlobalSemaphoreESaISF_EERKS8_ISF_EjRKS8_INS4_12MemoryConfigEESQ_NS3_9tt_fabric8TopologyES8_IN4ttsl10StrongTypeIhNS4_14SubDeviceIdTagEEEES8_IjESY_SY_SY_+0x62c) [0x7fe063a3279c]
 --- /localdev/jameszianxu/tt-xla/third_party/tt-mlir/install/lib/_ttnncpp.so(_ZN4ttnn10operations12experimental3ccl32ExecuteReduceScatterMinimalAsync6invokeERKN2tt8tt_metal6TensorERKSt8optionalISt6vectorIS6_SaIS6_EEEiRKSA_INS5_15GlobalSemaphoreESaISG_EERKS9_ISG_EjRKS9_INS5_12MemoryConfigEESR_NS4_9tt_fabric8TopologyES9_IN4ttsl10StrongTypeIhNS5_14SubDeviceIdTagEEEES9_IjESZ_SZ_SZ_+0xe1) [0x7fe063a5d091]
 --- /localdev/jameszianxu/tt-xla/third_party/tt-mlir/install/lib/libTTMLIRRuntime.so(+0x22ca87) [0x7fe2844aca87]
 --- /localdev/jameszianxu/tt-xla/third_party/tt-mlir/install/lib/libTTMLIRRuntime.so(+0x22c570) [0x7fe2844ac570]
 --- tt::runtime::ttnn::operations::ccl::run(tt::target::ttnn::ReduceScatterOp const*, tt::runtime::ttnn::ProgramContext&)
 --- tt::runtime::ttnn::ProgramExecutor::execute()
 --- tt::runtime::ttnn::submit(tt::runtime::Device, tt::runtime::Binary, unsigned int, std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> >&)
 --- tt::runtime::submit(tt::runtime::Device, tt::runtime::Binary, unsigned int, std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> >&)
 --- tt::pjrt::LoadedExecutableInstance::execute(PJRT_LoadedExecutable_Execute_Args*)
 --- tt::pjrt::internal::onLoadedExecutableExecute(PJRT_LoadedExecutable_Execute_Args*)
 --- xla::PjRtCApiLoadedExecutable::Execute(absl::lts_20230802::Span<std::vector<xla::PjRtBuffer*, std::allocator<xla::PjRtBuffer*> > const>, xla::ExecuteOptions const&, std::optional<std::vector<xla::PjRtFuture<void>, std::allocator<xla::PjRtFuture<void> > > >&)
 --- torch_xla::runtime::PjRtComputationClient::ExecuteReplicated(torch_xla::runtime::ComputationClient::Computation const&, absl::lts_20230802::Span<std::shared_ptr<torch_xla::runtime::ComputationClient::Data> const>, absl::lts_20230802::Span<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const>, torch_xla::runtime::ComputationClient::ExecuteReplicatedOptions const&)
 --- /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/_XLAC.cpython-311-x86_64-linux-gnu.so(+0x6f0531f) [0x7fe37445931f]
 --- torch::lazy::MultiWait::Complete(std::function<void ()> const&)
 --- Eigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)
 --- void absl::lts_20230802::internal_any_invocable::RemoteInvoker<false, void, tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}&>(absl::lts_20230802::internal_any_invocable::TypeErasedState*)
 --- /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/_XLAC.cpython-311-x86_64-linux-gnu.so(+0x11928751) [0x7fe37ee7c751]
 --- /lib/x86_64-linux-gnu/libc.so.6(+0x94ac3) [0x7fe4f179dac3]
 --- /lib/x86_64-linux-gnu/libc.so.6(+0x126850) [0x7fe4f182f850]

WARNING:pt-xla-profiler:================================================================================
WARNING:pt-xla-profiler:Unlowered Op usage summary (more of these ops, lower performance)
WARNING:pt-xla-profiler:Note: _local_scalar_dense typically indicates CPU context access
WARNING:pt-xla-profiler:--------------------------------------------------------------------------------
WARNING:pt-xla-profiler:================================================================================
2025-09-10 19:55:26.890 (  93.766s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:26.891 (  93.766s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:26.891 (  93.766s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:26.891 (  93.766s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:26.891 (  93.766s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:26.891 (  93.767s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:26.891 (  93.767s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:26.891 (  93.767s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.104 (  93.980s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.143 (  94.019s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.181 (  94.057s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.181 (  94.057s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.182 (  94.058s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.182 (  94.058s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.182 (  94.058s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.182 (  94.058s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.182 (  94.058s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.183 (  94.059s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.183 (  94.059s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.183 (  94.059s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.183 (  94.059s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.183 (  94.059s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.183 (  94.059s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.183 (  94.059s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.183 (  94.059s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.184 (  94.059s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.184 (  94.060s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.184 (  94.060s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.184 (  94.060s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.184 (  94.060s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.184 (  94.060s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.184 (  94.060s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.184 (  94.060s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.184 (  94.060s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.184 (  94.060s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.185 (  94.060s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.185 (  94.061s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.185 (  94.061s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.185 (  94.061s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.185 (  94.061s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.185 (  94.061s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.185 (  94.061s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.185 (  94.061s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.185 (  94.061s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.186 (  94.061s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.186 (  94.061s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.186 (  94.062s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.186 (  94.062s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.186 (  94.062s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.186 (  94.062s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.186 (  94.062s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.186 (  94.062s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.186 (  94.062s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.186 (  94.062s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.186 (  94.062s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.186 (  94.062s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.187 (  94.062s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.187 (  94.063s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.187 (  94.063s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.187 (  94.063s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.187 (  94.063s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.187 (  94.063s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.187 (  94.063s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.187 (  94.063s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.187 (  94.063s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.187 (  94.063s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.187 (  94.063s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.187 (  94.063s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.188 (  94.063s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.188 (  94.063s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.188 (  94.064s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.188 (  94.064s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.188 (  94.064s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.188 (  94.064s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.188 (  94.064s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.188 (  94.064s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.188 (  94.064s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.188 (  94.064s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.189 (  94.064s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.189 (  94.064s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.189 (  94.065s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.189 (  94.065s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.189 (  94.065s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.189 (  94.065s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.189 (  94.065s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.189 (  94.065s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.189 (  94.065s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.189 (  94.065s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.189 (  94.065s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.190 (  94.065s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.190 (  94.065s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.190 (  94.066s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.190 (  94.066s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.190 (  94.066s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.190 (  94.066s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.190 (  94.066s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.190 (  94.066s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.190 (  94.066s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.190 (  94.066s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.190 (  94.066s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.190 (  94.066s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.190 (  94.066s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.191 (  94.067s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.191 (  94.067s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.191 (  94.067s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.191 (  94.067s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.191 (  94.067s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.191 (  94.067s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.191 (  94.067s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.191 (  94.067s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.191 (  94.067s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.191 (  94.067s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.192 (  94.067s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.192 (  94.068s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.192 (  94.068s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.192 (  94.068s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.192 (  94.068s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.192 (  94.068s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.192 (  94.068s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.192 (  94.068s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.192 (  94.068s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.192 (  94.068s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.192 (  94.068s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.192 (  94.068s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.193 (  94.068s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.193 (  94.068s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.193 (  94.069s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.193 (  94.069s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.193 (  94.069s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.193 (  94.069s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.193 (  94.069s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.193 (  94.069s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.193 (  94.069s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.193 (  94.069s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.193 (  94.069s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.193 (  94.069s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.194 (  94.069s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.194 (  94.069s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.194 (  94.070s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.194 (  94.070s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.194 (  94.070s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.194 (  94.070s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.194 (  94.070s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.194 (  94.070s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.194 (  94.070s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.194 (  94.070s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.194 (  94.070s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.195 (  94.070s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.195 (  94.071s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.195 (  94.071s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.195 (  94.071s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.195 (  94.071s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.195 (  94.071s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.195 (  94.071s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.195 (  94.071s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.195 (  94.071s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.195 (  94.071s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.195 (  94.071s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.195 (  94.071s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.196 (  94.071s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.196 (  94.071s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.196 (  94.072s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.196 (  94.072s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.196 (  94.072s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.196 (  94.072s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.196 (  94.072s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.196 (  94.072s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.196 (  94.072s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.196 (  94.072s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.196 (  94.072s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.196 (  94.072s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.197 (  94.072s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.197 (  94.073s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.197 (  94.073s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.197 (  94.073s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.197 (  94.073s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.197 (  94.073s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.197 (  94.073s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.197 (  94.073s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.197 (  94.073s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.197 (  94.073s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.197 (  94.073s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.197 (  94.073s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.198 (  94.073s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.198 (  94.074s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.198 (  94.074s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.198 (  94.074s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.198 (  94.074s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.198 (  94.074s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.198 (  94.074s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.198 (  94.074s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.198 (  94.074s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.198 (  94.074s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.198 (  94.074s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.199 (  94.074s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.199 (  94.075s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.199 (  94.075s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.199 (  94.075s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.199 (  94.075s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.199 (  94.075s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.199 (  94.075s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.199 (  94.075s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.199 (  94.075s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.199 (  94.075s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.199 (  94.075s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.200 (  94.075s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.200 (  94.076s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.200 (  94.076s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.200 (  94.076s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.200 (  94.076s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.200 (  94.076s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.200 (  94.076s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.200 (  94.076s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.200 (  94.076s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.200 (  94.076s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.200 (  94.076s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.200 (  94.076s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.200 (  94.076s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.201 (  94.076s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.201 (  94.076s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.201 (  94.077s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.201 (  94.077s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.201 (  94.077s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.201 (  94.077s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.201 (  94.077s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.201 (  94.077s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.201 (  94.077s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.201 (  94.077s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.201 (  94.077s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.201 (  94.077s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.202 (  94.077s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.202 (  94.078s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.202 (  94.078s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.202 (  94.078s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.202 (  94.078s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.202 (  94.078s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.202 (  94.078s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.202 (  94.078s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.202 (  94.078s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.202 (  94.078s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.202 (  94.078s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.202 (  94.078s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.203 (  94.078s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.203 (  94.079s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.203 (  94.079s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.203 (  94.079s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.203 (  94.079s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.203 (  94.079s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.203 (  94.079s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.203 (  94.079s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.203 (  94.079s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.203 (  94.079s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.203 (  94.079s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.203 (  94.079s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.203 (  94.079s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.204 (  94.079s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.204 (  94.080s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.204 (  94.080s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.204 (  94.080s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.204 (  94.080s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.204 (  94.080s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.204 (  94.080s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.204 (  94.080s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.204 (  94.080s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.204 (  94.080s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.204 (  94.080s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.204 (  94.080s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.205 (  94.080s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.205 (  94.080s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.205 (  94.081s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.205 (  94.081s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.205 (  94.081s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.205 (  94.081s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.205 (  94.081s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.205 (  94.081s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.205 (  94.081s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.205 (  94.081s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.205 (  94.081s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.205 (  94.081s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.206 (  94.081s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.206 (  94.082s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.206 (  94.082s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.206 (  94.082s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.206 (  94.082s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.206 (  94.082s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.206 (  94.082s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.206 (  94.082s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.206 (  94.082s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.206 (  94.082s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.206 (  94.082s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.206 (  94.082s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.206 (  94.082s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.207 (  94.082s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.207 (  94.083s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.207 (  94.083s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.207 (  94.083s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.207 (  94.083s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.207 (  94.083s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.207 (  94.083s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.207 (  94.083s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.207 (  94.083s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.207 (  94.083s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.207 (  94.083s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.208 (  94.083s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.208 (  94.083s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.208 (  94.084s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.208 (  94.084s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.208 (  94.084s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.208 (  94.084s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.208 (  94.084s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.208 (  94.084s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.208 (  94.084s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.208 (  94.084s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.208 (  94.084s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.208 (  94.084s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.208 (  94.084s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.209 (  94.084s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.209 (  94.085s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.209 (  94.085s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.209 (  94.085s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.209 (  94.085s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.209 (  94.085s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.209 (  94.085s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.209 (  94.085s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.209 (  94.085s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.209 (  94.085s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.209 (  94.085s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.209 (  94.085s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.210 (  94.085s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.210 (  94.086s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.210 (  94.086s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.210 (  94.086s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.210 (  94.086s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.210 (  94.086s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.210 (  94.086s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.210 (  94.086s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.210 (  94.086s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.210 (  94.086s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.210 (  94.086s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.211 (  94.086s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.211 (  94.086s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.211 (  94.087s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.211 (  94.087s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.211 (  94.087s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.211 (  94.087s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.211 (  94.087s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.211 (  94.087s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.211 (  94.087s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.211 (  94.087s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.211 (  94.087s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.212 (  94.087s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.212 (  94.087s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.212 (  94.088s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.212 (  94.088s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.212 (  94.088s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.212 (  94.088s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.212 (  94.088s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.212 (  94.088s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.212 (  94.088s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.212 (  94.088s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.212 (  94.088s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.212 (  94.088s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.212 (  94.088s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.212 (  94.088s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.213 (  94.088s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.213 (  94.089s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.213 (  94.089s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.213 (  94.089s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.213 (  94.089s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.213 (  94.089s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.213 (  94.089s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.213 (  94.089s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.213 (  94.089s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.213 (  94.089s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.213 (  94.089s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.213 (  94.089s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.213 (  94.089s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.214 (  94.089s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.214 (  94.090s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.214 (  94.090s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.214 (  94.090s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.214 (  94.090s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.214 (  94.090s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.214 (  94.090s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.214 (  94.090s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.214 (  94.090s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.214 (  94.090s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.214 (  94.090s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.214 (  94.090s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.215 (  94.090s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.215 (  94.091s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.215 (  94.091s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.215 (  94.091s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.215 (  94.091s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.215 (  94.091s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.215 (  94.091s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.215 (  94.091s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.215 (  94.091s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.215 (  94.091s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.215 (  94.091s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.215 (  94.091s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.215 (  94.091s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.216 (  94.091s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.216 (  94.091s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.216 (  94.092s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.216 (  94.092s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.216 (  94.092s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.635 (  94.511s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.636 (  94.512s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.636 (  94.512s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.636 (  94.512s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
2025-09-10 19:55:27.681 (  94.557s) [        F1708000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
Merging ToLayoutOp into ttnn.constant
Merging ToLayoutOp into ttnn.constant
Merging ToLayoutOp into ttnn.constant
2025-09-10 19:55:28.557 | info     |          Device | Closing user mode device drivers (tt_cluster.cpp:393)
