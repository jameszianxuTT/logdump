WARNING:root:Defaulting to PJRT_DEVICE=CPU
2025-09-15 15:40:51.673 (   0.000s) [        F073E000]      dylib_platform.cc:47       1| DylibPlatform::SubclassInitialize
2025-09-15 15:40:51.675 (   0.002s) [        F073E000]     client_instance.cc:38       1| ClientInstance::ClientInstance
2025-09-15 15:40:51.676 (   0.002s) [        F073E000]              client.cc:18       1| TTClientInstance::TTClientInstance
2025-09-15 15:40:51.676 (   0.002s) [        F073E000]     client_instance.cc:59       1| ClientInstance::Initialize
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Device grid size = { 8, 8 }
2025-09-15 15:40:56.254 (   4.580s) [        F073E000]              stubs.inc:112   WARN| STUB: PJRT_Client_TopologyDescription
2025-09-15 15:40:56.254 (   4.580s) [        F073E000]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-09-15 15:40:56.254 (   4.580s) [        F073E000]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-09-15 15:40:56.254 (   4.580s) [        F073E000]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-09-15 15:40:56.254 (   4.580s) [        F073E000]     client_instance.cc:340      1| ClientInstance::PJRT_Client_PlatformVersion
2025-09-15 15:40:56.254 (   4.580s) [        F073E000]     client_instance.cc:320      1| ClientInstance::PJRT_Client_PlatformName
2025-09-15 15:40:56.254 (   4.580s) [        F073E000]     client_instance.cc:352      1| ClientInstance::PJRT_Client_Devices
2025-09-15 15:40:56.254 (   4.580s) [        F073E000]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-09-15 15:40:56.254 (   4.580s) [        F073E000]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-09-15 15:40:56.254 (   4.580s) [        F073E000]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-09-15 15:40:56.254 (   4.580s) [        F073E000]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-09-15 15:40:56.254 (   4.580s) [        F073E000]     client_instance.cc:365      1| ClientInstance::PJRT_Client_AddressableDevices
2025-09-15 15:40:56.254 (   4.580s) [        F073E000]     client_instance.cc:415      1| ClientInstance::PJRT_Client_AddressableMemories
2025-09-15 15:40:56.254 (   4.580s) [        F073E000]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-09-15 15:40:56.254 (   4.580s) [        F073E000]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-09-15 15:40:56.254 (   4.580s) [        F073E000]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-09-15 15:40:56.254 (   4.580s) [        F073E000]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-09-15 15:40:56.254 (   4.580s) [        F073E000]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-09-15 15:40:56.254 (   4.580s) [        F073E000]        api_bindings.cc:76       1| PJRT_Plugin_Attributes
2025-09-15 15:40:56.254663: W torch_xla/csrc/runtime/profiler.cpp:88] Profiler API not found for PJRT plugin
2025-09-15 15:40:56.254 (   4.581s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:56.254 (   4.581s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:56.254 (   4.581s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:56.254 (   4.581s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:56.254 (   4.581s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:56.254 (   4.581s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:56.254 (   4.581s) [        F073E000]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-09-15 15:40:56.254 (   4.581s) [        F073E000]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-09-15 15:40:56.254 (   4.581s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:56.254 (   4.581s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:56.254 (   4.581s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:56.254 (   4.581s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
/localdev/jameszianxu/tt-xla/examples/pytorch/llama.py:68: DeprecationWarning: Use torch_xla.device instead
  device = xm.xla_device()
Using TT-Metal from the source tree: /localdev/jameszianxu/tt-xla/third_party/tt-mlir/src/tt-mlir/third_party/tt-metal/src/tt-metal
WARNING: TT plugin is setting XLA_STABLEHLO_COMPILE to 1. This is required for TT PJRT plugin to work correctly.
Setting up XLA environment...
XLA environment configured.
Created device mesh: (1, 2) with 2 devices
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 51.34it/s]
2025-09-15 15:40:57.732 (   6.059s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:57.732 (   6.059s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:57.732 (   6.059s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:57.732 (   6.059s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:57.732 (   6.059s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-09-15 15:40:57.733 (   6.060s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:57.733 (   6.060s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:57.734 (   6.060s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:57.734 (   6.060s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:57.734 (   6.060s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-09-15 15:40:57.734 (   6.060s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:57.734 (   6.060s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:57.734 (   6.060s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:57.734 (   6.060s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:57.734 (   6.060s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:57.734 (   6.060s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:57.734 (   6.060s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-09-15 15:40:57.734 (   6.061s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:57.734 (   6.061s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:57.734 (   6.061s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:57.734 (   6.061s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:57.734 (   6.061s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-09-15 15:40:57.734 (   6.061s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:57.734 (   6.061s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:57.734 (   6.061s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:57.735 (   6.061s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:57.778 (   6.104s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:57.778 (   6.104s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:57.778 (   6.104s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:57.778 (   6.105s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:57.778 (   6.105s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:57.778 (   6.105s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:57.778 (   6.105s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:57.778 (   6.105s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:57.779 (   6.105s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:57.779 (   6.105s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:57.779 (   6.105s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:57.779 (   6.105s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:57.827 (   6.154s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:57.827 (   6.154s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:57.827 (   6.154s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:57.828 (   6.154s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:57.828 (   6.154s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:57.828 (   6.154s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:57.828 (   6.154s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:57.828 (   6.154s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:57.828 (   6.154s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:57.828 (   6.154s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.617 (   6.944s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.617 (   6.944s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.623 (   6.950s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.623 (   6.950s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.623 (   6.950s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.624 (   6.950s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.624 (   6.950s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.624 (   6.950s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.624 (   6.950s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.624 (   6.950s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.624 (   6.950s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.624 (   6.950s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.629 (   6.955s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.629 (   6.955s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.635 (   6.961s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.635 (   6.961s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.635 (   6.961s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.635 (   6.961s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.635 (   6.961s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.635 (   6.961s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.635 (   6.961s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.635 (   6.961s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.635 (   6.961s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.635 (   6.961s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.640 (   6.966s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.640 (   6.966s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.647 (   6.973s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.647 (   6.973s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.647 (   6.973s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.647 (   6.974s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.647 (   6.974s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.647 (   6.974s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.647 (   6.974s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.647 (   6.974s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.647 (   6.974s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.648 (   6.974s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.652 (   6.978s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.652 (   6.978s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.653 (   6.980s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.653 (   6.980s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.653 (   6.980s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.653 (   6.980s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.654 (   6.980s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.654 (   6.980s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.654 (   6.980s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.654 (   6.980s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.654 (   6.980s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.654 (   6.980s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.654 (   6.980s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.654 (   6.980s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.654 (   6.981s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.654 (   6.981s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.654 (   6.981s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.655 (   6.981s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.655 (   6.981s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.655 (   6.981s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.655 (   6.981s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.655 (   6.981s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.655 (   6.981s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.655 (   6.981s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.655 (   6.981s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.655 (   6.981s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.655 (   6.982s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.655 (   6.982s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.655 (   6.982s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.655 (   6.982s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.656 (   6.982s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.656 (   6.982s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.656 (   6.982s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.656 (   6.982s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.656 (   6.982s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.656 (   6.982s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.656 (   6.982s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.656 (   6.982s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.657 (   6.984s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.657 (   6.984s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.657 (   6.984s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.658 (   6.984s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.658 (   6.984s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.658 (   6.984s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.658 (   6.984s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.658 (   6.984s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.658 (   6.984s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.658 (   6.984s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.658 (   6.984s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.658 (   6.984s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.662 (   6.989s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.662 (   6.989s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.662 (   6.989s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.663 (   6.989s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.663 (   6.989s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.663 (   6.989s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.663 (   6.989s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.663 (   6.989s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.663 (   6.989s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.663 (   6.989s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.667 (   6.994s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.667 (   6.994s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.673 (   7.000s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.673 (   7.000s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.673 (   7.000s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.674 (   7.000s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.674 (   7.000s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.674 (   7.000s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.674 (   7.000s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.674 (   7.000s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.674 (   7.000s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.674 (   7.000s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.678 (   7.005s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.678 (   7.005s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.685 (   7.012s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.685 (   7.012s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.685 (   7.012s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.685 (   7.012s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.685 (   7.012s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.685 (   7.012s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.685 (   7.012s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.685 (   7.012s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.686 (   7.012s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.686 (   7.012s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.690 (   7.017s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.690 (   7.017s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.691 (   7.018s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.691 (   7.018s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.691 (   7.018s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.692 (   7.018s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.692 (   7.018s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.692 (   7.018s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.692 (   7.018s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.692 (   7.018s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.692 (   7.018s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.692 (   7.018s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.692 (   7.018s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.692 (   7.018s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.692 (   7.019s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.692 (   7.019s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.692 (   7.019s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.692 (   7.019s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.692 (   7.019s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.692 (   7.019s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.692 (   7.019s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.692 (   7.019s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.693 (   7.019s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.693 (   7.019s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.693 (   7.019s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.693 (   7.019s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.693 (   7.019s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.693 (   7.019s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.693 (   7.019s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.693 (   7.020s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.693 (   7.020s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.693 (   7.020s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.693 (   7.020s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.693 (   7.020s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.693 (   7.020s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.693 (   7.020s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.693 (   7.020s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.693 (   7.020s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.695 (   7.021s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.695 (   7.021s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.695 (   7.021s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.695 (   7.021s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.695 (   7.021s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.695 (   7.021s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.695 (   7.021s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.695 (   7.021s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.695 (   7.022s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.695 (   7.022s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.695 (   7.022s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.695 (   7.022s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.701 (   7.027s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.701 (   7.027s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.701 (   7.027s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.701 (   7.027s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.701 (   7.027s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.701 (   7.027s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.701 (   7.027s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.701 (   7.027s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.701 (   7.027s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.701 (   7.028s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.706 (   7.032s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.706 (   7.032s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.712 (   7.038s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.712 (   7.038s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.712 (   7.038s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.712 (   7.038s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.712 (   7.038s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.712 (   7.038s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.712 (   7.038s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.712 (   7.038s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.712 (   7.038s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.712 (   7.038s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.717 (   7.043s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.717 (   7.043s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.724 (   7.050s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.724 (   7.050s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.724 (   7.050s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.724 (   7.050s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.724 (   7.050s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.724 (   7.051s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.724 (   7.051s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.724 (   7.051s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.724 (   7.051s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.724 (   7.051s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.729 (   7.055s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.729 (   7.055s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.730 (   7.057s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.730 (   7.057s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.730 (   7.057s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.731 (   7.057s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.731 (   7.057s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.731 (   7.057s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.731 (   7.057s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.731 (   7.057s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.731 (   7.057s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.731 (   7.057s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.731 (   7.057s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.731 (   7.057s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.731 (   7.058s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.731 (   7.058s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.731 (   7.058s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.732 (   7.058s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.732 (   7.058s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.732 (   7.058s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.732 (   7.058s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.732 (   7.058s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.732 (   7.058s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.732 (   7.058s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.732 (   7.058s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.732 (   7.058s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.732 (   7.058s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.732 (   7.059s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.732 (   7.059s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.732 (   7.059s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.732 (   7.059s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.732 (   7.059s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.732 (   7.059s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.732 (   7.059s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.732 (   7.059s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.732 (   7.059s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.733 (   7.059s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.733 (   7.059s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.734 (   7.060s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.734 (   7.060s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.734 (   7.060s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.734 (   7.060s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.734 (   7.060s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.734 (   7.060s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.734 (   7.060s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.734 (   7.060s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.734 (   7.061s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.734 (   7.061s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.734 (   7.061s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.734 (   7.061s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.740 (   7.066s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.740 (   7.066s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.740 (   7.066s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.740 (   7.066s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.740 (   7.066s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.740 (   7.066s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.740 (   7.066s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.740 (   7.066s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.740 (   7.066s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.740 (   7.067s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.745 (   7.071s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.745 (   7.071s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.750 (   7.077s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.750 (   7.077s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.750 (   7.077s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.750 (   7.077s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.750 (   7.077s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.750 (   7.077s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.750 (   7.077s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.750 (   7.077s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.751 (   7.077s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.751 (   7.077s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.755 (   7.081s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.755 (   7.081s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.762 (   7.089s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.762 (   7.089s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.762 (   7.089s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.763 (   7.089s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.763 (   7.089s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.763 (   7.089s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.763 (   7.089s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.763 (   7.089s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.763 (   7.089s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.763 (   7.089s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.767 (   7.094s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.767 (   7.094s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.769 (   7.096s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.769 (   7.096s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.769 (   7.096s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.769 (   7.096s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.769 (   7.096s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.769 (   7.096s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.769 (   7.096s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.769 (   7.096s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.770 (   7.096s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.770 (   7.096s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.770 (   7.096s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.770 (   7.096s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.770 (   7.097s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.770 (   7.097s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.770 (   7.097s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.770 (   7.097s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.770 (   7.097s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.771 (   7.097s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.771 (   7.097s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.771 (   7.097s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.771 (   7.097s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.771 (   7.097s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.771 (   7.097s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.771 (   7.097s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.772 (   7.098s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.772 (   7.098s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.772 (   7.098s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.772 (   7.098s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.772 (   7.098s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.772 (   7.098s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.772 (   7.098s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.772 (   7.098s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.772 (   7.098s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.772 (   7.098s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.772 (   7.099s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.772 (   7.099s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.774 (   7.100s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.774 (   7.100s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.774 (   7.100s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.774 (   7.100s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.774 (   7.100s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.774 (   7.100s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.774 (   7.100s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.774 (   7.100s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.774 (   7.101s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.774 (   7.101s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.774 (   7.101s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.774 (   7.101s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.780 (   7.106s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.780 (   7.106s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.780 (   7.106s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.780 (   7.107s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.780 (   7.107s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.780 (   7.107s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.780 (   7.107s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.780 (   7.107s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.780 (   7.107s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.780 (   7.107s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.785 (   7.111s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.785 (   7.111s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.789 (   7.116s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.789 (   7.116s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.789 (   7.116s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.790 (   7.116s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.790 (   7.116s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.790 (   7.116s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.790 (   7.116s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.790 (   7.116s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.790 (   7.116s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.790 (   7.116s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.794 (   7.121s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.794 (   7.121s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.801 (   7.128s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.801 (   7.128s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.801 (   7.128s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.802 (   7.128s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.802 (   7.128s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.802 (   7.128s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.802 (   7.128s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.802 (   7.128s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.802 (   7.128s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.802 (   7.128s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.806 (   7.133s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.806 (   7.133s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.807 (   7.134s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.807 (   7.134s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.807 (   7.134s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.808 (   7.134s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.808 (   7.134s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.808 (   7.134s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.808 (   7.134s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.808 (   7.134s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.808 (   7.134s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.808 (   7.134s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.808 (   7.134s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.808 (   7.134s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.808 (   7.135s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.808 (   7.135s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.808 (   7.135s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.808 (   7.135s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.808 (   7.135s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.808 (   7.135s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.808 (   7.135s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.808 (   7.135s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.809 (   7.135s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.809 (   7.135s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.809 (   7.135s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.809 (   7.135s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.809 (   7.135s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.809 (   7.135s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.809 (   7.135s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.809 (   7.136s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.809 (   7.136s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.809 (   7.136s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.809 (   7.136s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.809 (   7.136s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.809 (   7.136s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.809 (   7.136s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.809 (   7.136s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.809 (   7.136s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.811 (   7.138s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.811 (   7.138s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.811 (   7.138s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.811 (   7.138s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.811 (   7.138s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.811 (   7.138s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.811 (   7.138s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.811 (   7.138s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.812 (   7.138s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.812 (   7.138s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.812 (   7.138s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.812 (   7.138s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.817 (   7.143s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.817 (   7.143s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.817 (   7.144s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.817 (   7.144s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.817 (   7.144s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.817 (   7.144s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.817 (   7.144s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.817 (   7.144s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.817 (   7.144s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.817 (   7.144s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.822 (   7.148s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.822 (   7.148s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.828 (   7.154s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.828 (   7.154s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.828 (   7.154s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.828 (   7.154s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.828 (   7.154s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.828 (   7.154s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.828 (   7.154s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.828 (   7.154s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.828 (   7.154s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.828 (   7.154s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.832 (   7.159s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.832 (   7.159s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.839 (   7.166s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.839 (   7.166s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.839 (   7.166s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.840 (   7.166s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.840 (   7.166s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.840 (   7.166s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.840 (   7.166s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.840 (   7.166s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.840 (   7.166s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.840 (   7.166s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.844 (   7.171s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.844 (   7.171s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.846 (   7.172s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.846 (   7.172s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.846 (   7.172s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.846 (   7.172s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.846 (   7.172s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.846 (   7.172s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.846 (   7.172s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.846 (   7.172s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.846 (   7.172s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.846 (   7.172s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.846 (   7.172s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.846 (   7.172s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.846 (   7.173s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.846 (   7.173s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.846 (   7.173s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.847 (   7.173s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.847 (   7.173s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.847 (   7.173s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.847 (   7.173s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.847 (   7.173s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.847 (   7.173s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.847 (   7.173s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.847 (   7.173s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.847 (   7.173s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.847 (   7.174s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.847 (   7.174s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.847 (   7.174s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.847 (   7.174s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.847 (   7.174s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.847 (   7.174s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.847 (   7.174s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.847 (   7.174s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.848 (   7.174s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.848 (   7.174s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.848 (   7.174s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.848 (   7.174s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.849 (   7.176s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.849 (   7.176s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.849 (   7.176s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.849 (   7.176s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.849 (   7.176s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.849 (   7.176s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.849 (   7.176s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.849 (   7.176s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.849 (   7.176s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.849 (   7.176s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.850 (   7.176s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.850 (   7.176s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.855 (   7.181s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.855 (   7.181s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.855 (   7.181s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.855 (   7.182s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.855 (   7.182s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.855 (   7.182s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.855 (   7.182s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.855 (   7.182s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.855 (   7.182s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.855 (   7.182s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.860 (   7.186s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.860 (   7.187s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.865 (   7.191s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.865 (   7.191s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.865 (   7.191s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.865 (   7.191s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.865 (   7.191s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.865 (   7.191s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.865 (   7.191s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.865 (   7.191s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.865 (   7.191s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.865 (   7.191s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.870 (   7.196s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.870 (   7.196s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.877 (   7.203s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.877 (   7.203s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.877 (   7.203s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.877 (   7.204s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.877 (   7.204s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.877 (   7.204s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.877 (   7.204s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.877 (   7.204s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.877 (   7.204s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.877 (   7.204s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.882 (   7.208s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.882 (   7.208s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.883 (   7.210s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.883 (   7.210s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.883 (   7.210s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.883 (   7.210s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.883 (   7.210s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.883 (   7.210s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.883 (   7.210s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.883 (   7.210s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.884 (   7.210s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.884 (   7.210s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.884 (   7.210s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.884 (   7.210s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.884 (   7.211s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.884 (   7.211s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.884 (   7.211s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.884 (   7.211s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.884 (   7.211s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.884 (   7.211s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.884 (   7.211s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.884 (   7.211s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.885 (   7.211s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.885 (   7.211s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.885 (   7.211s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.885 (   7.211s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.885 (   7.211s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.885 (   7.211s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.885 (   7.211s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.885 (   7.212s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.885 (   7.212s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.885 (   7.212s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.885 (   7.212s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.885 (   7.212s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.885 (   7.212s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.885 (   7.212s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.886 (   7.212s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.886 (   7.212s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.887 (   7.214s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.887 (   7.214s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.887 (   7.214s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.887 (   7.214s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.887 (   7.214s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.887 (   7.214s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.887 (   7.214s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.887 (   7.214s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.888 (   7.214s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.888 (   7.214s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.888 (   7.214s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.888 (   7.214s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.893 (   7.220s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.893 (   7.220s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.893 (   7.220s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.894 (   7.220s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.894 (   7.220s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.894 (   7.220s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.894 (   7.220s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.894 (   7.220s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.894 (   7.220s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.894 (   7.220s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.899 (   7.225s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.899 (   7.225s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.904 (   7.231s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.904 (   7.231s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.904 (   7.231s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.904 (   7.231s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.904 (   7.231s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.904 (   7.231s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.904 (   7.231s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.904 (   7.231s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.905 (   7.231s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.905 (   7.231s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.909 (   7.236s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.909 (   7.236s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.916 (   7.243s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.916 (   7.243s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.916 (   7.243s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.916 (   7.243s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.916 (   7.243s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.917 (   7.243s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.917 (   7.243s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.917 (   7.243s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.917 (   7.243s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.917 (   7.243s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.921 (   7.248s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.921 (   7.248s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.923 (   7.249s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.923 (   7.249s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.923 (   7.249s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.923 (   7.249s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.923 (   7.249s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.923 (   7.249s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.923 (   7.249s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.923 (   7.249s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.923 (   7.249s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.923 (   7.249s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.923 (   7.249s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.923 (   7.249s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.923 (   7.250s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.923 (   7.250s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.923 (   7.250s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.924 (   7.250s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.924 (   7.250s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.924 (   7.250s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.924 (   7.250s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.924 (   7.250s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.924 (   7.250s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.924 (   7.250s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.924 (   7.250s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.924 (   7.250s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.924 (   7.251s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.924 (   7.251s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.924 (   7.251s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.924 (   7.251s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.924 (   7.251s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.924 (   7.251s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.924 (   7.251s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.924 (   7.251s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.924 (   7.251s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.924 (   7.251s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.925 (   7.251s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.925 (   7.251s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.926 (   7.252s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.926 (   7.252s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.926 (   7.252s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.926 (   7.252s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.926 (   7.252s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.926 (   7.252s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.926 (   7.252s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.926 (   7.252s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.926 (   7.252s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.926 (   7.252s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.926 (   7.253s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.926 (   7.253s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.931 (   7.257s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.931 (   7.257s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.931 (   7.257s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.931 (   7.257s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.931 (   7.257s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.931 (   7.257s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.931 (   7.257s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.931 (   7.257s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.931 (   7.257s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.931 (   7.257s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.936 (   7.262s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.936 (   7.262s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.942 (   7.268s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.942 (   7.268s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.942 (   7.268s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.942 (   7.268s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.942 (   7.268s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.942 (   7.268s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.942 (   7.268s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.942 (   7.268s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.942 (   7.268s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.942 (   7.268s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.946 (   7.273s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.946 (   7.273s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.952 (   7.279s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.952 (   7.279s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.952 (   7.279s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.953 (   7.279s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.953 (   7.279s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.953 (   7.279s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.953 (   7.279s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.953 (   7.279s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.953 (   7.279s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.953 (   7.279s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.957 (   7.284s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.957 (   7.284s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.958 (   7.285s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.958 (   7.285s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.958 (   7.285s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.959 (   7.285s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.959 (   7.285s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.959 (   7.285s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.959 (   7.285s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.959 (   7.285s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.959 (   7.285s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.959 (   7.285s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.959 (   7.285s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.959 (   7.285s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.959 (   7.286s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.959 (   7.286s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.959 (   7.286s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.960 (   7.286s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.960 (   7.286s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.960 (   7.286s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.960 (   7.286s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.960 (   7.286s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.960 (   7.286s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.960 (   7.286s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.960 (   7.286s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.960 (   7.286s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.960 (   7.287s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.960 (   7.287s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.960 (   7.287s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.960 (   7.287s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.960 (   7.287s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.960 (   7.287s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.960 (   7.287s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.960 (   7.287s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.961 (   7.287s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.961 (   7.287s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.961 (   7.287s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.961 (   7.287s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.962 (   7.288s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.962 (   7.288s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.962 (   7.288s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.962 (   7.288s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.962 (   7.288s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.962 (   7.288s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.962 (   7.288s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.962 (   7.288s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.962 (   7.289s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.962 (   7.289s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.962 (   7.289s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.962 (   7.289s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.968 (   7.294s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.968 (   7.294s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.968 (   7.294s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.968 (   7.294s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.968 (   7.294s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.968 (   7.294s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.968 (   7.294s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.968 (   7.294s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.968 (   7.295s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.968 (   7.295s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.973 (   7.299s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.973 (   7.300s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.979 (   7.305s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.979 (   7.305s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.979 (   7.305s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.979 (   7.305s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.979 (   7.305s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.979 (   7.305s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.979 (   7.305s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.979 (   7.305s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.979 (   7.306s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.979 (   7.306s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.984 (   7.310s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.984 (   7.310s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.990 (   7.316s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.990 (   7.316s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.990 (   7.316s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.990 (   7.317s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.990 (   7.317s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.990 (   7.317s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.990 (   7.317s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.990 (   7.317s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.990 (   7.317s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.991 (   7.317s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.995 (   7.322s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.995 (   7.322s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.997 (   7.323s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.997 (   7.323s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.997 (   7.323s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.997 (   7.323s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.997 (   7.323s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.997 (   7.323s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.997 (   7.323s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.997 (   7.323s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.997 (   7.323s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.997 (   7.323s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.997 (   7.324s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.997 (   7.324s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.998 (   7.324s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.998 (   7.324s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.998 (   7.324s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.998 (   7.324s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.998 (   7.324s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.998 (   7.324s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.998 (   7.324s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.998 (   7.324s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.998 (   7.324s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.998 (   7.324s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.998 (   7.325s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.998 (   7.325s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.999 (   7.325s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.999 (   7.325s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.999 (   7.325s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.999 (   7.325s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.999 (   7.325s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.999 (   7.325s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:58.999 (   7.325s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:58.999 (   7.325s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:58.999 (   7.325s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:58.999 (   7.325s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:58.999 (   7.325s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:58.999 (   7.325s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.000 (   7.327s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.000 (   7.327s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.000 (   7.327s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.001 (   7.327s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.001 (   7.327s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.001 (   7.327s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.001 (   7.327s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.001 (   7.327s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.001 (   7.327s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.001 (   7.327s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.001 (   7.327s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.001 (   7.327s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.006 (   7.333s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.006 (   7.333s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.006 (   7.333s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.007 (   7.333s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.007 (   7.333s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.007 (   7.333s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.007 (   7.333s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.007 (   7.333s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.007 (   7.333s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.007 (   7.333s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.012 (   7.338s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.012 (   7.338s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.017 (   7.344s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.017 (   7.344s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.017 (   7.344s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.017 (   7.344s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.017 (   7.344s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.017 (   7.344s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.017 (   7.344s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.017 (   7.344s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.018 (   7.344s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.018 (   7.344s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.022 (   7.348s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.022 (   7.348s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.029 (   7.355s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.029 (   7.355s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.029 (   7.355s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.029 (   7.356s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.029 (   7.356s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.029 (   7.356s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.029 (   7.356s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.029 (   7.356s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.029 (   7.356s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.030 (   7.356s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.034 (   7.361s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.034 (   7.361s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.035 (   7.362s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.035 (   7.362s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.035 (   7.362s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.036 (   7.362s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.036 (   7.362s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.036 (   7.362s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.036 (   7.362s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.036 (   7.362s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.036 (   7.362s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.036 (   7.362s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.036 (   7.362s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.036 (   7.362s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.036 (   7.363s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.036 (   7.363s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.036 (   7.363s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.036 (   7.363s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.036 (   7.363s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.036 (   7.363s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.036 (   7.363s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.036 (   7.363s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.037 (   7.363s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.037 (   7.363s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.037 (   7.363s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.037 (   7.363s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.037 (   7.363s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.037 (   7.363s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.037 (   7.363s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.037 (   7.364s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.037 (   7.364s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.037 (   7.364s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.037 (   7.364s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.037 (   7.364s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.037 (   7.364s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.037 (   7.364s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.037 (   7.364s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.037 (   7.364s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.039 (   7.366s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.039 (   7.366s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.039 (   7.366s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.039 (   7.366s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.039 (   7.366s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.039 (   7.366s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.039 (   7.366s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.039 (   7.366s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.040 (   7.366s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.040 (   7.366s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.040 (   7.366s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.040 (   7.366s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.045 (   7.371s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.045 (   7.371s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.045 (   7.371s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.045 (   7.372s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.045 (   7.372s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.045 (   7.372s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.045 (   7.372s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.045 (   7.372s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.045 (   7.372s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.045 (   7.372s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.050 (   7.377s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.050 (   7.377s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.056 (   7.382s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.056 (   7.382s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.056 (   7.382s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.056 (   7.382s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.056 (   7.382s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.056 (   7.382s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.056 (   7.382s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.056 (   7.382s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.056 (   7.383s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.056 (   7.383s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.061 (   7.387s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.061 (   7.387s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.067 (   7.393s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.067 (   7.393s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.067 (   7.393s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.067 (   7.393s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.067 (   7.393s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.067 (   7.393s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.067 (   7.393s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.067 (   7.393s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.067 (   7.394s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.067 (   7.394s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.072 (   7.398s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.072 (   7.398s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.073 (   7.399s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.073 (   7.399s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.073 (   7.399s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.073 (   7.399s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.073 (   7.399s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.073 (   7.399s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.073 (   7.399s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.073 (   7.399s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.073 (   7.400s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.073 (   7.400s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.073 (   7.400s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.073 (   7.400s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.074 (   7.400s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.074 (   7.400s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.074 (   7.400s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.074 (   7.400s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.074 (   7.400s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.074 (   7.400s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.074 (   7.400s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.074 (   7.400s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.074 (   7.400s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.074 (   7.400s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.074 (   7.400s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.074 (   7.400s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.074 (   7.401s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.074 (   7.401s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.074 (   7.401s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.075 (   7.401s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.075 (   7.401s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.075 (   7.401s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.075 (   7.401s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.075 (   7.401s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.075 (   7.401s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.075 (   7.401s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.075 (   7.401s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.075 (   7.401s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.076 (   7.403s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.076 (   7.403s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.076 (   7.403s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.077 (   7.403s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.077 (   7.403s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.077 (   7.403s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.077 (   7.403s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.077 (   7.403s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.077 (   7.403s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.077 (   7.403s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.077 (   7.403s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.077 (   7.403s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.082 (   7.409s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.082 (   7.409s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.082 (   7.409s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.082 (   7.409s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.083 (   7.409s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.083 (   7.409s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.083 (   7.409s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.083 (   7.409s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.083 (   7.409s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.083 (   7.409s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.087 (   7.414s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.087 (   7.414s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.092 (   7.418s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.092 (   7.418s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.092 (   7.418s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.092 (   7.419s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.092 (   7.419s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.092 (   7.419s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.092 (   7.419s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.092 (   7.419s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.092 (   7.419s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.092 (   7.419s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.097 (   7.424s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.097 (   7.424s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.105 (   7.431s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.105 (   7.431s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.105 (   7.431s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.105 (   7.432s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.105 (   7.432s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.105 (   7.432s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.105 (   7.432s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.105 (   7.432s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.105 (   7.432s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.105 (   7.432s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.110 (   7.437s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.110 (   7.437s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.111 (   7.438s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.111 (   7.438s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.111 (   7.438s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.112 (   7.438s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.112 (   7.438s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.112 (   7.438s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.112 (   7.438s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.112 (   7.438s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.112 (   7.438s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.112 (   7.438s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.112 (   7.438s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.112 (   7.438s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.113 (   7.439s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.113 (   7.439s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.113 (   7.439s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.113 (   7.439s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.113 (   7.439s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.113 (   7.439s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.113 (   7.439s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.113 (   7.439s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.113 (   7.439s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.113 (   7.439s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.113 (   7.439s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.113 (   7.439s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.113 (   7.440s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.113 (   7.440s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.113 (   7.440s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.113 (   7.440s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.114 (   7.440s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.114 (   7.440s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.114 (   7.440s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.114 (   7.440s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.114 (   7.440s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.114 (   7.440s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.114 (   7.440s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.114 (   7.440s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.116 (   7.442s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.116 (   7.442s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.116 (   7.442s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.116 (   7.442s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.116 (   7.442s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.116 (   7.442s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.116 (   7.442s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.116 (   7.442s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.116 (   7.443s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.116 (   7.443s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.116 (   7.443s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.116 (   7.443s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.122 (   7.448s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.122 (   7.448s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.122 (   7.448s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.122 (   7.449s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.122 (   7.449s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.122 (   7.449s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.122 (   7.449s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.122 (   7.449s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.122 (   7.449s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.122 (   7.449s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.127 (   7.454s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.127 (   7.454s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.132 (   7.458s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.132 (   7.458s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.132 (   7.458s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.132 (   7.459s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.132 (   7.459s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.132 (   7.459s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.132 (   7.459s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.132 (   7.459s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.133 (   7.459s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.133 (   7.459s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.137 (   7.464s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.137 (   7.464s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.144 (   7.471s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.144 (   7.471s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.144 (   7.471s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.145 (   7.471s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.145 (   7.471s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.145 (   7.471s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.145 (   7.471s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.145 (   7.471s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.145 (   7.471s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.145 (   7.471s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.150 (   7.476s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.150 (   7.476s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.151 (   7.477s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.151 (   7.477s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.151 (   7.477s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.151 (   7.478s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.151 (   7.478s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.151 (   7.478s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.151 (   7.478s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.151 (   7.478s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.151 (   7.478s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.152 (   7.478s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.152 (   7.478s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.152 (   7.478s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.152 (   7.478s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.152 (   7.478s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.152 (   7.478s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.152 (   7.479s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.152 (   7.479s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.152 (   7.479s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.152 (   7.479s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.152 (   7.479s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.152 (   7.479s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.152 (   7.479s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.152 (   7.479s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.152 (   7.479s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.153 (   7.479s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.153 (   7.479s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.153 (   7.479s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.153 (   7.479s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.153 (   7.479s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.153 (   7.479s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.153 (   7.479s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.153 (   7.479s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.153 (   7.479s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.153 (   7.479s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.153 (   7.480s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.153 (   7.480s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.155 (   7.481s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.155 (   7.482s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.155 (   7.482s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.155 (   7.482s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.155 (   7.482s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.155 (   7.482s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.155 (   7.482s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.155 (   7.482s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.155 (   7.482s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.155 (   7.482s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.156 (   7.482s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.156 (   7.482s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.161 (   7.487s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.161 (   7.487s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.161 (   7.487s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.161 (   7.488s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.161 (   7.488s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.161 (   7.488s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.161 (   7.488s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.161 (   7.488s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.161 (   7.488s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.161 (   7.488s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.166 (   7.493s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.166 (   7.493s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.172 (   7.499s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.172 (   7.499s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.172 (   7.499s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.173 (   7.499s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.173 (   7.499s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.173 (   7.499s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.173 (   7.499s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.173 (   7.499s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.173 (   7.499s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.173 (   7.499s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.178 (   7.504s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.178 (   7.504s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.185 (   7.512s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.185 (   7.512s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.185 (   7.512s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.186 (   7.512s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.186 (   7.512s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.186 (   7.512s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.186 (   7.512s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.186 (   7.512s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.186 (   7.512s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.186 (   7.512s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.191 (   7.517s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.191 (   7.517s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.192 (   7.518s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.192 (   7.518s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.192 (   7.518s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.192 (   7.519s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.192 (   7.519s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.192 (   7.519s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.192 (   7.519s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.192 (   7.519s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.193 (   7.519s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.193 (   7.519s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.193 (   7.519s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.193 (   7.519s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.193 (   7.519s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.193 (   7.519s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.193 (   7.519s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.193 (   7.520s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.193 (   7.520s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.193 (   7.520s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.193 (   7.520s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.193 (   7.520s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.193 (   7.520s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.193 (   7.520s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.194 (   7.520s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.194 (   7.520s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.194 (   7.520s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.194 (   7.520s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.194 (   7.520s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.194 (   7.520s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.194 (   7.520s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.194 (   7.520s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.194 (   7.520s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.194 (   7.520s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.194 (   7.521s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.194 (   7.521s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.194 (   7.521s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.194 (   7.521s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.196 (   7.522s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.196 (   7.522s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.196 (   7.522s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.196 (   7.522s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.196 (   7.522s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.196 (   7.522s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.196 (   7.522s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.196 (   7.522s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.196 (   7.523s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.196 (   7.523s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.196 (   7.523s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.196 (   7.523s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.202 (   7.528s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.202 (   7.528s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.202 (   7.528s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.202 (   7.528s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.202 (   7.528s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.202 (   7.528s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.202 (   7.528s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.202 (   7.528s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.202 (   7.529s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.202 (   7.529s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.207 (   7.534s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.207 (   7.534s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.213 (   7.539s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.213 (   7.539s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.213 (   7.539s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.213 (   7.540s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.213 (   7.540s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.213 (   7.540s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.213 (   7.540s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.213 (   7.540s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.213 (   7.540s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.213 (   7.540s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.218 (   7.545s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.218 (   7.545s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.225 (   7.551s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.225 (   7.551s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.225 (   7.551s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.225 (   7.552s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.225 (   7.552s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.225 (   7.552s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.225 (   7.552s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.225 (   7.552s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.225 (   7.552s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.225 (   7.552s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.230 (   7.557s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.230 (   7.557s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.232 (   7.558s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.232 (   7.558s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.232 (   7.558s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.232 (   7.558s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.232 (   7.558s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.232 (   7.558s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.232 (   7.558s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.232 (   7.558s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.232 (   7.559s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.232 (   7.559s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.232 (   7.559s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.232 (   7.559s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.233 (   7.559s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.233 (   7.559s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.233 (   7.559s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.233 (   7.559s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.233 (   7.559s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.233 (   7.559s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.233 (   7.559s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.233 (   7.559s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.233 (   7.559s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.233 (   7.559s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.233 (   7.560s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.233 (   7.560s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.234 (   7.560s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.234 (   7.560s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.234 (   7.560s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.234 (   7.560s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.234 (   7.560s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.234 (   7.560s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.234 (   7.560s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.234 (   7.560s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.234 (   7.560s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.234 (   7.560s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.234 (   7.560s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.234 (   7.560s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.236 (   7.562s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.236 (   7.562s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.236 (   7.562s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.236 (   7.562s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.236 (   7.562s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.236 (   7.562s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.236 (   7.562s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.236 (   7.562s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.236 (   7.562s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.236 (   7.562s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.236 (   7.562s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.236 (   7.562s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.242 (   7.568s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.242 (   7.568s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.242 (   7.568s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.242 (   7.568s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.242 (   7.568s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.242 (   7.568s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.242 (   7.568s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.242 (   7.568s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.242 (   7.568s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.242 (   7.568s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.247 (   7.573s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.247 (   7.573s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.253 (   7.579s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.253 (   7.579s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.253 (   7.579s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.253 (   7.579s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.253 (   7.579s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.253 (   7.579s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.253 (   7.580s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.253 (   7.580s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.253 (   7.580s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.253 (   7.580s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.258 (   7.584s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.258 (   7.584s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.264 (   7.590s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.264 (   7.590s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.264 (   7.590s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.264 (   7.591s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.264 (   7.591s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.264 (   7.591s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.264 (   7.591s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.264 (   7.591s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.265 (   7.591s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.265 (   7.591s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.269 (   7.596s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.270 (   7.596s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.271 (   7.597s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.271 (   7.597s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.271 (   7.597s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.271 (   7.598s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.271 (   7.598s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.271 (   7.598s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.271 (   7.598s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.271 (   7.598s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.271 (   7.598s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.271 (   7.598s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.272 (   7.598s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.272 (   7.598s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.272 (   7.598s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.272 (   7.598s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.272 (   7.598s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.272 (   7.599s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.272 (   7.599s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.272 (   7.599s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.272 (   7.599s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.272 (   7.599s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.272 (   7.599s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.272 (   7.599s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.272 (   7.599s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.272 (   7.599s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.273 (   7.599s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.273 (   7.599s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.273 (   7.599s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.273 (   7.599s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.273 (   7.599s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.273 (   7.599s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.273 (   7.599s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.273 (   7.599s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.273 (   7.599s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.273 (   7.599s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.273 (   7.600s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.273 (   7.600s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.276 (   7.602s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.276 (   7.602s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.276 (   7.602s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.276 (   7.602s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.276 (   7.602s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.276 (   7.602s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.276 (   7.602s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.276 (   7.602s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.276 (   7.602s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.276 (   7.602s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.276 (   7.603s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.276 (   7.603s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.282 (   7.608s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.282 (   7.608s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.282 (   7.608s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.282 (   7.608s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.282 (   7.608s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.282 (   7.608s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.282 (   7.608s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.282 (   7.608s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.282 (   7.609s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.282 (   7.609s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.287 (   7.614s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.287 (   7.614s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.293 (   7.619s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.293 (   7.619s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.293 (   7.619s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.293 (   7.620s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.293 (   7.620s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.293 (   7.620s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.293 (   7.620s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.293 (   7.620s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.293 (   7.620s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.293 (   7.620s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.298 (   7.625s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.298 (   7.625s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.304 (   7.631s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.304 (   7.631s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.304 (   7.631s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.305 (   7.631s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.305 (   7.631s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.305 (   7.631s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.305 (   7.631s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.305 (   7.631s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.305 (   7.631s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.305 (   7.631s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.310 (   7.636s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.310 (   7.636s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.311 (   7.637s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.311 (   7.637s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.311 (   7.637s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.311 (   7.638s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.311 (   7.638s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.311 (   7.638s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.311 (   7.638s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.311 (   7.638s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.312 (   7.638s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.312 (   7.638s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.312 (   7.638s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.312 (   7.638s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.312 (   7.638s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.312 (   7.638s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.312 (   7.638s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.312 (   7.639s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.312 (   7.639s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.312 (   7.639s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.312 (   7.639s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.312 (   7.639s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.312 (   7.639s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.312 (   7.639s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.313 (   7.639s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.313 (   7.639s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.313 (   7.639s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.313 (   7.639s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.313 (   7.639s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.313 (   7.639s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.313 (   7.639s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.313 (   7.639s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.313 (   7.639s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.313 (   7.639s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.313 (   7.640s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.313 (   7.640s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.313 (   7.640s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.313 (   7.640s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.315 (   7.641s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.315 (   7.641s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.315 (   7.641s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.315 (   7.641s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.315 (   7.641s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.315 (   7.641s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.315 (   7.641s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.315 (   7.641s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.315 (   7.642s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.315 (   7.642s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.315 (   7.642s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.315 (   7.642s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.320 (   7.647s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.321 (   7.647s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.321 (   7.647s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.321 (   7.647s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.321 (   7.647s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.321 (   7.647s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.321 (   7.647s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.321 (   7.647s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.321 (   7.647s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.321 (   7.647s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.326 (   7.652s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.326 (   7.652s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.331 (   7.658s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.331 (   7.658s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.331 (   7.658s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.332 (   7.658s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.332 (   7.658s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.332 (   7.658s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.332 (   7.658s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.332 (   7.658s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.332 (   7.658s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.332 (   7.658s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.337 (   7.663s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.337 (   7.663s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.343 (   7.670s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.343 (   7.670s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.343 (   7.670s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.344 (   7.670s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.344 (   7.670s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.344 (   7.670s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.344 (   7.670s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.344 (   7.670s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.344 (   7.670s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.344 (   7.670s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.349 (   7.675s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.349 (   7.675s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.350 (   7.677s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.350 (   7.677s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.350 (   7.677s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.351 (   7.677s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.351 (   7.677s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.351 (   7.677s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.351 (   7.677s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.351 (   7.677s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.351 (   7.677s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.351 (   7.677s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.351 (   7.677s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.351 (   7.677s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.351 (   7.678s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.351 (   7.678s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.351 (   7.678s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.352 (   7.678s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.352 (   7.678s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.352 (   7.678s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.352 (   7.678s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.352 (   7.678s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.352 (   7.678s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.352 (   7.678s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.352 (   7.678s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.352 (   7.678s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.352 (   7.679s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.352 (   7.679s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.352 (   7.679s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.353 (   7.679s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.353 (   7.679s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.353 (   7.679s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.353 (   7.679s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.353 (   7.679s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.353 (   7.679s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.353 (   7.679s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.353 (   7.679s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.353 (   7.679s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.354 (   7.681s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.354 (   7.681s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.354 (   7.681s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.354 (   7.681s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.355 (   7.681s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.355 (   7.681s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.355 (   7.681s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.355 (   7.681s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.355 (   7.681s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.355 (   7.681s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.355 (   7.681s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.355 (   7.681s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.360 (   7.687s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.360 (   7.687s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.360 (   7.687s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.361 (   7.687s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.361 (   7.687s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.361 (   7.687s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.361 (   7.687s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.361 (   7.687s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.361 (   7.687s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.361 (   7.687s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.366 (   7.692s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.366 (   7.692s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.371 (   7.697s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.371 (   7.698s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.371 (   7.698s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.372 (   7.698s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.372 (   7.698s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.372 (   7.698s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.372 (   7.698s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.372 (   7.698s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.372 (   7.698s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.372 (   7.698s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.377 (   7.703s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.377 (   7.703s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.382 (   7.709s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.382 (   7.709s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.382 (   7.709s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.383 (   7.709s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.383 (   7.709s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.383 (   7.709s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.383 (   7.709s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.383 (   7.709s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.383 (   7.709s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.383 (   7.709s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.388 (   7.714s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.388 (   7.714s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.389 (   7.715s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.389 (   7.715s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.389 (   7.715s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.389 (   7.715s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.389 (   7.715s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.389 (   7.715s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.389 (   7.715s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.389 (   7.715s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.389 (   7.716s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.389 (   7.716s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.389 (   7.716s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.389 (   7.716s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.390 (   7.716s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.390 (   7.716s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.390 (   7.716s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.390 (   7.716s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.390 (   7.716s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.390 (   7.716s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.390 (   7.716s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.390 (   7.716s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.390 (   7.716s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.390 (   7.716s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.390 (   7.717s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.390 (   7.717s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.390 (   7.717s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.390 (   7.717s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.390 (   7.717s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.391 (   7.717s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.391 (   7.717s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.391 (   7.717s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.391 (   7.717s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.391 (   7.717s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.391 (   7.717s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.391 (   7.717s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.391 (   7.717s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.391 (   7.717s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.392 (   7.719s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.392 (   7.719s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.392 (   7.719s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.392 (   7.719s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.392 (   7.719s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.392 (   7.719s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.392 (   7.719s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.392 (   7.719s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.393 (   7.719s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.393 (   7.719s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.393 (   7.719s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.393 (   7.719s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.398 (   7.724s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.398 (   7.724s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.398 (   7.724s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.398 (   7.725s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.398 (   7.725s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.398 (   7.725s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.398 (   7.725s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.398 (   7.725s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.398 (   7.725s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.398 (   7.725s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.403 (   7.729s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.403 (   7.729s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.408 (   7.735s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.408 (   7.735s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.408 (   7.735s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.409 (   7.735s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.409 (   7.735s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.409 (   7.735s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.409 (   7.735s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.409 (   7.735s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.409 (   7.735s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.409 (   7.735s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.413 (   7.740s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.413 (   7.740s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.419 (   7.745s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.419 (   7.746s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.419 (   7.746s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.420 (   7.746s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.420 (   7.746s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.420 (   7.746s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.420 (   7.746s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.420 (   7.746s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.420 (   7.746s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.420 (   7.746s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.424 (   7.750s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.424 (   7.750s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.425 (   7.752s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.425 (   7.752s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.425 (   7.752s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.426 (   7.752s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.426 (   7.752s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.426 (   7.752s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.426 (   7.752s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.426 (   7.752s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.426 (   7.752s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.426 (   7.752s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.426 (   7.752s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.426 (   7.752s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.426 (   7.753s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.426 (   7.753s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.426 (   7.753s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.427 (   7.753s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.427 (   7.753s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.427 (   7.753s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.427 (   7.753s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.427 (   7.753s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.427 (   7.753s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.427 (   7.753s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.427 (   7.753s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.427 (   7.753s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.427 (   7.754s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.427 (   7.754s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.427 (   7.754s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.427 (   7.754s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.427 (   7.754s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.427 (   7.754s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.427 (   7.754s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.427 (   7.754s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.427 (   7.754s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.427 (   7.754s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.428 (   7.754s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.428 (   7.754s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.429 (   7.755s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.429 (   7.755s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.429 (   7.755s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.429 (   7.755s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.429 (   7.755s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.429 (   7.755s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.429 (   7.755s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.429 (   7.755s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.429 (   7.756s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.429 (   7.756s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.429 (   7.756s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.429 (   7.756s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.434 (   7.761s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.434 (   7.761s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.434 (   7.761s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.435 (   7.761s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.435 (   7.761s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.435 (   7.761s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.435 (   7.761s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.435 (   7.761s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.435 (   7.761s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.435 (   7.761s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.439 (   7.766s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.439 (   7.766s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.445 (   7.771s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.445 (   7.771s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.445 (   7.771s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.445 (   7.771s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.445 (   7.771s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.445 (   7.771s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.445 (   7.771s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.445 (   7.771s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.445 (   7.772s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.445 (   7.772s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.449 (   7.776s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.450 (   7.776s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.455 (   7.782s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.455 (   7.782s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.455 (   7.782s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.456 (   7.782s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.456 (   7.782s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.456 (   7.782s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.456 (   7.782s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.456 (   7.782s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.456 (   7.782s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.456 (   7.782s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.460 (   7.787s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.460 (   7.787s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.462 (   7.788s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.462 (   7.788s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.462 (   7.788s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.462 (   7.789s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.462 (   7.789s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.462 (   7.789s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.462 (   7.789s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.462 (   7.789s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.462 (   7.789s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.462 (   7.789s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.463 (   7.789s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.463 (   7.789s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.463 (   7.790s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.463 (   7.790s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.463 (   7.790s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.463 (   7.790s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.463 (   7.790s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.463 (   7.790s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.463 (   7.790s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.463 (   7.790s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.463 (   7.790s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.463 (   7.790s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.464 (   7.790s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.464 (   7.790s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.464 (   7.790s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.464 (   7.790s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.464 (   7.790s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.464 (   7.790s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.464 (   7.790s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.464 (   7.790s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.464 (   7.790s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.464 (   7.790s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.464 (   7.791s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.464 (   7.791s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.464 (   7.791s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.464 (   7.791s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.466 (   7.793s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.466 (   7.793s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.466 (   7.793s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.466 (   7.793s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.466 (   7.793s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.466 (   7.793s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.466 (   7.793s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.466 (   7.793s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.467 (   7.793s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.467 (   7.793s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.467 (   7.793s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.467 (   7.793s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.472 (   7.798s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.472 (   7.798s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.472 (   7.798s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.472 (   7.799s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.472 (   7.799s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.472 (   7.799s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.472 (   7.799s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.472 (   7.799s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.472 (   7.799s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.472 (   7.799s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.477 (   7.803s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.477 (   7.803s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.481 (   7.808s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.482 (   7.808s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.482 (   7.808s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.482 (   7.808s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.482 (   7.808s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.482 (   7.808s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.482 (   7.808s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.482 (   7.808s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.482 (   7.808s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.482 (   7.808s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.487 (   7.813s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.487 (   7.813s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.497 (   7.823s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.497 (   7.823s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.497 (   7.823s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.497 (   7.823s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.497 (   7.823s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.497 (   7.823s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.497 (   7.823s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.497 (   7.823s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.497 (   7.824s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.497 (   7.824s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.502 (   7.828s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.502 (   7.828s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.503 (   7.829s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.503 (   7.829s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.503 (   7.829s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.503 (   7.830s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.503 (   7.830s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.503 (   7.830s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.503 (   7.830s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.503 (   7.830s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.503 (   7.830s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.503 (   7.830s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.504 (   7.830s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.504 (   7.830s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.504 (   7.830s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.504 (   7.830s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.504 (   7.830s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.504 (   7.830s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.504 (   7.830s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.504 (   7.830s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.504 (   7.830s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.504 (   7.830s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.504 (   7.831s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.504 (   7.831s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.504 (   7.831s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.504 (   7.831s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.505 (   7.831s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.505 (   7.831s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.505 (   7.831s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.505 (   7.831s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.505 (   7.831s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.505 (   7.831s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.505 (   7.831s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.505 (   7.831s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.505 (   7.831s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.505 (   7.831s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.505 (   7.831s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.505 (   7.831s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.507 (   7.833s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.507 (   7.833s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.507 (   7.833s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.507 (   7.833s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.507 (   7.833s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.507 (   7.833s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.507 (   7.833s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.507 (   7.833s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.507 (   7.833s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.507 (   7.834s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.507 (   7.834s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.507 (   7.834s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.512 (   7.839s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.512 (   7.839s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.512 (   7.839s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.513 (   7.839s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.513 (   7.839s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.513 (   7.839s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.513 (   7.839s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.513 (   7.839s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.513 (   7.839s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.513 (   7.839s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.517 (   7.844s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.517 (   7.844s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.522 (   7.848s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.522 (   7.848s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.522 (   7.848s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.522 (   7.848s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.522 (   7.848s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.522 (   7.848s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.522 (   7.848s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.522 (   7.848s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.522 (   7.848s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.522 (   7.848s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.526 (   7.853s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.526 (   7.853s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.533 (   7.860s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.533 (   7.860s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.533 (   7.860s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.534 (   7.860s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.534 (   7.860s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.534 (   7.860s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.534 (   7.860s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.534 (   7.860s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.534 (   7.860s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.534 (   7.860s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.538 (   7.864s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.538 (   7.864s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.539 (   7.865s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.539 (   7.865s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.539 (   7.865s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.539 (   7.866s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.539 (   7.866s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.539 (   7.866s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.539 (   7.866s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.539 (   7.866s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.540 (   7.866s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.540 (   7.866s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.540 (   7.866s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.540 (   7.866s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.540 (   7.866s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.540 (   7.866s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.540 (   7.866s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.540 (   7.867s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.540 (   7.867s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.540 (   7.867s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.540 (   7.867s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.540 (   7.867s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.540 (   7.867s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.540 (   7.867s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.540 (   7.867s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.540 (   7.867s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.541 (   7.867s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.541 (   7.867s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.541 (   7.867s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.541 (   7.867s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.541 (   7.867s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.541 (   7.867s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.541 (   7.867s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.541 (   7.867s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.541 (   7.867s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.541 (   7.867s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.541 (   7.868s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.541 (   7.868s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.543 (   7.869s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.543 (   7.869s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.543 (   7.869s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.543 (   7.869s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.543 (   7.869s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.543 (   7.869s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.543 (   7.869s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.543 (   7.869s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.543 (   7.869s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.543 (   7.869s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.543 (   7.869s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.543 (   7.869s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.548 (   7.875s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.548 (   7.875s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.548 (   7.875s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.548 (   7.875s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.548 (   7.875s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.548 (   7.875s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.548 (   7.875s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.548 (   7.875s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.549 (   7.875s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.549 (   7.875s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.553 (   7.879s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.553 (   7.879s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.556 (   7.883s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.556 (   7.883s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.556 (   7.883s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.557 (   7.883s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.557 (   7.883s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.557 (   7.883s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.557 (   7.883s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.557 (   7.883s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.557 (   7.883s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.557 (   7.883s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.561 (   7.887s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.561 (   7.887s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.568 (   7.894s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.568 (   7.894s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.568 (   7.894s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.568 (   7.895s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.568 (   7.895s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.568 (   7.895s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.568 (   7.895s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.568 (   7.895s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.568 (   7.895s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.568 (   7.895s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.573 (   7.899s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.573 (   7.899s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.574 (   7.900s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.574 (   7.900s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.574 (   7.900s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.574 (   7.901s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.574 (   7.901s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.574 (   7.901s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.574 (   7.901s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.574 (   7.901s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.574 (   7.901s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.574 (   7.901s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.574 (   7.901s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.574 (   7.901s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.575 (   7.901s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.575 (   7.901s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.575 (   7.901s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.575 (   7.902s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.575 (   7.902s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.575 (   7.902s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.575 (   7.902s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.575 (   7.902s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.575 (   7.902s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.575 (   7.902s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.575 (   7.902s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.575 (   7.902s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.576 (   7.902s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.576 (   7.902s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.576 (   7.902s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.576 (   7.902s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.576 (   7.902s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.576 (   7.902s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.576 (   7.902s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.576 (   7.902s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.576 (   7.902s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.576 (   7.903s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.576 (   7.903s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.576 (   7.903s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.578 (   7.905s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.578 (   7.905s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.578 (   7.905s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.578 (   7.905s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.578 (   7.905s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.578 (   7.905s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.578 (   7.905s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.578 (   7.905s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.579 (   7.905s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.579 (   7.905s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.579 (   7.905s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.579 (   7.905s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.584 (   7.910s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.584 (   7.910s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.584 (   7.910s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.584 (   7.910s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.584 (   7.910s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.584 (   7.910s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.584 (   7.910s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.584 (   7.910s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.584 (   7.911s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.584 (   7.911s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.588 (   7.915s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.588 (   7.915s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.593 (   7.919s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.593 (   7.919s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.593 (   7.919s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.593 (   7.919s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.593 (   7.919s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.593 (   7.919s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.593 (   7.919s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.593 (   7.920s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.593 (   7.920s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.593 (   7.920s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.597 (   7.924s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.597 (   7.924s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.603 (   7.929s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.603 (   7.929s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.603 (   7.929s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.603 (   7.929s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.603 (   7.929s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.603 (   7.929s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.603 (   7.929s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.603 (   7.929s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.603 (   7.930s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.603 (   7.930s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.607 (   7.934s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.607 (   7.934s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.609 (   7.935s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.609 (   7.935s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.609 (   7.935s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.609 (   7.935s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.609 (   7.935s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.609 (   7.935s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.609 (   7.935s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.609 (   7.935s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.609 (   7.935s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.609 (   7.935s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.609 (   7.935s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.609 (   7.935s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.609 (   7.936s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.609 (   7.936s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.609 (   7.936s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.609 (   7.936s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.609 (   7.936s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.609 (   7.936s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.609 (   7.936s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.609 (   7.936s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.610 (   7.936s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.610 (   7.936s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.610 (   7.936s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.610 (   7.936s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.610 (   7.936s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.610 (   7.936s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.610 (   7.936s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.610 (   7.937s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.610 (   7.937s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.610 (   7.937s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.610 (   7.937s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.610 (   7.937s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.610 (   7.937s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.610 (   7.937s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.610 (   7.937s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.610 (   7.937s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.612 (   7.938s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.612 (   7.938s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.612 (   7.938s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.612 (   7.939s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.612 (   7.939s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.612 (   7.939s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.612 (   7.939s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.612 (   7.939s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.612 (   7.939s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.612 (   7.939s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.612 (   7.939s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.612 (   7.939s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.617 (   7.944s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.617 (   7.944s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.618 (   7.944s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.618 (   7.944s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.618 (   7.944s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.618 (   7.944s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.618 (   7.944s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.618 (   7.944s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.618 (   7.944s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.618 (   7.944s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.622 (   7.949s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.622 (   7.949s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.626 (   7.953s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.626 (   7.953s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.626 (   7.953s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.627 (   7.953s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.627 (   7.953s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.627 (   7.953s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.627 (   7.953s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.627 (   7.953s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.627 (   7.953s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.627 (   7.953s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.631 (   7.957s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.631 (   7.957s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.636 (   7.963s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.636 (   7.963s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.636 (   7.963s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.637 (   7.963s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.637 (   7.963s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.637 (   7.963s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.637 (   7.963s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.637 (   7.963s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.637 (   7.963s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.637 (   7.963s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.641 (   7.967s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.641 (   7.967s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.642 (   7.968s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.642 (   7.968s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.642 (   7.968s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.642 (   7.969s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.642 (   7.969s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.642 (   7.969s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.642 (   7.969s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.642 (   7.969s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.642 (   7.969s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.642 (   7.969s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.642 (   7.969s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.642 (   7.969s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.643 (   7.969s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.643 (   7.969s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.643 (   7.969s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.643 (   7.969s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.643 (   7.969s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.643 (   7.969s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.643 (   7.969s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.643 (   7.969s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.643 (   7.970s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.643 (   7.970s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.643 (   7.970s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.643 (   7.970s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.644 (   7.970s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.644 (   7.970s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.644 (   7.970s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.644 (   7.970s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.644 (   7.970s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.644 (   7.970s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.644 (   7.970s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.644 (   7.970s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.644 (   7.970s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.644 (   7.970s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.644 (   7.970s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.644 (   7.970s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.645 (   7.972s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.645 (   7.972s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.645 (   7.972s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.646 (   7.972s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.646 (   7.972s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.646 (   7.972s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.646 (   7.972s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.646 (   7.972s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.646 (   7.972s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.646 (   7.972s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.646 (   7.972s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.646 (   7.972s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.651 (   7.977s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.651 (   7.977s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.651 (   7.977s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.651 (   7.977s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.651 (   7.977s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.651 (   7.977s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.651 (   7.977s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.651 (   7.977s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.651 (   7.978s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.651 (   7.978s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.656 (   7.982s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.656 (   7.982s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.661 (   7.987s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.661 (   7.987s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.661 (   7.987s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.661 (   7.987s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.661 (   7.987s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.661 (   7.987s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.661 (   7.987s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.661 (   7.987s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.661 (   7.987s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.661 (   7.987s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.665 (   7.991s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.665 (   7.991s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.672 (   7.998s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.672 (   7.998s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.672 (   7.998s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.672 (   7.999s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.672 (   7.999s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.672 (   7.999s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.672 (   7.999s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.672 (   7.999s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.672 (   7.999s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.672 (   7.999s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.676 (   8.003s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.676 (   8.003s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.678 (   8.004s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.678 (   8.004s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.678 (   8.004s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.678 (   8.004s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.678 (   8.004s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.678 (   8.004s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.678 (   8.004s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.678 (   8.004s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.678 (   8.005s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.678 (   8.005s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.678 (   8.005s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.678 (   8.005s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.679 (   8.005s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.679 (   8.005s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.679 (   8.005s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.679 (   8.005s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.679 (   8.005s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.679 (   8.005s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.679 (   8.005s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.679 (   8.005s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.679 (   8.005s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.679 (   8.005s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.679 (   8.006s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.679 (   8.006s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.680 (   8.006s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.680 (   8.006s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.680 (   8.006s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.680 (   8.006s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.680 (   8.006s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.680 (   8.006s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.680 (   8.006s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.680 (   8.006s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.680 (   8.006s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.680 (   8.006s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.680 (   8.007s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.680 (   8.007s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:40:59.682 (   8.008s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.682 (   8.008s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.682 (   8.008s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.682 (   8.008s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.682 (   8.008s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:40:59.682 (   8.008s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:40:59.682 (   8.008s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:40:59.682 (   8.008s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:40:59.682 (   8.008s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:40:59.682 (   8.008s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:00.141 (   8.467s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:00.141 (   8.467s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:00.141 (   8.467s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:41:00.141 (   8.467s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:41:00.141 (   8.467s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:41:00.141 (   8.467s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:41:00.141 (   8.468s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User is requesting to copy the data from a runtime tensor with data type: Int32 into buffer with expected data type: Int64, the values will be casted, this may impact the throughput and the integrity of the data.
2025-09-15 15:41:00.142 (   8.468s) [        FAEC5640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:00.149 (   8.476s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:00.149 (   8.476s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:00.149 (   8.476s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:41:00.149 (   8.476s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:41:00.149 (   8.476s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:41:00.149 (   8.476s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:41:00.150 (   8.476s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User is requesting to copy the data from a runtime tensor with data type: Int32 into buffer with expected data type: Int64, the values will be casted, this may impact the throughput and the integrity of the data.
2025-09-15 15:41:00.150 (   8.476s) [        F27FC640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:00.150 (   8.476s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:00.150 (   8.476s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:00.150 (   8.476s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:00.156 (   8.482s) [        F073E000]     client_instance.cc:428      1| ClientInstance::PJRT_Client_Compile
2025-09-15 15:41:00.156 (   8.482s) [        F073E000]      module_builder.cc:103      1| ModuleBuilder::buildModule
2025-09-15 15:41:00.159 (   8.486s) [        F073E000]      module_builder.cc:165      1| VHLO Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) {
    %0 = "vhlo.custom_call_v1"(%arg0) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"xla.sdy.FuncResultSharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<true>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {}, {}, {}]>]>">}>} : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>
    "vhlo.return_v1"(%0) : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>]>, res_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, sym_visibility = #vhlo.string_v1<"">}
}
2025-09-15 15:41:00.165 (   8.492s) [        F073E000]      module_builder.cc:185      1| Is using shardy? true
2025-09-15 15:41:00.167 (   8.494s) [        F073E000]      module_builder.cc:203      1| SHLO Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}) -> (tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>}) {
    return %arg0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:41:00.168 (   8.495s) [        F073E000]      module_builder.cc:212      1| SHLO Module after frontend StableHLO pipeline:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}) -> (tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>}) {
    return %arg0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:41:00.177 (   8.503s) [        F073E000]      module_builder.cc:486      1| SHLO Module after compiler StableHLO pipeline:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = sdy.manual_computation(%arg0) in_shardings=[<@mesh, [{}, {"_axis_0"}, {}, {}]>] out_shardings=[<@mesh, [{}, {}, {}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg1: tensor<1x4x1024x128xbf16>) {
      %1 = "stablehlo.all_gather"(%arg1) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> : (tensor<1x4x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
      sdy.return %1 : tensor<1x8x1024x128xbf16>
    } : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    return %0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:41:00.180 (   8.506s) [        F073E000]      module_builder.cc:510      1| TTIR Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = ttir.empty() : tensor<1x4x1024x128xbf16>
    %1 = "ttir.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16>, tensor<1x4x1024x128xbf16>) -> tensor<1x4x1024x128xbf16>
    %2 = ttir.empty() : tensor<1x8x1024x128xbf16>
    %3 = "ttir.all_gather"(%1, %2) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<1x4x1024x128xbf16>, tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %4 = ttir.empty() : tensor<1x8x1024x128xbf16>
    %5 = "ttir.mesh_shard"(%3, %4) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16>, tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    return %5 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:41:00.180 (   8.507s) [        F073E000]      module_builder.cc:561   WARN| `mhlo.num_partitions` attribute not found, assuming default number of partitions: 1
2025-09-15 15:41:00.180 (   8.507s) [        F073E000]      module_builder.cc:575   WARN| `mhlo.num_replicas` attribute not found, assuming default number of replicas: 1
2025-09-15 15:41:00.180 (   8.507s) [        F073E000]      module_builder.cc:585   WARN| Num replicas and num partitions are not set, inferring the number of devices from mesh shape
2025-09-15 15:41:00.217 (   8.543s) [        F073E000]      module_builder.cc:630      1| TTNN Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073185088, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073193408, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x16, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 2)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 2, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %2 = "ttnn.all_gather"(%1, %0) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.mesh_shard"(%2, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %3 : tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
    }
  }
}
2025-09-15 15:41:00.227 (   8.553s) [        F073E000]loaded_executable_insta:441      1| LoadedExecutableInstance::PJRT_LoadedExecutable_GetExecutable
2025-09-15 15:41:00.227 (   8.553s) [        F073E000]loaded_executable_insta:460      1| LoadedExecutableInstance::PJRT_LoadedExecutable_AddressableDevices
2025-09-15 15:41:00.227 (   8.554s) [        F073E000]              stubs.inc:76    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2025-09-15 15:41:00.227 (   8.554s) [        F073E000]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-09-15 15:41:00.227 (   8.554s) [        F073E000]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-09-15 15:41:00.227 (   8.554s) [        F073E000]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-09-15 15:41:00.227 (   8.554s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:41:00.227 (   8.554s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:41:00.231 (   8.557s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:41:00.231 (   8.557s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:41:00.233 (   8.559s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:00.233 (   8.559s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:00.233 (   8.559s) [        F073E000]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:00.233 (   8.559s) [        F073E000]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:00.233 (   8.559s) [        F073E000]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:00.233 (   8.559s) [        F073E000]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:00.233 (   8.559s) [        F073E000] executable_instance.cc:138      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-09-15 15:41:00.233 (   8.559s) [        F073E000]loaded_executable_insta:497      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-09-15 15:41:00.233 (   8.559s) [        F073E000]loaded_executable_insta:81       1| LoadedExecutableInstance::Execute
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Device grid size = { 8, 8 }
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.all_gather"(%1, %0) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("p0.1")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("p0.1")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.mesh_shard"(%2, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main
2025-09-15 15:41:06.295 (  14.622s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:06.296 (  14.622s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:06.296 (  14.622s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:06.296 (  14.622s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:06.296 (  14.622s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:41:06.296 (  14.622s) [        F073E000]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:41:06.296 (  14.622s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:41:06.296 (  14.622s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:41:06.296 (  14.622s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:41:06.296 (  14.622s) [        F073E000]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:41:06.296 (  14.622s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:41:06.296 (  14.622s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:41:06.296 (  14.622s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:41:06.296 (  14.622s) [        F073E000] executable_instance.cc:58       1| ExecutableInstance::PJRT_Executable_Destroy
2025-09-15 15:41:06.296 (  14.622s) [        F073E000]loaded_executable_insta:432      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Destroy
2025-09-15 15:41:06.296 (  14.622s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:41:06.296 (  14.622s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:41:06.296 (  14.622s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:41:06.296 (  14.622s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:41:06.296 (  14.623s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:06.296 (  14.623s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:41:06.297 (  14.623s) [        E9FFB640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:06.303 (  14.629s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:06.303 (  14.629s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:06.303 (  14.630s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:06.310 (  14.636s) [        F073E000]     client_instance.cc:428      1| ClientInstance::PJRT_Client_Compile
2025-09-15 15:41:06.310 (  14.636s) [        F073E000]      module_builder.cc:103      1| ModuleBuilder::buildModule
2025-09-15 15:41:06.310 (  14.637s) [        F073E000]      module_builder.cc:165      1| VHLO Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) {
    %0 = "vhlo.custom_call_v1"(%arg0) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"xla.sdy.FuncResultSharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<true>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {}, {}, {}]>]>">}>} : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>
    "vhlo.return_v1"(%0) : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>]>, res_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, sym_visibility = #vhlo.string_v1<"">}
}
2025-09-15 15:41:06.312 (  14.638s) [        F073E000]      module_builder.cc:185      1| Is using shardy? true
2025-09-15 15:41:06.314 (  14.641s) [        F073E000]      module_builder.cc:203      1| SHLO Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}) -> (tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>}) {
    return %arg0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:41:06.314 (  14.641s) [        F073E000]      module_builder.cc:212      1| SHLO Module after frontend StableHLO pipeline:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}) -> (tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>}) {
    return %arg0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:41:06.318 (  14.645s) [        F073E000]      module_builder.cc:486      1| SHLO Module after compiler StableHLO pipeline:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = sdy.manual_computation(%arg0) in_shardings=[<@mesh, [{}, {"_axis_0"}, {}, {}]>] out_shardings=[<@mesh, [{}, {}, {}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg1: tensor<1x4x1024x128xbf16>) {
      %1 = "stablehlo.all_gather"(%arg1) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> : (tensor<1x4x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
      sdy.return %1 : tensor<1x8x1024x128xbf16>
    } : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    return %0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:41:06.321 (  14.647s) [        F073E000]      module_builder.cc:510      1| TTIR Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = ttir.empty() : tensor<1x4x1024x128xbf16>
    %1 = "ttir.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16>, tensor<1x4x1024x128xbf16>) -> tensor<1x4x1024x128xbf16>
    %2 = ttir.empty() : tensor<1x8x1024x128xbf16>
    %3 = "ttir.all_gather"(%1, %2) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<1x4x1024x128xbf16>, tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %4 = ttir.empty() : tensor<1x8x1024x128xbf16>
    %5 = "ttir.mesh_shard"(%3, %4) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16>, tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    return %5 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:41:06.321 (  14.648s) [        F073E000]      module_builder.cc:561   WARN| `mhlo.num_partitions` attribute not found, assuming default number of partitions: 1
2025-09-15 15:41:06.321 (  14.648s) [        F073E000]      module_builder.cc:575   WARN| `mhlo.num_replicas` attribute not found, assuming default number of replicas: 1
2025-09-15 15:41:06.321 (  14.648s) [        F073E000]      module_builder.cc:585   WARN| Num replicas and num partitions are not set, inferring the number of devices from mesh shape
2025-09-15 15:41:06.354 (  14.680s) [        F073E000]      module_builder.cc:630      1| TTNN Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073185088, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073193408, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x16, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 2)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 2, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %2 = "ttnn.all_gather"(%1, %0) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.mesh_shard"(%2, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %3 : tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
    }
  }
}
2025-09-15 15:41:06.365 (  14.691s) [        F073E000]loaded_executable_insta:441      1| LoadedExecutableInstance::PJRT_LoadedExecutable_GetExecutable
2025-09-15 15:41:06.365 (  14.691s) [        F073E000]loaded_executable_insta:460      1| LoadedExecutableInstance::PJRT_LoadedExecutable_AddressableDevices
2025-09-15 15:41:06.366 (  14.692s) [        F073E000]              stubs.inc:76    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2025-09-15 15:41:06.366 (  14.692s) [        F073E000]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-09-15 15:41:06.366 (  14.692s) [        F073E000]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-09-15 15:41:06.366 (  14.692s) [        F073E000]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-09-15 15:41:06.366 (  14.692s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:41:06.366 (  14.692s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:41:06.369 (  14.695s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:41:06.369 (  14.695s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:41:06.371 (  14.697s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:06.371 (  14.697s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:06.371 (  14.697s) [        F073E000]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:06.371 (  14.697s) [        F073E000]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:06.371 (  14.697s) [        F073E000]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:06.371 (  14.697s) [        F073E000]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:06.371 (  14.697s) [        F073E000] executable_instance.cc:138      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-09-15 15:41:06.371 (  14.697s) [        F073E000]loaded_executable_insta:497      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-09-15 15:41:06.371 (  14.697s) [        F073E000]loaded_executable_insta:81       1| LoadedExecutableInstance::Execute
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Device grid size = { 8, 8 }
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.all_gather"(%1, %0) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("p0.1")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("p0.1")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.mesh_shard"(%2, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main
2025-09-15 15:41:06.672 (  14.999s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:06.673 (  14.999s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:06.673 (  14.999s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:06.673 (  14.999s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:06.673 (  14.999s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:41:06.673 (  14.999s) [        F073E000]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:41:06.673 (  14.999s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:41:06.673 (  14.999s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:41:06.673 (  14.999s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:41:06.673 (  14.999s) [        F073E000]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:41:06.673 (  14.999s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:41:06.673 (  14.999s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:41:06.673 (  14.999s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:41:06.673 (  14.999s) [        F073E000] executable_instance.cc:58       1| ExecutableInstance::PJRT_Executable_Destroy
2025-09-15 15:41:06.673 (  14.999s) [        F073E000]loaded_executable_insta:432      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Destroy
2025-09-15 15:41:06.673 (  14.999s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:41:06.673 (  14.999s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:41:06.673 (  14.999s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:41:06.673 (  15.000s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:41:06.673 (  15.000s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:06.673 (  15.000s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:41:06.674 (  15.000s) [        EA7FC640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
Example inputs: [tensor([[128000,     40,   1093,   4737,  23291,    304,    279]],
       device='xla:0'), tensor([0, 1, 2, 3, 4, 5, 6], device='xla:0'), tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='xla:0',
       dtype=torch.bfloat16), tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='xla:0',
       dtype=torch.bfloat16)]
gm graph before export for training
opcode         name                                                                  target                                                                       args                                                                                                                                                   kwargs
-------------  --------------------------------------------------------------------  ---------------------------------------------------------------------------  -----------------------------------------------------------------------------------------------------------------------------------------------------  -------------------------------------------------------------------------------------------------------
placeholder    l_kwargs_input_ids_                                                   L_kwargs_input_ids_                                                          ()                                                                                                                                                     {}
placeholder    l_kwargs_cache_position_                                              L_kwargs_cache_position_                                                     ()                                                                                                                                                     {}
placeholder    l_kwargs_past_key_values_key_cache_0_                                 L_kwargs_past_key_values_key_cache_0_                                        ()                                                                                                                                                     {}
placeholder    l_kwargs_past_key_values_value_cache_0_                               L_kwargs_past_key_values_value_cache_0_                                      ()                                                                                                                                                     {}
call_module    inputs_embeds                                                         L__self___model_embed_tokens                                                 (l_kwargs_input_ids_,)                                                                                                                                 {}
call_method    position_ids                                                          unsqueeze                                                                    (l_kwargs_cache_position_, 0)                                                                                                                          {}
call_function  getitem                                                               <built-in function getitem>                                                  (l_kwargs_past_key_values_key_cache_0_, (0, 0))                                                                                                        {}
call_method    any_1                                                                 any                                                                          (getitem,)                                                                                                                                             {'dim': -1}
call_method    past_seen_tokens                                                      sum                                                                          (any_1,)                                                                                                                                               {}
call_function  causal_mask                                                           <built-in method full of type object at 0x7f26e82f6f80>                      ((7, 1024),)                                                                                                                                           {'fill_value': -3.3895313892515355e+38, 'dtype': torch.bfloat16, 'device': device(type='xla', index=0)}
call_function  causal_mask_1                                                         <built-in method triu of type object at 0x7f26e82f6f80>                      (causal_mask,)                                                                                                                                         {'diagonal': 1}
call_function  arange                                                                <built-in method arange of type object at 0x7f26e82f6f80>                    (1024,)                                                                                                                                                {'device': device(type='xla', index=0)}
call_method    reshape                                                               reshape                                                                      (l_kwargs_cache_position_, -1, 1)                                                                                                                      {}
call_function  gt                                                                    <built-in function gt>                                                       (arange, reshape)                                                                                                                                      {}
call_function  causal_mask_2                                                         <built-in function imul>                                                     (causal_mask_1, gt)                                                                                                                                    {}
call_function  getitem_1                                                             <built-in function getitem>                                                  (causal_mask_2, (None, None, slice(None, None, None), slice(None, None, None)))                                                                        {}
call_method    causal_mask_3                                                         expand                                                                       (getitem_1, 1, 1, -1, -1)                                                                                                                              {}
get_attr       l__self___model_rotary_emb_inv_freq                                   L__self___model_rotary_emb_inv_freq                                          ()                                                                                                                                                     {}
call_function  getitem_2                                                             <built-in function getitem>                                                  (l__self___model_rotary_emb_inv_freq, (None, slice(None, None, None), None))                                                                           {}
call_method    float_1                                                               float                                                                        (getitem_2,)                                                                                                                                           {}
call_method    expand_1                                                              expand                                                                       (float_1, 1, -1, 1)                                                                                                                                    {}
call_method    inv_freq_expanded                                                     to                                                                           (expand_1, device(type='xla', index=0))                                                                                                                {}
call_function  getitem_3                                                             <built-in function getitem>                                                  (position_ids, (slice(None, None, None), None, slice(None, None, None)))                                                                               {}
call_method    position_ids_expanded                                                 float                                                                        (getitem_3,)                                                                                                                                           {}
call_function  _enter_autocast                                                       <function _enter_autocast at 0x7f26224fa160>                                 ('xla', None, False, None)                                                                                                                             {}
call_method    float_3                                                               float                                                                        (inv_freq_expanded,)                                                                                                                                   {}
call_method    float_4                                                               float                                                                        (position_ids_expanded,)                                                                                                                               {}
call_function  matmul                                                                <built-in function matmul>                                                   (float_3, float_4)                                                                                                                                     {}
call_method    freqs                                                                 transpose                                                                    (matmul, 1, 2)                                                                                                                                         {}
call_function  emb                                                                   <built-in method cat of type object at 0x7f26e82f6f80>                       ((freqs, freqs),)                                                                                                                                      {'dim': -1}
call_method    cos                                                                   cos                                                                          (emb,)                                                                                                                                                 {}
call_function  cos_1                                                                 <built-in function mul>                                                      (cos, 1.0)                                                                                                                                             {}
call_method    sin                                                                   sin                                                                          (emb,)                                                                                                                                                 {}
call_function  sin_1                                                                 <built-in function mul>                                                      (sin, 1.0)                                                                                                                                             {}
call_function  _exit_autocast                                                        <function _exit_autocast at 0x7f26224fa480>                                  (_enter_autocast,)                                                                                                                                     {}
call_method    cos_2                                                                 to                                                                           (cos_1,)                                                                                                                                               {'dtype': torch.bfloat16}
call_method    sin_2                                                                 to                                                                           (sin_1,)                                                                                                                                               {'dtype': torch.bfloat16}
call_function  _log_api_usage_once                                                   <built-in method _log_api_usage_once of PyCapsule object at 0x7f2655575740>  ('python.nn_module',)                                                                                                                                  {}
call_method    hidden_states                                                         to                                                                           (inputs_embeds, torch.float32)                                                                                                                         {}
call_method    pow_1                                                                 pow                                                                          (hidden_states, 2)                                                                                                                                     {}
call_method    variance                                                              mean                                                                         (pow_1, -1)                                                                                                                                            {'keepdim': True}
call_function  add                                                                   <built-in function add>                                                      (variance, 1e-05)                                                                                                                                      {}
call_function  rsqrt                                                                 <built-in method rsqrt of type object at 0x7f26e82f6f80>                     (add,)                                                                                                                                                 {}
call_function  hidden_states_1                                                       <built-in function mul>                                                      (hidden_states, rsqrt)                                                                                                                                 {}
get_attr       l__self___model_layers__modules__0___input_layernorm_weight           L__self___model_layers__modules__0___input_layernorm_weight                  ()                                                                                                                                                     {}
call_method    to_4                                                                  to                                                                           (hidden_states_1, torch.bfloat16)                                                                                                                      {}
call_function  hidden_states_2                                                       <built-in function mul>                                                      (l__self___model_layers__modules__0___input_layernorm_weight, to_4)                                                                                    {}
call_module    l__self___model_layers__modules__0___self_attn_q_proj                 L__self___model_layers__modules__0___self_attn_q_proj                        (hidden_states_2,)                                                                                                                                     {}
call_method    view                                                                  view                                                                         (l__self___model_layers__modules__0___self_attn_q_proj, (1, 7, -1, 128))                                                                               {}
call_method    query_states                                                          transpose                                                                    (view, 1, 2)                                                                                                                                           {}
call_module    l__self___model_layers__modules__0___self_attn_k_proj                 L__self___model_layers__modules__0___self_attn_k_proj                        (hidden_states_2,)                                                                                                                                     {}
call_method    view_1                                                                view                                                                         (l__self___model_layers__modules__0___self_attn_k_proj, (1, 7, -1, 128))                                                                               {}
call_method    key_states                                                            transpose                                                                    (view_1, 1, 2)                                                                                                                                         {}
call_module    l__self___model_layers__modules__0___self_attn_v_proj                 L__self___model_layers__modules__0___self_attn_v_proj                        (hidden_states_2,)                                                                                                                                     {}
call_method    view_2                                                                view                                                                         (l__self___model_layers__modules__0___self_attn_v_proj, (1, 7, -1, 128))                                                                               {}
call_method    value_states                                                          transpose                                                                    (view_2, 1, 2)                                                                                                                                         {}
call_method    cos_3                                                                 unsqueeze                                                                    (cos_2, 1)                                                                                                                                             {}
call_method    sin_3                                                                 unsqueeze                                                                    (sin_2, 1)                                                                                                                                             {}
call_function  mul_4                                                                 <built-in function mul>                                                      (query_states, cos_3)                                                                                                                                  {}
call_function  x1                                                                    <built-in function getitem>                                                  (query_states, (Ellipsis, slice(None, 64, None)))                                                                                                      {}
call_function  x2                                                                    <built-in function getitem>                                                  (query_states, (Ellipsis, slice(64, None, None)))                                                                                                      {}
call_function  neg                                                                   <built-in function neg>                                                      (x2,)                                                                                                                                                  {}
call_function  cat_1                                                                 <built-in method cat of type object at 0x7f26e82f6f80>                       ((neg, x1),)                                                                                                                                           {'dim': -1}
call_function  mul_5                                                                 <built-in function mul>                                                      (cat_1, sin_3)                                                                                                                                         {}
call_function  q_embed                                                               <built-in function add>                                                      (mul_4, mul_5)                                                                                                                                         {}
call_function  mul_6                                                                 <built-in function mul>                                                      (key_states, cos_3)                                                                                                                                    {}
call_function  x1_1                                                                  <built-in function getitem>                                                  (key_states, (Ellipsis, slice(None, 64, None)))                                                                                                        {}
call_function  x2_1                                                                  <built-in function getitem>                                                  (key_states, (Ellipsis, slice(64, None, None)))                                                                                                        {}
call_function  neg_1                                                                 <built-in function neg>                                                      (x2_1,)                                                                                                                                                {}
call_function  cat_2                                                                 <built-in method cat of type object at 0x7f26e82f6f80>                       ((neg_1, x1_1),)                                                                                                                                       {'dim': -1}
call_function  mul_7                                                                 <built-in function mul>                                                      (cat_2, sin_3)                                                                                                                                         {}
call_function  k_embed                                                               <built-in function add>                                                      (mul_6, mul_7)                                                                                                                                         {}
call_method    key_states_1                                                          to                                                                           (k_embed, torch.bfloat16)                                                                                                                              {}
call_method    value_states_1                                                        to                                                                           (value_states, torch.bfloat16)                                                                                                                         {}
call_method    index_copy_                                                           index_copy_                                                                  (l_kwargs_past_key_values_key_cache_0_, 2, l_kwargs_cache_position_, key_states_1)                                                                     {}
call_method    index_copy__1                                                         index_copy_                                                                  (l_kwargs_past_key_values_value_cache_0_, 2, l_kwargs_cache_position_, value_states_1)                                                                 {}
call_function  getitem_8                                                             <built-in function getitem>                                                  (l_kwargs_past_key_values_key_cache_0_, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))    {}
call_method    hidden_states_3                                                       expand                                                                       (getitem_8, 1, 8, 3, 1024, 128)                                                                                                                        {}
call_method    key_states_2                                                          reshape                                                                      (hidden_states_3, 1, 24, 1024, 128)                                                                                                                    {}
call_function  getitem_9                                                             <built-in function getitem>                                                  (l_kwargs_past_key_values_value_cache_0_, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))  {}
call_method    hidden_states_4                                                       expand                                                                       (getitem_9, 1, 8, 3, 1024, 128)                                                                                                                        {}
call_method    value_states_2                                                        reshape                                                                      (hidden_states_4, 1, 24, 1024, 128)                                                                                                                    {}
call_method    transpose_4                                                           transpose                                                                    (key_states_2, 2, 3)                                                                                                                                   {}
call_function  matmul_1                                                              <built-in method matmul of type object at 0x7f26e82f6f80>                    (q_embed, transpose_4)                                                                                                                                 {}
call_function  attn_weights                                                          <built-in function mul>                                                      (matmul_1, 0.08838834764831845)                                                                                                                        {}
call_function  causal_mask_4                                                         <built-in function getitem>                                                  (causal_mask_3, (slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 1024, None)))                                  {}
call_function  attn_weights_1                                                        <built-in function add>                                                      (attn_weights, causal_mask_4)                                                                                                                          {}
call_function  softmax                                                               <function softmax at 0x7f25835fb600>                                         (attn_weights_1,)                                                                                                                                      {'dim': -1, 'dtype': torch.float32}
call_method    attn_weights_2                                                        to                                                                           (softmax, torch.bfloat16)                                                                                                                              {}
call_function  attn_weights_3                                                        <function dropout at 0x7f25835fa980>                                         (attn_weights_2,)                                                                                                                                      {'p': 0.0, 'training': False}
call_function  attn_output                                                           <built-in method matmul of type object at 0x7f26e82f6f80>                    (attn_weights_3, value_states_2)                                                                                                                       {}
call_method    transpose_5                                                           transpose                                                                    (attn_output, 1, 2)                                                                                                                                    {}
call_method    attn_output_1                                                         contiguous                                                                   (transpose_5,)                                                                                                                                         {}
call_method    reshape_3                                                             reshape                                                                      (attn_output_1, 1, 7, -1)                                                                                                                              {}
call_method    attn_output_2                                                         contiguous                                                                   (reshape_3,)                                                                                                                                           {}
call_module    attn_output_3                                                         L__self___model_layers__modules__0___self_attn_o_proj                        (attn_output_2,)                                                                                                                                       {}
call_function  hidden_states_5                                                       <built-in function add>                                                      (inputs_embeds, attn_output_3)                                                                                                                         {}
call_method    hidden_states_6                                                       to                                                                           (hidden_states_5, torch.float32)                                                                                                                       {}
call_method    pow_2                                                                 pow                                                                          (hidden_states_6, 2)                                                                                                                                   {}
call_method    variance_1                                                            mean                                                                         (pow_2, -1)                                                                                                                                            {'keepdim': True}
call_function  add_5                                                                 <built-in function add>                                                      (variance_1, 1e-05)                                                                                                                                    {}
call_function  rsqrt_1                                                               <built-in method rsqrt of type object at 0x7f26e82f6f80>                     (add_5,)                                                                                                                                               {}
call_function  hidden_states_7                                                       <built-in function mul>                                                      (hidden_states_6, rsqrt_1)                                                                                                                             {}
get_attr       l__self___model_layers__modules__0___post_attention_layernorm_weight  L__self___model_layers__modules__0___post_attention_layernorm_weight         ()                                                                                                                                                     {}
call_method    to_9                                                                  to                                                                           (hidden_states_7, torch.bfloat16)                                                                                                                      {}
call_function  hidden_states_8                                                       <built-in function mul>                                                      (l__self___model_layers__modules__0___post_attention_layernorm_weight, to_9)                                                                           {}
call_module    l__self___model_layers__modules__0___mlp_gate_proj                    L__self___model_layers__modules__0___mlp_gate_proj                           (hidden_states_8,)                                                                                                                                     {}
call_module    l__self___model_layers__modules__0___mlp_act_fn                       L__self___model_layers__modules__0___mlp_act_fn                              (l__self___model_layers__modules__0___mlp_gate_proj,)                                                                                                  {}
call_module    l__self___model_layers__modules__0___mlp_up_proj                      L__self___model_layers__modules__0___mlp_up_proj                             (hidden_states_8,)                                                                                                                                     {}
call_function  mul_11                                                                <built-in function mul>                                                      (l__self___model_layers__modules__0___mlp_act_fn, l__self___model_layers__modules__0___mlp_up_proj)                                                    {}
call_module    down_proj                                                             L__self___model_layers__modules__0___mlp_down_proj                           (mul_11,)                                                                                                                                              {}
call_function  hidden_states_9                                                       <built-in function add>                                                      (hidden_states_5, down_proj)                                                                                                                           {}
call_method    hidden_states_10                                                      to                                                                           (hidden_states_9, torch.float32)                                                                                                                       {}
call_method    pow_3                                                                 pow                                                                          (hidden_states_10, 2)                                                                                                                                  {}
call_method    variance_2                                                            mean                                                                         (pow_3, -1)                                                                                                                                            {'keepdim': True}
call_function  add_7                                                                 <built-in function add>                                                      (variance_2, 1e-05)                                                                                                                                    {}
call_function  rsqrt_2                                                               <built-in method rsqrt of type object at 0x7f26e82f6f80>                     (add_7,)                                                                                                                                               {}
call_function  hidden_states_11                                                      <built-in function mul>                                                      (hidden_states_10, rsqrt_2)                                                                                                                            {}
get_attr       l__self___model_norm_weight                                           L__self___model_norm_weight                                                  ()                                                                                                                                                     {}
call_method    to_11                                                                 to                                                                           (hidden_states_11, torch.bfloat16)                                                                                                                     {}
call_function  hidden_states_12                                                      <built-in function mul>                                                      (l__self___model_norm_weight, to_11)                                                                                                                   {}
call_function  getitem_11                                                            <built-in function getitem>                                                  (hidden_states_12, (slice(None, None, None), slice(0, None, None), slice(None, None, None)))                                                           {}
call_module    logits                                                                L__self___lm_head                                                            (getitem_11,)                                                                                                                                          {}
output         output                                                                output                                                                       ((logits,),)                                                                                                                                           {}
[james] override use torch.export.export
program.graph_signature: ExportGraphSignature(input_specs=[InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_layers__modules__0___input_layernorm_weight'), target='L__self___model_layers__modules__0___input_layernorm_weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_layers__modules__0___post_attention_layernorm_weight'), target='L__self___model_layers__modules__0___post_attention_layernorm_weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_norm_weight'), target='L__self___model_norm_weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_embed_tokens_weight'), target='L__self___model_embed_tokens.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_layers__modules__0___self_attn_q_proj_weight'), target='L__self___model_layers__modules__0___self_attn_q_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_layers__modules__0___self_attn_k_proj_weight'), target='L__self___model_layers__modules__0___self_attn_k_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_layers__modules__0___self_attn_v_proj_weight'), target='L__self___model_layers__modules__0___self_attn_v_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_layers__modules__0___self_attn_o_proj_weight'), target='L__self___model_layers__modules__0___self_attn_o_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_layers__modules__0___mlp_gate_proj_weight'), target='L__self___model_layers__modules__0___mlp_gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_layers__modules__0___mlp_up_proj_weight'), target='L__self___model_layers__modules__0___mlp_up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_layers__modules__0___mlp_down_proj_weight'), target='L__self___model_layers__modules__0___mlp_down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_lm_head_weight'), target='L__self___lm_head.weight', persistent=None), InputSpec(kind=<InputKind.BUFFER: 3>, arg=TensorArgument(name='b_model_rotary_emb_inv_freq'), target='L__self___model_rotary_emb_inv_freq', persistent=True), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='args_0'), target=None, persistent=None), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='args_1'), target=None, persistent=None), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='args_2'), target=None, persistent=None), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='args_3'), target=None, persistent=None)], output_specs=[OutputSpec(kind=<OutputKind.USER_INPUT_MUTATION: 6>, arg=TensorArgument(name='index_put'), target='args_2'), OutputSpec(kind=<OutputKind.USER_INPUT_MUTATION: 6>, arg=TensorArgument(name='index_put_1'), target='args_3'), OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='view_31'), target=None)])
opcode         name                                                                  target                                                                args                                                                                 kwargs
-------------  --------------------------------------------------------------------  --------------------------------------------------------------------  -----------------------------------------------------------------------------------  --------------------------------------------------------------------------------------------------------------
get_attr       l__self___model_layers__modules__0___input_layernorm_weight           L__self___model_layers__modules__0___input_layernorm_weight           ()                                                                                   {}
call_function  mark_argument_attributes                                              tt.mark_argument_attributes                                           (l__self___model_layers__modules__0___input_layernorm_weight,)                       {'argument_type': 'parameter', 'name': 'l__self___model_layers__modules__0___input_layernorm_weight'}
get_attr       l__self___model_layers__modules__0___post_attention_layernorm_weight  L__self___model_layers__modules__0___post_attention_layernorm_weight  ()                                                                                   {}
call_function  mark_argument_attributes_1                                            tt.mark_argument_attributes                                           (l__self___model_layers__modules__0___post_attention_layernorm_weight,)              {'argument_type': 'parameter', 'name': 'l__self___model_layers__modules__0___post_attention_layernorm_weight'}
get_attr       l__self___model_norm_weight                                           L__self___model_norm_weight                                           ()                                                                                   {}
call_function  mark_argument_attributes_2                                            tt.mark_argument_attributes                                           (l__self___model_norm_weight,)                                                       {'argument_type': 'parameter', 'name': 'l__self___model_norm_weight'}
get_attr       l__self___model_embed_tokens_weight                                   L__self___model_embed_tokens.weight                                   ()                                                                                   {}
call_function  mark_argument_attributes_3                                            tt.mark_argument_attributes                                           (l__self___model_embed_tokens_weight,)                                               {'argument_type': 'parameter', 'name': 'l__self___model_embed_tokens_weight'}
get_attr       l__self___model_layers__modules__0___self_attn_q_proj_weight          L__self___model_layers__modules__0___self_attn_q_proj.weight          ()                                                                                   {}
call_function  mark_argument_attributes_4                                            tt.mark_argument_attributes                                           (l__self___model_layers__modules__0___self_attn_q_proj_weight,)                      {'argument_type': 'parameter', 'name': 'l__self___model_layers__modules__0___self_attn_q_proj_weight'}
get_attr       l__self___model_layers__modules__0___self_attn_k_proj_weight          L__self___model_layers__modules__0___self_attn_k_proj.weight          ()                                                                                   {}
call_function  mark_argument_attributes_5                                            tt.mark_argument_attributes                                           (l__self___model_layers__modules__0___self_attn_k_proj_weight,)                      {'argument_type': 'parameter', 'name': 'l__self___model_layers__modules__0___self_attn_k_proj_weight'}
get_attr       l__self___model_layers__modules__0___self_attn_v_proj_weight          L__self___model_layers__modules__0___self_attn_v_proj.weight          ()                                                                                   {}
call_function  mark_argument_attributes_6                                            tt.mark_argument_attributes                                           (l__self___model_layers__modules__0___self_attn_v_proj_weight,)                      {'argument_type': 'parameter', 'name': 'l__self___model_layers__modules__0___self_attn_v_proj_weight'}
get_attr       l__self___model_layers__modules__0___self_attn_o_proj_weight          L__self___model_layers__modules__0___self_attn_o_proj.weight          ()                                                                                   {}
call_function  mark_argument_attributes_7                                            tt.mark_argument_attributes                                           (l__self___model_layers__modules__0___self_attn_o_proj_weight,)                      {'argument_type': 'parameter', 'name': 'l__self___model_layers__modules__0___self_attn_o_proj_weight'}
get_attr       l__self___model_layers__modules__0___mlp_gate_proj_weight             L__self___model_layers__modules__0___mlp_gate_proj.weight             ()                                                                                   {}
call_function  mark_argument_attributes_8                                            tt.mark_argument_attributes                                           (l__self___model_layers__modules__0___mlp_gate_proj_weight,)                         {'argument_type': 'parameter', 'name': 'l__self___model_layers__modules__0___mlp_gate_proj_weight'}
get_attr       l__self___model_layers__modules__0___mlp_up_proj_weight               L__self___model_layers__modules__0___mlp_up_proj.weight               ()                                                                                   {}
call_function  mark_argument_attributes_9                                            tt.mark_argument_attributes                                           (l__self___model_layers__modules__0___mlp_up_proj_weight,)                           {'argument_type': 'parameter', 'name': 'l__self___model_layers__modules__0___mlp_up_proj_weight'}
get_attr       l__self___model_layers__modules__0___mlp_down_proj_weight             L__self___model_layers__modules__0___mlp_down_proj.weight             ()                                                                                   {}
call_function  mark_argument_attributes_10                                           tt.mark_argument_attributes                                           (l__self___model_layers__modules__0___mlp_down_proj_weight,)                         {'argument_type': 'parameter', 'name': 'l__self___model_layers__modules__0___mlp_down_proj_weight'}
get_attr       l__self___lm_head_weight                                              L__self___lm_head.weight                                              ()                                                                                   {}
call_function  mark_argument_attributes_11                                           tt.mark_argument_attributes                                           (l__self___lm_head_weight,)                                                          {'argument_type': 'parameter', 'name': 'l__self___lm_head_weight'}
get_attr       l__self___model_rotary_emb_inv_freq                                   L__self___model_rotary_emb_inv_freq                                   ()                                                                                   {}
call_function  mark_argument_attributes_12                                           tt.mark_argument_attributes                                           (l__self___model_rotary_emb_inv_freq,)                                               {'argument_type': 'input', 'name': 'l__self___model_rotary_emb_inv_freq'}
placeholder    args_0                                                                args_0                                                                ()                                                                                   {}
call_function  mark_argument_attributes_13                                           tt.mark_argument_attributes                                           (args_0,)                                                                            {'argument_type': 'input', 'name': 'args_0'}
placeholder    args_1                                                                args_1                                                                ()                                                                                   {}
call_function  mark_argument_attributes_14                                           tt.mark_argument_attributes                                           (args_1,)                                                                            {'argument_type': 'input', 'name': 'args_1'}
placeholder    args_2                                                                args_2                                                                ()                                                                                   {}
call_function  mark_argument_attributes_15                                           tt.mark_argument_attributes                                           (args_2,)                                                                            {'argument_type': 'input', 'name': 'args_2'}
placeholder    args_3                                                                args_3                                                                ()                                                                                   {}
call_function  mark_argument_attributes_16                                           tt.mark_argument_attributes                                           (args_3,)                                                                            {'argument_type': 'input', 'name': 'args_3'}
call_function  embedding                                                             aten.embedding.default                                                (mark_argument_attributes_3, mark_argument_attributes_13)                            {}
call_function  unsqueeze                                                             aten.unsqueeze.default                                                (mark_argument_attributes_14, 0)                                                     {}
call_function  empty_strided                                                         aten.empty_strided.default                                            ([7, 1024], [1024, 1])                                                               {'dtype': torch.bfloat16, 'layout': torch.strided, 'device': device(type='xla', index=0), 'pin_memory': False}
call_function  full_like                                                             aten.full_like.default                                                (empty_strided, -3.3895313892515355e+38)                                             {'pin_memory': False}
call_function  arange                                                                aten.arange.start_step                                                (0, 1024)                                                                            {'layout': torch.strided, 'device': device(type='xla', index=0), 'pin_memory': False}
call_function  unsqueeze_1                                                           aten.unsqueeze.default                                                (arange, -2)                                                                         {}
call_function  arange_1                                                              aten.arange.start_step                                                (0, 7)                                                                               {'layout': torch.strided, 'device': device(type='xla', index=0), 'pin_memory': False}
call_function  unsqueeze_2                                                           aten.unsqueeze.default                                                (arange_1, -1)                                                                       {}
call_function  sub                                                                   aten.sub.Tensor                                                       (unsqueeze_1, unsqueeze_2)                                                           {}
call_function  ge                                                                    aten.ge.Scalar                                                        (sub, 1)                                                                             {}
call_function  scalar_tensor                                                         aten.scalar_tensor.default                                            (0,)                                                                                 {'dtype': torch.bfloat16, 'layout': torch.strided, 'device': device(type='xla', index=0)}
call_function  where                                                                 aten.where.self                                                       (ge, full_like, scalar_tensor)                                                       {}
call_function  arange_2                                                              aten.arange.start_step                                                (0, 1024)                                                                            {'layout': torch.strided, 'device': device(type='xla', index=0), 'pin_memory': False}
call_function  view                                                                  aten.view.default                                                     (mark_argument_attributes_14, [-1, 1])                                               {}
call_function  gt                                                                    aten.gt.Tensor                                                        (arange_2, view)                                                                     {}
call_function  mul                                                                   aten.mul.Tensor                                                       (where, gt)                                                                          {}
call_function  unsqueeze_5                                                           aten.unsqueeze.default                                                (mark_argument_attributes_12, 0)                                                     {}
call_function  slice_3                                                               aten.slice.Tensor                                                     (unsqueeze_5, 1, 0, 9223372036854775807)                                             {}
call_function  unsqueeze_6                                                           aten.unsqueeze.default                                                (slice_3, 2)                                                                         {}
call_function  convert_element_type                                                  prims.convert_element_type.default                                    (unsqueeze_6, torch.float32)                                                         {}
call_function  expand_1                                                              aten.expand.default                                                   (unsqueeze_6, [1, -1, 1])                                                            {}
call_function  convert_element_type_1                                                prims.convert_element_type.default                                    (expand_1, torch.float32)                                                            {}
call_function  slice_4                                                               aten.slice.Tensor                                                     (unsqueeze, 0, 0, 9223372036854775807)                                               {}
call_function  unsqueeze_7                                                           aten.unsqueeze.default                                                (slice_4, 1)                                                                         {}
call_function  slice_5                                                               aten.slice.Tensor                                                     (unsqueeze_7, 2, 0, 9223372036854775807)                                             {}
call_function  convert_element_type_2                                                prims.convert_element_type.default                                    (slice_5, torch.float32)                                                             {}
call_function  convert_element_type_3                                                prims.convert_element_type.default                                    (expand_1, torch.float32)                                                            {}
call_function  convert_element_type_4                                                prims.convert_element_type.default                                    (convert_element_type_2, torch.float32)                                              {}
call_function  expand_2                                                              aten.expand.default                                                   (expand_1, [1, 64, 1])                                                               {}
call_function  view_1                                                                aten.view.default                                                     (expand_2, [1, 64, 1])                                                               {}
call_function  expand_3                                                              aten.expand.default                                                   (convert_element_type_2, [1, 1, 7])                                                  {}
call_function  view_2                                                                aten.view.default                                                     (expand_3, [1, 1, 7])                                                                {}
call_function  bmm                                                                   aten.bmm.default                                                      (view_1, view_2)                                                                     {}
call_function  view_3                                                                aten.view.default                                                     (bmm, [1, 64, 7])                                                                    {}
call_function  permute                                                               aten.permute.default                                                  (view_3, [0, 2, 1])                                                                  {}
call_function  cat                                                                   aten.cat.default                                                      ([permute, permute], -1)                                                             {}
call_function  cos                                                                   aten.cos.default                                                      (cat,)                                                                               {}
call_function  mul_1                                                                 aten.mul.Tensor                                                       (cos, 1.0)                                                                           {}
call_function  sin                                                                   aten.sin.default                                                      (cat,)                                                                               {}
call_function  mul_2                                                                 aten.mul.Tensor                                                       (sin, 1.0)                                                                           {}
call_function  convert_element_type_5                                                prims.convert_element_type.default                                    (mul_1, torch.bfloat16)                                                              {}
call_function  convert_element_type_6                                                prims.convert_element_type.default                                    (mul_2, torch.bfloat16)                                                              {}
call_function  convert_element_type_7                                                prims.convert_element_type.default                                    (embedding, torch.float32)                                                           {}
call_function  pow_1                                                                 aten.pow.Tensor_Scalar                                                (convert_element_type_7, 2)                                                          {}
call_function  mean                                                                  aten.mean.dim                                                         (pow_1, [-1], True)                                                                  {}
call_function  add                                                                   aten.add.Tensor                                                       (mean, 1e-05)                                                                        {}
call_function  rsqrt                                                                 aten.rsqrt.default                                                    (add,)                                                                               {}
call_function  mul_3                                                                 aten.mul.Tensor                                                       (convert_element_type_7, rsqrt)                                                      {}
call_function  convert_element_type_8                                                prims.convert_element_type.default                                    (mul_3, torch.bfloat16)                                                              {}
call_function  mul_4                                                                 aten.mul.Tensor                                                       (mark_argument_attributes, convert_element_type_8)                                   {}
call_function  permute_1                                                             aten.permute.default                                                  (mark_argument_attributes_4, [1, 0])                                                 {}
call_function  view_4                                                                aten.view.default                                                     (mul_4, [7, 3072])                                                                   {}
call_function  mm                                                                    aten.mm.default                                                       (view_4, permute_1)                                                                  {}
call_function  view_5                                                                aten.view.default                                                     (mm, [1, 7, 3072])                                                                   {}
call_function  view_6                                                                aten.view.default                                                     (view_5, [1, 7, -1, 128])                                                            {}
call_function  permute_2                                                             aten.permute.default                                                  (view_6, [0, 2, 1, 3])                                                               {}
call_function  permute_3                                                             aten.permute.default                                                  (mark_argument_attributes_5, [1, 0])                                                 {}
call_function  view_7                                                                aten.view.default                                                     (mul_4, [7, 3072])                                                                   {}
call_function  mm_1                                                                  aten.mm.default                                                       (view_7, permute_3)                                                                  {}
call_function  view_8                                                                aten.view.default                                                     (mm_1, [1, 7, 1024])                                                                 {}
call_function  view_9                                                                aten.view.default                                                     (view_8, [1, 7, -1, 128])                                                            {}
call_function  permute_4                                                             aten.permute.default                                                  (view_9, [0, 2, 1, 3])                                                               {}
call_function  permute_5                                                             aten.permute.default                                                  (mark_argument_attributes_6, [1, 0])                                                 {}
call_function  view_10                                                               aten.view.default                                                     (mul_4, [7, 3072])                                                                   {}
call_function  mm_2                                                                  aten.mm.default                                                       (view_10, permute_5)                                                                 {}
call_function  view_11                                                               aten.view.default                                                     (mm_2, [1, 7, 1024])                                                                 {}
call_function  view_12                                                               aten.view.default                                                     (view_11, [1, 7, -1, 128])                                                           {}
call_function  permute_6                                                             aten.permute.default                                                  (view_12, [0, 2, 1, 3])                                                              {}
call_function  unsqueeze_8                                                           aten.unsqueeze.default                                                (convert_element_type_5, 1)                                                          {}
call_function  unsqueeze_9                                                           aten.unsqueeze.default                                                (convert_element_type_6, 1)                                                          {}
call_function  mul_5                                                                 aten.mul.Tensor                                                       (permute_2, unsqueeze_8)                                                             {}
call_function  slice_6                                                               aten.slice.Tensor                                                     (permute_2, 3, 0, 64)                                                                {}
call_function  slice_7                                                               aten.slice.Tensor                                                     (permute_2, 3, 64, 9223372036854775807)                                              {}
call_function  neg                                                                   aten.neg.default                                                      (slice_7,)                                                                           {}
call_function  cat_1                                                                 aten.cat.default                                                      ([neg, slice_6], -1)                                                                 {}
call_function  mul_6                                                                 aten.mul.Tensor                                                       (cat_1, unsqueeze_9)                                                                 {}
call_function  add_1                                                                 aten.add.Tensor                                                       (mul_5, mul_6)                                                                       {}
call_function  mul_7                                                                 aten.mul.Tensor                                                       (permute_4, unsqueeze_8)                                                             {}
call_function  slice_8                                                               aten.slice.Tensor                                                     (permute_4, 3, 0, 64)                                                                {}
call_function  slice_9                                                               aten.slice.Tensor                                                     (permute_4, 3, 64, 9223372036854775807)                                              {}
call_function  neg_1                                                                 aten.neg.default                                                      (slice_9,)                                                                           {}
call_function  cat_2                                                                 aten.cat.default                                                      ([neg_1, slice_8], -1)                                                               {}
call_function  mul_8                                                                 aten.mul.Tensor                                                       (cat_2, unsqueeze_9)                                                                 {}
call_function  add_2                                                                 aten.add.Tensor                                                       (mul_7, mul_8)                                                                       {}
call_function  convert_element_type_9                                                prims.convert_element_type.default                                    (add_2, torch.bfloat16)                                                              {}
call_function  convert_element_type_10                                               prims.convert_element_type.default                                    (permute_6, torch.bfloat16)                                                          {}
call_function  index_put                                                             aten.index_put.default                                                (mark_argument_attributes_15, [None, None, mark_argument_attributes_14], add_2)      {}
call_function  index_put_1                                                           aten.index_put.default                                                (mark_argument_attributes_16, [None, None, mark_argument_attributes_14], permute_6)  {}
call_function  slice_14                                                              aten.slice.Tensor                                                     (index_put, 0, 0, 9223372036854775807)                                               {}
call_function  slice_15                                                              aten.slice.Tensor                                                     (slice_14, 1, 0, 9223372036854775807)                                                {}
call_function  unsqueeze_11                                                          aten.unsqueeze.default                                                (slice_15, 2)                                                                        {}
call_function  slice_16                                                              aten.slice.Tensor                                                     (unsqueeze_11, 3, 0, 9223372036854775807)                                            {}
call_function  slice_17                                                              aten.slice.Tensor                                                     (slice_16, 4, 0, 9223372036854775807)                                                {}
call_function  expand_5                                                              aten.expand.default                                                   (slice_17, [1, 8, 3, 1024, 128])                                                     {}
call_function  clone                                                                 aten.clone.default                                                    (expand_5,)                                                                          {'memory_format': torch.contiguous_format}
call_function  view_13                                                               aten.view.default                                                     (clone, [1, 24, 1024, 128])                                                          {}
call_function  slice_22                                                              aten.slice.Tensor                                                     (index_put_1, 0, 0, 9223372036854775807)                                             {}
call_function  slice_23                                                              aten.slice.Tensor                                                     (slice_22, 1, 0, 9223372036854775807)                                                {}
call_function  unsqueeze_13                                                          aten.unsqueeze.default                                                (slice_23, 2)                                                                        {}
call_function  slice_24                                                              aten.slice.Tensor                                                     (unsqueeze_13, 3, 0, 9223372036854775807)                                            {}
call_function  slice_25                                                              aten.slice.Tensor                                                     (slice_24, 4, 0, 9223372036854775807)                                                {}
call_function  expand_7                                                              aten.expand.default                                                   (slice_25, [1, 8, 3, 1024, 128])                                                     {}
call_function  clone_1                                                               aten.clone.default                                                    (expand_7,)                                                                          {'memory_format': torch.contiguous_format}
call_function  view_14                                                               aten.view.default                                                     (clone_1, [1, 24, 1024, 128])                                                        {}
call_function  permute_7                                                             aten.permute.default                                                  (view_13, [0, 1, 3, 2])                                                              {}
call_function  expand_8                                                              aten.expand.default                                                   (add_1, [1, 24, 7, 128])                                                             {}
call_function  view_15                                                               aten.view.default                                                     (expand_8, [24, 7, 128])                                                             {}
call_function  expand_9                                                              aten.expand.default                                                   (permute_7, [1, 24, 128, 1024])                                                      {}
call_function  view_16                                                               aten.view.default                                                     (expand_9, [24, 128, 1024])                                                          {}
call_function  bmm_1                                                                 aten.bmm.default                                                      (view_15, view_16)                                                                   {}
call_function  view_17                                                               aten.view.default                                                     (bmm_1, [1, 24, 7, 1024])                                                            {}
call_function  mul_9                                                                 aten.mul.Tensor                                                       (view_17, 0.08838834764831845)                                                       {}
call_function  unsqueeze_14                                                          aten.unsqueeze.default                                                (mul, 0)                                                                             {}
call_function  unsqueeze_15                                                          aten.unsqueeze.default                                                (unsqueeze_14, 1)                                                                    {}
call_function  slice_29                                                              aten.slice.Tensor                                                     (unsqueeze_15, 2, 0, 9223372036854775807)                                            {}
call_function  slice_30                                                              aten.slice.Tensor                                                     (slice_29, 3, 0, 9223372036854775807)                                                {}
call_function  expand_10                                                             aten.expand.default                                                   (slice_30, [1, 1, -1, -1])                                                           {}
call_function  slice_31                                                              aten.slice.Tensor                                                     (expand_10, 0, 0, 9223372036854775807)                                               {}
call_function  slice_32                                                              aten.slice.Tensor                                                     (slice_31, 1, 0, 9223372036854775807)                                                {}
call_function  slice_33                                                              aten.slice.Tensor                                                     (slice_32, 2, 0, 9223372036854775807)                                                {}
call_function  add_3                                                                 aten.add.Tensor                                                       (mul_9, slice_33)                                                                    {}
call_function  convert_element_type_11                                               prims.convert_element_type.default                                    (add_3, torch.float32)                                                               {}
call_function  _softmax                                                              aten._softmax.default                                                 (convert_element_type_11, -1, False)                                                 {}
call_function  convert_element_type_12                                               prims.convert_element_type.default                                    (_softmax, torch.bfloat16)                                                           {}
call_function  clone_2                                                               aten.clone.default                                                    (convert_element_type_12,)                                                           {}
call_function  expand_11                                                             aten.expand.default                                                   (clone_2, [1, 24, 7, 1024])                                                          {}
call_function  view_18                                                               aten.view.default                                                     (expand_11, [24, 7, 1024])                                                           {}
call_function  expand_12                                                             aten.expand.default                                                   (view_14, [1, 24, 1024, 128])                                                        {}
call_function  view_19                                                               aten.view.default                                                     (expand_12, [24, 1024, 128])                                                         {}
call_function  bmm_2                                                                 aten.bmm.default                                                      (view_18, view_19)                                                                   {}
call_function  view_20                                                               aten.view.default                                                     (bmm_2, [1, 24, 7, 128])                                                             {}
call_function  permute_8                                                             aten.permute.default                                                  (view_20, [0, 2, 1, 3])                                                              {}
call_function  clone_3                                                               aten.clone.default                                                    (permute_8,)                                                                         {'memory_format': torch.contiguous_format}
call_function  view_21                                                               aten.view.default                                                     (clone_3, [1, 7, -1])                                                                {}
call_function  permute_9                                                             aten.permute.default                                                  (mark_argument_attributes_7, [1, 0])                                                 {}
call_function  view_22                                                               aten.view.default                                                     (view_21, [7, 3072])                                                                 {}
call_function  mm_3                                                                  aten.mm.default                                                       (view_22, permute_9)                                                                 {}
call_function  view_23                                                               aten.view.default                                                     (mm_3, [1, 7, 3072])                                                                 {}
call_function  add_4                                                                 aten.add.Tensor                                                       (embedding, view_23)                                                                 {}
call_function  convert_element_type_13                                               prims.convert_element_type.default                                    (add_4, torch.float32)                                                               {}
call_function  pow_2                                                                 aten.pow.Tensor_Scalar                                                (convert_element_type_13, 2)                                                         {}
call_function  mean_1                                                                aten.mean.dim                                                         (pow_2, [-1], True)                                                                  {}
call_function  add_5                                                                 aten.add.Tensor                                                       (mean_1, 1e-05)                                                                      {}
call_function  rsqrt_1                                                               aten.rsqrt.default                                                    (add_5,)                                                                             {}
call_function  mul_10                                                                aten.mul.Tensor                                                       (convert_element_type_13, rsqrt_1)                                                   {}
call_function  convert_element_type_14                                               prims.convert_element_type.default                                    (mul_10, torch.bfloat16)                                                             {}
call_function  mul_11                                                                aten.mul.Tensor                                                       (mark_argument_attributes_1, convert_element_type_14)                                {}
call_function  permute_10                                                            aten.permute.default                                                  (mark_argument_attributes_8, [1, 0])                                                 {}
call_function  view_24                                                               aten.view.default                                                     (mul_11, [7, 3072])                                                                  {}
call_function  mm_4                                                                  aten.mm.default                                                       (view_24, permute_10)                                                                {}
call_function  view_25                                                               aten.view.default                                                     (mm_4, [1, 7, 8192])                                                                 {}
call_function  convert_element_type_15                                               prims.convert_element_type.default                                    (view_25, torch.float32)                                                             {}
call_function  sigmoid                                                               aten.sigmoid.default                                                  (view_25,)                                                                           {}
call_function  mul_12                                                                aten.mul.Tensor                                                       (view_25, sigmoid)                                                                   {}
call_function  convert_element_type_16                                               prims.convert_element_type.default                                    (mul_12, torch.bfloat16)                                                             {}
call_function  permute_11                                                            aten.permute.default                                                  (mark_argument_attributes_9, [1, 0])                                                 {}
call_function  view_26                                                               aten.view.default                                                     (mul_11, [7, 3072])                                                                  {}
call_function  mm_5                                                                  aten.mm.default                                                       (view_26, permute_11)                                                                {}
call_function  view_27                                                               aten.view.default                                                     (mm_5, [1, 7, 8192])                                                                 {}
call_function  mul_13                                                                aten.mul.Tensor                                                       (convert_element_type_16, view_27)                                                   {}
call_function  permute_12                                                            aten.permute.default                                                  (mark_argument_attributes_10, [1, 0])                                                {}
call_function  view_28                                                               aten.view.default                                                     (mul_13, [7, 8192])                                                                  {}
call_function  mm_6                                                                  aten.mm.default                                                       (view_28, permute_12)                                                                {}
call_function  view_29                                                               aten.view.default                                                     (mm_6, [1, 7, 3072])                                                                 {}
call_function  add_6                                                                 aten.add.Tensor                                                       (add_4, view_29)                                                                     {}
call_function  convert_element_type_17                                               prims.convert_element_type.default                                    (add_6, torch.float32)                                                               {}
call_function  pow_3                                                                 aten.pow.Tensor_Scalar                                                (convert_element_type_17, 2)                                                         {}
call_function  mean_2                                                                aten.mean.dim                                                         (pow_3, [-1], True)                                                                  {}
call_function  add_7                                                                 aten.add.Tensor                                                       (mean_2, 1e-05)                                                                      {}
call_function  rsqrt_2                                                               aten.rsqrt.default                                                    (add_7,)                                                                             {}
call_function  mul_14                                                                aten.mul.Tensor                                                       (convert_element_type_17, rsqrt_2)                                                   {}
call_function  convert_element_type_18                                               prims.convert_element_type.default                                    (mul_14, torch.bfloat16)                                                             {}
call_function  mul_15                                                                aten.mul.Tensor                                                       (mark_argument_attributes_2, convert_element_type_18)                                {}
call_function  slice_34                                                              aten.slice.Tensor                                                     (mul_15, 0, 0, 9223372036854775807)                                                  {}
call_function  slice_35                                                              aten.slice.Tensor                                                     (slice_34, 1, 0, 9223372036854775807)                                                {}
call_function  slice_36                                                              aten.slice.Tensor                                                     (slice_35, 2, 0, 9223372036854775807)                                                {}
call_function  permute_13                                                            aten.permute.default                                                  (mark_argument_attributes_11, [1, 0])                                                {}
call_function  view_30                                                               aten.view.default                                                     (slice_36, [7, 3072])                                                                {}
call_function  mm_7                                                                  aten.mm.default                                                       (view_30, permute_13)                                                                {}
call_function  view_31                                                               aten.view.default                                                     (mm_7, [1, 7, 128256])                                                               {}
call_function  copy__default                                                         aten.copy_.default                                                    (mark_argument_attributes_15, index_put)                                             {}
call_function  copy__default_1                                                       aten.copy_.default                                                    (mark_argument_attributes_16, index_put_1)                                           {}
output         output                                                                output                                                                ((view_31,),)                                                                        {}
readable graph module
class GraphModule(torch.nn.Module):
    def forward(self, args_0, args_1, args_2, args_3):
        args_0: "i64[1, 7]"; args_1: "i64[7]"; args_2: "bf16[1, 8, 1024, 128]"; args_3: "bf16[1, 8, 1024, 128]"; 
    
        args_0, args_1, args_2, args_3, = fx_pytree.tree_flatten_spec(([args_0, args_1, args_2, args_3], {}), self._in_spec)
        # No stacktrace found for following nodes
        l__self___model_layers__modules__0___input_layernorm_weight: "bf16[3072]" = self.L__self___model_layers__modules__0___input_layernorm_weight
        mark_argument_attributes = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___input_layernorm_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___input_layernorm_weight');  l__self___model_layers__modules__0___input_layernorm_weight = None
        l__self___model_layers__modules__0___post_attention_layernorm_weight: "bf16[3072]" = self.L__self___model_layers__modules__0___post_attention_layernorm_weight
        mark_argument_attributes_1 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___post_attention_layernorm_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___post_attention_layernorm_weight');  l__self___model_layers__modules__0___post_attention_layernorm_weight = None
        l__self___model_norm_weight: "bf16[3072]" = self.L__self___model_norm_weight
        mark_argument_attributes_2 = torch.ops.tt.mark_argument_attributes(l__self___model_norm_weight, argument_type = 'parameter', name = 'l__self___model_norm_weight');  l__self___model_norm_weight = None
        l__self___model_embed_tokens_weight: "bf16[128256, 3072]" = self.L__self___model_embed_tokens.weight
        mark_argument_attributes_3 = torch.ops.tt.mark_argument_attributes(l__self___model_embed_tokens_weight, argument_type = 'parameter', name = 'l__self___model_embed_tokens_weight');  l__self___model_embed_tokens_weight = None
        l__self___model_layers__modules__0___self_attn_q_proj_weight: "bf16[3072, 3072]" = self.L__self___model_layers__modules__0___self_attn_q_proj.weight
        mark_argument_attributes_4 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___self_attn_q_proj_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___self_attn_q_proj_weight');  l__self___model_layers__modules__0___self_attn_q_proj_weight = None
        l__self___model_layers__modules__0___self_attn_k_proj_weight: "bf16[1024, 3072]" = self.L__self___model_layers__modules__0___self_attn_k_proj.weight
        mark_argument_attributes_5 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___self_attn_k_proj_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___self_attn_k_proj_weight');  l__self___model_layers__modules__0___self_attn_k_proj_weight = None
        l__self___model_layers__modules__0___self_attn_v_proj_weight: "bf16[1024, 3072]" = self.L__self___model_layers__modules__0___self_attn_v_proj.weight
        mark_argument_attributes_6 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___self_attn_v_proj_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___self_attn_v_proj_weight');  l__self___model_layers__modules__0___self_attn_v_proj_weight = None
        l__self___model_layers__modules__0___self_attn_o_proj_weight: "bf16[3072, 3072]" = self.L__self___model_layers__modules__0___self_attn_o_proj.weight
        mark_argument_attributes_7 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___self_attn_o_proj_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___self_attn_o_proj_weight');  l__self___model_layers__modules__0___self_attn_o_proj_weight = None
        l__self___model_layers__modules__0___mlp_gate_proj_weight: "bf16[8192, 3072]" = self.L__self___model_layers__modules__0___mlp_gate_proj.weight
        mark_argument_attributes_8 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___mlp_gate_proj_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___mlp_gate_proj_weight');  l__self___model_layers__modules__0___mlp_gate_proj_weight = None
        l__self___model_layers__modules__0___mlp_up_proj_weight: "bf16[8192, 3072]" = self.L__self___model_layers__modules__0___mlp_up_proj.weight
        mark_argument_attributes_9 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___mlp_up_proj_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___mlp_up_proj_weight');  l__self___model_layers__modules__0___mlp_up_proj_weight = None
        l__self___model_layers__modules__0___mlp_down_proj_weight: "bf16[3072, 8192]" = self.L__self___model_layers__modules__0___mlp_down_proj.weight
        mark_argument_attributes_10 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___mlp_down_proj_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___mlp_down_proj_weight');  l__self___model_layers__modules__0___mlp_down_proj_weight = None
        l__self___lm_head_weight: "bf16[128256, 3072]" = self.L__self___lm_head.weight
        mark_argument_attributes_11 = torch.ops.tt.mark_argument_attributes(l__self___lm_head_weight, argument_type = 'parameter', name = 'l__self___lm_head_weight');  l__self___lm_head_weight = None
        l__self___model_rotary_emb_inv_freq: "f32[64]" = self.L__self___model_rotary_emb_inv_freq
        mark_argument_attributes_12 = torch.ops.tt.mark_argument_attributes(l__self___model_rotary_emb_inv_freq, argument_type = 'input', name = 'l__self___model_rotary_emb_inv_freq');  l__self___model_rotary_emb_inv_freq = None
        mark_argument_attributes_13 = torch.ops.tt.mark_argument_attributes(args_0, argument_type = 'input', name = 'args_0');  args_0 = None
        mark_argument_attributes_14 = torch.ops.tt.mark_argument_attributes(args_1, argument_type = 'input', name = 'args_1');  args_1 = None
        mark_argument_attributes_15 = torch.ops.tt.mark_argument_attributes(args_2, argument_type = 'input', name = 'args_2');  args_2 = None
        mark_argument_attributes_16 = torch.ops.tt.mark_argument_attributes(args_3, argument_type = 'input', name = 'args_3');  args_3 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:422 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        embedding: "bf16[1, 7, 3072]" = torch.ops.aten.embedding.default(mark_argument_attributes_3, mark_argument_attributes_13);  mark_argument_attributes_3 = mark_argument_attributes_13 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:434 in forward, code: position_ids = cache_position.unsqueeze(0)
        unsqueeze: "i64[1, 7]" = torch.ops.aten.unsqueeze.default(mark_argument_attributes_14, 0)
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:586 in _prepare_4d_causal_attention_mask_with_cache_position, code: causal_mask = torch.full(
        empty_strided: "bf16[7, 1024]" = torch.ops.aten.empty_strided.default([7, 1024], [1024, 1], dtype = torch.bfloat16, layout = torch.strided, device = device(type='xla', index=0), pin_memory = False)
        full_like: "bf16[7, 1024]" = torch.ops.aten.full_like.default(empty_strided, -3.3895313892515355e+38, pin_memory = False);  empty_strided = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:590 in _prepare_4d_causal_attention_mask_with_cache_position, code: causal_mask = torch.triu(causal_mask, diagonal=1)
        arange: "i64[1024]" = torch.ops.aten.arange.start_step(0, 1024, layout = torch.strided, device = device(type='xla', index=0), pin_memory = False)
        unsqueeze_1: "i64[1, 1024]" = torch.ops.aten.unsqueeze.default(arange, -2);  arange = None
        arange_1: "i64[7]" = torch.ops.aten.arange.start_step(0, 7, layout = torch.strided, device = device(type='xla', index=0), pin_memory = False)
        unsqueeze_2: "i64[7, 1]" = torch.ops.aten.unsqueeze.default(arange_1, -1);  arange_1 = None
        sub: "i64[7, 1024]" = torch.ops.aten.sub.Tensor(unsqueeze_1, unsqueeze_2);  unsqueeze_1 = unsqueeze_2 = None
        ge: "b8[7, 1024]" = torch.ops.aten.ge.Scalar(sub, 1);  sub = None
        scalar_tensor: "bf16[]" = torch.ops.aten.scalar_tensor.default(0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='xla', index=0))
        where: "bf16[7, 1024]" = torch.ops.aten.where.self(ge, full_like, scalar_tensor);  ge = full_like = scalar_tensor = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:591 in _prepare_4d_causal_attention_mask_with_cache_position, code: causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)
        arange_2: "i64[1024]" = torch.ops.aten.arange.start_step(0, 1024, layout = torch.strided, device = device(type='xla', index=0), pin_memory = False)
        view: "i64[7, 1]" = torch.ops.aten.view.default(mark_argument_attributes_14, [-1, 1])
        gt: "b8[7, 1024]" = torch.ops.aten.gt.Tensor(arange_2, view);  arange_2 = view = None
        mul: "bf16[7, 1024]" = torch.ops.aten.mul.Tensor(where, gt);  where = gt = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:103 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        unsqueeze_5: "f32[1, 64]" = torch.ops.aten.unsqueeze.default(mark_argument_attributes_12, 0);  mark_argument_attributes_12 = None
        slice_3: "f32[1, 64]" = torch.ops.aten.slice.Tensor(unsqueeze_5, 1, 0, 9223372036854775807);  unsqueeze_5 = None
        unsqueeze_6: "f32[1, 64, 1]" = torch.ops.aten.unsqueeze.default(slice_3, 2);  slice_3 = None
        convert_element_type: "f32[1, 64, 1]" = torch.ops.prims.convert_element_type.default(unsqueeze_6, torch.float32);  convert_element_type = None
        expand_1: "f32[1, 64, 1]" = torch.ops.aten.expand.default(unsqueeze_6, [1, -1, 1]);  unsqueeze_6 = None
        convert_element_type_1: "f32[1, 64, 1]" = torch.ops.prims.convert_element_type.default(expand_1, torch.float32);  convert_element_type_1 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:104 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        slice_4: "i64[1, 7]" = torch.ops.aten.slice.Tensor(unsqueeze, 0, 0, 9223372036854775807);  unsqueeze = None
        unsqueeze_7: "i64[1, 1, 7]" = torch.ops.aten.unsqueeze.default(slice_4, 1);  slice_4 = None
        slice_5: "i64[1, 1, 7]" = torch.ops.aten.slice.Tensor(unsqueeze_7, 2, 0, 9223372036854775807);  unsqueeze_7 = None
        convert_element_type_2: "f32[1, 1, 7]" = torch.ops.prims.convert_element_type.default(slice_5, torch.float32);  slice_5 = None
        
         # File: <eval_with_key>.9:5 in forward, code: to_3 = torch.ops.aten.to.dtype(to_1, torch.float32);  to_1 = None
        convert_element_type_3: "f32[1, 64, 1]" = torch.ops.prims.convert_element_type.default(expand_1, torch.float32);  convert_element_type_3 = None
        
         # File: <eval_with_key>.9:6 in forward, code: to_4 = torch.ops.aten.to.dtype(to_2, torch.float32);  to_2 = None
        convert_element_type_4: "f32[1, 1, 7]" = torch.ops.prims.convert_element_type.default(convert_element_type_2, torch.float32);  convert_element_type_4 = None
        
         # File: <eval_with_key>.9:7 in forward, code: matmul = torch.ops.aten.matmul.default(to_3, to_4);  to_3 = to_4 = None
        expand_2: "f32[1, 64, 1]" = torch.ops.aten.expand.default(expand_1, [1, 64, 1]);  expand_1 = None
        view_1: "f32[1, 64, 1]" = torch.ops.aten.view.default(expand_2, [1, 64, 1]);  expand_2 = None
        expand_3: "f32[1, 1, 7]" = torch.ops.aten.expand.default(convert_element_type_2, [1, 1, 7]);  convert_element_type_2 = None
        view_2: "f32[1, 1, 7]" = torch.ops.aten.view.default(expand_3, [1, 1, 7]);  expand_3 = None
        bmm: "f32[1, 64, 7]" = torch.ops.aten.bmm.default(view_1, view_2);  view_1 = view_2 = None
        view_3: "f32[1, 64, 7]" = torch.ops.aten.view.default(bmm, [1, 64, 7]);  bmm = None
        
         # File: <eval_with_key>.9:8 in forward, code: transpose = torch.ops.aten.transpose.int(matmul, 1, 2);  matmul = None
        permute: "f32[1, 7, 64]" = torch.ops.aten.permute.default(view_3, [0, 2, 1]);  view_3 = None
        
         # File: <eval_with_key>.9:9 in forward, code: cat = torch.ops.aten.cat.default([transpose, transpose], -1);  transpose = None
        cat: "f32[1, 7, 128]" = torch.ops.aten.cat.default([permute, permute], -1);  permute = None
        
         # File: <eval_with_key>.9:10 in forward, code: cos = torch.ops.aten.cos.default(cat)
        cos: "f32[1, 7, 128]" = torch.ops.aten.cos.default(cat)
        
         # File: <eval_with_key>.9:11 in forward, code: mul = torch.ops.aten.mul.Tensor(cos, 1.0);  cos = None
        mul_1: "f32[1, 7, 128]" = torch.ops.aten.mul.Tensor(cos, 1.0);  cos = None
        
         # File: <eval_with_key>.9:12 in forward, code: sin = torch.ops.aten.sin.default(cat);  cat = None
        sin: "f32[1, 7, 128]" = torch.ops.aten.sin.default(cat);  cat = None
        
         # File: <eval_with_key>.9:13 in forward, code: mul_1 = torch.ops.aten.mul.Tensor(sin, 1.0);  sin = None
        mul_2: "f32[1, 7, 128]" = torch.ops.aten.mul.Tensor(sin, 1.0);  sin = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        convert_element_type_5: "bf16[1, 7, 128]" = torch.ops.prims.convert_element_type.default(mul_1, torch.bfloat16);  mul_1 = None
        convert_element_type_6: "bf16[1, 7, 128]" = torch.ops.prims.convert_element_type.default(mul_2, torch.bfloat16);  mul_2 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:70 in forward, code: hidden_states = hidden_states.to(torch.float32)
        convert_element_type_7: "f32[1, 7, 3072]" = torch.ops.prims.convert_element_type.default(embedding, torch.float32)
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:71 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 7, 3072]" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_7, 2)
        mean: "f32[1, 7, 1]" = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:72 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 7, 1]" = torch.ops.aten.add.Tensor(mean, 1e-05);  mean = None
        rsqrt: "f32[1, 7, 1]" = torch.ops.aten.rsqrt.default(add);  add = None
        mul_3: "f32[1, 7, 3072]" = torch.ops.aten.mul.Tensor(convert_element_type_7, rsqrt);  convert_element_type_7 = rsqrt = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:73 in forward, code: return self.weight * hidden_states.to(input_dtype)
        convert_element_type_8: "bf16[1, 7, 3072]" = torch.ops.prims.convert_element_type.default(mul_3, torch.bfloat16);  mul_3 = None
        mul_4: "bf16[1, 7, 3072]" = torch.ops.aten.mul.Tensor(mark_argument_attributes, convert_element_type_8);  mark_argument_attributes = convert_element_type_8 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:242 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_1: "bf16[3072, 3072]" = torch.ops.aten.permute.default(mark_argument_attributes_4, [1, 0]);  mark_argument_attributes_4 = None
        view_4: "bf16[7, 3072]" = torch.ops.aten.view.default(mul_4, [7, 3072])
        mm: "bf16[7, 3072]" = torch.ops.aten.mm.default(view_4, permute_1);  view_4 = permute_1 = None
        view_5: "bf16[1, 7, 3072]" = torch.ops.aten.view.default(mm, [1, 7, 3072]);  mm = None
        view_6: "bf16[1, 7, 24, 128]" = torch.ops.aten.view.default(view_5, [1, 7, -1, 128]);  view_5 = None
        permute_2: "bf16[1, 24, 7, 128]" = torch.ops.aten.permute.default(view_6, [0, 2, 1, 3]);  view_6 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:243 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_3: "bf16[3072, 1024]" = torch.ops.aten.permute.default(mark_argument_attributes_5, [1, 0]);  mark_argument_attributes_5 = None
        view_7: "bf16[7, 3072]" = torch.ops.aten.view.default(mul_4, [7, 3072])
        mm_1: "bf16[7, 1024]" = torch.ops.aten.mm.default(view_7, permute_3);  view_7 = permute_3 = None
        view_8: "bf16[1, 7, 1024]" = torch.ops.aten.view.default(mm_1, [1, 7, 1024]);  mm_1 = None
        view_9: "bf16[1, 7, 8, 128]" = torch.ops.aten.view.default(view_8, [1, 7, -1, 128]);  view_8 = None
        permute_4: "bf16[1, 8, 7, 128]" = torch.ops.aten.permute.default(view_9, [0, 2, 1, 3]);  view_9 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:244 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_5: "bf16[3072, 1024]" = torch.ops.aten.permute.default(mark_argument_attributes_6, [1, 0]);  mark_argument_attributes_6 = None
        view_10: "bf16[7, 3072]" = torch.ops.aten.view.default(mul_4, [7, 3072]);  mul_4 = None
        mm_2: "bf16[7, 1024]" = torch.ops.aten.mm.default(view_10, permute_5);  view_10 = permute_5 = None
        view_11: "bf16[1, 7, 1024]" = torch.ops.aten.view.default(mm_2, [1, 7, 1024]);  mm_2 = None
        view_12: "bf16[1, 7, 8, 128]" = torch.ops.aten.view.default(view_11, [1, 7, -1, 128]);  view_11 = None
        permute_6: "bf16[1, 8, 7, 128]" = torch.ops.aten.permute.default(view_12, [0, 2, 1, 3]);  view_12 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:143 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_8: "bf16[1, 1, 7, 128]" = torch.ops.aten.unsqueeze.default(convert_element_type_5, 1);  convert_element_type_5 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:144 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_9: "bf16[1, 1, 7, 128]" = torch.ops.aten.unsqueeze.default(convert_element_type_6, 1);  convert_element_type_6 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:145 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 24, 7, 128]" = torch.ops.aten.mul.Tensor(permute_2, unsqueeze_8)
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_6: "bf16[1, 24, 7, 64]" = torch.ops.aten.slice.Tensor(permute_2, 3, 0, 64)
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_7: "bf16[1, 24, 7, 64]" = torch.ops.aten.slice.Tensor(permute_2, 3, 64, 9223372036854775807);  permute_2 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 24, 7, 64]" = torch.ops.aten.neg.default(slice_7);  slice_7 = None
        cat_1: "bf16[1, 24, 7, 128]" = torch.ops.aten.cat.default([neg, slice_6], -1);  neg = slice_6 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:145 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_6: "bf16[1, 24, 7, 128]" = torch.ops.aten.mul.Tensor(cat_1, unsqueeze_9);  cat_1 = None
        add_1: "bf16[1, 24, 7, 128]" = torch.ops.aten.add.Tensor(mul_5, mul_6);  mul_5 = mul_6 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:146 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 7, 128]" = torch.ops.aten.mul.Tensor(permute_4, unsqueeze_8);  unsqueeze_8 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_8: "bf16[1, 8, 7, 64]" = torch.ops.aten.slice.Tensor(permute_4, 3, 0, 64)
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_9: "bf16[1, 8, 7, 64]" = torch.ops.aten.slice.Tensor(permute_4, 3, 64, 9223372036854775807);  permute_4 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 7, 64]" = torch.ops.aten.neg.default(slice_9);  slice_9 = None
        cat_2: "bf16[1, 8, 7, 128]" = torch.ops.aten.cat.default([neg_1, slice_8], -1);  neg_1 = slice_8 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:146 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_8: "bf16[1, 8, 7, 128]" = torch.ops.aten.mul.Tensor(cat_2, unsqueeze_9);  cat_2 = unsqueeze_9 = None
        add_2: "bf16[1, 8, 7, 128]" = torch.ops.aten.add.Tensor(mul_7, mul_8);  mul_7 = mul_8 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/cache_utils.py:1368 in update, code: key_states = key_states.to(self.key_cache[layer_idx].dtype)
        convert_element_type_9: "bf16[1, 8, 7, 128]" = torch.ops.prims.convert_element_type.default(add_2, torch.bfloat16);  convert_element_type_9 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/cache_utils.py:1369 in update, code: value_states = value_states.to(self.value_cache[layer_idx].dtype)
        convert_element_type_10: "bf16[1, 8, 7, 128]" = torch.ops.prims.convert_element_type.default(permute_6, torch.bfloat16);  convert_element_type_10 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/cache_utils.py:54 in _static_cache_update, code: k_cache.index_copy_(2, cache_position, key_states)
        index_put: "bf16[1, 8, 1024, 128]" = torch.ops.aten.index_put.default(mark_argument_attributes_15, [None, None, mark_argument_attributes_14], add_2);  add_2 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/cache_utils.py:55 in _static_cache_update, code: v_cache.index_copy_(2, cache_position, value_states)
        index_put_1: "bf16[1, 8, 1024, 128]" = torch.ops.aten.index_put.default(mark_argument_attributes_16, [None, None, mark_argument_attributes_14], permute_6);  mark_argument_attributes_14 = permute_6 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        slice_14: "bf16[1, 8, 1024, 128]" = torch.ops.aten.slice.Tensor(index_put, 0, 0, 9223372036854775807)
        slice_15: "bf16[1, 8, 1024, 128]" = torch.ops.aten.slice.Tensor(slice_14, 1, 0, 9223372036854775807);  slice_14 = None
        unsqueeze_11: "bf16[1, 8, 1, 1024, 128]" = torch.ops.aten.unsqueeze.default(slice_15, 2);  slice_15 = None
        slice_16: "bf16[1, 8, 1, 1024, 128]" = torch.ops.aten.slice.Tensor(unsqueeze_11, 3, 0, 9223372036854775807);  unsqueeze_11 = None
        slice_17: "bf16[1, 8, 1, 1024, 128]" = torch.ops.aten.slice.Tensor(slice_16, 4, 0, 9223372036854775807);  slice_16 = None
        expand_5: "bf16[1, 8, 3, 1024, 128]" = torch.ops.aten.expand.default(slice_17, [1, 8, 3, 1024, 128]);  slice_17 = None
        clone: "bf16[1, 8, 3, 1024, 128]" = torch.ops.aten.clone.default(expand_5, memory_format = torch.contiguous_format);  expand_5 = None
        view_13: "bf16[1, 24, 1024, 128]" = torch.ops.aten.view.default(clone, [1, 24, 1024, 128]);  clone = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        slice_22: "bf16[1, 8, 1024, 128]" = torch.ops.aten.slice.Tensor(index_put_1, 0, 0, 9223372036854775807)
        slice_23: "bf16[1, 8, 1024, 128]" = torch.ops.aten.slice.Tensor(slice_22, 1, 0, 9223372036854775807);  slice_22 = None
        unsqueeze_13: "bf16[1, 8, 1, 1024, 128]" = torch.ops.aten.unsqueeze.default(slice_23, 2);  slice_23 = None
        slice_24: "bf16[1, 8, 1, 1024, 128]" = torch.ops.aten.slice.Tensor(unsqueeze_13, 3, 0, 9223372036854775807);  unsqueeze_13 = None
        slice_25: "bf16[1, 8, 1, 1024, 128]" = torch.ops.aten.slice.Tensor(slice_24, 4, 0, 9223372036854775807);  slice_24 = None
        expand_7: "bf16[1, 8, 3, 1024, 128]" = torch.ops.aten.expand.default(slice_25, [1, 8, 3, 1024, 128]);  slice_25 = None
        clone_1: "bf16[1, 8, 3, 1024, 128]" = torch.ops.aten.clone.default(expand_7, memory_format = torch.contiguous_format);  expand_7 = None
        view_14: "bf16[1, 24, 1024, 128]" = torch.ops.aten.view.default(clone_1, [1, 24, 1024, 128]);  clone_1 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:191 in eager_attention_forward, code: attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling
        permute_7: "bf16[1, 24, 128, 1024]" = torch.ops.aten.permute.default(view_13, [0, 1, 3, 2]);  view_13 = None
        expand_8: "bf16[1, 24, 7, 128]" = torch.ops.aten.expand.default(add_1, [1, 24, 7, 128]);  add_1 = None
        view_15: "bf16[24, 7, 128]" = torch.ops.aten.view.default(expand_8, [24, 7, 128]);  expand_8 = None
        expand_9: "bf16[1, 24, 128, 1024]" = torch.ops.aten.expand.default(permute_7, [1, 24, 128, 1024]);  permute_7 = None
        view_16: "bf16[24, 128, 1024]" = torch.ops.aten.view.default(expand_9, [24, 128, 1024]);  expand_9 = None
        bmm_1: "bf16[24, 7, 1024]" = torch.ops.aten.bmm.default(view_15, view_16);  view_15 = view_16 = None
        view_17: "bf16[1, 24, 7, 1024]" = torch.ops.aten.view.default(bmm_1, [1, 24, 7, 1024]);  bmm_1 = None
        mul_9: "bf16[1, 24, 7, 1024]" = torch.ops.aten.mul.Tensor(view_17, 0.08838834764831845);  view_17 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:194 in eager_attention_forward, code: attn_weights = attn_weights + causal_mask
        unsqueeze_14: "bf16[1, 7, 1024]" = torch.ops.aten.unsqueeze.default(mul, 0);  mul = None
        unsqueeze_15: "bf16[1, 1, 7, 1024]" = torch.ops.aten.unsqueeze.default(unsqueeze_14, 1);  unsqueeze_14 = None
        slice_29: "bf16[1, 1, 7, 1024]" = torch.ops.aten.slice.Tensor(unsqueeze_15, 2, 0, 9223372036854775807);  unsqueeze_15 = None
        slice_30: "bf16[1, 1, 7, 1024]" = torch.ops.aten.slice.Tensor(slice_29, 3, 0, 9223372036854775807);  slice_29 = None
        expand_10: "bf16[1, 1, 7, 1024]" = torch.ops.aten.expand.default(slice_30, [1, 1, -1, -1]);  slice_30 = None
        slice_31: "bf16[1, 1, 7, 1024]" = torch.ops.aten.slice.Tensor(expand_10, 0, 0, 9223372036854775807);  expand_10 = None
        slice_32: "bf16[1, 1, 7, 1024]" = torch.ops.aten.slice.Tensor(slice_31, 1, 0, 9223372036854775807);  slice_31 = None
        slice_33: "bf16[1, 1, 7, 1024]" = torch.ops.aten.slice.Tensor(slice_32, 2, 0, 9223372036854775807);  slice_32 = None
        add_3: "bf16[1, 24, 7, 1024]" = torch.ops.aten.add.Tensor(mul_9, slice_33);  mul_9 = slice_33 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:196 in eager_attention_forward, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
        convert_element_type_11: "f32[1, 24, 7, 1024]" = torch.ops.prims.convert_element_type.default(add_3, torch.float32);  add_3 = None
        _softmax: "f32[1, 24, 7, 1024]" = torch.ops.aten._softmax.default(convert_element_type_11, -1, False);  convert_element_type_11 = None
        convert_element_type_12: "bf16[1, 24, 7, 1024]" = torch.ops.prims.convert_element_type.default(_softmax, torch.bfloat16);  _softmax = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:197 in eager_attention_forward, code: attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
        clone_2: "bf16[1, 24, 7, 1024]" = torch.ops.aten.clone.default(convert_element_type_12);  convert_element_type_12 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:198 in eager_attention_forward, code: attn_output = torch.matmul(attn_weights, value_states)
        expand_11: "bf16[1, 24, 7, 1024]" = torch.ops.aten.expand.default(clone_2, [1, 24, 7, 1024]);  clone_2 = None
        view_18: "bf16[24, 7, 1024]" = torch.ops.aten.view.default(expand_11, [24, 7, 1024]);  expand_11 = None
        expand_12: "bf16[1, 24, 1024, 128]" = torch.ops.aten.expand.default(view_14, [1, 24, 1024, 128]);  view_14 = None
        view_19: "bf16[24, 1024, 128]" = torch.ops.aten.view.default(expand_12, [24, 1024, 128]);  expand_12 = None
        bmm_2: "bf16[24, 7, 128]" = torch.ops.aten.bmm.default(view_18, view_19);  view_18 = view_19 = None
        view_20: "bf16[1, 24, 7, 128]" = torch.ops.aten.view.default(bmm_2, [1, 24, 7, 128]);  bmm_2 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:199 in eager_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_8: "bf16[1, 7, 24, 128]" = torch.ops.aten.permute.default(view_20, [0, 2, 1, 3]);  view_20 = None
        clone_3: "bf16[1, 7, 24, 128]" = torch.ops.aten.clone.default(permute_8, memory_format = torch.contiguous_format);  permute_8 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:276 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_21: "bf16[1, 7, 3072]" = torch.ops.aten.view.default(clone_3, [1, 7, -1]);  clone_3 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:277 in forward, code: attn_output = self.o_proj(attn_output)
        permute_9: "bf16[3072, 3072]" = torch.ops.aten.permute.default(mark_argument_attributes_7, [1, 0]);  mark_argument_attributes_7 = None
        view_22: "bf16[7, 3072]" = torch.ops.aten.view.default(view_21, [7, 3072]);  view_21 = None
        mm_3: "bf16[7, 3072]" = torch.ops.aten.mm.default(view_22, permute_9);  view_22 = permute_9 = None
        view_23: "bf16[1, 7, 3072]" = torch.ops.aten.view.default(mm_3, [1, 7, 3072]);  mm_3 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:319 in forward, code: hidden_states = residual + hidden_states
        add_4: "bf16[1, 7, 3072]" = torch.ops.aten.add.Tensor(embedding, view_23);  embedding = view_23 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:70 in forward, code: hidden_states = hidden_states.to(torch.float32)
        convert_element_type_13: "f32[1, 7, 3072]" = torch.ops.prims.convert_element_type.default(add_4, torch.float32)
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:71 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_2: "f32[1, 7, 3072]" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_13, 2)
        mean_1: "f32[1, 7, 1]" = torch.ops.aten.mean.dim(pow_2, [-1], True);  pow_2 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:72 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add_5: "f32[1, 7, 1]" = torch.ops.aten.add.Tensor(mean_1, 1e-05);  mean_1 = None
        rsqrt_1: "f32[1, 7, 1]" = torch.ops.aten.rsqrt.default(add_5);  add_5 = None
        mul_10: "f32[1, 7, 3072]" = torch.ops.aten.mul.Tensor(convert_element_type_13, rsqrt_1);  convert_element_type_13 = rsqrt_1 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:73 in forward, code: return self.weight * hidden_states.to(input_dtype)
        convert_element_type_14: "bf16[1, 7, 3072]" = torch.ops.prims.convert_element_type.default(mul_10, torch.bfloat16);  mul_10 = None
        mul_11: "bf16[1, 7, 3072]" = torch.ops.aten.mul.Tensor(mark_argument_attributes_1, convert_element_type_14);  mark_argument_attributes_1 = convert_element_type_14 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:162 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_10: "bf16[3072, 8192]" = torch.ops.aten.permute.default(mark_argument_attributes_8, [1, 0]);  mark_argument_attributes_8 = None
        view_24: "bf16[7, 3072]" = torch.ops.aten.view.default(mul_11, [7, 3072])
        mm_4: "bf16[7, 8192]" = torch.ops.aten.mm.default(view_24, permute_10);  view_24 = permute_10 = None
        view_25: "bf16[1, 7, 8192]" = torch.ops.aten.view.default(mm_4, [1, 7, 8192]);  mm_4 = None
        convert_element_type_15: "f32[1, 7, 8192]" = torch.ops.prims.convert_element_type.default(view_25, torch.float32);  convert_element_type_15 = None
        sigmoid: "f32[1, 7, 8192]" = torch.ops.aten.sigmoid.default(view_25)
        mul_12: "f32[1, 7, 8192]" = torch.ops.aten.mul.Tensor(view_25, sigmoid);  view_25 = sigmoid = None
        convert_element_type_16: "bf16[1, 7, 8192]" = torch.ops.prims.convert_element_type.default(mul_12, torch.bfloat16);  mul_12 = None
        permute_11: "bf16[3072, 8192]" = torch.ops.aten.permute.default(mark_argument_attributes_9, [1, 0]);  mark_argument_attributes_9 = None
        view_26: "bf16[7, 3072]" = torch.ops.aten.view.default(mul_11, [7, 3072]);  mul_11 = None
        mm_5: "bf16[7, 8192]" = torch.ops.aten.mm.default(view_26, permute_11);  view_26 = permute_11 = None
        view_27: "bf16[1, 7, 8192]" = torch.ops.aten.view.default(mm_5, [1, 7, 8192]);  mm_5 = None
        mul_13: "bf16[1, 7, 8192]" = torch.ops.aten.mul.Tensor(convert_element_type_16, view_27);  convert_element_type_16 = view_27 = None
        permute_12: "bf16[8192, 3072]" = torch.ops.aten.permute.default(mark_argument_attributes_10, [1, 0]);  mark_argument_attributes_10 = None
        view_28: "bf16[7, 8192]" = torch.ops.aten.view.default(mul_13, [7, 8192]);  mul_13 = None
        mm_6: "bf16[7, 3072]" = torch.ops.aten.mm.default(view_28, permute_12);  view_28 = permute_12 = None
        view_29: "bf16[1, 7, 3072]" = torch.ops.aten.view.default(mm_6, [1, 7, 3072]);  mm_6 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325 in forward, code: hidden_states = residual + hidden_states
        add_6: "bf16[1, 7, 3072]" = torch.ops.aten.add.Tensor(add_4, view_29);  add_4 = view_29 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:70 in forward, code: hidden_states = hidden_states.to(torch.float32)
        convert_element_type_17: "f32[1, 7, 3072]" = torch.ops.prims.convert_element_type.default(add_6, torch.float32);  add_6 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:71 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_3: "f32[1, 7, 3072]" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_17, 2)
        mean_2: "f32[1, 7, 1]" = torch.ops.aten.mean.dim(pow_3, [-1], True);  pow_3 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:72 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add_7: "f32[1, 7, 1]" = torch.ops.aten.add.Tensor(mean_2, 1e-05);  mean_2 = None
        rsqrt_2: "f32[1, 7, 1]" = torch.ops.aten.rsqrt.default(add_7);  add_7 = None
        mul_14: "f32[1, 7, 3072]" = torch.ops.aten.mul.Tensor(convert_element_type_17, rsqrt_2);  convert_element_type_17 = rsqrt_2 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:73 in forward, code: return self.weight * hidden_states.to(input_dtype)
        convert_element_type_18: "bf16[1, 7, 3072]" = torch.ops.prims.convert_element_type.default(mul_14, torch.bfloat16);  mul_14 = None
        mul_15: "bf16[1, 7, 3072]" = torch.ops.aten.mul.Tensor(mark_argument_attributes_2, convert_element_type_18);  mark_argument_attributes_2 = convert_element_type_18 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:704 in forward, code: logits = self.lm_head(hidden_states[:, slice_indices, :])
        slice_34: "bf16[1, 7, 3072]" = torch.ops.aten.slice.Tensor(mul_15, 0, 0, 9223372036854775807);  mul_15 = None
        slice_35: "bf16[1, 7, 3072]" = torch.ops.aten.slice.Tensor(slice_34, 1, 0, 9223372036854775807);  slice_34 = None
        slice_36: "bf16[1, 7, 3072]" = torch.ops.aten.slice.Tensor(slice_35, 2, 0, 9223372036854775807);  slice_35 = None
        permute_13: "bf16[3072, 128256]" = torch.ops.aten.permute.default(mark_argument_attributes_11, [1, 0]);  mark_argument_attributes_11 = None
        view_30: "bf16[7, 3072]" = torch.ops.aten.view.default(slice_36, [7, 3072]);  slice_36 = None
        mm_7: "bf16[7, 128256]" = torch.ops.aten.mm.default(view_30, permute_13);  view_30 = permute_13 = None
        view_31: "bf16[1, 7, 128256]" = torch.ops.aten.view.default(mm_7, [1, 7, 128256]);  mm_7 = None
        
        # No stacktrace found for following nodes
        copy__default = torch.ops.aten.copy_.default(mark_argument_attributes_15, index_put);  mark_argument_attributes_15 = index_put = copy__default = None
        copy__default_1 = torch.ops.aten.copy_.default(mark_argument_attributes_16, index_put_1);  mark_argument_attributes_16 = index_put_1 = copy__default_1 = None
        return pytree.tree_unflatten((view_31,), self._out_spec)
        2025-09-15 15:41:07.467 (  15.793s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.467 (  15.793s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.467 (  15.793s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.467 (  15.793s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.467 (  15.793s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.467 (  15.794s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.467 (  15.794s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.467 (  15.794s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.468 (  15.794s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.468 (  15.794s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.468 (  15.794s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.468 (  15.794s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.468 (  15.794s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.468 (  15.794s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.468 (  15.794s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.468 (  15.794s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.468 (  15.794s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.468 (  15.794s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.468 (  15.794s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.468 (  15.794s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.468 (  15.794s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.468 (  15.794s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.468 (  15.795s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.468 (  15.795s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.468 (  15.795s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.468 (  15.795s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.468 (  15.795s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.468 (  15.795s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.468 (  15.795s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.469 (  15.795s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.469 (  15.795s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.469 (  15.795s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.469 (  15.795s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.469 (  15.795s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.469 (  15.795s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.469 (  15.795s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.469 (  15.795s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.469 (  15.795s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.554 (  15.880s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.554 (  15.880s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.554 (  15.880s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.554 (  15.881s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.554 (  15.881s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.554 (  15.881s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.554 (  15.881s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.554 (  15.881s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.555 (  15.881s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.555 (  15.881s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.615 (  15.941s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.615 (  15.941s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.701 (  16.027s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.701 (  16.027s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.701 (  16.027s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.701 (  16.027s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.701 (  16.027s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.701 (  16.028s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.701 (  16.028s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.701 (  16.028s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.701 (  16.028s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.701 (  16.028s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.755 (  16.081s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.755 (  16.081s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.755 (  16.081s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.755 (  16.081s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.755 (  16.081s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.755 (  16.082s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.755 (  16.082s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.755 (  16.082s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.755 (  16.082s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.755 (  16.082s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.756 (  16.082s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.756 (  16.082s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.756 (  16.083s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.756 (  16.083s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.756 (  16.083s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.756 (  16.083s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.757 (  16.083s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.757 (  16.083s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.757 (  16.083s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.757 (  16.083s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.757 (  16.083s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.757 (  16.083s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.757 (  16.083s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.757 (  16.083s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.760 (  16.087s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.760 (  16.087s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.760 (  16.087s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.760 (  16.087s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.760 (  16.087s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.760 (  16.087s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.760 (  16.087s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.761 (  16.087s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.761 (  16.087s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.761 (  16.087s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.761 (  16.087s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.761 (  16.087s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.762 (  16.089s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.762 (  16.089s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.762 (  16.089s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.762 (  16.089s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.762 (  16.089s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-09-15 15:41:07.763 (  16.089s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.763 (  16.089s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.763 (  16.089s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.763 (  16.089s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.763 (  16.089s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-09-15 15:41:07.763 (  16.089s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.763 (  16.089s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.764 (  16.090s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.764 (  16.090s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.764 (  16.090s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.764 (  16.090s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.764 (  16.090s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.764 (  16.090s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.764 (  16.090s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.764 (  16.090s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.764 (  16.090s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.764 (  16.090s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.764 (  16.090s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.764 (  16.090s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.768 (  16.094s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.768 (  16.094s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.768 (  16.094s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.768 (  16.094s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.768 (  16.094s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.768 (  16.094s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.768 (  16.094s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.768 (  16.094s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.768 (  16.094s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.768 (  16.094s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.768 (  16.094s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.768 (  16.094s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.768 (  16.094s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.768 (  16.094s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.768 (  16.094s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.768 (  16.094s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.768 (  16.094s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.768 (  16.095s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.768 (  16.095s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.768 (  16.095s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.768 (  16.095s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.768 (  16.095s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.768 (  16.095s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.768 (  16.095s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.768 (  16.095s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.768 (  16.095s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.768 (  16.095s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.768 (  16.095s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.768 (  16.095s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.768 (  16.095s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.769 (  16.095s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.769 (  16.095s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.769 (  16.095s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.769 (  16.095s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.769 (  16.095s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.769 (  16.095s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.769 (  16.095s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.769 (  16.095s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.769 (  16.095s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.769 (  16.095s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.769 (  16.095s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.769 (  16.095s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.769 (  16.095s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.769 (  16.095s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.769 (  16.095s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.769 (  16.095s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.769 (  16.095s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.769 (  16.095s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.769 (  16.095s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.769 (  16.095s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.769 (  16.095s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.769 (  16.095s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.769 (  16.095s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.769 (  16.096s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.769 (  16.096s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.769 (  16.096s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.769 (  16.096s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.769 (  16.096s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.769 (  16.096s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.769 (  16.096s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.769 (  16.096s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.769 (  16.096s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.769 (  16.096s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.769 (  16.096s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.769 (  16.096s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.770 (  16.096s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.770 (  16.096s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.770 (  16.096s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.770 (  16.096s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.770 (  16.096s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.770 (  16.096s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.770 (  16.096s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.770 (  16.096s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.770 (  16.096s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.770 (  16.096s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.770 (  16.096s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.770 (  16.096s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.770 (  16.096s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.770 (  16.096s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.770 (  16.096s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.770 (  16.096s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.770 (  16.096s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.770 (  16.096s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.770 (  16.097s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.770 (  16.097s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.770 (  16.097s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.770 (  16.097s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.770 (  16.097s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.770 (  16.097s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.770 (  16.097s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.770 (  16.097s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.770 (  16.097s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.770 (  16.097s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.770 (  16.097s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.770 (  16.097s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.770 (  16.097s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.770 (  16.097s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.770 (  16.097s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.771 (  16.097s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.771 (  16.097s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.771 (  16.097s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.771 (  16.097s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.771 (  16.097s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.771 (  16.097s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.771 (  16.097s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.771 (  16.097s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.771 (  16.097s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.771 (  16.097s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.771 (  16.097s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.771 (  16.097s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.771 (  16.097s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.771 (  16.097s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.771 (  16.097s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.771 (  16.097s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.771 (  16.097s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.771 (  16.097s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.771 (  16.097s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.771 (  16.097s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.771 (  16.098s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.771 (  16.098s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.771 (  16.098s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.771 (  16.098s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.771 (  16.098s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.771 (  16.098s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.771 (  16.098s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.771 (  16.098s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.771 (  16.098s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.771 (  16.098s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.771 (  16.098s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.771 (  16.098s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.772 (  16.098s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.772 (  16.098s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.772 (  16.098s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.772 (  16.098s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.772 (  16.098s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.772 (  16.098s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.772 (  16.098s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.772 (  16.098s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.772 (  16.098s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.772 (  16.098s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.772 (  16.098s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.772 (  16.098s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.772 (  16.098s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.772 (  16.098s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.772 (  16.098s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.772 (  16.098s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.772 (  16.098s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.772 (  16.098s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.772 (  16.098s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.772 (  16.099s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.772 (  16.099s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.772 (  16.099s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.772 (  16.099s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.772 (  16.099s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.772 (  16.099s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.772 (  16.099s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.772 (  16.099s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.772 (  16.099s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.772 (  16.099s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.772 (  16.099s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.772 (  16.099s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.773 (  16.099s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.773 (  16.099s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.773 (  16.099s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.773 (  16.099s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.773 (  16.099s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.773 (  16.099s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.773 (  16.099s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.773 (  16.099s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.773 (  16.099s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.773 (  16.099s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.773 (  16.099s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.773 (  16.099s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.773 (  16.099s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.773 (  16.099s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.773 (  16.099s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.773 (  16.099s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.773 (  16.099s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.773 (  16.099s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.773 (  16.099s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.773 (  16.099s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.773 (  16.099s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.773 (  16.099s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.773 (  16.099s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.773 (  16.100s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.773 (  16.100s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.773 (  16.100s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.773 (  16.100s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.773 (  16.100s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.773 (  16.100s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.773 (  16.100s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.773 (  16.100s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.773 (  16.100s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.773 (  16.100s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.773 (  16.100s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.773 (  16.100s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.773 (  16.100s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.774 (  16.100s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.774 (  16.100s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.774 (  16.100s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.774 (  16.100s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.774 (  16.100s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.774 (  16.100s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.774 (  16.100s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.774 (  16.100s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.774 (  16.100s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.774 (  16.100s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.774 (  16.100s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.774 (  16.100s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.774 (  16.101s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.774 (  16.101s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.774 (  16.101s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.774 (  16.101s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.774 (  16.101s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.774 (  16.101s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.774 (  16.101s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.774 (  16.101s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.774 (  16.101s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.774 (  16.101s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.774 (  16.101s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.774 (  16.101s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.775 (  16.101s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.775 (  16.101s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.775 (  16.101s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.775 (  16.101s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.775 (  16.101s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.775 (  16.101s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.775 (  16.101s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.775 (  16.101s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.775 (  16.101s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.775 (  16.101s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.775 (  16.101s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.775 (  16.101s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.775 (  16.101s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.775 (  16.101s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.775 (  16.101s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.775 (  16.101s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.775 (  16.101s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.775 (  16.101s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.775 (  16.101s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.775 (  16.102s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.775 (  16.102s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.775 (  16.102s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.775 (  16.102s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.775 (  16.102s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.775 (  16.102s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.775 (  16.102s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.775 (  16.102s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.775 (  16.102s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.775 (  16.102s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.775 (  16.102s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.775 (  16.102s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.775 (  16.102s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.775 (  16.102s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.776 (  16.102s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.776 (  16.102s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.776 (  16.102s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.776 (  16.102s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.776 (  16.102s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.776 (  16.102s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.776 (  16.102s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.776 (  16.102s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.776 (  16.102s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.776 (  16.102s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.776 (  16.102s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.776 (  16.102s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.776 (  16.102s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.776 (  16.102s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.776 (  16.102s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.776 (  16.102s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.776 (  16.102s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.776 (  16.102s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.776 (  16.102s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.776 (  16.102s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.776 (  16.103s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.776 (  16.103s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.776 (  16.103s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.776 (  16.103s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.776 (  16.103s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.776 (  16.103s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.776 (  16.103s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.776 (  16.103s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.776 (  16.103s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.776 (  16.103s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.776 (  16.103s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.776 (  16.103s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.777 (  16.103s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.777 (  16.103s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.777 (  16.103s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.777 (  16.103s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.777 (  16.103s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.777 (  16.103s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.777 (  16.103s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.777 (  16.103s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.777 (  16.103s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.777 (  16.103s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.777 (  16.103s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.777 (  16.103s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.777 (  16.104s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.777 (  16.104s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.777 (  16.104s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.777 (  16.104s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.777 (  16.104s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.777 (  16.104s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.777 (  16.104s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.777 (  16.104s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.777 (  16.104s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.777 (  16.104s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.777 (  16.104s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.777 (  16.104s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.777 (  16.104s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.777 (  16.104s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.778 (  16.104s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.778 (  16.104s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.778 (  16.104s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.778 (  16.104s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.778 (  16.104s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.778 (  16.104s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.778 (  16.104s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.778 (  16.104s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.778 (  16.104s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.778 (  16.104s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.778 (  16.104s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.778 (  16.104s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.778 (  16.104s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.778 (  16.104s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.778 (  16.104s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.778 (  16.104s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.778 (  16.104s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.778 (  16.104s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.778 (  16.104s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.778 (  16.104s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.778 (  16.104s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.778 (  16.104s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.778 (  16.105s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.778 (  16.105s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.778 (  16.105s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.778 (  16.105s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.778 (  16.105s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.778 (  16.105s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.778 (  16.105s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.778 (  16.105s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.778 (  16.105s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.778 (  16.105s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.778 (  16.105s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.778 (  16.105s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.779 (  16.105s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.779 (  16.105s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.779 (  16.105s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.779 (  16.105s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.779 (  16.105s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.779 (  16.105s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.779 (  16.105s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.779 (  16.105s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.779 (  16.105s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.779 (  16.105s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.779 (  16.105s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.779 (  16.105s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.779 (  16.105s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.779 (  16.105s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.779 (  16.105s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.779 (  16.105s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.779 (  16.105s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.779 (  16.105s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.779 (  16.105s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.779 (  16.106s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.779 (  16.106s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.779 (  16.106s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.779 (  16.106s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.779 (  16.106s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.779 (  16.106s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.779 (  16.106s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.779 (  16.106s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.779 (  16.106s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.779 (  16.106s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.779 (  16.106s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.779 (  16.106s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.780 (  16.106s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.780 (  16.106s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.780 (  16.106s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.780 (  16.106s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.780 (  16.106s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.780 (  16.106s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.780 (  16.106s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.780 (  16.106s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.780 (  16.106s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.780 (  16.106s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.780 (  16.106s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.780 (  16.106s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.780 (  16.106s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.780 (  16.106s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.780 (  16.106s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.780 (  16.106s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.780 (  16.106s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.780 (  16.106s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.780 (  16.106s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.780 (  16.106s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.780 (  16.106s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.780 (  16.107s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.780 (  16.107s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.780 (  16.107s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.780 (  16.107s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.780 (  16.107s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.780 (  16.107s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.780 (  16.107s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.780 (  16.107s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.780 (  16.107s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.780 (  16.107s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.780 (  16.107s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.780 (  16.107s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.780 (  16.107s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.781 (  16.107s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.781 (  16.107s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.781 (  16.107s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.781 (  16.107s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.781 (  16.107s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.781 (  16.107s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.781 (  16.107s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.781 (  16.107s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.781 (  16.107s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.781 (  16.107s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.781 (  16.107s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.781 (  16.107s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.781 (  16.107s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.781 (  16.107s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.781 (  16.107s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.781 (  16.107s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.781 (  16.107s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.781 (  16.107s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.781 (  16.107s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.781 (  16.108s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.781 (  16.108s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.781 (  16.108s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.781 (  16.108s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.781 (  16.108s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.781 (  16.108s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.781 (  16.108s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.781 (  16.108s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.781 (  16.108s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.781 (  16.108s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.781 (  16.108s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.781 (  16.108s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.781 (  16.108s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.782 (  16.108s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.782 (  16.108s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.782 (  16.108s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.782 (  16.108s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.782 (  16.108s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.782 (  16.108s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.782 (  16.108s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.782 (  16.108s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.782 (  16.108s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.782 (  16.108s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.782 (  16.108s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.782 (  16.108s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.782 (  16.108s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.782 (  16.108s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.782 (  16.108s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.782 (  16.108s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.782 (  16.108s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.782 (  16.108s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.782 (  16.108s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.782 (  16.108s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.782 (  16.108s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.782 (  16.108s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.782 (  16.108s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.782 (  16.109s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.782 (  16.109s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.782 (  16.109s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.782 (  16.109s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.782 (  16.109s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.782 (  16.109s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.782 (  16.109s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.782 (  16.109s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.782 (  16.109s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.782 (  16.109s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.782 (  16.109s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.782 (  16.109s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.783 (  16.109s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.783 (  16.109s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.783 (  16.109s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.783 (  16.109s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.783 (  16.109s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.783 (  16.109s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.783 (  16.109s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.783 (  16.109s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.783 (  16.109s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.783 (  16.109s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.783 (  16.109s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.783 (  16.109s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.783 (  16.109s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.783 (  16.109s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.783 (  16.109s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.783 (  16.109s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.783 (  16.109s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.783 (  16.109s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.783 (  16.109s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.783 (  16.110s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.783 (  16.110s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.783 (  16.110s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.783 (  16.110s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.783 (  16.110s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.783 (  16.110s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.783 (  16.110s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.783 (  16.110s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.783 (  16.110s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.783 (  16.110s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.783 (  16.110s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.783 (  16.110s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.784 (  16.110s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.784 (  16.110s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.784 (  16.110s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.784 (  16.110s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.784 (  16.110s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.784 (  16.110s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.784 (  16.110s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.784 (  16.110s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.784 (  16.110s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.784 (  16.110s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.784 (  16.110s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.784 (  16.110s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.784 (  16.110s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.784 (  16.110s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.784 (  16.110s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.784 (  16.110s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.784 (  16.110s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.784 (  16.110s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.784 (  16.110s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.784 (  16.110s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.784 (  16.110s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.784 (  16.110s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.784 (  16.111s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.784 (  16.111s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.784 (  16.111s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.784 (  16.111s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.784 (  16.111s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.784 (  16.111s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.784 (  16.111s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.784 (  16.111s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.784 (  16.111s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.784 (  16.111s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.784 (  16.111s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.784 (  16.111s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.784 (  16.111s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.784 (  16.111s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.785 (  16.111s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.785 (  16.111s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.785 (  16.111s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.785 (  16.111s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.785 (  16.111s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.785 (  16.111s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.785 (  16.111s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.785 (  16.111s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.785 (  16.111s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.785 (  16.111s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.785 (  16.111s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.785 (  16.111s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.785 (  16.111s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.785 (  16.111s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.785 (  16.111s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.785 (  16.111s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.785 (  16.111s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.785 (  16.112s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.785 (  16.112s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.785 (  16.112s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.785 (  16.112s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.785 (  16.112s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.785 (  16.112s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.785 (  16.112s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.785 (  16.112s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.785 (  16.112s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.785 (  16.112s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.785 (  16.112s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.785 (  16.112s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.785 (  16.112s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.785 (  16.112s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.786 (  16.112s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.786 (  16.112s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.786 (  16.112s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.786 (  16.112s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.786 (  16.112s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.786 (  16.112s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.786 (  16.112s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.786 (  16.112s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.786 (  16.112s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.786 (  16.112s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.786 (  16.112s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.786 (  16.112s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.786 (  16.112s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.786 (  16.112s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.786 (  16.112s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.786 (  16.112s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.786 (  16.112s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.786 (  16.112s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.786 (  16.112s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.786 (  16.112s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.786 (  16.112s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.786 (  16.112s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.786 (  16.113s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.786 (  16.113s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.786 (  16.113s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.786 (  16.113s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.786 (  16.113s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.786 (  16.113s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.786 (  16.113s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.786 (  16.113s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.786 (  16.113s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.786 (  16.113s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.786 (  16.113s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.786 (  16.113s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.787 (  16.113s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.787 (  16.113s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.787 (  16.113s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.787 (  16.113s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.787 (  16.113s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.787 (  16.113s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.787 (  16.113s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.787 (  16.113s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.787 (  16.113s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.787 (  16.113s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.787 (  16.113s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.787 (  16.113s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.787 (  16.113s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.787 (  16.113s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.787 (  16.113s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.787 (  16.113s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.787 (  16.113s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.787 (  16.113s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.787 (  16.113s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.787 (  16.114s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.787 (  16.114s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.787 (  16.114s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.787 (  16.114s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.787 (  16.114s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.787 (  16.114s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.787 (  16.114s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.787 (  16.114s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.787 (  16.114s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:07.787 (  16.114s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.787 (  16.114s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.787 (  16.114s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.787 (  16.114s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.787 (  16.114s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.787 (  16.114s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:41:07.787 (  16.114s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:41:07.787 (  16.114s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.114s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.115s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.115s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.115s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.115s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.115s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.115s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.115s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.115s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.115s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.115s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.115s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.115s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.115s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.115s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.115s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.115s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.115s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.115s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.115s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.788 (  16.115s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.789 (  16.115s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.789 (  16.115s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.789 (  16.115s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.789 (  16.115s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.789 (  16.115s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.789 (  16.115s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.789 (  16.116s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.789 (  16.116s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.789 (  16.116s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.789 (  16.116s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.790 (  16.116s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.790 (  16.116s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.790 (  16.116s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.790 (  16.116s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.790 (  16.117s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:41:07.822 (  16.149s) [        F073E000]     client_instance.cc:428      1| ClientInstance::PJRT_Client_Compile
2025-09-15 15:41:07.822 (  16.149s) [        F073E000]      module_builder.cc:103      1| ModuleBuilder::buildModule
2025-09-15 15:41:07.826 (  16.152s) [        F073E000]      module_builder.cc:165      1| VHLO Module:
module @SyncTensorsGraph.448 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, %arg1: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg2: !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>, %arg4: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg5: !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>, %arg6: !vhlo.tensor_v1<1x7x!vhlo.i64_v1>, %arg7: !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, %arg8: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg9: !vhlo.tensor_v1<7x!vhlo.i64_v1>, %arg10: !vhlo.tensor_v1<!vhlo.i64_v1>, %arg11: !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg13: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg14: !vhlo.tensor_v1<64x!vhlo.f32_v1>, %arg15: !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>, %arg16: !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>, %arg17: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg18: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg19: !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>, %arg20: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<"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tensor<1024xi64>>}> : () -> !vhlo.tensor_v1<1024x!vhlo.i64_v1>
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>>}> : () -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %3 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF800000> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %4 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %5 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<3.25520843E-4> : tensor<1x7xf32>>}> : () -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %6 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0> : tensor<7xi64>>}> : () -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %7 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1> : tensor<i64>>}> : () -> !vhlo.tensor_v1<!vhlo.i64_v1>
    %8 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %9 = "vhlo.broadcast_in_dim_v1"(%8) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>
    %10 = "vhlo.broadcast_in_dim_v1"(%7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.i64_v1>
    %11 = "vhlo.broadcast_in_dim_v1"(%4) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %12 = "vhlo.reshape_v1"(%arg20) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %13 = "vhlo.custom_call_v1"(%12) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_norm_weight">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1>
    %15 = "vhlo.convert_v1"(%14) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %16 = "vhlo.broadcast_in_dim_v1"(%15) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %17 = "vhlo.reshape_v1"(%arg7) : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>
    %18 = "vhlo.custom_call_v1"(%17) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_embed_tokens_weight">}>} : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>
    %19 = "vhlo.reshape_v1"(%18) : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>
    %20 = "vhlo.reshape_v1"(%arg6) : (!vhlo.tensor_v1<1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>
    %21 = "vhlo.custom_call_v1"(%20) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_0">}>} : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>
    %22 = "vhlo.reshape_v1"(%21) : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.ui32_v1>
    %24 = "vhlo.gather_v2"(%19, %23) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 3072]> : tensor<2xi64>>, start_index_map = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x!vhlo.ui32_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %25 = "vhlo.reshape_v1"(%24) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %26 = "vhlo.reshape_v1"(%arg8) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %27 = "vhlo.custom_call_v1"(%26) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___input_layernorm_weight">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %28 = "vhlo.reshape_v1"(%27) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1>
    %29 = "vhlo.convert_v1"(%28) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %30 = "vhlo.broadcast_in_dim_v1"(%29) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %31 = "vhlo.convert_v1"(%25) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %32 = "vhlo.power_v1"(%31, %11) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %33 = "vhlo.reduce_v1"(%32, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %242 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%242) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %34 = "vhlo.multiply_v1"(%33, %5) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %35 = "vhlo.reshape_v1"(%34) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %36 = "vhlo.broadcast_in_dim_v1"(%arg1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %37 = "vhlo.add_v1"(%35, %36) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %38 = "vhlo.rsqrt_v2"(%37) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %39 = "vhlo.reshape_v1"(%38) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %40 = "vhlo.broadcast_in_dim_v1"(%39) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %41 = "vhlo.multiply_v1"(%31, %40) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %42 = "vhlo.convert_v1"(%41) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %43 = "vhlo.convert_v1"(%42) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %44 = "vhlo.multiply_v1"(%30, %43) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %45 = "vhlo.convert_v1"(%44) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %46 = "vhlo.reshape_v1"(%45) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %47 = "vhlo.reshape_v1"(%arg17) : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>
    %48 = "vhlo.custom_call_v1"(%47) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>
    %49 = "vhlo.reshape_v1"(%48) : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %50 = "vhlo.transpose_v1"(%49) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,3072]{0,1}">} : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %51 = "vhlo.dot_general_v2"(%46, %50) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %52 = "vhlo.reshape_v1"(%51) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>
    %53 = "vhlo.transpose_v1"(%52) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,24,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %54 = "vhlo.convert_v1"(%53) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,24,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %55 = "vhlo.reshape_v1"(%arg14) : (!vhlo.tensor_v1<64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>
    %56 = "vhlo.custom_call_v1"(%55) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_rotary_emb_inv_freq">}>} : (!vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>
    %57 = "vhlo.reshape_v1"(%56) : (!vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>
    %58 = "vhlo.reshape_v1"(%arg9) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>
    %59 = "vhlo.custom_call_v1"(%58) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_1">}>} : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>
    %60 = "vhlo.reshape_v1"(%59) : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %61 = "vhlo.convert_v1"(%59) : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.f32_v1>
    %62 = "vhlo.dot_general_v2"(%57, %61) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>
    %63 = "vhlo.transpose_v1"(%62) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1]> : tensor<3xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[1, 2, 0]> : tensor<3xindex>>, xla_shape = #vhlo.string_v1<"f32[1,7,64]{1,2,0}">} : (!vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>
    %64 = "vhlo.concatenate_v1"(%63, %63) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %65 = "vhlo.cosine_v2"(%64) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %66 = "vhlo.convert_v1"(%65) : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>
    %67 = "vhlo.reshape_v1"(%66) : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>
    %68 = "vhlo.convert_v1"(%67) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>
    %69 = "vhlo.reshape_v1"(%68) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %70 = "vhlo.broadcast_in_dim_v1"(%69) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %71 = "vhlo.multiply_v1"(%54, %70) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %72 = "vhlo.convert_v1"(%71) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %73 = "vhlo.slice_v1"(%53) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 7, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>
    %74 = "vhlo.negate_v1"(%73) : (!vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>
    %75 = "vhlo.slice_v1"(%53) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 7, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>
    %76 = "vhlo.concatenate_v1"(%74, %75) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %77 = "vhlo.convert_v1"(%76) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %78 = "vhlo.sine_v2"(%64) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %79 = "vhlo.convert_v1"(%78) : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>
    %80 = "vhlo.reshape_v1"(%79) : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>
    %81 = "vhlo.convert_v1"(%80) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>
    %82 = "vhlo.reshape_v1"(%81) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %83 = "vhlo.broadcast_in_dim_v1"(%82) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %84 = "vhlo.multiply_v1"(%77, %83) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %85 = "vhlo.convert_v1"(%84) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %86 = "vhlo.add_v1"(%72, %85) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %87 = "vhlo.reshape_v1"(%86) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>
    %88 = "vhlo.custom_call_v1"(%arg16) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_2">}>} : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>
    %89 = "vhlo.compare_v1"(%60, %6) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 LT>}> : (!vhlo.tensor_v1<7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.bool_v1>
    %90 = "vhlo.broadcast_in_dim_v1"(%arg10) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %91 = "vhlo.add_v1"(%60, %90) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %92 = "vhlo.select_v1"(%89, %91, %60) : (!vhlo.tensor_v1<7x!vhlo.bool_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %93 = "vhlo.reshape_v1"(%92) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x1x!vhlo.i64_v1>
    %94 = "vhlo.reshape_v1"(%arg15) : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>
    %95 = "vhlo.custom_call_v1"(%94) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>
    %96 = "vhlo.reshape_v1"(%95) : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>
    %97 = "vhlo.transpose_v1"(%96) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,1024]{0,1}">} : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>
    %98 = "vhlo.dot_general_v2"(%46, %97) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>
    %99 = "vhlo.reshape_v1"(%98) : (!vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>
    %100 = "vhlo.transpose_v1"(%99) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %101 = "vhlo.convert_v1"(%100) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %102 = "vhlo.broadcast_in_dim_v1"(%69) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %103 = "vhlo.multiply_v1"(%101, %102) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %104 = "vhlo.convert_v1"(%103) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %105 = "vhlo.slice_v1"(%100) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 7, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %106 = "vhlo.negate_v1"(%105) : (!vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %107 = "vhlo.slice_v1"(%100) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 7, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %108 = "vhlo.concatenate_v1"(%106, %107) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %109 = "vhlo.convert_v1"(%108) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %110 = "vhlo.broadcast_in_dim_v1"(%82) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %111 = "vhlo.multiply_v1"(%109, %110) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %112 = "vhlo.convert_v1"(%111) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %113 = "vhlo.add_v1"(%104, %112) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %114 = "vhlo.scatter_v2"(%88, %93, %113) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg22: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg22) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>
    %115 = "vhlo.broadcast_in_dim_v1"(%114) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x1024x128x!vhlo.bf16_v1>
    %116 = "vhlo.reshape_v1"(%115) : (!vhlo.tensor_v1<1x8x3x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1024x128x!vhlo.bf16_v1>
    %117 = "vhlo.transpose_v1"(%116) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 3, 1, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,24,128,1024]{2,3,1,0}">} : (!vhlo.tensor_v1<1x24x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x128x1024x!vhlo.bf16_v1>
    %118 = "vhlo.reshape_v1"(%117) : (!vhlo.tensor_v1<1x24x128x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x128x1024x!vhlo.bf16_v1>
    %119 = "vhlo.dot_general_v2"(%87, %118) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x128x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x1024x!vhlo.bf16_v1>
    %120 = "vhlo.reshape_v1"(%119) : (!vhlo.tensor_v1<24x7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x1024x!vhlo.bf16_v1>
    %121 = "vhlo.convert_v1"(%120) : (!vhlo.tensor_v1<1x24x7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>
    %122 = "vhlo.broadcast_in_dim_v1"(%arg13) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>
    %123 = "vhlo.multiply_v1"(%121, %122) : (!vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>
    %124 = "vhlo.convert_v1"(%123) : (!vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x1024x!vhlo.bf16_v1>
    %125 = "vhlo.broadcast_in_dim_v1"(%1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1024x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.i64_v1>
    %126 = "vhlo.broadcast_in_dim_v1"(%2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.i64_v1>
    %127 = "vhlo.subtract_v1"(%125, %126) : (!vhlo.tensor_v1<7x1024x!vhlo.i64_v1>, !vhlo.tensor_v1<7x1024x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.i64_v1>
    %128 = "vhlo.compare_v1"(%127, %10) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GE>}> : (!vhlo.tensor_v1<7x1024x!vhlo.i64_v1>, !vhlo.tensor_v1<7x1024x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bool_v1>
    %129 = "vhlo.broadcast_in_dim_v1"(%arg12) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>
    %130 = "vhlo.select_v1"(%128, %129, %9) : (!vhlo.tensor_v1<7x1024x!vhlo.bool_v1>, !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>
    %131 = "vhlo.convert_v1"(%130) : (!vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.f32_v1>
    %132 = "vhlo.broadcast_in_dim_v1"(%60) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.i64_v1>
    %133 = "vhlo.compare_v1"(%125, %132) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<7x1024x!vhlo.i64_v1>, !vhlo.tensor_v1<7x1024x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bool_v1>
    %134 = "vhlo.convert_v1"(%133) : (!vhlo.tensor_v1<7x1024x!vhlo.bool_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.f32_v1>
    %135 = "vhlo.multiply_v1"(%131, %134) : (!vhlo.tensor_v1<7x1024x!vhlo.f32_v1>, !vhlo.tensor_v1<7x1024x!vhlo.f32_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.f32_v1>
    %136 = "vhlo.convert_v1"(%135) : (!vhlo.tensor_v1<7x1024x!vhlo.f32_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>
    %137 = "vhlo.reshape_v1"(%136) : (!vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x1024x!vhlo.bf16_v1>
    %138 = "vhlo.broadcast_in_dim_v1"(%137) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x1024x!vhlo.bf16_v1>
    %139 = "vhlo.add_v1"(%124, %138) : (!vhlo.tensor_v1<1x24x7x1024x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x1024x!vhlo.bf16_v1>
    %140 = "vhlo.convert_v1"(%139) : (!vhlo.tensor_v1<1x24x7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>
    %141 = "vhlo.reduce_v1"(%140, %3) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %242 = "vhlo.maximum_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%242) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>
    %142 = "vhlo.broadcast_in_dim_v1"(%141) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>
    %143 = "vhlo.subtract_v1"(%140, %142) : (!vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>
    %144 = "vhlo.exponential_v2"(%143) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>
    %145 = "vhlo.reduce_v1"(%144, %0) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %242 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%242) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>
    %146 = "vhlo.broadcast_in_dim_v1"(%145) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>
    %147 = "vhlo.divide_v1"(%144, %146) : (!vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>
    %148 = "vhlo.convert_v1"(%147) : (!vhlo.tensor_v1<1x24x7x1024x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x1024x!vhlo.bf16_v1>
    %149 = "vhlo.reshape_v1"(%148) : (!vhlo.tensor_v1<1x24x7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x1024x!vhlo.bf16_v1>
    %150 = "vhlo.custom_call_v1"(%arg11) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_3">}>} : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>
    %151 = "vhlo.reshape_v1"(%arg5) : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>
    %152 = "vhlo.custom_call_v1"(%151) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>
    %153 = "vhlo.reshape_v1"(%152) : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>
    %154 = "vhlo.transpose_v1"(%153) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,1024]{0,1}">} : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>
    %155 = "vhlo.dot_general_v2"(%46, %154) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>
    %156 = "vhlo.reshape_v1"(%155) : (!vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>
    %157 = "vhlo.transpose_v1"(%156) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %158 = "vhlo.scatter_v2"(%150, %93, %157) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg22: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg22) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>
    %159 = "vhlo.broadcast_in_dim_v1"(%158) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x1024x128x!vhlo.bf16_v1>
    %160 = "vhlo.reshape_v1"(%159) : (!vhlo.tensor_v1<1x8x3x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x1024x128x!vhlo.bf16_v1>
    %161 = "vhlo.dot_general_v2"(%149, %160) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x7x1024x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>
    %162 = "vhlo.reshape_v1"(%161) : (!vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %163 = "vhlo.transpose_v1"(%162) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,7,24,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>
    %164 = "vhlo.reshape_v1"(%163) : (!vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %165 = "vhlo.reshape_v1"(%arg4) : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>
    %166 = "vhlo.custom_call_v1"(%165) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_o_proj_weight">}>} : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>
    %167 = "vhlo.reshape_v1"(%166) : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %168 = "vhlo.transpose_v1"(%167) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,3072]{0,1}">} : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %169 = "vhlo.dot_general_v2"(%164, %168) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %170 = "vhlo.reshape_v1"(%169) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %171 = "vhlo.add_v1"(%25, %170) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %172 = "vhlo.reshape_v1"(%arg18) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %173 = "vhlo.custom_call_v1"(%172) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___post_attention_layernorm_weight">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %174 = "vhlo.reshape_v1"(%173) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1>
    %175 = "vhlo.convert_v1"(%174) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %176 = "vhlo.broadcast_in_dim_v1"(%175) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %177 = "vhlo.convert_v1"(%171) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %178 = "vhlo.power_v1"(%177, %11) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %179 = "vhlo.reduce_v1"(%178, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %242 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%242) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %180 = "vhlo.multiply_v1"(%179, %5) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %181 = "vhlo.reshape_v1"(%180) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %182 = "vhlo.add_v1"(%181, %36) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %183 = "vhlo.rsqrt_v2"(%182) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %184 = "vhlo.reshape_v1"(%183) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %185 = "vhlo.broadcast_in_dim_v1"(%184) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %186 = "vhlo.multiply_v1"(%177, %185) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %187 = "vhlo.convert_v1"(%186) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %188 = "vhlo.convert_v1"(%187) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %189 = "vhlo.multiply_v1"(%176, %188) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %190 = "vhlo.convert_v1"(%189) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %191 = "vhlo.reshape_v1"(%190) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %192 = "vhlo.reshape_v1"(%arg19) : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>
    %193 = "vhlo.custom_call_v1"(%192) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_gate_proj_weight">}>} : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>
    %194 = "vhlo.reshape_v1"(%193) : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>
    %195 = "vhlo.transpose_v1"(%194) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,8192]{0,1}">} : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>
    %196 = "vhlo.dot_general_v2"(%191, %195) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %197 = "vhlo.reshape_v1"(%196) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %198 = "vhlo.convert_v1"(%197) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %199 = "vhlo.logistic_v2"(%197) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %200 = "vhlo.convert_v1"(%199) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %201 = "vhlo.multiply_v1"(%198, %200) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %202 = "vhlo.convert_v1"(%201) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %203 = "vhlo.convert_v1"(%202) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %204 = "vhlo.reshape_v1"(%arg3) : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>
    %205 = "vhlo.custom_call_v1"(%204) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_up_proj_weight">}>} : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>
    %206 = "vhlo.reshape_v1"(%205) : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>
    %207 = "vhlo.transpose_v1"(%206) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,8192]{0,1}">} : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>
    %208 = "vhlo.dot_general_v2"(%191, %207) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %209 = "vhlo.reshape_v1"(%208) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %210 = "vhlo.convert_v1"(%209) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %211 = "vhlo.multiply_v1"(%203, %210) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %212 = "vhlo.convert_v1"(%211) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %213 = "vhlo.reshape_v1"(%212) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %214 = "vhlo.reshape_v1"(%arg2) : (!vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1>
    %215 = "vhlo.custom_call_v1"(%214) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_down_proj_weight">}>} : (!vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1>
    %216 = "vhlo.reshape_v1"(%215) : (!vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>
    %217 = "vhlo.transpose_v1"(%216) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,3072]{0,1}">} : (!vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>
    %218 = "vhlo.dot_general_v2"(%213, %217) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %219 = "vhlo.reshape_v1"(%218) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %220 = "vhlo.add_v1"(%171, %219) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %221 = "vhlo.convert_v1"(%220) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %222 = "vhlo.power_v1"(%221, %11) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %223 = "vhlo.reduce_v1"(%222, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %242 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%242) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %224 = "vhlo.multiply_v1"(%223, %5) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %225 = "vhlo.reshape_v1"(%224) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %226 = "vhlo.add_v1"(%225, %36) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %227 = "vhlo.rsqrt_v2"(%226) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %228 = "vhlo.reshape_v1"(%227) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %229 = "vhlo.broadcast_in_dim_v1"(%228) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %230 = "vhlo.multiply_v1"(%221, %229) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %231 = "vhlo.convert_v1"(%230) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %232 = "vhlo.convert_v1"(%231) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %233 = "vhlo.multiply_v1"(%16, %232) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %234 = "vhlo.convert_v1"(%233) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %235 = "vhlo.reshape_v1"(%234) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %236 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>
    %237 = "vhlo.custom_call_v1"(%236) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___lm_head_weight">}>} : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>
    %238 = "vhlo.reshape_v1"(%237) : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>
    %239 = "vhlo.transpose_v1"(%238) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,128256]{0,1}">} : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x128256x!vhlo.bf16_v1>
    %240 = "vhlo.dot_general_v2"(%235, %239) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>
    %241 = "vhlo.reshape_v1"(%240) : (!vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>
    "vhlo.return_v1"(%240, %241) : (!vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}
2025-09-15 15:41:07.864 (  16.190s) [        F073E000]      module_builder.cc:185      1| Is using shardy? true
2025-09-15 15:41:07.884 (  16.211s) [        F073E000]      module_builder.cc:203      1| SHLO Module:
module @SyncTensorsGraph.448 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg1: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg2: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>}, %arg3: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg4: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>}, %arg5: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg6: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg7: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg8: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg9: tensor<7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg10: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg11: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}, %arg12: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg13: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg14: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg15: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg16: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}) -> (tensor<7x128256xbf16>, tensor<1x7x128256xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<"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tensor<1024xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_4 = stablehlo.constant dense<0> : tensor<7xi64>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x1024xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x1024xi64>
    %2 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %4 = stablehlo.custom_call @tt.mark_argument(%3) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_norm_weight"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %5 = stablehlo.convert %4 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %6 = stablehlo.reshape %5 : (tensor<1x1x3072xf32>) -> tensor<3072xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %8 = stablehlo.reshape %arg7 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %9 = stablehlo.custom_call @tt.mark_argument(%8) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_embed_tokens_weight"}} : (tensor<1x128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %10 = stablehlo.reshape %9 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %11 = stablehlo.reshape %arg6 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %12 = stablehlo.custom_call @tt.mark_argument(%11) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_0"}} : (tensor<1x1x7xi64>) -> tensor<1x1x7xi64>
    %13 = stablehlo.convert %12 : (tensor<1x1x7xi64>) -> tensor<1x1x7xui32>
    %14 = stablehlo.reshape %13 : (tensor<1x1x7xui32>) -> tensor<7xui32>
    %15 = "stablehlo.gather"(%10, %14) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %16 = stablehlo.reshape %15 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %17 = stablehlo.reshape %arg8 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %18 = stablehlo.custom_call @tt.mark_argument(%17) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %19 = stablehlo.convert %18 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x1x3072xf32>) -> tensor<3072xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.convert %16 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %23 = stablehlo.power %22, %2 : tensor<1x7x3072xf32>
    %24 = stablehlo.reduce(%23 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %25 = stablehlo.multiply %24, %cst_3 : tensor<1x7xf32>
    %26 = stablehlo.reshape %25 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %27 = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %28 = stablehlo.add %26, %27 : tensor<1x7x1xf32>
    %29 = stablehlo.rsqrt %28 : tensor<1x7x1xf32>
    %30 = stablehlo.reshape %29 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %31 = stablehlo.broadcast_in_dim %30, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %32 = stablehlo.multiply %22, %31 : tensor<1x7x3072xf32>
    %33 = stablehlo.convert %32 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %35 = stablehlo.multiply %21, %34 : tensor<1x7x3072xf32>
    %36 = stablehlo.convert %35 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %37 = stablehlo.reshape %36 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %38 = stablehlo.reshape %arg17 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %39 = stablehlo.custom_call @tt.mark_argument(%38) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}} : (tensor<1x3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %40 = stablehlo.reshape %39 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %41 = stablehlo.transpose %40, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %42 = stablehlo.dot_general %37, %41, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %43 = stablehlo.reshape %42 : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %44 = stablehlo.transpose %43, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %45 = stablehlo.convert %44 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %46 = stablehlo.reshape %arg14 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %47 = stablehlo.custom_call @tt.mark_argument(%46) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "l__self___model_rotary_emb_inv_freq"}} : (tensor<1x1x64xf32>) -> tensor<1x1x64xf32>
    %48 = stablehlo.reshape %47 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %49 = stablehlo.reshape %arg9 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %50 = stablehlo.custom_call @tt.mark_argument(%49) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_1"}} : (tensor<1x1x7xi64>) -> tensor<1x1x7xi64>
    %51 = stablehlo.reshape %50 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %52 = stablehlo.convert %50 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %53 = stablehlo.dot_general %48, %52, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %54 = stablehlo.transpose %53, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %55 = stablehlo.concatenate %54, %54, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %56 = stablehlo.cosine %55 : tensor<1x7x128xf32>
    %57 = stablehlo.convert %56 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %58 = stablehlo.convert %57 : (tensor<1x7x128xbf16>) -> tensor<1x7x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %60 = stablehlo.multiply %45, %59 : tensor<1x24x7x128xf32>
    %61 = stablehlo.convert %60 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %62 = stablehlo.slice %44 [0:1, 0:24, 0:7, 64:128] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %63 = stablehlo.negate %62 : tensor<1x24x7x64xbf16>
    %64 = stablehlo.slice %44 [0:1, 0:24, 0:7, 0:64] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %65 = stablehlo.concatenate %63, %64, dim = 3 : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %66 = stablehlo.convert %65 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %67 = stablehlo.sine %55 : tensor<1x7x128xf32>
    %68 = stablehlo.convert %67 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %69 = stablehlo.convert %68 : (tensor<1x7x128xbf16>) -> tensor<1x7x128xf32>
    %70 = stablehlo.broadcast_in_dim %69, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %71 = stablehlo.multiply %66, %70 : tensor<1x24x7x128xf32>
    %72 = stablehlo.convert %71 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %73 = stablehlo.add %61, %72 : tensor<1x24x7x128xbf16>
    %74 = stablehlo.reshape %73 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %75 = stablehlo.custom_call @tt.mark_argument(%arg16) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_2"}} : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %76 = stablehlo.compare  LT, %51, %c_4 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %77 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %78 = stablehlo.add %51, %77 : tensor<7xi64>
    %79 = stablehlo.select %76, %78, %51 : tensor<7xi1>, tensor<7xi64>
    %80 = stablehlo.reshape %79 : (tensor<7xi64>) -> tensor<7x1xi64>
    %81 = stablehlo.reshape %arg15 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %82 = stablehlo.custom_call @tt.mark_argument(%81) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}} : (tensor<1x1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %83 = stablehlo.reshape %82 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %84 = stablehlo.transpose %83, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %85 = stablehlo.dot_general %37, %84, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %86 = stablehlo.reshape %85 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %87 = stablehlo.transpose %86, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %88 = stablehlo.convert %87 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %89 = stablehlo.broadcast_in_dim %58, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %90 = stablehlo.multiply %88, %89 : tensor<1x8x7x128xf32>
    %91 = stablehlo.convert %90 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %92 = stablehlo.slice %87 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %93 = stablehlo.negate %92 : tensor<1x8x7x64xbf16>
    %94 = stablehlo.slice %87 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %95 = stablehlo.concatenate %93, %94, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %96 = stablehlo.convert %95 : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %97 = stablehlo.broadcast_in_dim %69, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %98 = stablehlo.multiply %96, %97 : tensor<1x8x7x128xf32>
    %99 = stablehlo.convert %98 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %100 = stablehlo.add %91, %99 : tensor<1x8x7x128xbf16>
    %101 = "stablehlo.scatter"(%75, %80, %100) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x1024x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %102 = stablehlo.broadcast_in_dim %101, dims = [0, 1, 3, 4] : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x3x1024x128xbf16>
    %103 = stablehlo.reshape %102 : (tensor<1x8x3x1024x128xbf16>) -> tensor<1x24x1024x128xbf16>
    %104 = stablehlo.transpose %103, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,1024]{2,3,1,0}"} : (tensor<1x24x1024x128xbf16>) -> tensor<1x24x128x1024xbf16>
    %105 = stablehlo.reshape %104 : (tensor<1x24x128x1024xbf16>) -> tensor<24x128x1024xbf16>
    %106 = stablehlo.dot_general %74, %105, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x1024xbf16>) -> tensor<24x7x1024xbf16>
    %107 = stablehlo.convert %106 : (tensor<24x7x1024xbf16>) -> tensor<24x7x1024xf32>
    %108 = stablehlo.reshape %107 : (tensor<24x7x1024xf32>) -> tensor<1x24x7x1024xf32>
    %109 = stablehlo.broadcast_in_dim %arg13, dims = [] : (tensor<f32>) -> tensor<1x24x7x1024xf32>
    %110 = stablehlo.multiply %108, %109 : tensor<1x24x7x1024xf32>
    %111 = stablehlo.convert %110 : (tensor<1x24x7x1024xf32>) -> tensor<1x24x7x1024xbf16>
    %112 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<1024xi64>) -> tensor<7x1024xi64>
    %113 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x1024xi64>
    %114 = stablehlo.subtract %112, %113 : tensor<7x1024xi64>
    %115 = stablehlo.compare  GE, %114, %1 : (tensor<7x1024xi64>, tensor<7x1024xi64>) -> tensor<7x1024xi1>
    %116 = stablehlo.broadcast_in_dim %arg12, dims = [] : (tensor<bf16>) -> tensor<7x1024xbf16>
    %117 = stablehlo.select %115, %116, %0 : tensor<7x1024xi1>, tensor<7x1024xbf16>
    %118 = stablehlo.convert %117 : (tensor<7x1024xbf16>) -> tensor<7x1024xf32>
    %119 = stablehlo.broadcast_in_dim %51, dims = [0] : (tensor<7xi64>) -> tensor<7x1024xi64>
    %120 = stablehlo.compare  GT, %112, %119 : (tensor<7x1024xi64>, tensor<7x1024xi64>) -> tensor<7x1024xi1>
    %121 = stablehlo.convert %120 : (tensor<7x1024xi1>) -> tensor<7x1024xf32>
    %122 = stablehlo.multiply %118, %121 : tensor<7x1024xf32>
    %123 = stablehlo.convert %122 : (tensor<7x1024xf32>) -> tensor<7x1024xbf16>
    %124 = stablehlo.reshape %123 : (tensor<7x1024xbf16>) -> tensor<1x7x1024xbf16>
    %125 = stablehlo.broadcast_in_dim %124, dims = [0, 2, 3] : (tensor<1x7x1024xbf16>) -> tensor<1x24x7x1024xbf16>
    %126 = stablehlo.add %111, %125 : tensor<1x24x7x1024xbf16>
    %127 = stablehlo.convert %126 : (tensor<1x24x7x1024xbf16>) -> tensor<1x24x7x1024xf32>
    %128 = stablehlo.reduce(%127 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x7x1024xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %129 = stablehlo.broadcast_in_dim %128, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x1024xf32>
    %130 = stablehlo.subtract %127, %129 : tensor<1x24x7x1024xf32>
    %131 = stablehlo.exponential %130 : tensor<1x24x7x1024xf32>
    %132 = stablehlo.reduce(%131 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x7x1024xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x1024xf32>
    %134 = stablehlo.divide %131, %133 : tensor<1x24x7x1024xf32>
    %135 = stablehlo.convert %134 : (tensor<1x24x7x1024xf32>) -> tensor<1x24x7x1024xbf16>
    %136 = stablehlo.reshape %135 : (tensor<1x24x7x1024xbf16>) -> tensor<24x7x1024xbf16>
    %137 = stablehlo.custom_call @tt.mark_argument(%arg11) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_3"}} : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %138 = stablehlo.reshape %arg5 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %139 = stablehlo.custom_call @tt.mark_argument(%138) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}} : (tensor<1x1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %140 = stablehlo.reshape %139 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %141 = stablehlo.transpose %140, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %142 = stablehlo.dot_general %37, %141, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %143 = stablehlo.reshape %142 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %144 = stablehlo.transpose %143, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %145 = "stablehlo.scatter"(%137, %80, %144) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x1024x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %146 = stablehlo.broadcast_in_dim %145, dims = [0, 1, 3, 4] : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x3x1024x128xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x8x3x1024x128xbf16>) -> tensor<24x1024x128xbf16>
    %148 = stablehlo.dot_general %136, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x1024xbf16>, tensor<24x1024x128xbf16>) -> tensor<24x7x128xbf16>
    %149 = stablehlo.reshape %148 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %152 = stablehlo.reshape %arg4 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %153 = stablehlo.custom_call @tt.mark_argument(%152) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}} : (tensor<1x3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %155 = stablehlo.transpose %154, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %156 = stablehlo.dot_general %151, %155, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %157 = stablehlo.reshape %156 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %158 = stablehlo.add %16, %157 : tensor<1x7x3072xbf16>
    %159 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %160 = stablehlo.custom_call @tt.mark_argument(%159) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %161 = stablehlo.convert %160 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %162 = stablehlo.reshape %161 : (tensor<1x1x3072xf32>) -> tensor<3072xf32>
    %163 = stablehlo.broadcast_in_dim %162, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %164 = stablehlo.convert %158 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %165 = stablehlo.power %164, %2 : tensor<1x7x3072xf32>
    %166 = stablehlo.reduce(%165 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %167 = stablehlo.multiply %166, %cst_3 : tensor<1x7xf32>
    %168 = stablehlo.reshape %167 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %169 = stablehlo.add %168, %27 : tensor<1x7x1xf32>
    %170 = stablehlo.rsqrt %169 : tensor<1x7x1xf32>
    %171 = stablehlo.reshape %170 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %172 = stablehlo.broadcast_in_dim %171, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %173 = stablehlo.multiply %164, %172 : tensor<1x7x3072xf32>
    %174 = stablehlo.convert %173 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %175 = stablehlo.convert %174 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %176 = stablehlo.multiply %163, %175 : tensor<1x7x3072xf32>
    %177 = stablehlo.convert %176 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %178 = stablehlo.reshape %177 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %179 = stablehlo.reshape %arg19 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %180 = stablehlo.custom_call @tt.mark_argument(%179) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}} : (tensor<1x8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %181 = stablehlo.reshape %180 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %182 = stablehlo.transpose %181, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %183 = stablehlo.dot_general %178, %182, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %184 = stablehlo.reshape %183 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %185 = stablehlo.convert %184 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %186 = stablehlo.logistic %184 : tensor<1x7x8192xbf16>
    %187 = stablehlo.convert %186 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %188 = stablehlo.multiply %185, %187 : tensor<1x7x8192xf32>
    %189 = stablehlo.convert %188 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %190 = stablehlo.convert %189 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %191 = stablehlo.reshape %arg3 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %192 = stablehlo.custom_call @tt.mark_argument(%191) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}} : (tensor<1x8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %193 = stablehlo.reshape %192 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %194 = stablehlo.transpose %193, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %195 = stablehlo.dot_general %178, %194, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %196 = stablehlo.convert %195 : (tensor<7x8192xbf16>) -> tensor<7x8192xf32>
    %197 = stablehlo.reshape %196 : (tensor<7x8192xf32>) -> tensor<1x7x8192xf32>
    %198 = stablehlo.multiply %190, %197 : tensor<1x7x8192xf32>
    %199 = stablehlo.convert %198 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %200 = stablehlo.reshape %199 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %201 = stablehlo.reshape %arg2 : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %202 = stablehlo.custom_call @tt.mark_argument(%201) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}} : (tensor<1x3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %203 = stablehlo.reshape %202 : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %204 = stablehlo.transpose %203, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %205 = stablehlo.dot_general %200, %204, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %206 = stablehlo.reshape %205 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %207 = stablehlo.add %158, %206 : tensor<1x7x3072xbf16>
    %208 = stablehlo.convert %207 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %209 = stablehlo.power %208, %2 : tensor<1x7x3072xf32>
    %210 = stablehlo.reduce(%209 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %211 = stablehlo.multiply %210, %cst_3 : tensor<1x7xf32>
    %212 = stablehlo.reshape %211 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %213 = stablehlo.add %212, %27 : tensor<1x7x1xf32>
    %214 = stablehlo.rsqrt %213 : tensor<1x7x1xf32>
    %215 = stablehlo.reshape %214 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %216 = stablehlo.broadcast_in_dim %215, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %217 = stablehlo.multiply %208, %216 : tensor<1x7x3072xf32>
    %218 = stablehlo.convert %217 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %219 = stablehlo.convert %218 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %220 = stablehlo.multiply %7, %219 : tensor<1x7x3072xf32>
    %221 = stablehlo.convert %220 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %222 = stablehlo.reshape %221 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %223 = stablehlo.reshape %arg0 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %224 = stablehlo.custom_call @tt.mark_argument(%223) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___lm_head_weight"}} : (tensor<1x128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %225 = stablehlo.reshape %224 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %226 = stablehlo.transpose %225, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %227 = stablehlo.dot_general %222, %226, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %228 = stablehlo.reshape %227 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %227, %228 : tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}
2025-09-15 15:41:07.899 (  16.225s) [        F073E000]      module_builder.cc:212      1| SHLO Module after frontend StableHLO pipeline:
module @SyncTensorsGraph.448 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___lm_head_weight"}, %arg1: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_0"}, %arg2: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg3: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg4: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg5: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg6: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"}, %arg7: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg8: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg9: tensor<7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_1"}, %arg10: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_1"}, %arg11: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_3"}, %arg12: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_2"}, %arg13: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_3"}, %arg14: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg15: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg16: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_2"}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<7x128256xbf16>, tensor<1x7x128256xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<"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tensor<1024xi64>
    %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %c_4 = stablehlo.constant dense<0> : tensor<7xi64>
    %c_5 = stablehlo.constant dense<1> : tensor<i64>
    %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x1024xbf16>
    %1 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x1024xi64>
    %2 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %3 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %4 = stablehlo.convert %3 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %5 = stablehlo.reshape %4 : (tensor<1x1x3072xf32>) -> tensor<3072xf32>
    %6 = stablehlo.broadcast_in_dim %5, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %7 = stablehlo.reshape %arg7 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %8 = stablehlo.reshape %7 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %9 = stablehlo.reshape %arg6 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %10 = stablehlo.convert %9 : (tensor<1x1x7xi64>) -> tensor<1x1x7xui32>
    %11 = stablehlo.reshape %10 : (tensor<1x1x7xui32>) -> tensor<7xui32>
    %12 = "stablehlo.gather"(%8, %11) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %13 = stablehlo.reshape %12 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %14 = stablehlo.reshape %arg8 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %15 = stablehlo.convert %14 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x1x3072xf32>) -> tensor<3072xf32>
    %17 = stablehlo.broadcast_in_dim %16, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %18 = stablehlo.convert %13 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %19 = stablehlo.power %18, %2 : tensor<1x7x3072xf32>
    %20 = stablehlo.reduce(%19 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %21 = stablehlo.multiply %20, %cst_3 : tensor<1x7xf32>
    %22 = stablehlo.reshape %21 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %23 = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %24 = stablehlo.add %22, %23 : tensor<1x7x1xf32>
    %25 = stablehlo.rsqrt %24 : tensor<1x7x1xf32>
    %26 = stablehlo.reshape %25 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %27 = stablehlo.broadcast_in_dim %26, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %28 = stablehlo.multiply %18, %27 : tensor<1x7x3072xf32>
    %29 = stablehlo.convert %28 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %30 = stablehlo.convert %29 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %31 = stablehlo.multiply %17, %30 : tensor<1x7x3072xf32>
    %32 = stablehlo.convert %31 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %33 = stablehlo.reshape %32 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %34 = stablehlo.reshape %arg17 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %35 = stablehlo.reshape %34 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %36 = stablehlo.transpose %35, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %37 = stablehlo.dot_general %33, %36, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %38 = stablehlo.reshape %37 : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %39 = stablehlo.transpose %38, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %40 = stablehlo.convert %39 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %41 = stablehlo.reshape %arg14 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %42 = stablehlo.reshape %41 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %43 = stablehlo.reshape %arg9 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %44 = stablehlo.reshape %43 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %45 = stablehlo.convert %43 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %46 = stablehlo.dot_general %42, %45, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %47 = stablehlo.transpose %46, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %48 = stablehlo.concatenate %47, %47, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %49 = stablehlo.cosine %48 : tensor<1x7x128xf32>
    %50 = stablehlo.convert %49 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %51 = stablehlo.convert %50 : (tensor<1x7x128xbf16>) -> tensor<1x7x128xf32>
    %52 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %53 = stablehlo.multiply %40, %52 : tensor<1x24x7x128xf32>
    %54 = stablehlo.convert %53 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %55 = stablehlo.slice %39 [0:1, 0:24, 0:7, 64:128] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %56 = stablehlo.negate %55 : tensor<1x24x7x64xbf16>
    %57 = stablehlo.slice %39 [0:1, 0:24, 0:7, 0:64] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %58 = stablehlo.concatenate %56, %57, dim = 3 : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %59 = stablehlo.convert %58 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %60 = stablehlo.sine %48 : tensor<1x7x128xf32>
    %61 = stablehlo.convert %60 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %62 = stablehlo.convert %61 : (tensor<1x7x128xbf16>) -> tensor<1x7x128xf32>
    %63 = stablehlo.broadcast_in_dim %62, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %64 = stablehlo.multiply %59, %63 : tensor<1x24x7x128xf32>
    %65 = stablehlo.convert %64 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %66 = stablehlo.add %54, %65 : tensor<1x24x7x128xbf16>
    %67 = stablehlo.reshape %66 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %68 = stablehlo.compare  LT, %44, %c_4 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %69 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %70 = stablehlo.add %44, %69 : tensor<7xi64>
    %71 = stablehlo.select %68, %70, %44 : tensor<7xi1>, tensor<7xi64>
    %72 = stablehlo.reshape %71 : (tensor<7xi64>) -> tensor<7x1xi64>
    %73 = stablehlo.reshape %arg15 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %74 = stablehlo.reshape %73 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %75 = stablehlo.transpose %74, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %76 = stablehlo.dot_general %33, %75, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %77 = stablehlo.reshape %76 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %78 = stablehlo.transpose %77, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %79 = stablehlo.convert %78 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %80 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %81 = stablehlo.multiply %79, %80 : tensor<1x8x7x128xf32>
    %82 = stablehlo.convert %81 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %83 = stablehlo.slice %78 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %84 = stablehlo.negate %83 : tensor<1x8x7x64xbf16>
    %85 = stablehlo.slice %78 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %86 = stablehlo.concatenate %84, %85, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %87 = stablehlo.convert %86 : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %88 = stablehlo.broadcast_in_dim %62, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %89 = stablehlo.multiply %87, %88 : tensor<1x8x7x128xf32>
    %90 = stablehlo.convert %89 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %91 = stablehlo.add %82, %90 : tensor<1x8x7x128xbf16>
    %92 = "stablehlo.scatter"(%arg16, %72, %91) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x1024x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %93 = stablehlo.broadcast_in_dim %92, dims = [0, 1, 3, 4] : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x3x1024x128xbf16>
    %94 = stablehlo.reshape %93 : (tensor<1x8x3x1024x128xbf16>) -> tensor<1x24x1024x128xbf16>
    %95 = stablehlo.transpose %94, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,1024]{2,3,1,0}"} : (tensor<1x24x1024x128xbf16>) -> tensor<1x24x128x1024xbf16>
    %96 = stablehlo.reshape %95 : (tensor<1x24x128x1024xbf16>) -> tensor<24x128x1024xbf16>
    %97 = stablehlo.dot_general %67, %96, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x1024xbf16>) -> tensor<24x7x1024xbf16>
    %98 = stablehlo.convert %97 : (tensor<24x7x1024xbf16>) -> tensor<24x7x1024xf32>
    %99 = stablehlo.reshape %98 : (tensor<24x7x1024xf32>) -> tensor<1x24x7x1024xf32>
    %100 = stablehlo.broadcast_in_dim %arg13, dims = [] : (tensor<f32>) -> tensor<1x24x7x1024xf32>
    %101 = stablehlo.multiply %99, %100 : tensor<1x24x7x1024xf32>
    %102 = stablehlo.convert %101 : (tensor<1x24x7x1024xf32>) -> tensor<1x24x7x1024xbf16>
    %103 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<1024xi64>) -> tensor<7x1024xi64>
    %104 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x1024xi64>
    %105 = stablehlo.subtract %103, %104 : tensor<7x1024xi64>
    %106 = stablehlo.compare  GE, %105, %1 : (tensor<7x1024xi64>, tensor<7x1024xi64>) -> tensor<7x1024xi1>
    %107 = stablehlo.broadcast_in_dim %arg12, dims = [] : (tensor<bf16>) -> tensor<7x1024xbf16>
    %108 = stablehlo.select %106, %107, %0 : tensor<7x1024xi1>, tensor<7x1024xbf16>
    %109 = stablehlo.convert %108 : (tensor<7x1024xbf16>) -> tensor<7x1024xf32>
    %110 = stablehlo.broadcast_in_dim %44, dims = [0] : (tensor<7xi64>) -> tensor<7x1024xi64>
    %111 = stablehlo.compare  GT, %103, %110 : (tensor<7x1024xi64>, tensor<7x1024xi64>) -> tensor<7x1024xi1>
    %112 = stablehlo.convert %111 : (tensor<7x1024xi1>) -> tensor<7x1024xf32>
    %113 = stablehlo.multiply %109, %112 : tensor<7x1024xf32>
    %114 = stablehlo.convert %113 : (tensor<7x1024xf32>) -> tensor<7x1024xbf16>
    %115 = stablehlo.reshape %114 : (tensor<7x1024xbf16>) -> tensor<1x7x1024xbf16>
    %116 = stablehlo.broadcast_in_dim %115, dims = [0, 2, 3] : (tensor<1x7x1024xbf16>) -> tensor<1x24x7x1024xbf16>
    %117 = stablehlo.add %102, %116 : tensor<1x24x7x1024xbf16>
    %118 = stablehlo.convert %117 : (tensor<1x24x7x1024xbf16>) -> tensor<1x24x7x1024xf32>
    %119 = stablehlo.reduce(%118 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x7x1024xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %120 = stablehlo.broadcast_in_dim %119, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x1024xf32>
    %121 = stablehlo.subtract %118, %120 : tensor<1x24x7x1024xf32>
    %122 = stablehlo.exponential %121 : tensor<1x24x7x1024xf32>
    %123 = stablehlo.reduce(%122 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x7x1024xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %124 = stablehlo.broadcast_in_dim %123, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x1024xf32>
    %125 = stablehlo.divide %122, %124 : tensor<1x24x7x1024xf32>
    %126 = stablehlo.convert %125 : (tensor<1x24x7x1024xf32>) -> tensor<1x24x7x1024xbf16>
    %127 = stablehlo.reshape %126 : (tensor<1x24x7x1024xbf16>) -> tensor<24x7x1024xbf16>
    %128 = stablehlo.reshape %arg5 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %129 = stablehlo.reshape %128 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %130 = stablehlo.transpose %129, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %131 = stablehlo.dot_general %33, %130, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %132 = stablehlo.reshape %131 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %133 = stablehlo.transpose %132, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %134 = "stablehlo.scatter"(%arg11, %72, %133) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x1024x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 3, 4] : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x3x1024x128xbf16>
    %136 = stablehlo.reshape %135 : (tensor<1x8x3x1024x128xbf16>) -> tensor<24x1024x128xbf16>
    %137 = stablehlo.dot_general %127, %136, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x1024xbf16>, tensor<24x1024x128xbf16>) -> tensor<24x7x128xbf16>
    %138 = stablehlo.reshape %137 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %139 = stablehlo.transpose %138, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %140 = stablehlo.reshape %139 : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %141 = stablehlo.reshape %arg4 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %142 = stablehlo.reshape %141 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %143 = stablehlo.transpose %142, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %144 = stablehlo.dot_general %140, %143, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %145 = stablehlo.reshape %144 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %146 = stablehlo.add %13, %145 : tensor<1x7x3072xbf16>
    %147 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %148 = stablehlo.convert %147 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %149 = stablehlo.reshape %148 : (tensor<1x1x3072xf32>) -> tensor<3072xf32>
    %150 = stablehlo.broadcast_in_dim %149, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %151 = stablehlo.convert %146 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %152 = stablehlo.power %151, %2 : tensor<1x7x3072xf32>
    %153 = stablehlo.reduce(%152 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %154 = stablehlo.multiply %153, %cst_3 : tensor<1x7xf32>
    %155 = stablehlo.reshape %154 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %156 = stablehlo.add %155, %23 : tensor<1x7x1xf32>
    %157 = stablehlo.rsqrt %156 : tensor<1x7x1xf32>
    %158 = stablehlo.reshape %157 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %159 = stablehlo.broadcast_in_dim %158, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %160 = stablehlo.multiply %151, %159 : tensor<1x7x3072xf32>
    %161 = stablehlo.convert %160 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %162 = stablehlo.convert %161 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %163 = stablehlo.multiply %150, %162 : tensor<1x7x3072xf32>
    %164 = stablehlo.convert %163 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %165 = stablehlo.reshape %164 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %166 = stablehlo.reshape %arg19 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %167 = stablehlo.reshape %166 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %168 = stablehlo.transpose %167, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %169 = stablehlo.dot_general %165, %168, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %170 = stablehlo.reshape %169 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %171 = stablehlo.convert %170 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %172 = stablehlo.logistic %170 : tensor<1x7x8192xbf16>
    %173 = stablehlo.convert %172 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %174 = stablehlo.multiply %171, %173 : tensor<1x7x8192xf32>
    %175 = stablehlo.convert %174 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %176 = stablehlo.convert %175 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %177 = stablehlo.reshape %arg3 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %178 = stablehlo.reshape %177 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %179 = stablehlo.transpose %178, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %180 = stablehlo.dot_general %165, %179, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %181 = stablehlo.convert %180 : (tensor<7x8192xbf16>) -> tensor<7x8192xf32>
    %182 = stablehlo.reshape %181 : (tensor<7x8192xf32>) -> tensor<1x7x8192xf32>
    %183 = stablehlo.multiply %176, %182 : tensor<1x7x8192xf32>
    %184 = stablehlo.convert %183 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %185 = stablehlo.reshape %184 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %186 = stablehlo.reshape %arg2 : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %187 = stablehlo.reshape %186 : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %188 = stablehlo.transpose %187, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %189 = stablehlo.dot_general %185, %188, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %190 = stablehlo.reshape %189 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %191 = stablehlo.add %146, %190 : tensor<1x7x3072xbf16>
    %192 = stablehlo.convert %191 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %193 = stablehlo.power %192, %2 : tensor<1x7x3072xf32>
    %194 = stablehlo.reduce(%193 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %195 = stablehlo.multiply %194, %cst_3 : tensor<1x7xf32>
    %196 = stablehlo.reshape %195 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %197 = stablehlo.add %196, %23 : tensor<1x7x1xf32>
    %198 = stablehlo.rsqrt %197 : tensor<1x7x1xf32>
    %199 = stablehlo.reshape %198 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %200 = stablehlo.broadcast_in_dim %199, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %201 = stablehlo.multiply %192, %200 : tensor<1x7x3072xf32>
    %202 = stablehlo.convert %201 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %203 = stablehlo.convert %202 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %204 = stablehlo.multiply %6, %203 : tensor<1x7x3072xf32>
    %205 = stablehlo.convert %204 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %206 = stablehlo.reshape %205 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %207 = stablehlo.reshape %arg0 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %208 = stablehlo.reshape %207 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %209 = stablehlo.transpose %208, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %210 = stablehlo.dot_general %206, %209, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %211 = stablehlo.reshape %210 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %210, %211 : tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}
2025-09-15 15:41:08.037 (  16.363s) [        F073E000]      module_builder.cc:486      1| SHLO Module after compiler StableHLO pipeline:
module @SyncTensorsGraph.448 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<128256x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg1: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg2: tensor<3072x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg3: tensor<8192x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg4: tensor<3072x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg5: tensor<1024x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg6: tensor<1x7xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg7: tensor<128256x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg8: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg9: tensor<7xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg10: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg11: tensor<1x8x1024x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg12: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg13: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg14: tensor<64xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg15: tensor<1024x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg16: tensor<1x8x1024x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg17: tensor<3072x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:2 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %arg16, %arg17, %arg18, %arg19, %arg20) in_shardings=[<@mesh, [{}, {}]>, <@mesh, []>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, []>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, []>, <@mesh, []>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>] out_shardings=[<@mesh, [{}, {}]>, <@mesh, [{}, {}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg21: tensor<128256x3072xbf16>, %arg22: tensor<f32>, %arg23: tensor<3072x4096xbf16>, %arg24: tensor<4096x3072xbf16>, %arg25: tensor<3072x1536xbf16>, %arg26: tensor<512x3072xbf16>, %arg27: tensor<1x7xi64>, %arg28: tensor<128256x3072xbf16>, %arg29: tensor<3072xbf16>, %arg30: tensor<7xi64>, %arg31: tensor<i64>, %arg32: tensor<1x4x1024x128xbf16>, %arg33: tensor<bf16>, %arg34: tensor<f32>, %arg35: tensor<64xf32>, %arg36: tensor<512x3072xbf16>, %arg37: tensor<1x4x1024x128xbf16>, %arg38: tensor<1536x3072xbf16>, %arg39: tensor<3072xbf16>, %arg40: tensor<4096x3072xbf16>, %arg41: tensor<3072xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
      %c = stablehlo.constant dense<"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tensor<1024xi64>
      %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
      %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
      %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %cst_3 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
      %c_4 = stablehlo.constant dense<0> : tensor<7xi64>
      %c_5 = stablehlo.constant dense<1> : tensor<i64>
      %cst_6 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<7x1024xbf16>
      %2 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<7x1024xi64>
      %3 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
      %4 = stablehlo.reshape %arg41 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
      %5 = stablehlo.convert %4 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %6 = stablehlo.reshape %5 : (tensor<1x1x3072xf32>) -> tensor<3072xf32>
      %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
      %8 = stablehlo.reshape %arg28 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
      %9 = stablehlo.reshape %8 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
      %10 = stablehlo.reshape %arg27 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
      %11 = stablehlo.convert %10 : (tensor<1x1x7xi64>) -> tensor<1x1x7xui32>
      %12 = stablehlo.reshape %11 : (tensor<1x1x7xui32>) -> tensor<7xui32>
      %13 = "stablehlo.gather"(%9, %12) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
      %14 = stablehlo.reshape %13 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
      %15 = stablehlo.reshape %arg29 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
      %16 = stablehlo.convert %15 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %17 = stablehlo.reshape %16 : (tensor<1x1x3072xf32>) -> tensor<3072xf32>
      %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
      %19 = stablehlo.convert %14 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
      %20 = stablehlo.power %19, %3 : tensor<1x7x3072xf32>
      %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
      %22 = stablehlo.multiply %21, %cst_3 : tensor<1x7xf32>
      %23 = stablehlo.reshape %22 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
      %24 = stablehlo.broadcast_in_dim %arg22, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
      %25 = stablehlo.add %23, %24 : tensor<1x7x1xf32>
      %26 = stablehlo.rsqrt %25 : tensor<1x7x1xf32>
      %27 = stablehlo.reshape %26 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
      %28 = stablehlo.broadcast_in_dim %27, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
      %29 = stablehlo.multiply %19, %28 : tensor<1x7x3072xf32>
      %30 = stablehlo.convert %29 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
      %31 = stablehlo.convert %30 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
      %32 = stablehlo.multiply %18, %31 : tensor<1x7x3072xf32>
      %33 = stablehlo.convert %32 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
      %34 = stablehlo.reshape %33 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
      %35 = stablehlo.reshape %arg38 : (tensor<1536x3072xbf16>) -> tensor<1x1536x3072xbf16>
      %36 = stablehlo.reshape %35 : (tensor<1x1536x3072xbf16>) -> tensor<1536x3072xbf16>
      %37 = stablehlo.transpose %36, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<1536x3072xbf16>) -> tensor<3072x1536xbf16>
      %38 = stablehlo.dot_general %34, %37, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1536xbf16>) -> tensor<7x1536xbf16>
      %39 = stablehlo.reshape %38 : (tensor<7x1536xbf16>) -> tensor<1x7x12x128xbf16>
      %40 = stablehlo.transpose %39, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x12x128xbf16>) -> tensor<1x12x7x128xbf16>
      %41 = stablehlo.convert %40 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x12x7x128xbf16>) -> tensor<1x12x7x128xf32>
      %42 = stablehlo.reshape %arg35 : (tensor<64xf32>) -> tensor<1x1x64xf32>
      %43 = stablehlo.reshape %42 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
      %44 = stablehlo.reshape %arg30 : (tensor<7xi64>) -> tensor<1x1x7xi64>
      %45 = stablehlo.reshape %44 : (tensor<1x1x7xi64>) -> tensor<7xi64>
      %46 = stablehlo.convert %44 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
      %47 = stablehlo.dot_general %43, %46, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
      %48 = stablehlo.transpose %47, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
      %49 = stablehlo.concatenate %48, %48, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
      %50 = stablehlo.cosine %49 : tensor<1x7x128xf32>
      %51 = stablehlo.convert %50 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
      %52 = stablehlo.convert %51 : (tensor<1x7x128xbf16>) -> tensor<1x7x128xf32>
      %53 = stablehlo.broadcast_in_dim %52, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x12x7x128xf32>
      %54 = stablehlo.multiply %41, %53 : tensor<1x12x7x128xf32>
      %55 = stablehlo.convert %54 : (tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xbf16>
      %56 = stablehlo.slice %40 [0:1, 0:12, 0:7, 64:128] : (tensor<1x12x7x128xbf16>) -> tensor<1x12x7x64xbf16>
      %57 = stablehlo.negate %56 : tensor<1x12x7x64xbf16>
      %58 = stablehlo.slice %40 [0:1, 0:12, 0:7, 0:64] : (tensor<1x12x7x128xbf16>) -> tensor<1x12x7x64xbf16>
      %59 = stablehlo.concatenate %57, %58, dim = 3 : (tensor<1x12x7x64xbf16>, tensor<1x12x7x64xbf16>) -> tensor<1x12x7x128xbf16>
      %60 = stablehlo.convert %59 : (tensor<1x12x7x128xbf16>) -> tensor<1x12x7x128xf32>
      %61 = stablehlo.sine %49 : tensor<1x7x128xf32>
      %62 = stablehlo.convert %61 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
      %63 = stablehlo.convert %62 : (tensor<1x7x128xbf16>) -> tensor<1x7x128xf32>
      %64 = stablehlo.broadcast_in_dim %63, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x12x7x128xf32>
      %65 = stablehlo.multiply %60, %64 : tensor<1x12x7x128xf32>
      %66 = stablehlo.convert %65 : (tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xbf16>
      %67 = stablehlo.add %55, %66 : tensor<1x12x7x128xbf16>
      %68 = stablehlo.reshape %67 : (tensor<1x12x7x128xbf16>) -> tensor<12x7x128xbf16>
      %69 = stablehlo.compare  LT, %45, %c_4 : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
      %70 = stablehlo.broadcast_in_dim %arg31, dims = [] : (tensor<i64>) -> tensor<7xi64>
      %71 = stablehlo.add %45, %70 : tensor<7xi64>
      %72 = stablehlo.select %69, %71, %45 : tensor<7xi1>, tensor<7xi64>
      %73 = stablehlo.reshape %72 : (tensor<7xi64>) -> tensor<7x1xi64>
      %74 = stablehlo.reshape %arg36 : (tensor<512x3072xbf16>) -> tensor<1x512x3072xbf16>
      %75 = stablehlo.reshape %74 : (tensor<1x512x3072xbf16>) -> tensor<512x3072xbf16>
      %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<512x3072xbf16>) -> tensor<3072x512xbf16>
      %77 = stablehlo.dot_general %34, %76, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x512xbf16>) -> tensor<7x512xbf16>
      %78 = stablehlo.reshape %77 : (tensor<7x512xbf16>) -> tensor<1x7x4x128xbf16>
      %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x4x128xbf16>) -> tensor<1x4x7x128xbf16>
      %80 = stablehlo.convert %79 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x4x7x128xbf16>) -> tensor<1x4x7x128xf32>
      %81 = stablehlo.broadcast_in_dim %52, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x4x7x128xf32>
      %82 = stablehlo.multiply %80, %81 : tensor<1x4x7x128xf32>
      %83 = stablehlo.convert %82 : (tensor<1x4x7x128xf32>) -> tensor<1x4x7x128xbf16>
      %84 = stablehlo.slice %79 [0:1, 0:4, 0:7, 64:128] : (tensor<1x4x7x128xbf16>) -> tensor<1x4x7x64xbf16>
      %85 = stablehlo.negate %84 : tensor<1x4x7x64xbf16>
      %86 = stablehlo.slice %79 [0:1, 0:4, 0:7, 0:64] : (tensor<1x4x7x128xbf16>) -> tensor<1x4x7x64xbf16>
      %87 = stablehlo.concatenate %85, %86, dim = 3 : (tensor<1x4x7x64xbf16>, tensor<1x4x7x64xbf16>) -> tensor<1x4x7x128xbf16>
      %88 = stablehlo.convert %87 : (tensor<1x4x7x128xbf16>) -> tensor<1x4x7x128xf32>
      %89 = stablehlo.broadcast_in_dim %63, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x4x7x128xf32>
      %90 = stablehlo.multiply %88, %89 : tensor<1x4x7x128xf32>
      %91 = stablehlo.convert %90 : (tensor<1x4x7x128xf32>) -> tensor<1x4x7x128xbf16>
      %92 = stablehlo.add %83, %91 : tensor<1x4x7x128xbf16>
      %93 = "stablehlo.scatter"(%arg37, %73, %92) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg42: tensor<bf16>, %arg43: tensor<bf16>):
        stablehlo.return %arg43 : tensor<bf16>
      }) : (tensor<1x4x1024x128xbf16>, tensor<7x1xi64>, tensor<1x4x7x128xbf16>) -> tensor<1x4x1024x128xbf16>
      %94 = stablehlo.broadcast_in_dim %93, dims = [0, 1, 3, 4] : (tensor<1x4x1024x128xbf16>) -> tensor<1x4x3x1024x128xbf16>
      %95 = stablehlo.reshape %94 : (tensor<1x4x3x1024x128xbf16>) -> tensor<1x12x1024x128xbf16>
      %96 = stablehlo.transpose %95, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,1024]{2,3,1,0}"} : (tensor<1x12x1024x128xbf16>) -> tensor<1x12x128x1024xbf16>
      %97 = stablehlo.reshape %96 : (tensor<1x12x128x1024xbf16>) -> tensor<12x128x1024xbf16>
      %98 = stablehlo.dot_general %68, %97, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<12x7x128xbf16>, tensor<12x128x1024xbf16>) -> tensor<12x7x1024xbf16>
      %99 = stablehlo.convert %98 : (tensor<12x7x1024xbf16>) -> tensor<12x7x1024xf32>
      %100 = stablehlo.reshape %99 : (tensor<12x7x1024xf32>) -> tensor<1x12x7x1024xf32>
      %101 = stablehlo.broadcast_in_dim %arg34, dims = [] : (tensor<f32>) -> tensor<1x12x7x1024xf32>
      %102 = stablehlo.multiply %100, %101 : tensor<1x12x7x1024xf32>
      %103 = stablehlo.convert %102 : (tensor<1x12x7x1024xf32>) -> tensor<1x12x7x1024xbf16>
      %104 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<1024xi64>) -> tensor<7x1024xi64>
      %105 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<7xi64>) -> tensor<7x1024xi64>
      %106 = stablehlo.subtract %104, %105 : tensor<7x1024xi64>
      %107 = stablehlo.compare  GE, %106, %2 : (tensor<7x1024xi64>, tensor<7x1024xi64>) -> tensor<7x1024xi1>
      %108 = stablehlo.broadcast_in_dim %arg33, dims = [] : (tensor<bf16>) -> tensor<7x1024xbf16>
      %109 = stablehlo.select %107, %108, %1 : tensor<7x1024xi1>, tensor<7x1024xbf16>
      %110 = stablehlo.convert %109 : (tensor<7x1024xbf16>) -> tensor<7x1024xf32>
      %111 = stablehlo.broadcast_in_dim %45, dims = [0] : (tensor<7xi64>) -> tensor<7x1024xi64>
      %112 = stablehlo.compare  GT, %104, %111 : (tensor<7x1024xi64>, tensor<7x1024xi64>) -> tensor<7x1024xi1>
      %113 = stablehlo.convert %112 : (tensor<7x1024xi1>) -> tensor<7x1024xf32>
      %114 = stablehlo.multiply %110, %113 : tensor<7x1024xf32>
      %115 = stablehlo.convert %114 : (tensor<7x1024xf32>) -> tensor<7x1024xbf16>
      %116 = stablehlo.reshape %115 : (tensor<7x1024xbf16>) -> tensor<1x7x1024xbf16>
      %117 = stablehlo.broadcast_in_dim %116, dims = [0, 2, 3] : (tensor<1x7x1024xbf16>) -> tensor<1x12x7x1024xbf16>
      %118 = stablehlo.add %103, %117 : tensor<1x12x7x1024xbf16>
      %119 = stablehlo.convert %118 : (tensor<1x12x7x1024xbf16>) -> tensor<1x12x7x1024xf32>
      %120 = stablehlo.reduce(%119 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x12x7x1024xf32>, tensor<f32>) -> tensor<1x12x7xf32>
      %121 = stablehlo.broadcast_in_dim %120, dims = [0, 1, 2] : (tensor<1x12x7xf32>) -> tensor<1x12x7x1024xf32>
      %122 = stablehlo.subtract %119, %121 : tensor<1x12x7x1024xf32>
      %123 = stablehlo.exponential %122 : tensor<1x12x7x1024xf32>
      %124 = stablehlo.reduce(%123 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x12x7x1024xf32>, tensor<f32>) -> tensor<1x12x7xf32>
      %125 = stablehlo.broadcast_in_dim %124, dims = [0, 1, 2] : (tensor<1x12x7xf32>) -> tensor<1x12x7x1024xf32>
      %126 = stablehlo.divide %123, %125 : tensor<1x12x7x1024xf32>
      %127 = stablehlo.convert %126 : (tensor<1x12x7x1024xf32>) -> tensor<1x12x7x1024xbf16>
      %128 = stablehlo.reshape %127 : (tensor<1x12x7x1024xbf16>) -> tensor<12x7x1024xbf16>
      %129 = stablehlo.reshape %arg26 : (tensor<512x3072xbf16>) -> tensor<1x512x3072xbf16>
      %130 = stablehlo.reshape %129 : (tensor<1x512x3072xbf16>) -> tensor<512x3072xbf16>
      %131 = stablehlo.transpose %130, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<512x3072xbf16>) -> tensor<3072x512xbf16>
      %132 = stablehlo.dot_general %34, %131, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x512xbf16>) -> tensor<7x512xbf16>
      %133 = stablehlo.reshape %132 : (tensor<7x512xbf16>) -> tensor<1x7x4x128xbf16>
      %134 = stablehlo.transpose %133, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x4x128xbf16>) -> tensor<1x4x7x128xbf16>
      %135 = "stablehlo.scatter"(%arg32, %73, %134) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg42: tensor<bf16>, %arg43: tensor<bf16>):
        stablehlo.return %arg43 : tensor<bf16>
      }) : (tensor<1x4x1024x128xbf16>, tensor<7x1xi64>, tensor<1x4x7x128xbf16>) -> tensor<1x4x1024x128xbf16>
      %136 = stablehlo.broadcast_in_dim %135, dims = [0, 1, 3, 4] : (tensor<1x4x1024x128xbf16>) -> tensor<1x4x3x1024x128xbf16>
      %137 = stablehlo.reshape %136 : (tensor<1x4x3x1024x128xbf16>) -> tensor<12x1024x128xbf16>
      %138 = stablehlo.dot_general %128, %137, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<12x7x1024xbf16>, tensor<12x1024x128xbf16>) -> tensor<12x7x128xbf16>
      %139 = stablehlo.reshape %138 : (tensor<12x7x128xbf16>) -> tensor<1x12x7x128xbf16>
      %140 = stablehlo.transpose %139, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x12x7x128xbf16>) -> tensor<1x7x12x128xbf16>
      %141 = stablehlo.reshape %140 : (tensor<1x7x12x128xbf16>) -> tensor<7x1536xbf16>
      %142 = stablehlo.reshape %arg25 : (tensor<3072x1536xbf16>) -> tensor<1x3072x1536xbf16>
      %143 = stablehlo.reshape %142 : (tensor<1x3072x1536xbf16>) -> tensor<3072x1536xbf16>
      %144 = stablehlo.transpose %143, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x1536xbf16>) -> tensor<1536x3072xbf16>
      %145 = stablehlo.dot_general %141, %144, contracting_dims = [1] x [0] : (tensor<7x1536xbf16>, tensor<1536x3072xbf16>) -> tensor<7x3072xbf16>
      %146 = "stablehlo.all_reduce"(%145) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> ({
      ^bb0(%arg42: tensor<bf16>, %arg43: tensor<bf16>):
        %215 = stablehlo.add %arg42, %arg43 : tensor<bf16>
        stablehlo.return %215 : tensor<bf16>
      }) : (tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
      %147 = stablehlo.reshape %146 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
      %148 = stablehlo.add %14, %147 : tensor<1x7x3072xbf16>
      %149 = stablehlo.reshape %arg39 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
      %150 = stablehlo.convert %149 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %151 = stablehlo.reshape %150 : (tensor<1x1x3072xf32>) -> tensor<3072xf32>
      %152 = stablehlo.broadcast_in_dim %151, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
      %153 = stablehlo.convert %148 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
      %154 = stablehlo.power %153, %3 : tensor<1x7x3072xf32>
      %155 = stablehlo.reduce(%154 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
      %156 = stablehlo.multiply %155, %cst_3 : tensor<1x7xf32>
      %157 = stablehlo.reshape %156 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
      %158 = stablehlo.add %157, %24 : tensor<1x7x1xf32>
      %159 = stablehlo.rsqrt %158 : tensor<1x7x1xf32>
      %160 = stablehlo.reshape %159 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
      %161 = stablehlo.broadcast_in_dim %160, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
      %162 = stablehlo.multiply %153, %161 : tensor<1x7x3072xf32>
      %163 = stablehlo.convert %162 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
      %164 = stablehlo.convert %163 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
      %165 = stablehlo.multiply %152, %164 : tensor<1x7x3072xf32>
      %166 = stablehlo.convert %165 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
      %167 = stablehlo.reshape %166 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
      %168 = stablehlo.reshape %arg40 : (tensor<4096x3072xbf16>) -> tensor<1x4096x3072xbf16>
      %169 = stablehlo.reshape %168 : (tensor<1x4096x3072xbf16>) -> tensor<4096x3072xbf16>
      %170 = stablehlo.transpose %169, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<4096x3072xbf16>) -> tensor<3072x4096xbf16>
      %171 = stablehlo.dot_general %167, %170, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<7x4096xbf16>
      %172 = stablehlo.reshape %171 : (tensor<7x4096xbf16>) -> tensor<1x7x4096xbf16>
      %173 = stablehlo.convert %172 : (tensor<1x7x4096xbf16>) -> tensor<1x7x4096xf32>
      %174 = stablehlo.logistic %172 : tensor<1x7x4096xbf16>
      %175 = stablehlo.convert %174 : (tensor<1x7x4096xbf16>) -> tensor<1x7x4096xf32>
      %176 = stablehlo.multiply %173, %175 : tensor<1x7x4096xf32>
      %177 = stablehlo.convert %176 : (tensor<1x7x4096xf32>) -> tensor<1x7x4096xbf16>
      %178 = stablehlo.convert %177 : (tensor<1x7x4096xbf16>) -> tensor<1x7x4096xf32>
      %179 = stablehlo.reshape %arg24 : (tensor<4096x3072xbf16>) -> tensor<1x4096x3072xbf16>
      %180 = stablehlo.reshape %179 : (tensor<1x4096x3072xbf16>) -> tensor<4096x3072xbf16>
      %181 = stablehlo.transpose %180, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<4096x3072xbf16>) -> tensor<3072x4096xbf16>
      %182 = stablehlo.dot_general %167, %181, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<7x4096xbf16>
      %183 = stablehlo.convert %182 : (tensor<7x4096xbf16>) -> tensor<7x4096xf32>
      %184 = stablehlo.reshape %183 : (tensor<7x4096xf32>) -> tensor<1x7x4096xf32>
      %185 = stablehlo.multiply %178, %184 : tensor<1x7x4096xf32>
      %186 = stablehlo.convert %185 : (tensor<1x7x4096xf32>) -> tensor<1x7x4096xbf16>
      %187 = stablehlo.reshape %186 : (tensor<1x7x4096xbf16>) -> tensor<7x4096xbf16>
      %188 = stablehlo.reshape %arg23 : (tensor<3072x4096xbf16>) -> tensor<1x3072x4096xbf16>
      %189 = stablehlo.reshape %188 : (tensor<1x3072x4096xbf16>) -> tensor<3072x4096xbf16>
      %190 = stablehlo.transpose %189, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x4096xbf16>) -> tensor<4096x3072xbf16>
      %191 = stablehlo.dot_general %187, %190, contracting_dims = [1] x [0] : (tensor<7x4096xbf16>, tensor<4096x3072xbf16>) -> tensor<7x3072xbf16>
      %192 = "stablehlo.all_reduce"(%191) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> ({
      ^bb0(%arg42: tensor<bf16>, %arg43: tensor<bf16>):
        %215 = stablehlo.add %arg42, %arg43 : tensor<bf16>
        stablehlo.return %215 : tensor<bf16>
      }) : (tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
      %193 = stablehlo.reshape %192 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
      %194 = stablehlo.add %148, %193 : tensor<1x7x3072xbf16>
      %195 = stablehlo.convert %194 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
      %196 = stablehlo.power %195, %3 : tensor<1x7x3072xf32>
      %197 = stablehlo.reduce(%196 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
      %198 = stablehlo.multiply %197, %cst_3 : tensor<1x7xf32>
      %199 = stablehlo.reshape %198 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
      %200 = stablehlo.add %199, %24 : tensor<1x7x1xf32>
      %201 = stablehlo.rsqrt %200 : tensor<1x7x1xf32>
      %202 = stablehlo.reshape %201 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
      %203 = stablehlo.broadcast_in_dim %202, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
      %204 = stablehlo.multiply %195, %203 : tensor<1x7x3072xf32>
      %205 = stablehlo.convert %204 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
      %206 = stablehlo.convert %205 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
      %207 = stablehlo.multiply %7, %206 : tensor<1x7x3072xf32>
      %208 = stablehlo.convert %207 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
      %209 = stablehlo.reshape %208 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
      %210 = stablehlo.reshape %arg21 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
      %211 = stablehlo.reshape %210 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
      %212 = stablehlo.transpose %211, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
      %213 = stablehlo.dot_general %209, %212, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
      %214 = stablehlo.reshape %213 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
      sdy.return %213, %214 : tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
    } : (tensor<128256x3072xbf16>, tensor<f32>, tensor<3072x8192xbf16>, tensor<8192x3072xbf16>, tensor<3072x3072xbf16>, tensor<1024x3072xbf16>, tensor<1x7xi64>, tensor<128256x3072xbf16>, tensor<3072xbf16>, tensor<7xi64>, tensor<i64>, tensor<1x8x1024x128xbf16>, tensor<bf16>, tensor<f32>, tensor<64xf32>, tensor<1024x3072xbf16>, tensor<1x8x1024x128xbf16>, tensor<3072x3072xbf16>, tensor<3072xbf16>, tensor<8192x3072xbf16>, tensor<3072xbf16>) -> (tensor<7x128256xbf16>, tensor<1x7x128256xbf16>)
    return %0#0, %0#1 : tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}
2025-09-15 15:41:08.074 (  16.401s) [        F073E000]      module_builder.cc:510      1| TTIR Module:
module @SyncTensorsGraph.448 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  func.func @main(%arg0: tensor<128256x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg1: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg2: tensor<3072x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg3: tensor<8192x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg4: tensor<3072x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg5: tensor<1024x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg6: tensor<1x7xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg7: tensor<128256x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg8: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg9: tensor<7xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg10: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg11: tensor<1x8x1024x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg12: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg13: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg14: tensor<64xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg15: tensor<1024x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg16: tensor<1x8x1024x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg17: tensor<3072x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = ttir.empty() : tensor<128256x3072xbf16>
    %1 = "ttir.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16>, tensor<128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %2 = ttir.empty() : tensor<f32>
    %3 = "ttir.mesh_shard"(%arg1, %2) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %4 = ttir.empty() : tensor<3072x4096xbf16>
    %5 = "ttir.mesh_shard"(%arg2, %4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x8192xbf16>, tensor<3072x4096xbf16>) -> tensor<3072x4096xbf16>
    %6 = ttir.empty() : tensor<4096x3072xbf16>
    %7 = "ttir.mesh_shard"(%arg3, %6) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16>, tensor<4096x3072xbf16>) -> tensor<4096x3072xbf16>
    %8 = ttir.empty() : tensor<3072x1536xbf16>
    %9 = "ttir.mesh_shard"(%arg4, %8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16>, tensor<3072x1536xbf16>) -> tensor<3072x1536xbf16>
    %10 = ttir.empty() : tensor<512x3072xbf16>
    %11 = "ttir.mesh_shard"(%arg5, %10) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16>, tensor<512x3072xbf16>) -> tensor<512x3072xbf16>
    %12 = ttir.empty() : tensor<1x7xi64>
    %13 = "ttir.mesh_shard"(%arg6, %12) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x7xi64>, tensor<1x7xi64>) -> tensor<1x7xi64>
    %14 = ttir.empty() : tensor<128256x3072xbf16>
    %15 = "ttir.mesh_shard"(%arg7, %14) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16>, tensor<128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = ttir.empty() : tensor<3072xbf16>
    %17 = "ttir.mesh_shard"(%arg8, %16) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16>, tensor<3072xbf16>) -> tensor<3072xbf16>
    %18 = ttir.empty() : tensor<7xi64>
    %19 = "ttir.mesh_shard"(%arg9, %18) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi64>
    %20 = ttir.empty() : tensor<i64>
    %21 = "ttir.mesh_shard"(%arg10, %20) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %22 = ttir.empty() : tensor<1x4x1024x128xbf16>
    %23 = "ttir.mesh_shard"(%arg11, %22) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16>, tensor<1x4x1024x128xbf16>) -> tensor<1x4x1024x128xbf16>
    %24 = ttir.empty() : tensor<bf16>
    %25 = "ttir.mesh_shard"(%arg12, %24) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
    %26 = ttir.empty() : tensor<f32>
    %27 = "ttir.mesh_shard"(%arg13, %26) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %28 = ttir.empty() : tensor<64xf32>
    %29 = "ttir.mesh_shard"(%arg14, %28) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32>, tensor<64xf32>) -> tensor<64xf32>
    %30 = ttir.empty() : tensor<512x3072xbf16>
    %31 = "ttir.mesh_shard"(%arg15, %30) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16>, tensor<512x3072xbf16>) -> tensor<512x3072xbf16>
    %32 = ttir.empty() : tensor<1x4x1024x128xbf16>
    %33 = "ttir.mesh_shard"(%arg16, %32) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16>, tensor<1x4x1024x128xbf16>) -> tensor<1x4x1024x128xbf16>
    %34 = ttir.empty() : tensor<1536x3072xbf16>
    %35 = "ttir.mesh_shard"(%arg17, %34) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16>, tensor<1536x3072xbf16>) -> tensor<1536x3072xbf16>
    %36 = ttir.empty() : tensor<3072xbf16>
    %37 = "ttir.mesh_shard"(%arg18, %36) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16>, tensor<3072xbf16>) -> tensor<3072xbf16>
    %38 = ttir.empty() : tensor<4096x3072xbf16>
    %39 = "ttir.mesh_shard"(%arg19, %38) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16>, tensor<4096x3072xbf16>) -> tensor<4096x3072xbf16>
    %40 = ttir.empty() : tensor<3072xbf16>
    %41 = "ttir.mesh_shard"(%arg20, %40) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16>, tensor<3072xbf16>) -> tensor<3072xbf16>
    %42 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<f32>}> : () -> tensor<f32>
    %43 = "ttir.constant"() <{value = dense<"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tensor<1024xi64>}> : () -> tensor<1024xi64>
    %44 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>}> : () -> tensor<7xi64>
    %45 = "ttir.constant"() <{value = dense<0xFF800000> : tensor<f32>}> : () -> tensor<f32>
    %46 = "ttir.constant"() <{value = dense<2.000000e+00> : tensor<f32>}> : () -> tensor<f32>
    %47 = "ttir.constant"() <{value = dense<3.25520843E-4> : tensor<1x7xf32>}> : () -> tensor<1x7xf32>
    %48 = "ttir.constant"() <{value = dense<0> : tensor<7xi64>}> : () -> tensor<7xi64>
    %49 = "ttir.constant"() <{value = dense<1> : tensor<i64>}> : () -> tensor<i64>
    %50 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<bf16>}> : () -> tensor<bf16>
    %51 = ttir.empty() : tensor<1x1xbf16>
    %52 = "ttir.reshape"(%50, %51) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %53 = ttir.empty() : tensor<7x1024xbf16>
    %54 = "ttir.broadcast"(%52, %53) <{broadcast_dimensions = array<i64: 7, 1024>}> : (tensor<1x1xbf16>, tensor<7x1024xbf16>) -> tensor<7x1024xbf16>
    %55 = ttir.empty() : tensor<1x1xi64>
    %56 = "ttir.reshape"(%49, %55) <{shape = [1 : i32, 1 : i32]}> : (tensor<i64>, tensor<1x1xi64>) -> tensor<1x1xi64>
    %57 = ttir.empty() : tensor<7x1024xi64>
    %58 = "ttir.broadcast"(%56, %57) <{broadcast_dimensions = array<i64: 7, 1024>}> : (tensor<1x1xi64>, tensor<7x1024xi64>) -> tensor<7x1024xi64>
    %59 = ttir.empty() : tensor<1x1x1xf32>
    %60 = "ttir.reshape"(%46, %59) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %61 = ttir.empty() : tensor<1x7x3072xf32>
    %62 = "ttir.broadcast"(%60, %61) <{broadcast_dimensions = array<i64: 1, 7, 3072>}> : (tensor<1x1x1xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %63 = ttir.empty() : tensor<1x1x3072xbf16>
    %64 = "ttir.reshape"(%41, %63) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %65 = ttir.empty() : tensor<1x1x3072xf32>
    %66 = "ttir.typecast"(%64, %65) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %67 = ttir.empty() : tensor<3072xf32>
    %68 = "ttir.reshape"(%66, %67) <{shape = [3072 : i32]}> : (tensor<1x1x3072xf32>, tensor<3072xf32>) -> tensor<3072xf32>
    %69 = ttir.empty() : tensor<1x1x3072xf32>
    %70 = "ttir.reshape"(%68, %69) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %71 = ttir.empty() : tensor<1x7x3072xf32>
    %72 = "ttir.broadcast"(%70, %71) <{broadcast_dimensions = array<i64: 1, 7, 1>}> : (tensor<1x1x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %73 = ttir.empty() : tensor<1x128256x3072xbf16>
    %74 = "ttir.reshape"(%15, %73) <{shape = [1 : i32, 128256 : i32, 3072 : i32]}> : (tensor<128256x3072xbf16>, tensor<1x128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %75 = ttir.empty() : tensor<128256x3072xbf16>
    %76 = "ttir.reshape"(%74, %75) <{shape = [128256 : i32, 3072 : i32]}> : (tensor<1x128256x3072xbf16>, tensor<128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %77 = ttir.empty() : tensor<1x1x7xi64>
    %78 = "ttir.reshape"(%13, %77) <{shape = [1 : i32, 1 : i32, 7 : i32]}> : (tensor<1x7xi64>, tensor<1x1x7xi64>) -> tensor<1x1x7xi64>
    %79 = ttir.empty() : tensor<1x1x7xui32>
    %80 = "ttir.typecast"(%78, %79) <{conservative_folding = false}> : (tensor<1x1x7xi64>, tensor<1x1x7xui32>) -> tensor<1x1x7xui32>
    %81 = ttir.empty() : tensor<7xui32>
    %82 = "ttir.reshape"(%80, %81) <{shape = [7 : i32]}> : (tensor<1x1x7xui32>, tensor<7xui32>) -> tensor<7xui32>
    %83 = ttir.empty() : tensor<7x3072xbf16>
    %84 = "ttir.gather"(%76, %82, %83) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 3072>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<128256x3072xbf16>, tensor<7xui32>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %85 = ttir.empty() : tensor<1x7x3072xbf16>
    %86 = "ttir.reshape"(%84, %85) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %87 = ttir.empty() : tensor<1x1x3072xbf16>
    %88 = "ttir.reshape"(%17, %87) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %89 = ttir.empty() : tensor<1x1x3072xf32>
    %90 = "ttir.typecast"(%88, %89) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %91 = ttir.empty() : tensor<3072xf32>
    %92 = "ttir.reshape"(%90, %91) <{shape = [3072 : i32]}> : (tensor<1x1x3072xf32>, tensor<3072xf32>) -> tensor<3072xf32>
    %93 = ttir.empty() : tensor<1x1x3072xf32>
    %94 = "ttir.reshape"(%92, %93) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %95 = ttir.empty() : tensor<1x7x3072xf32>
    %96 = "ttir.broadcast"(%94, %95) <{broadcast_dimensions = array<i64: 1, 7, 1>}> : (tensor<1x1x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %97 = ttir.empty() : tensor<1x7x3072xf32>
    %98 = "ttir.typecast"(%86, %97) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %99 = ttir.empty() : tensor<1x7x3072xf32>
    %100 = "ttir.pow"(%98, %62, %99) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %101 = ttir.empty() : tensor<1x7xf32>
    %102 = "ttir.sum"(%100, %101) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %103 = ttir.empty() : tensor<1x7xf32>
    %104 = "ttir.multiply"(%102, %47, %103) : (tensor<1x7xf32>, tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %105 = ttir.empty() : tensor<1x7x1xf32>
    %106 = "ttir.reshape"(%104, %105) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %107 = ttir.empty() : tensor<1x1x1xf32>
    %108 = "ttir.reshape"(%3, %107) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %109 = ttir.empty() : tensor<1x7x1xf32>
    %110 = "ttir.broadcast"(%108, %109) <{broadcast_dimensions = array<i64: 1, 7, 1>}> : (tensor<1x1x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %111 = ttir.empty() : tensor<1x7x1xf32>
    %112 = "ttir.add"(%106, %110, %111) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %113 = ttir.empty() : tensor<1x7x1xf32>
    %114 = "ttir.rsqrt"(%112, %113) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %115 = ttir.empty() : tensor<1x7xf32>
    %116 = "ttir.reshape"(%114, %115) <{shape = [1 : i32, 7 : i32]}> : (tensor<1x7x1xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %117 = ttir.empty() : tensor<1x7x1xf32>
    %118 = "ttir.reshape"(%116, %117) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %119 = ttir.empty() : tensor<1x7x3072xf32>
    %120 = "ttir.broadcast"(%118, %119) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x7x1xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %121 = ttir.empty() : tensor<1x7x3072xf32>
    %122 = "ttir.multiply"(%98, %120, %121) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %123 = ttir.empty() : tensor<1x7x3072xbf16>
    %124 = "ttir.typecast"(%122, %123) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %125 = ttir.empty() : tensor<1x7x3072xf32>
    %126 = "ttir.typecast"(%124, %125) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %127 = ttir.empty() : tensor<1x7x3072xf32>
    %128 = "ttir.multiply"(%96, %126, %127) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %129 = ttir.empty() : tensor<1x7x3072xbf16>
    %130 = "ttir.typecast"(%128, %129) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %131 = ttir.empty() : tensor<7x3072xbf16>
    %132 = "ttir.reshape"(%130, %131) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x7x3072xbf16>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %133 = ttir.empty() : tensor<1x1536x3072xbf16>
    %134 = "ttir.reshape"(%35, %133) <{shape = [1 : i32, 1536 : i32, 3072 : i32]}> : (tensor<1536x3072xbf16>, tensor<1x1536x3072xbf16>) -> tensor<1x1536x3072xbf16>
    %135 = ttir.empty() : tensor<1536x3072xbf16>
    %136 = "ttir.reshape"(%134, %135) <{shape = [1536 : i32, 3072 : i32]}> : (tensor<1x1536x3072xbf16>, tensor<1536x3072xbf16>) -> tensor<1536x3072xbf16>
    %137 = ttir.empty() : tensor<3072x1536xbf16>
    %138 = "ttir.permute"(%136, %137) <{permutation = array<i64: 1, 0>}> : (tensor<1536x3072xbf16>, tensor<3072x1536xbf16>) -> tensor<3072x1536xbf16>
    %139 = "ttir.dot_general"(%132, %138) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x1536xbf16>) -> tensor<7x1536xbf16>
    %140 = ttir.empty() : tensor<1x7x12x128xbf16>
    %141 = "ttir.reshape"(%139, %140) <{shape = [1 : i32, 7 : i32, 12 : i32, 128 : i32]}> : (tensor<7x1536xbf16>, tensor<1x7x12x128xbf16>) -> tensor<1x7x12x128xbf16>
    %142 = ttir.empty() : tensor<1x12x7x128xbf16>
    %143 = "ttir.permute"(%141, %142) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x12x128xbf16>, tensor<1x12x7x128xbf16>) -> tensor<1x12x7x128xbf16>
    %144 = ttir.empty() : tensor<1x12x7x128xf32>
    %145 = "ttir.typecast"(%143, %144) <{conservative_folding = false}> : (tensor<1x12x7x128xbf16>, tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xf32>
    %146 = ttir.empty() : tensor<1x1x64xf32>
    %147 = "ttir.reshape"(%29, %146) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xf32>, tensor<1x1x64xf32>) -> tensor<1x1x64xf32>
    %148 = ttir.empty() : tensor<1x64x1xf32>
    %149 = "ttir.reshape"(%147, %148) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<1x1x64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
    %150 = ttir.empty() : tensor<1x1x7xi64>
    %151 = "ttir.reshape"(%19, %150) <{shape = [1 : i32, 1 : i32, 7 : i32]}> : (tensor<7xi64>, tensor<1x1x7xi64>) -> tensor<1x1x7xi64>
    %152 = ttir.empty() : tensor<7xi64>
    %153 = "ttir.reshape"(%151, %152) <{shape = [7 : i32]}> : (tensor<1x1x7xi64>, tensor<7xi64>) -> tensor<7xi64>
    %154 = ttir.empty() : tensor<1x1x7xf32>
    %155 = "ttir.typecast"(%151, %154) <{conservative_folding = false}> : (tensor<1x1x7xi64>, tensor<1x1x7xf32>) -> tensor<1x1x7xf32>
    %156 = "ttir.dot_general"(%149, %155) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %157 = ttir.empty() : tensor<1x7x64xf32>
    %158 = "ttir.permute"(%156, %157) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x7xf32>, tensor<1x7x64xf32>) -> tensor<1x7x64xf32>
    %159 = ttir.empty() : tensor<1x7x128xf32>
    %160 = "ttir.concat"(%158, %158, %159) <{dim = 2 : si32}> : (tensor<1x7x64xf32>, tensor<1x7x64xf32>, tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
    %161 = ttir.empty() : tensor<1x7x128xf32>
    %162 = "ttir.cos"(%160, %161) : (tensor<1x7x128xf32>, tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
    %163 = ttir.empty() : tensor<1x7x128xbf16>
    %164 = "ttir.typecast"(%162, %163) <{conservative_folding = false}> : (tensor<1x7x128xf32>, tensor<1x7x128xbf16>) -> tensor<1x7x128xbf16>
    %165 = ttir.empty() : tensor<1x7x128xf32>
    %166 = "ttir.typecast"(%164, %165) <{conservative_folding = false}> : (tensor<1x7x128xbf16>, tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
    %167 = ttir.empty() : tensor<1x1x7x128xf32>
    %168 = "ttir.reshape"(%166, %167) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32>, tensor<1x1x7x128xf32>) -> tensor<1x1x7x128xf32>
    %169 = ttir.empty() : tensor<1x12x7x128xf32>
    %170 = "ttir.broadcast"(%168, %169) <{broadcast_dimensions = array<i64: 1, 12, 1, 1>}> : (tensor<1x1x7x128xf32>, tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xf32>
    %171 = ttir.empty() : tensor<1x12x7x128xf32>
    %172 = "ttir.multiply"(%145, %170, %171) : (tensor<1x12x7x128xf32>, tensor<1x12x7x128xf32>, tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xf32>
    %173 = ttir.empty() : tensor<1x12x7x128xbf16>
    %174 = "ttir.typecast"(%172, %173) <{conservative_folding = false}> : (tensor<1x12x7x128xf32>, tensor<1x12x7x128xbf16>) -> tensor<1x12x7x128xbf16>
    %175 = ttir.empty() : tensor<1x12x7x64xbf16>
    %176 = "ttir.slice_static"(%143, %175) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 12 : i32, 7 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x7x128xbf16>, tensor<1x12x7x64xbf16>) -> tensor<1x12x7x64xbf16>
    %177 = ttir.empty() : tensor<1x12x7x64xbf16>
    %178 = "ttir.neg"(%176, %177) : (tensor<1x12x7x64xbf16>, tensor<1x12x7x64xbf16>) -> tensor<1x12x7x64xbf16>
    %179 = ttir.empty() : tensor<1x12x7x64xbf16>
    %180 = "ttir.slice_static"(%143, %179) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 12 : i32, 7 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x7x128xbf16>, tensor<1x12x7x64xbf16>) -> tensor<1x12x7x64xbf16>
    %181 = ttir.empty() : tensor<1x12x7x128xbf16>
    %182 = "ttir.concat"(%178, %180, %181) <{dim = 3 : si32}> : (tensor<1x12x7x64xbf16>, tensor<1x12x7x64xbf16>, tensor<1x12x7x128xbf16>) -> tensor<1x12x7x128xbf16>
    %183 = ttir.empty() : tensor<1x12x7x128xf32>
    %184 = "ttir.typecast"(%182, %183) <{conservative_folding = false}> : (tensor<1x12x7x128xbf16>, tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xf32>
    %185 = ttir.empty() : tensor<1x7x128xf32>
    %186 = "ttir.sin"(%160, %185) : (tensor<1x7x128xf32>, tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
    %187 = ttir.empty() : tensor<1x7x128xbf16>
    %188 = "ttir.typecast"(%186, %187) <{conservative_folding = false}> : (tensor<1x7x128xf32>, tensor<1x7x128xbf16>) -> tensor<1x7x128xbf16>
    %189 = ttir.empty() : tensor<1x7x128xf32>
    %190 = "ttir.typecast"(%188, %189) <{conservative_folding = false}> : (tensor<1x7x128xbf16>, tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
    %191 = ttir.empty() : tensor<1x1x7x128xf32>
    %192 = "ttir.reshape"(%190, %191) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32>, tensor<1x1x7x128xf32>) -> tensor<1x1x7x128xf32>
    %193 = ttir.empty() : tensor<1x12x7x128xf32>
    %194 = "ttir.broadcast"(%192, %193) <{broadcast_dimensions = array<i64: 1, 12, 1, 1>}> : (tensor<1x1x7x128xf32>, tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xf32>
    %195 = ttir.empty() : tensor<1x12x7x128xf32>
    %196 = "ttir.multiply"(%184, %194, %195) : (tensor<1x12x7x128xf32>, tensor<1x12x7x128xf32>, tensor<1x12x7x128xf32>) -> tensor<1x12x7x128xf32>
    %197 = ttir.empty() : tensor<1x12x7x128xbf16>
    %198 = "ttir.typecast"(%196, %197) <{conservative_folding = false}> : (tensor<1x12x7x128xf32>, tensor<1x12x7x128xbf16>) -> tensor<1x12x7x128xbf16>
    %199 = ttir.empty() : tensor<1x12x7x128xbf16>
    %200 = "ttir.add"(%174, %198, %199) : (tensor<1x12x7x128xbf16>, tensor<1x12x7x128xbf16>, tensor<1x12x7x128xbf16>) -> tensor<1x12x7x128xbf16>
    %201 = ttir.empty() : tensor<12x7x128xbf16>
    %202 = "ttir.reshape"(%200, %201) <{shape = [12 : i32, 7 : i32, 128 : i32]}> : (tensor<1x12x7x128xbf16>, tensor<12x7x128xbf16>) -> tensor<12x7x128xbf16>
    %203 = ttir.empty() : tensor<7xi1>
    %204 = "ttir.lt"(%153, %48, %203) : (tensor<7xi64>, tensor<7xi64>, tensor<7xi1>) -> tensor<7xi1>
    %205 = ttir.empty() : tensor<1xi64>
    %206 = "ttir.reshape"(%21, %205) <{shape = [1 : i32]}> : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %207 = ttir.empty() : tensor<7xi64>
    %208 = "ttir.broadcast"(%206, %207) <{broadcast_dimensions = array<i64: 7>}> : (tensor<1xi64>, tensor<7xi64>) -> tensor<7xi64>
    %209 = ttir.empty() : tensor<7xi64>
    %210 = "ttir.add"(%153, %208, %209) : (tensor<7xi64>, tensor<7xi64>, tensor<7xi64>) -> tensor<7xi64>
    %211 = ttir.empty() : tensor<7xi64>
    %212 = "ttir.where"(%204, %210, %153, %211) : (tensor<7xi1>, tensor<7xi64>, tensor<7xi64>, tensor<7xi64>) -> tensor<7xi64>
    %213 = ttir.empty() : tensor<7x1xi64>
    %214 = "ttir.reshape"(%212, %213) <{shape = [7 : i32, 1 : i32]}> : (tensor<7xi64>, tensor<7x1xi64>) -> tensor<7x1xi64>
    %215 = ttir.empty() : tensor<1x512x3072xbf16>
    %216 = "ttir.reshape"(%31, %215) <{shape = [1 : i32, 512 : i32, 3072 : i32]}> : (tensor<512x3072xbf16>, tensor<1x512x3072xbf16>) -> tensor<1x512x3072xbf16>
    %217 = ttir.empty() : tensor<512x3072xbf16>
    %218 = "ttir.reshape"(%216, %217) <{shape = [512 : i32, 3072 : i32]}> : (tensor<1x512x3072xbf16>, tensor<512x3072xbf16>) -> tensor<512x3072xbf16>
    %219 = ttir.empty() : tensor<3072x512xbf16>
    %220 = "ttir.permute"(%218, %219) <{permutation = array<i64: 1, 0>}> : (tensor<512x3072xbf16>, tensor<3072x512xbf16>) -> tensor<3072x512xbf16>
    %221 = "ttir.dot_general"(%132, %220) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x512xbf16>) -> tensor<7x512xbf16>
    %222 = ttir.empty() : tensor<1x7x4x128xbf16>
    %223 = "ttir.reshape"(%221, %222) <{shape = [1 : i32, 7 : i32, 4 : i32, 128 : i32]}> : (tensor<7x512xbf16>, tensor<1x7x4x128xbf16>) -> tensor<1x7x4x128xbf16>
    %224 = ttir.empty() : tensor<1x4x7x128xbf16>
    %225 = "ttir.permute"(%223, %224) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x4x128xbf16>, tensor<1x4x7x128xbf16>) -> tensor<1x4x7x128xbf16>
    %226 = ttir.empty() : tensor<1x4x7x128xf32>
    %227 = "ttir.typecast"(%225, %226) <{conservative_folding = false}> : (tensor<1x4x7x128xbf16>, tensor<1x4x7x128xf32>) -> tensor<1x4x7x128xf32>
    %228 = ttir.empty() : tensor<1x1x7x128xf32>
    %229 = "ttir.reshape"(%166, %228) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32>, tensor<1x1x7x128xf32>) -> tensor<1x1x7x128xf32>
    %230 = ttir.empty() : tensor<1x4x7x128xf32>
    %231 = "ttir.broadcast"(%229, %230) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x7x128xf32>, tensor<1x4x7x128xf32>) -> tensor<1x4x7x128xf32>
    %232 = ttir.empty() : tensor<1x4x7x128xf32>
    %233 = "ttir.multiply"(%227, %231, %232) : (tensor<1x4x7x128xf32>, tensor<1x4x7x128xf32>, tensor<1x4x7x128xf32>) -> tensor<1x4x7x128xf32>
    %234 = ttir.empty() : tensor<1x4x7x128xbf16>
    %235 = "ttir.typecast"(%233, %234) <{conservative_folding = false}> : (tensor<1x4x7x128xf32>, tensor<1x4x7x128xbf16>) -> tensor<1x4x7x128xbf16>
    %236 = ttir.empty() : tensor<1x4x7x64xbf16>
    %237 = "ttir.slice_static"(%225, %236) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 7 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x7x128xbf16>, tensor<1x4x7x64xbf16>) -> tensor<1x4x7x64xbf16>
    %238 = ttir.empty() : tensor<1x4x7x64xbf16>
    %239 = "ttir.neg"(%237, %238) : (tensor<1x4x7x64xbf16>, tensor<1x4x7x64xbf16>) -> tensor<1x4x7x64xbf16>
    %240 = ttir.empty() : tensor<1x4x7x64xbf16>
    %241 = "ttir.slice_static"(%225, %240) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 7 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x7x128xbf16>, tensor<1x4x7x64xbf16>) -> tensor<1x4x7x64xbf16>
    %242 = ttir.empty() : tensor<1x4x7x128xbf16>
    %243 = "ttir.concat"(%239, %241, %242) <{dim = 3 : si32}> : (tensor<1x4x7x64xbf16>, tensor<1x4x7x64xbf16>, tensor<1x4x7x128xbf16>) -> tensor<1x4x7x128xbf16>
    %244 = ttir.empty() : tensor<1x4x7x128xf32>
    %245 = "ttir.typecast"(%243, %244) <{conservative_folding = false}> : (tensor<1x4x7x128xbf16>, tensor<1x4x7x128xf32>) -> tensor<1x4x7x128xf32>
    %246 = ttir.empty() : tensor<1x1x7x128xf32>
    %247 = "ttir.reshape"(%190, %246) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32>, tensor<1x1x7x128xf32>) -> tensor<1x1x7x128xf32>
    %248 = ttir.empty() : tensor<1x4x7x128xf32>
    %249 = "ttir.broadcast"(%247, %248) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x7x128xf32>, tensor<1x4x7x128xf32>) -> tensor<1x4x7x128xf32>
    %250 = ttir.empty() : tensor<1x4x7x128xf32>
    %251 = "ttir.multiply"(%245, %249, %250) : (tensor<1x4x7x128xf32>, tensor<1x4x7x128xf32>, tensor<1x4x7x128xf32>) -> tensor<1x4x7x128xf32>
    %252 = ttir.empty() : tensor<1x4x7x128xbf16>
    %253 = "ttir.typecast"(%251, %252) <{conservative_folding = false}> : (tensor<1x4x7x128xf32>, tensor<1x4x7x128xbf16>) -> tensor<1x4x7x128xbf16>
    %254 = ttir.empty() : tensor<1x4x7x128xbf16>
    %255 = "ttir.add"(%235, %253, %254) : (tensor<1x4x7x128xbf16>, tensor<1x4x7x128xbf16>, tensor<1x4x7x128xbf16>) -> tensor<1x4x7x128xbf16>
    %256 = ttir.empty() : tensor<1x4x1024x128xbf16>
    %257 = "ttir.scatter"(%33, %214, %255, %256) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x4x1024x128xbf16>, tensor<7x1xi64>, tensor<1x4x7x128xbf16>, tensor<1x4x1024x128xbf16>) -> tensor<1x4x1024x128xbf16>
    %258 = ttir.empty() : tensor<1x4x1x1024x128xbf16>
    %259 = "ttir.reshape"(%257, %258) <{shape = [1 : i32, 4 : i32, 1 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x1024x128xbf16>, tensor<1x4x1x1024x128xbf16>) -> tensor<1x4x1x1024x128xbf16>
    %260 = ttir.empty() : tensor<1x4x3x1024x128xbf16>
    %261 = "ttir.broadcast"(%259, %260) <{broadcast_dimensions = array<i64: 1, 1, 3, 1, 1>}> : (tensor<1x4x1x1024x128xbf16>, tensor<1x4x3x1024x128xbf16>) -> tensor<1x4x3x1024x128xbf16>
    %262 = ttir.empty() : tensor<1x12x1024x128xbf16>
    %263 = "ttir.reshape"(%261, %262) <{shape = [1 : i32, 12 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x3x1024x128xbf16>, tensor<1x12x1024x128xbf16>) -> tensor<1x12x1024x128xbf16>
    %264 = ttir.empty() : tensor<1x12x128x1024xbf16>
    %265 = "ttir.permute"(%263, %264) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x12x1024x128xbf16>, tensor<1x12x128x1024xbf16>) -> tensor<1x12x128x1024xbf16>
    %266 = ttir.empty() : tensor<12x128x1024xbf16>
    %267 = "ttir.reshape"(%265, %266) <{shape = [12 : i32, 128 : i32, 1024 : i32]}> : (tensor<1x12x128x1024xbf16>, tensor<12x128x1024xbf16>) -> tensor<12x128x1024xbf16>
    %268 = "ttir.dot_general"(%202, %267) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<12x7x128xbf16>, tensor<12x128x1024xbf16>) -> tensor<12x7x1024xbf16>
    %269 = ttir.empty() : tensor<12x7x1024xf32>
    %270 = "ttir.typecast"(%268, %269) <{conservative_folding = false}> : (tensor<12x7x1024xbf16>, tensor<12x7x1024xf32>) -> tensor<12x7x1024xf32>
    %271 = ttir.empty() : tensor<1x12x7x1024xf32>
    %272 = "ttir.reshape"(%270, %271) <{shape = [1 : i32, 12 : i32, 7 : i32, 1024 : i32]}> : (tensor<12x7x1024xf32>, tensor<1x12x7x1024xf32>) -> tensor<1x12x7x1024xf32>
    %273 = ttir.empty() : tensor<1x1x1x1xf32>
    %274 = "ttir.reshape"(%27, %273) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %275 = ttir.empty() : tensor<1x12x7x1024xf32>
    %276 = "ttir.broadcast"(%274, %275) <{broadcast_dimensions = array<i64: 1, 12, 7, 1024>}> : (tensor<1x1x1x1xf32>, tensor<1x12x7x1024xf32>) -> tensor<1x12x7x1024xf32>
    %277 = ttir.empty() : tensor<1x12x7x1024xf32>
    %278 = "ttir.multiply"(%272, %276, %277) : (tensor<1x12x7x1024xf32>, tensor<1x12x7x1024xf32>, tensor<1x12x7x1024xf32>) -> tensor<1x12x7x1024xf32>
    %279 = ttir.empty() : tensor<1x12x7x1024xbf16>
    %280 = "ttir.typecast"(%278, %279) <{conservative_folding = false}> : (tensor<1x12x7x1024xf32>, tensor<1x12x7x1024xbf16>) -> tensor<1x12x7x1024xbf16>
    %281 = ttir.empty() : tensor<1x1024xi64>
    %282 = "ttir.reshape"(%43, %281) <{shape = [1 : i32, 1024 : i32]}> : (tensor<1024xi64>, tensor<1x1024xi64>) -> tensor<1x1024xi64>
    %283 = ttir.empty() : tensor<7x1024xi64>
    %284 = "ttir.broadcast"(%282, %283) <{broadcast_dimensions = array<i64: 7, 1>}> : (tensor<1x1024xi64>, tensor<7x1024xi64>) -> tensor<7x1024xi64>
    %285 = ttir.empty() : tensor<7x1xi64>
    %286 = "ttir.reshape"(%44, %285) <{shape = [7 : i32, 1 : i32]}> : (tensor<7xi64>, tensor<7x1xi64>) -> tensor<7x1xi64>
    %287 = ttir.empty() : tensor<7x1024xi64>
    %288 = "ttir.broadcast"(%286, %287) <{broadcast_dimensions = array<i64: 1, 1024>}> : (tensor<7x1xi64>, tensor<7x1024xi64>) -> tensor<7x1024xi64>
    %289 = ttir.empty() : tensor<7x1024xi64>
    %290 = "ttir.subtract"(%284, %288, %289) : (tensor<7x1024xi64>, tensor<7x1024xi64>, tensor<7x1024xi64>) -> tensor<7x1024xi64>
    %291 = ttir.empty() : tensor<7x1024xi1>
    %292 = "ttir.ge"(%290, %58, %291) : (tensor<7x1024xi64>, tensor<7x1024xi64>, tensor<7x1024xi1>) -> tensor<7x1024xi1>
    %293 = ttir.empty() : tensor<1x1xbf16>
    %294 = "ttir.reshape"(%25, %293) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %295 = ttir.empty() : tensor<7x1024xbf16>
    %296 = "ttir.broadcast"(%294, %295) <{broadcast_dimensions = array<i64: 7, 1024>}> : (tensor<1x1xbf16>, tensor<7x1024xbf16>) -> tensor<7x1024xbf16>
    %297 = ttir.empty() : tensor<7x1024xbf16>
    %298 = "ttir.where"(%292, %296, %54, %297) : (tensor<7x1024xi1>, tensor<7x1024xbf16>, tensor<7x1024xbf16>, tensor<7x1024xbf16>) -> tensor<7x1024xbf16>
    %299 = ttir.empty() : tensor<7x1024xf32>
    %300 = "ttir.typecast"(%298, %299) <{conservative_folding = false}> : (tensor<7x1024xbf16>, tensor<7x1024xf32>) -> tensor<7x1024xf32>
    %301 = ttir.empty() : tensor<7x1xi64>
    %302 = "ttir.reshape"(%153, %301) <{shape = [7 : i32, 1 : i32]}> : (tensor<7xi64>, tensor<7x1xi64>) -> tensor<7x1xi64>
    %303 = ttir.empty() : tensor<7x1024xi64>
    %304 = "ttir.broadcast"(%302, %303) <{broadcast_dimensions = array<i64: 1, 1024>}> : (tensor<7x1xi64>, tensor<7x1024xi64>) -> tensor<7x1024xi64>
    %305 = ttir.empty() : tensor<7x1024xi1>
    %306 = "ttir.gt"(%284, %304, %305) : (tensor<7x1024xi64>, tensor<7x1024xi64>, tensor<7x1024xi1>) -> tensor<7x1024xi1>
    %307 = ttir.empty() : tensor<7x1024xf32>
    %308 = "ttir.typecast"(%306, %307) <{conservative_folding = false}> : (tensor<7x1024xi1>, tensor<7x1024xf32>) -> tensor<7x1024xf32>
    %309 = ttir.empty() : tensor<7x1024xf32>
    %310 = "ttir.multiply"(%300, %308, %309) : (tensor<7x1024xf32>, tensor<7x1024xf32>, tensor<7x1024xf32>) -> tensor<7x1024xf32>
    %311 = ttir.empty() : tensor<7x1024xbf16>
    %312 = "ttir.typecast"(%310, %311) <{conservative_folding = false}> : (tensor<7x1024xf32>, tensor<7x1024xbf16>) -> tensor<7x1024xbf16>
    %313 = ttir.empty() : tensor<1x7x1024xbf16>
    %314 = "ttir.reshape"(%312, %313) <{shape = [1 : i32, 7 : i32, 1024 : i32]}> : (tensor<7x1024xbf16>, tensor<1x7x1024xbf16>) -> tensor<1x7x1024xbf16>
    %315 = ttir.empty() : tensor<1x1x7x1024xbf16>
    %316 = "ttir.reshape"(%314, %315) <{shape = [1 : i32, 1 : i32, 7 : i32, 1024 : i32]}> : (tensor<1x7x1024xbf16>, tensor<1x1x7x1024xbf16>) -> tensor<1x1x7x1024xbf16>
    %317 = ttir.empty() : tensor<1x12x7x1024xbf16>
    %318 = "ttir.broadcast"(%316, %317) <{broadcast_dimensions = array<i64: 1, 12, 1, 1>}> : (tensor<1x1x7x1024xbf16>, tensor<1x12x7x1024xbf16>) -> tensor<1x12x7x1024xbf16>
    %319 = ttir.empty() : tensor<1x12x7x1024xbf16>
    %320 = "ttir.add"(%280, %318, %319) : (tensor<1x12x7x1024xbf16>, tensor<1x12x7x1024xbf16>, tensor<1x12x7x1024xbf16>) -> tensor<1x12x7x1024xbf16>
    %321 = ttir.empty() : tensor<1x12x7x1024xf32>
    %322 = "ttir.typecast"(%320, %321) <{conservative_folding = false}> : (tensor<1x12x7x1024xbf16>, tensor<1x12x7x1024xf32>) -> tensor<1x12x7x1024xf32>
    %323 = ttir.empty() : tensor<1x12x7xf32>
    %324 = "ttir.max"(%322, %323) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x12x7x1024xf32>, tensor<1x12x7xf32>) -> tensor<1x12x7xf32>
    %325 = ttir.empty() : tensor<1x12x7x1xf32>
    %326 = "ttir.reshape"(%324, %325) <{shape = [1 : i32, 12 : i32, 7 : i32, 1 : i32]}> : (tensor<1x12x7xf32>, tensor<1x12x7x1xf32>) -> tensor<1x12x7x1xf32>
    %327 = ttir.empty() : tensor<1x12x7x1024xf32>
    %328 = "ttir.broadcast"(%326, %327) <{broadcast_dimensions = array<i64: 1, 1, 1, 1024>}> : (tensor<1x12x7x1xf32>, tensor<1x12x7x1024xf32>) -> tensor<1x12x7x1024xf32>
    %329 = ttir.empty() : tensor<1x12x7x1024xf32>
    %330 = "ttir.subtract"(%322, %328, %329) : (tensor<1x12x7x1024xf32>, tensor<1x12x7x1024xf32>, tensor<1x12x7x1024xf32>) -> tensor<1x12x7x1024xf32>
    %331 = ttir.empty() : tensor<1x12x7x1024xf32>
    %332 = "ttir.exp"(%330, %331) : (tensor<1x12x7x1024xf32>, tensor<1x12x7x1024xf32>) -> tensor<1x12x7x1024xf32>
    %333 = ttir.empty() : tensor<1x12x7xf32>
    %334 = "ttir.sum"(%332, %333) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x12x7x1024xf32>, tensor<1x12x7xf32>) -> tensor<1x12x7xf32>
    %335 = ttir.empty() : tensor<1x12x7x1xf32>
    %336 = "ttir.reshape"(%334, %335) <{shape = [1 : i32, 12 : i32, 7 : i32, 1 : i32]}> : (tensor<1x12x7xf32>, tensor<1x12x7x1xf32>) -> tensor<1x12x7x1xf32>
    %337 = ttir.empty() : tensor<1x12x7x1024xf32>
    %338 = "ttir.broadcast"(%336, %337) <{broadcast_dimensions = array<i64: 1, 1, 1, 1024>}> : (tensor<1x12x7x1xf32>, tensor<1x12x7x1024xf32>) -> tensor<1x12x7x1024xf32>
    %339 = ttir.empty() : tensor<1x12x7x1024xf32>
    %340 = "ttir.div"(%332, %338, %339) : (tensor<1x12x7x1024xf32>, tensor<1x12x7x1024xf32>, tensor<1x12x7x1024xf32>) -> tensor<1x12x7x1024xf32>
    %341 = ttir.empty() : tensor<1x12x7x1024xbf16>
    %342 = "ttir.typecast"(%340, %341) <{conservative_folding = false}> : (tensor<1x12x7x1024xf32>, tensor<1x12x7x1024xbf16>) -> tensor<1x12x7x1024xbf16>
    %343 = ttir.empty() : tensor<12x7x1024xbf16>
    %344 = "ttir.reshape"(%342, %343) <{shape = [12 : i32, 7 : i32, 1024 : i32]}> : (tensor<1x12x7x1024xbf16>, tensor<12x7x1024xbf16>) -> tensor<12x7x1024xbf16>
    %345 = ttir.empty() : tensor<1x512x3072xbf16>
    %346 = "ttir.reshape"(%11, %345) <{shape = [1 : i32, 512 : i32, 3072 : i32]}> : (tensor<512x3072xbf16>, tensor<1x512x3072xbf16>) -> tensor<1x512x3072xbf16>
    %347 = ttir.empty() : tensor<512x3072xbf16>
    %348 = "ttir.reshape"(%346, %347) <{shape = [512 : i32, 3072 : i32]}> : (tensor<1x512x3072xbf16>, tensor<512x3072xbf16>) -> tensor<512x3072xbf16>
    %349 = ttir.empty() : tensor<3072x512xbf16>
    %350 = "ttir.permute"(%348, %349) <{permutation = array<i64: 1, 0>}> : (tensor<512x3072xbf16>, tensor<3072x512xbf16>) -> tensor<3072x512xbf16>
    %351 = "ttir.dot_general"(%132, %350) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x512xbf16>) -> tensor<7x512xbf16>
    %352 = ttir.empty() : tensor<1x7x4x128xbf16>
    %353 = "ttir.reshape"(%351, %352) <{shape = [1 : i32, 7 : i32, 4 : i32, 128 : i32]}> : (tensor<7x512xbf16>, tensor<1x7x4x128xbf16>) -> tensor<1x7x4x128xbf16>
    %354 = ttir.empty() : tensor<1x4x7x128xbf16>
    %355 = "ttir.permute"(%353, %354) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x4x128xbf16>, tensor<1x4x7x128xbf16>) -> tensor<1x4x7x128xbf16>
    %356 = ttir.empty() : tensor<1x4x1024x128xbf16>
    %357 = "ttir.scatter"(%23, %214, %355, %356) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x4x1024x128xbf16>, tensor<7x1xi64>, tensor<1x4x7x128xbf16>, tensor<1x4x1024x128xbf16>) -> tensor<1x4x1024x128xbf16>
    %358 = ttir.empty() : tensor<1x4x1x1024x128xbf16>
    %359 = "ttir.reshape"(%357, %358) <{shape = [1 : i32, 4 : i32, 1 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x1024x128xbf16>, tensor<1x4x1x1024x128xbf16>) -> tensor<1x4x1x1024x128xbf16>
    %360 = ttir.empty() : tensor<1x4x3x1024x128xbf16>
    %361 = "ttir.broadcast"(%359, %360) <{broadcast_dimensions = array<i64: 1, 1, 3, 1, 1>}> : (tensor<1x4x1x1024x128xbf16>, tensor<1x4x3x1024x128xbf16>) -> tensor<1x4x3x1024x128xbf16>
    %362 = ttir.empty() : tensor<12x1024x128xbf16>
    %363 = "ttir.reshape"(%361, %362) <{shape = [12 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x3x1024x128xbf16>, tensor<12x1024x128xbf16>) -> tensor<12x1024x128xbf16>
    %364 = "ttir.dot_general"(%344, %363) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<12x7x1024xbf16>, tensor<12x1024x128xbf16>) -> tensor<12x7x128xbf16>
    %365 = ttir.empty() : tensor<1x12x7x128xbf16>
    %366 = "ttir.reshape"(%364, %365) <{shape = [1 : i32, 12 : i32, 7 : i32, 128 : i32]}> : (tensor<12x7x128xbf16>, tensor<1x12x7x128xbf16>) -> tensor<1x12x7x128xbf16>
    %367 = ttir.empty() : tensor<1x7x12x128xbf16>
    %368 = "ttir.permute"(%366, %367) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x12x7x128xbf16>, tensor<1x7x12x128xbf16>) -> tensor<1x7x12x128xbf16>
    %369 = ttir.empty() : tensor<7x1536xbf16>
    %370 = "ttir.reshape"(%368, %369) <{shape = [7 : i32, 1536 : i32]}> : (tensor<1x7x12x128xbf16>, tensor<7x1536xbf16>) -> tensor<7x1536xbf16>
    %371 = ttir.empty() : tensor<1x3072x1536xbf16>
    %372 = "ttir.reshape"(%9, %371) <{shape = [1 : i32, 3072 : i32, 1536 : i32]}> : (tensor<3072x1536xbf16>, tensor<1x3072x1536xbf16>) -> tensor<1x3072x1536xbf16>
    %373 = ttir.empty() : tensor<3072x1536xbf16>
    %374 = "ttir.reshape"(%372, %373) <{shape = [3072 : i32, 1536 : i32]}> : (tensor<1x3072x1536xbf16>, tensor<3072x1536xbf16>) -> tensor<3072x1536xbf16>
    %375 = ttir.empty() : tensor<1536x3072xbf16>
    %376 = "ttir.permute"(%374, %375) <{permutation = array<i64: 1, 0>}> : (tensor<3072x1536xbf16>, tensor<1536x3072xbf16>) -> tensor<1536x3072xbf16>
    %377 = "ttir.dot_general"(%370, %376) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x1536xbf16>, tensor<1536x3072xbf16>) -> tensor<7x3072xbf16>
    %378 = ttir.empty() : tensor<7x3072xbf16>
    %379 = "ttir.all_reduce"(%377, %378) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<7x3072xbf16>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %380 = ttir.empty() : tensor<1x7x3072xbf16>
    %381 = "ttir.reshape"(%379, %380) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %382 = ttir.empty() : tensor<1x7x3072xbf16>
    %383 = "ttir.add"(%86, %381, %382) : (tensor<1x7x3072xbf16>, tensor<1x7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %384 = ttir.empty() : tensor<1x1x3072xbf16>
    %385 = "ttir.reshape"(%37, %384) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %386 = ttir.empty() : tensor<1x1x3072xf32>
    %387 = "ttir.typecast"(%385, %386) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %388 = ttir.empty() : tensor<3072xf32>
    %389 = "ttir.reshape"(%387, %388) <{shape = [3072 : i32]}> : (tensor<1x1x3072xf32>, tensor<3072xf32>) -> tensor<3072xf32>
    %390 = ttir.empty() : tensor<1x1x3072xf32>
    %391 = "ttir.reshape"(%389, %390) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %392 = ttir.empty() : tensor<1x7x3072xf32>
    %393 = "ttir.broadcast"(%391, %392) <{broadcast_dimensions = array<i64: 1, 7, 1>}> : (tensor<1x1x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %394 = ttir.empty() : tensor<1x7x3072xf32>
    %395 = "ttir.typecast"(%383, %394) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %396 = ttir.empty() : tensor<1x7x3072xf32>
    %397 = "ttir.pow"(%395, %62, %396) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %398 = ttir.empty() : tensor<1x7xf32>
    %399 = "ttir.sum"(%397, %398) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %400 = ttir.empty() : tensor<1x7xf32>
    %401 = "ttir.multiply"(%399, %47, %400) : (tensor<1x7xf32>, tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %402 = ttir.empty() : tensor<1x7x1xf32>
    %403 = "ttir.reshape"(%401, %402) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %404 = ttir.empty() : tensor<1x7x1xf32>
    %405 = "ttir.add"(%403, %110, %404) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %406 = ttir.empty() : tensor<1x7x1xf32>
    %407 = "ttir.rsqrt"(%405, %406) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %408 = ttir.empty() : tensor<1x7xf32>
    %409 = "ttir.reshape"(%407, %408) <{shape = [1 : i32, 7 : i32]}> : (tensor<1x7x1xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %410 = ttir.empty() : tensor<1x7x1xf32>
    %411 = "ttir.reshape"(%409, %410) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %412 = ttir.empty() : tensor<1x7x3072xf32>
    %413 = "ttir.broadcast"(%411, %412) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x7x1xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %414 = ttir.empty() : tensor<1x7x3072xf32>
    %415 = "ttir.multiply"(%395, %413, %414) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %416 = ttir.empty() : tensor<1x7x3072xbf16>
    %417 = "ttir.typecast"(%415, %416) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %418 = ttir.empty() : tensor<1x7x3072xf32>
    %419 = "ttir.typecast"(%417, %418) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %420 = ttir.empty() : tensor<1x7x3072xf32>
    %421 = "ttir.multiply"(%393, %419, %420) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %422 = ttir.empty() : tensor<1x7x3072xbf16>
    %423 = "ttir.typecast"(%421, %422) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %424 = ttir.empty() : tensor<7x3072xbf16>
    %425 = "ttir.reshape"(%423, %424) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x7x3072xbf16>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %426 = ttir.empty() : tensor<1x4096x3072xbf16>
    %427 = "ttir.reshape"(%39, %426) <{shape = [1 : i32, 4096 : i32, 3072 : i32]}> : (tensor<4096x3072xbf16>, tensor<1x4096x3072xbf16>) -> tensor<1x4096x3072xbf16>
    %428 = ttir.empty() : tensor<4096x3072xbf16>
    %429 = "ttir.reshape"(%427, %428) <{shape = [4096 : i32, 3072 : i32]}> : (tensor<1x4096x3072xbf16>, tensor<4096x3072xbf16>) -> tensor<4096x3072xbf16>
    %430 = ttir.empty() : tensor<3072x4096xbf16>
    %431 = "ttir.permute"(%429, %430) <{permutation = array<i64: 1, 0>}> : (tensor<4096x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<3072x4096xbf16>
    %432 = "ttir.dot_general"(%425, %431) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<7x4096xbf16>
    %433 = ttir.empty() : tensor<1x7x4096xbf16>
    %434 = "ttir.reshape"(%432, %433) <{shape = [1 : i32, 7 : i32, 4096 : i32]}> : (tensor<7x4096xbf16>, tensor<1x7x4096xbf16>) -> tensor<1x7x4096xbf16>
    %435 = ttir.empty() : tensor<1x7x4096xf32>
    %436 = "ttir.typecast"(%434, %435) <{conservative_folding = false}> : (tensor<1x7x4096xbf16>, tensor<1x7x4096xf32>) -> tensor<1x7x4096xf32>
    %437 = ttir.empty() : tensor<1x7x4096xbf16>
    %438 = "ttir.sigmoid"(%434, %437) : (tensor<1x7x4096xbf16>, tensor<1x7x4096xbf16>) -> tensor<1x7x4096xbf16>
    %439 = ttir.empty() : tensor<1x7x4096xf32>
    %440 = "ttir.typecast"(%438, %439) <{conservative_folding = false}> : (tensor<1x7x4096xbf16>, tensor<1x7x4096xf32>) -> tensor<1x7x4096xf32>
    %441 = ttir.empty() : tensor<1x7x4096xf32>
    %442 = "ttir.multiply"(%436, %440, %441) : (tensor<1x7x4096xf32>, tensor<1x7x4096xf32>, tensor<1x7x4096xf32>) -> tensor<1x7x4096xf32>
    %443 = ttir.empty() : tensor<1x7x4096xbf16>
    %444 = "ttir.typecast"(%442, %443) <{conservative_folding = false}> : (tensor<1x7x4096xf32>, tensor<1x7x4096xbf16>) -> tensor<1x7x4096xbf16>
    %445 = ttir.empty() : tensor<1x7x4096xf32>
    %446 = "ttir.typecast"(%444, %445) <{conservative_folding = false}> : (tensor<1x7x4096xbf16>, tensor<1x7x4096xf32>) -> tensor<1x7x4096xf32>
    %447 = ttir.empty() : tensor<1x4096x3072xbf16>
    %448 = "ttir.reshape"(%7, %447) <{shape = [1 : i32, 4096 : i32, 3072 : i32]}> : (tensor<4096x3072xbf16>, tensor<1x4096x3072xbf16>) -> tensor<1x4096x3072xbf16>
    %449 = ttir.empty() : tensor<4096x3072xbf16>
    %450 = "ttir.reshape"(%448, %449) <{shape = [4096 : i32, 3072 : i32]}> : (tensor<1x4096x3072xbf16>, tensor<4096x3072xbf16>) -> tensor<4096x3072xbf16>
    %451 = ttir.empty() : tensor<3072x4096xbf16>
    %452 = "ttir.permute"(%450, %451) <{permutation = array<i64: 1, 0>}> : (tensor<4096x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<3072x4096xbf16>
    %453 = "ttir.dot_general"(%425, %452) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<7x4096xbf16>
    %454 = ttir.empty() : tensor<7x4096xf32>
    %455 = "ttir.typecast"(%453, %454) <{conservative_folding = false}> : (tensor<7x4096xbf16>, tensor<7x4096xf32>) -> tensor<7x4096xf32>
    %456 = ttir.empty() : tensor<1x7x4096xf32>
    %457 = "ttir.reshape"(%455, %456) <{shape = [1 : i32, 7 : i32, 4096 : i32]}> : (tensor<7x4096xf32>, tensor<1x7x4096xf32>) -> tensor<1x7x4096xf32>
    %458 = ttir.empty() : tensor<1x7x4096xf32>
    %459 = "ttir.multiply"(%446, %457, %458) : (tensor<1x7x4096xf32>, tensor<1x7x4096xf32>, tensor<1x7x4096xf32>) -> tensor<1x7x4096xf32>
    %460 = ttir.empty() : tensor<1x7x4096xbf16>
    %461 = "ttir.typecast"(%459, %460) <{conservative_folding = false}> : (tensor<1x7x4096xf32>, tensor<1x7x4096xbf16>) -> tensor<1x7x4096xbf16>
    %462 = ttir.empty() : tensor<7x4096xbf16>
    %463 = "ttir.reshape"(%461, %462) <{shape = [7 : i32, 4096 : i32]}> : (tensor<1x7x4096xbf16>, tensor<7x4096xbf16>) -> tensor<7x4096xbf16>
    %464 = ttir.empty() : tensor<1x3072x4096xbf16>
    %465 = "ttir.reshape"(%5, %464) <{shape = [1 : i32, 3072 : i32, 4096 : i32]}> : (tensor<3072x4096xbf16>, tensor<1x3072x4096xbf16>) -> tensor<1x3072x4096xbf16>
    %466 = ttir.empty() : tensor<3072x4096xbf16>
    %467 = "ttir.reshape"(%465, %466) <{shape = [3072 : i32, 4096 : i32]}> : (tensor<1x3072x4096xbf16>, tensor<3072x4096xbf16>) -> tensor<3072x4096xbf16>
    %468 = ttir.empty() : tensor<4096x3072xbf16>
    %469 = "ttir.permute"(%467, %468) <{permutation = array<i64: 1, 0>}> : (tensor<3072x4096xbf16>, tensor<4096x3072xbf16>) -> tensor<4096x3072xbf16>
    %470 = "ttir.dot_general"(%463, %469) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x4096xbf16>, tensor<4096x3072xbf16>) -> tensor<7x3072xbf16>
    %471 = ttir.empty() : tensor<7x3072xbf16>
    %472 = "ttir.all_reduce"(%470, %471) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<7x3072xbf16>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %473 = ttir.empty() : tensor<1x7x3072xbf16>
    %474 = "ttir.reshape"(%472, %473) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %475 = ttir.empty() : tensor<1x7x3072xbf16>
    %476 = "ttir.add"(%383, %474, %475) : (tensor<1x7x3072xbf16>, tensor<1x7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %477 = ttir.empty() : tensor<1x7x3072xf32>
    %478 = "ttir.typecast"(%476, %477) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %479 = ttir.empty() : tensor<1x7x3072xf32>
    %480 = "ttir.pow"(%478, %62, %479) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %481 = ttir.empty() : tensor<1x7xf32>
    %482 = "ttir.sum"(%480, %481) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %483 = ttir.empty() : tensor<1x7xf32>
    %484 = "ttir.multiply"(%482, %47, %483) : (tensor<1x7xf32>, tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %485 = ttir.empty() : tensor<1x7x1xf32>
    %486 = "ttir.reshape"(%484, %485) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %487 = ttir.empty() : tensor<1x7x1xf32>
    %488 = "ttir.add"(%486, %110, %487) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %489 = ttir.empty() : tensor<1x7x1xf32>
    %490 = "ttir.rsqrt"(%488, %489) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %491 = ttir.empty() : tensor<1x7xf32>
    %492 = "ttir.reshape"(%490, %491) <{shape = [1 : i32, 7 : i32]}> : (tensor<1x7x1xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %493 = ttir.empty() : tensor<1x7x1xf32>
    %494 = "ttir.reshape"(%492, %493) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %495 = ttir.empty() : tensor<1x7x3072xf32>
    %496 = "ttir.broadcast"(%494, %495) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x7x1xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %497 = ttir.empty() : tensor<1x7x3072xf32>
    %498 = "ttir.multiply"(%478, %496, %497) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %499 = ttir.empty() : tensor<1x7x3072xbf16>
    %500 = "ttir.typecast"(%498, %499) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %501 = ttir.empty() : tensor<1x7x3072xf32>
    %502 = "ttir.typecast"(%500, %501) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %503 = ttir.empty() : tensor<1x7x3072xf32>
    %504 = "ttir.multiply"(%72, %502, %503) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %505 = ttir.empty() : tensor<1x7x3072xbf16>
    %506 = "ttir.typecast"(%504, %505) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %507 = ttir.empty() : tensor<7x3072xbf16>
    %508 = "ttir.reshape"(%506, %507) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x7x3072xbf16>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %509 = ttir.empty() : tensor<1x128256x3072xbf16>
    %510 = "ttir.reshape"(%1, %509) <{shape = [1 : i32, 128256 : i32, 3072 : i32]}> : (tensor<128256x3072xbf16>, tensor<1x128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %511 = ttir.empty() : tensor<128256x3072xbf16>
    %512 = "ttir.reshape"(%510, %511) <{shape = [128256 : i32, 3072 : i32]}> : (tensor<1x128256x3072xbf16>, tensor<128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %513 = ttir.empty() : tensor<3072x128256xbf16>
    %514 = "ttir.permute"(%512, %513) <{permutation = array<i64: 1, 0>}> : (tensor<128256x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<3072x128256xbf16>
    %515 = "ttir.dot_general"(%508, %514) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %516 = ttir.empty() : tensor<1x7x128256xbf16>
    %517 = "ttir.reshape"(%515, %516) <{shape = [1 : i32, 7 : i32, 128256 : i32]}> : (tensor<7x128256xbf16>, tensor<1x7x128256xbf16>) -> tensor<1x7x128256xbf16>
    %518 = ttir.empty() : tensor<7x128256xbf16>
    %519 = "ttir.mesh_shard"(%515, %518) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<7x128256xbf16>, tensor<7x128256xbf16>) -> tensor<7x128256xbf16>
    %520 = ttir.empty() : tensor<1x7x128256xbf16>
    %521 = "ttir.mesh_shard"(%517, %520) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x7x128256xbf16>, tensor<1x7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %519, %521 : tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}
2025-09-15 15:41:08.092 (  16.419s) [        F073E000]      module_builder.cc:561   WARN| `mhlo.num_partitions` attribute not found, assuming default number of partitions: 1
2025-09-15 15:41:08.092 (  16.419s) [        F073E000]      module_builder.cc:575   WARN| `mhlo.num_replicas` attribute not found, assuming default number of replicas: 1
2025-09-15 15:41:08.092 (  16.419s) [        F073E000]      module_builder.cc:585   WARN| Num replicas and num partitions are not set, inferring the number of devices from mesh shape
2025-09-15 15:41:08.407 (  16.733s) [        F073E000]      module_builder.cc:630      1| TTNN Module:
module @SyncTensorsGraph.448 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.448 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073185088, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073193408, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x16, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 2)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 2, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main_const_eval_0(%arg0: tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_1(%arg0: tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_2(%arg0: tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_3(%arg0: tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> (tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.constant"(%0) <{dtype = #ttcore.supportedDataTypes<si32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <interleaved>>, value = dense<"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tensor<1024xsi32>}> : (!ttnn.device) -> tensor<1024xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.constant"(%0) <{dtype = #ttcore.supportedDataTypes<si32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <interleaved>>, value = dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xsi32>}> : (!ttnn.device) -> tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %4 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<si32>, fill_value = 1 : i32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %5 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %6 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %7 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32]}> : (tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %8 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1024 : i32]}> : (tensor<1024xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1024xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %9 = "ttnn.repeat"(%8) <{repeat_dims = #ttnn.shape<7x1>}> : (tensor<1x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %10 = "ttnn.reshape"(%2) <{shape = [7 : i32, 1 : i32]}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %11 = "ttnn.neg"(%10) : (tensor<7x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<7x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %12 = "ttnn.add"(%8, %11) <{dtype = #ttcore.supportedDataTypes<si32>}> : (tensor<1x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<7x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %13 = "ttnn.typecast"(%12) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1024xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<7x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %14 = "ttnn.typecast"(%7) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %15 = "ttnn.ge"(%13, %14) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x1024xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1024xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<7x1024xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %16 = "ttnn.typecast"(%15) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x1024xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<7x1024xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %17 = "ttnn.reshape"(%5) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %18 = "ttnn.repeat"(%17) <{repeat_dims = #ttnn.shape<7x1024>}> : (tensor<1x1xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<1x1xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %19 = "ttnn.repeat"(%6) <{repeat_dims = #ttnn.shape<7x1024>}> : (tensor<1x1xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x1xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %20 = "ttnn.where"(%16, %18, %19) : (tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %21 = "ttnn.reshape"(%20) <{shape = [1 : i32, 1 : i32, 7 : i32, 1024 : i32]}> : (tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %22 = "ttnn.typecast"(%21) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%21) <{force = false}> : (tensor<1x1x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %23 = "ttnn.reshape"(%9) <{shape = [1 : i32, 1 : i32, 7 : i32, 1024 : i32]}> : (tensor<7x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x1024xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<7x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %24 = "ttnn.typecast"(%23) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x7x1024xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%23) <{force = false}> : (tensor<1x1x7x1024xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %22, %24 : tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_4(%arg0: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_5(%arg0: tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_6(%arg0: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.repeat"(%2) <{repeat_dims = #ttnn.shape<7x1>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 7 : i32]}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %5 = "ttnn.reshape"(%3) <{shape = [1 : i32, 7 : i32]}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %6 = "ttnn.reshape"(%3) <{shape = [1 : i32, 7 : i32]}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %4, %5, %6 : tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_7(%arg0: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %3 : tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_8(%arg0: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %3 : tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_9(%arg0: tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_10(%arg0: tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_11(%arg0: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_12(%arg0: tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_13(%arg0: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.repeat"(%2) <{repeat_dims = #ttnn.shape<1x12x7x1024>}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %4 = "ttnn.reshape"(%3) <{shape = [12 : i32, 7 : i32, 1024 : i32]}> : (tensor<1x12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %4 : tensor<12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_14(%arg0: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %3 : tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_15() -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %2 : tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main(%arg0: tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg1: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg2: tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg3: tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg4: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg5: tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg6: tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg7: tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg8: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg9: tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg10: tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg11: tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg12: tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg13: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg14: tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg15: tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg16: tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg17: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, [%arg0]) : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg10) <{force = false}> : (tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %1 = ttcore.load_cached(@main_const_eval_1, [%arg7]) : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg7) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %2 = ttcore.load_cached(@main_const_eval_2, [%arg15]) : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg15) <{force = false}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3:2 = ttcore.load_cached(@main_const_eval_3, [%arg12]) : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> (tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>)
        "ttnn.deallocate"(%arg12) <{force = false}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %4 = ttcore.load_cached(@main_const_eval_4, [%arg17]) : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg17) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %5 = ttcore.load_cached(@main_const_eval_5, [%arg5]) : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg5) <{force = false}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %6:3 = ttcore.load_cached(@main_const_eval_6, [%arg1]) : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>)
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %7 = ttcore.load_cached(@main_const_eval_7, [%arg8]) : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg8) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %8 = ttcore.load_cached(@main_const_eval_8, [%arg18]) : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg18) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %9 = ttcore.load_cached(@main_const_eval_9, [%arg19]) : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg19) <{force = false}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %10 = ttcore.load_cached(@main_const_eval_10, [%arg2]) : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %11 = ttcore.load_cached(@main_const_eval_11, [%arg4]) : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %12 = ttcore.load_cached(@main_const_eval_12, [%arg3]) : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %13 = ttcore.load_cached(@main_const_eval_13, [%arg13]) : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg13) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %14 = ttcore.load_cached(@main_const_eval_14, [%arg20]) : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg20) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %15 = ttcore.load_cached(@main_const_eval_15, []) : () -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %16 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %17 = "ttnn.full"(%16) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 3.25520843E-4 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x7>}> : (!ttnn.device) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %18 = "ttnn.mesh_shard"(%arg6, %16) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg6) <{force = false}> : (tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %19 = "ttnn.mesh_shard"(%arg9, %16) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg9) <{force = false}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %20 = "ttnn.mesh_shard"(%arg11, %16) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg11) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %21 = "ttnn.mesh_shard"(%arg14, %16) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg14) <{force = false}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %22 = "ttnn.mesh_shard"(%arg16, %16) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg16) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %23 = "ttnn.typecast"(%18) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %24 = "ttnn.reshape"(%23) <{shape = [7 : i32]}> : (tensor<1x7xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%23) <{force = false}> : (tensor<1x7xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %25 = "ttnn.from_device"(%24) : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%24) <{force = false}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %26 = "ttnn.to_layout"(%25) <{layout = #ttnn.layout<row_major>}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%25) <{force = false}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> ()
        %27 = "ttnn.to_device"(%26, %16) <{memory_config = #ttnn.memory_config<<dram>, <interleaved>>}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%26) <{force = false}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<system_memory>>>>) -> ()
        %28 = "ttnn.embedding"(%27, %1) : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%27) <{force = false}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %29 = "ttnn.typecast"(%28) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %30 = "ttnn.reshape"(%29) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %31 = "ttnn.pow"(%30, %15) : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%30) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %32 = "ttnn.sum"(%31) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%31) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %33 = "ttnn.multiply"(%32, %17) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%32) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %34 = "ttnn.add"(%33, %6#0) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%33) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%6#0) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %35 = "ttnn.rsqrt"(%34) : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%34) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %36 = "ttnn.reshape"(%35) <{shape = [7 : i32, 1 : i32]}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%35) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %37 = "ttnn.multiply"(%29, %36) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%36) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%29) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %38 = "ttnn.multiply"(%7, %37) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%37) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %39 = "ttnn.typecast"(%38) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%38) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %40 = "ttnn.matmul"(%39, %4) <{transpose_a = false, transpose_b = true}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %41 = "ttnn.reshape"(%40) <{shape = [1 : i32, 7 : i32, 12 : i32, 128 : i32]}> : (tensor<7x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x12x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%40) <{force = false}> : (tensor<7x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %42 = "ttnn.permute"(%41) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x12x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%41) <{force = false}> : (tensor<1x7x12x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %43 = "ttnn.typecast"(%42) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %44 = "ttnn.reshape"(%43) <{shape = [12 : i32, 7 : i32, 128 : i32]}> : (tensor<1x12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%43) <{force = false}> : (tensor<1x12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %45 = "ttnn.reshape"(%21) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%21) <{force = false}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %46 = "ttnn.typecast"(%19) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %47 = "ttnn.reshape"(%46) <{shape = [1 : i32, 1 : i32, 7 : i32]}> : (tensor<7xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%46) <{force = false}> : (tensor<7xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %48 = "ttnn.matmul"(%45, %47) <{transpose_a = false, transpose_b = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%47) <{force = false}> : (tensor<1x1x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%45) <{force = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %49 = "ttnn.permute"(%48) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%48) <{force = false}> : (tensor<1x64x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %50 = "ttnn.concat"(%49, %49) <{dim = 2 : si32}> : (tensor<1x7x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%49) <{force = false}> : (tensor<1x7x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %51 = "ttnn.cos"(%50) : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %52 = "ttnn.multiply"(%44, %51) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%44) <{force = false}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %53 = "ttnn.typecast"(%52) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%52) <{force = false}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %54 = "ttnn.slice_static"(%42) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 12 : i32, 7 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %55 = "ttnn.neg"(%54) : (tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%54) <{force = false}> : (tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %56 = "ttnn.reshape"(%55) <{shape = [12 : i32, 7 : i32, 64 : i32]}> : (tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%55) <{force = false}> : (tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %57 = "ttnn.slice_static"(%42) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 12 : i32, 7 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%42) <{force = false}> : (tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %58 = "ttnn.reshape"(%57) <{shape = [12 : i32, 7 : i32, 64 : i32]}> : (tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%57) <{force = false}> : (tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %59 = "ttnn.concat"(%56, %58) <{dim = 2 : si32}> : (tensor<12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%58) <{force = false}> : (tensor<12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%56) <{force = false}> : (tensor<12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %60 = "ttnn.typecast"(%59) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%59) <{force = false}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %61 = "ttnn.sin"(%50) : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%50) <{force = false}> : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %62 = "ttnn.multiply"(%60, %61) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%60) <{force = false}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %63 = "ttnn.typecast"(%62) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%62) <{force = false}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %64 = "ttnn.add"(%53, %63) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%63) <{force = false}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%53) <{force = false}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %65 = "ttnn.matmul"(%39, %2) <{transpose_a = false, transpose_b = true}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %66 = "ttnn.reshape"(%65) <{shape = [1 : i32, 7 : i32, 4 : i32, 128 : i32]}> : (tensor<7x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x4x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%65) <{force = false}> : (tensor<7x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %67 = "ttnn.permute"(%66) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x4x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%66) <{force = false}> : (tensor<1x7x4x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %68 = "ttnn.typecast"(%67) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %69 = "ttnn.reshape"(%51) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%51) <{force = false}> : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %70 = "ttnn.multiply"(%68, %69) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%69) <{force = false}> : (tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%68) <{force = false}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %71 = "ttnn.typecast"(%70) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%70) <{force = false}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %72 = "ttnn.slice_static"(%67) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 7 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %73 = "ttnn.neg"(%72) : (tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%72) <{force = false}> : (tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %74 = "ttnn.slice_static"(%67) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 7 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%67) <{force = false}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %75 = "ttnn.concat"(%73, %74) <{dim = 3 : si32}> : (tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%74) <{force = false}> : (tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%73) <{force = false}> : (tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %76 = "ttnn.typecast"(%75) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%75) <{force = false}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %77 = "ttnn.reshape"(%61) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%61) <{force = false}> : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %78 = "ttnn.multiply"(%76, %77) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%77) <{force = false}> : (tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%76) <{force = false}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %79 = "ttnn.typecast"(%78) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%78) <{force = false}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %80 = "ttnn.add"(%71, %79) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%79) <{force = false}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%71) <{force = false}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.fill_cache"(%22, %80) <{batch_offset = 0 : i32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%80) <{force = false}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %81 = "ttnn.reshape"(%22) <{shape = [1 : i32, 4 : i32, 1 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%22) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %82 = "ttnn.repeat"(%81) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%81) <{force = false}> : (tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %83 = "ttnn.reshape"(%82) <{shape = [1 : i32, 12 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12288 + d1 * 1024 + d2, d3), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%82) <{force = false}> : (tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %84 = "ttnn.permute"(%83) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12288 + d1 * 1024 + d2, d3), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%83) <{force = false}> : (tensor<1x12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12288 + d1 * 1024 + d2, d3), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %85 = "ttnn.reshape"(%84) <{shape = [12 : i32, 128 : i32, 1024 : i32]}> : (tensor<1x12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%84) <{force = false}> : (tensor<1x12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %86 = "ttnn.matmul"(%64, %85) <{transpose_a = false, transpose_b = false}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%85) <{force = false}> : (tensor<12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%64) <{force = false}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %87 = "ttnn.typecast"(%86) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%86) <{force = false}> : (tensor<12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %88 = "ttnn.multiply"(%87, %13) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%87) <{force = false}> : (tensor<12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %89 = "ttnn.typecast"(%88) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%88) <{force = false}> : (tensor<12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %90 = "ttnn.reshape"(%89) <{shape = [1 : i32, 12 : i32, 7 : i32, 1024 : i32]}> : (tensor<12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%89) <{force = false}> : (tensor<12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %91 = "ttnn.reshape"(%19) <{shape = [1 : i32, 1 : i32, 7 : i32, 1 : i32]}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %92 = "ttnn.typecast"(%91) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x7x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%91) <{force = false}> : (tensor<1x1x7x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %93 = "ttnn.gt"(%3#1, %92) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%92) <{force = false}> : (tensor<1x1x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%3#1) <{force = false}> : (tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %94 = "ttnn.typecast"(%93) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%93) <{force = false}> : (tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %95 = "ttnn.typecast"(%94) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%94) <{force = false}> : (tensor<1x1x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %96 = "ttnn.multiply"(%3#0, %95) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%95) <{force = false}> : (tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%3#0) <{force = false}> : (tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %97 = "ttnn.typecast"(%96) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%96) <{force = false}> : (tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %98 = "ttnn.add"(%90, %97) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%97) <{force = false}> : (tensor<1x1x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%90) <{force = false}> : (tensor<1x12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %99 = "ttnn.typecast"(%98) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%98) <{force = false}> : (tensor<1x12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %100 = "ttnn.softmax"(%99) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%99) <{force = false}> : (tensor<1x12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %101 = "ttnn.typecast"(%100) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%100) <{force = false}> : (tensor<1x12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %102 = "ttnn.reshape"(%101) <{shape = [12 : i32, 7 : i32, 1024 : i32]}> : (tensor<1x12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%101) <{force = false}> : (tensor<1x12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %103 = "ttnn.matmul"(%39, %5) <{transpose_a = false, transpose_b = true}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%39) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %104 = "ttnn.reshape"(%103) <{shape = [1 : i32, 7 : i32, 4 : i32, 128 : i32]}> : (tensor<7x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x4x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%103) <{force = false}> : (tensor<7x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %105 = "ttnn.permute"(%104) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x4x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%104) <{force = false}> : (tensor<1x7x4x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.fill_cache"(%20, %105) <{batch_offset = 0 : i32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%105) <{force = false}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %106 = "ttnn.reshape"(%20) <{shape = [1 : i32, 4 : i32, 1 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %107 = "ttnn.repeat"(%106) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%106) <{force = false}> : (tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %108 = "ttnn.reshape"(%107) <{shape = [12 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 1024 + d1, d2), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%107) <{force = false}> : (tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %109 = "ttnn.matmul"(%102, %108) <{transpose_a = false, transpose_b = false}> : (tensor<12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 1024 + d1, d2), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%108) <{force = false}> : (tensor<12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 1024 + d1, d2), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%102) <{force = false}> : (tensor<12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %110 = "ttnn.reshape"(%109) <{shape = [1 : i32, 12 : i32, 7 : i32, 128 : i32]}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%109) <{force = false}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %111 = "ttnn.concatenate_heads"(%110) : (tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%110) <{force = false}> : (tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %112 = "ttnn.reshape"(%111) <{shape = [7 : i32, 1536 : i32]}> : (tensor<1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%111) <{force = false}> : (tensor<1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %113 = "ttnn.matmul"(%112, %11) <{transpose_a = false, transpose_b = true}> : (tensor<7x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%112) <{force = false}> : (tensor<7x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %114 = "ttnn.reshape"(%113) <{shape = [1 : i32, 1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%113) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %115 = "ttnn.reduce_scatter"(%114, %16) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 3 : si32}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%114) <{force = false}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %116 = "ttnn.all_gather"(%115, %16) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%115) <{force = false}> : (tensor<1x1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %117 = "ttnn.reshape"(%116) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%116) <{force = false}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %118 = "ttnn.add"(%28, %117) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%117) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%28) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %119 = "ttnn.typecast"(%118) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %120 = "ttnn.reshape"(%119) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %121 = "ttnn.pow"(%120, %15) : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%120) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %122 = "ttnn.sum"(%121) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%121) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %123 = "ttnn.multiply"(%122, %17) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%122) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %124 = "ttnn.add"(%123, %6#1) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%123) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%6#1) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %125 = "ttnn.rsqrt"(%124) : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%124) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %126 = "ttnn.reshape"(%125) <{shape = [7 : i32, 1 : i32]}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%125) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %127 = "ttnn.multiply"(%119, %126) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%126) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%119) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %128 = "ttnn.multiply"(%8, %127) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%127) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %129 = "ttnn.typecast"(%128) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%128) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %130 = "ttnn.matmul"(%129, %9) <{transpose_a = false, transpose_b = true}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %131 = "ttnn.typecast"(%130) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %132 = "ttnn.sigmoid"(%130) : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%130) <{force = false}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %133 = "ttnn.typecast"(%132) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%132) <{force = false}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %134 = "ttnn.multiply"(%131, %133) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%133) <{force = false}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%131) <{force = false}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %135 = "ttnn.matmul"(%129, %12) <{transpose_a = false, transpose_b = true}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%129) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %136 = "ttnn.typecast"(%135) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%135) <{force = false}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %137 = "ttnn.multiply"(%134, %136) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%136) <{force = false}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%134) <{force = false}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %138 = "ttnn.typecast"(%137) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%137) <{force = false}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %139 = "ttnn.matmul"(%138, %10) <{transpose_a = false, transpose_b = true}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%138) <{force = false}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %140 = "ttnn.reshape"(%139) <{shape = [1 : i32, 1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%139) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %141 = "ttnn.reduce_scatter"(%140, %16) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 3 : si32}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%140) <{force = false}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %142 = "ttnn.all_gather"(%141, %16) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%141) <{force = false}> : (tensor<1x1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %143 = "ttnn.reshape"(%142) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%142) <{force = false}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %144 = "ttnn.add"(%118, %143) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%143) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%118) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %145 = "ttnn.typecast"(%144) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%144) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %146 = "ttnn.reshape"(%145) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %147 = "ttnn.pow"(%146, %15) : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%146) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %148 = "ttnn.sum"(%147) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%147) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %149 = "ttnn.multiply"(%148, %17) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%148) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %150 = "ttnn.add"(%149, %6#2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%149) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%6#2) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %151 = "ttnn.rsqrt"(%150) : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%150) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %152 = "ttnn.reshape"(%151) <{shape = [7 : i32, 1 : i32]}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%151) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %153 = "ttnn.multiply"(%145, %152) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%152) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%145) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %154 = "ttnn.multiply"(%14, %153) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%153) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %155 = "ttnn.typecast"(%154) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%154) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %156 = "ttnn.matmul"(%155, %0) <{transpose_a = false, transpose_b = true}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%155) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %157 = "ttnn.reshape"(%156) <{shape = [1 : i32, 7 : i32, 128256 : i32]}> : (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %158 = "ttnn.to_layout"(%156) <{layout = #ttnn.layout<row_major>}> : (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%156) <{force = false}> : (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %159 = "ttnn.from_device"(%158) : (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%158) <{force = false}> : (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %160 = "ttnn.mesh_shard"(%159, %16) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%159) <{force = false}> : (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>) -> ()
        %161 = "ttnn.to_layout"(%157) <{layout = #ttnn.layout<row_major>}> : (tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%157) <{force = false}> : (tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %162 = "ttnn.from_device"(%161) : (tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%161) <{force = false}> : (tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %163 = "ttnn.mesh_shard"(%162, %16) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%162) <{force = false}> : (tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>) -> ()
        return %160, %163 : tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>, tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>
      }
    }
  }
}
2025-09-15 15:41:08.495 (  16.821s) [        F073E000]loaded_executable_insta:441      1| LoadedExecutableInstance::PJRT_LoadedExecutable_GetExecutable
2025-09-15 15:41:08.495 (  16.821s) [        F073E000]loaded_executable_insta:460      1| LoadedExecutableInstance::PJRT_LoadedExecutable_AddressableDevices
2025-09-15 15:41:08.496 (  16.822s) [        F073E000]              stubs.inc:76    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2025-09-15 15:41:08.496 (  16.822s) [        F073E000]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-09-15 15:41:08.496 (  16.822s) [        F073E000]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-09-15 15:41:08.496 (  16.822s) [        F073E000]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-09-15 15:41:08.496 (  16.822s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:41:08.496 (  16.822s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:41:08.508 (  16.835s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:41:08.508 (  16.835s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:41:08.519 (  16.845s) [        F2FFD640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:08.519 (  16.845s) [        F2FFD640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:41:08.519 (  16.845s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.845s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.845s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.845s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        6FFFF640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        6FFFF640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        6FFFF640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        6FFFF640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:41:08.519 (  16.846s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.519 (  16.846s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:41:08.520 (  16.846s) [        F2FFD640] executable_instance.cc:138      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-09-15 15:41:08.520 (  16.846s) [        F2FFD640]loaded_executable_insta:497      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-09-15 15:41:08.520 (  16.846s) [        F2FFD640]loaded_executable_insta:81       1| LoadedExecutableInstance::Execute
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Device grid size = { 8, 8 }
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = ttcore.load_cached(@main_const_eval_0, [%arg0]) : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_0
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_0
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_0
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_0
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg10) <{force = false}> : (tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = ttcore.load_cached(@main_const_eval_1, [%arg7]) : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_1
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_1
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_1
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_1
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg7) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = ttcore.load_cached(@main_const_eval_2, [%arg15]) : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_2
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_2
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_2
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_2
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg15) <{force = false}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3:2 = ttcore.load_cached(@main_const_eval_3, [%arg12]) : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> (tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_3
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_3
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.constant"(%0) <{dtype = #ttcore.supportedDataTypes<si32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <interleaved>>, value = dense_resource<__elided__> : tensor<1024xsi32>}> : (!ttnn.device) -> tensor<1024xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.constant"(%0) <{dtype = #ttcore.supportedDataTypes<si32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <interleaved>>, value = dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xsi32>}> : (!ttnn.device) -> tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %4 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<si32>, fill_value = 1 : i32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %5 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %6 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%3) <{force = false}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %7 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32]}> : (tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%4) <{force = false}> : (tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %8 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1024 : i32]}> : (tensor<1024xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.154")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<1024xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.154")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %9 = "ttnn.repeat"(%8) <{repeat_dims = #ttnn.shape<7x1>}> : (tensor<1x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.154")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %10 = "ttnn.reshape"(%2) <{shape = [7 : i32, 1 : i32]}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.156")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.156")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %11 = "ttnn.neg"(%10) : (tensor<7x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("subtract.157_neg"("subtract.157"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%10) <{force = false}> : (tensor<7x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("subtract.157_neg"("subtract.157"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %12 = "ttnn.add"(%8, %11) <{dtype = #ttcore.supportedDataTypes<si32>}> : (tensor<1x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("subtract.157")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%11) <{force = false}> : (tensor<7x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("subtract.157")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("subtract.157")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %13 = "ttnn.typecast"(%12) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1024xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("compare.159_workaround"("compare.159"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%12) <{force = false}> : (tensor<7x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("compare.159_workaround"("compare.159"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %14 = "ttnn.typecast"(%7) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("compare.159_workaround"("compare.159"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("compare.159_workaround"("compare.159"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %15 = "ttnn.ge"(%13, %14) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x1024xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1024xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("compare.159")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("compare.159")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%13) <{force = false}> : (tensor<7x1024xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("compare.159")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %16 = "ttnn.typecast"(%15) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x1024xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("compare.159_workaround"("compare.159"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%15) <{force = false}> : (tensor<7x1024xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("compare.159_workaround"("compare.159"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %17 = "ttnn.reshape"(%5) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.143")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%5) <{force = false}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.143")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %18 = "ttnn.repeat"(%17) <{repeat_dims = #ttnn.shape<7x1024>}> : (tensor<1x1xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("select.161")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%17) <{force = false}> : (tensor<1x1xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("select.161")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %19 = "ttnn.repeat"(%6) <{repeat_dims = #ttnn.shape<7x1024>}> : (tensor<1x1xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("select.161")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x1xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("select.161")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %20 = "ttnn.where"(%16, %18, %19) : (tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("select.161")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%19) <{force = false}> : (tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("select.161")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%18) <{force = false}> : (tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("select.161")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%16) <{force = false}> : (tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("select.161")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %21 = "ttnn.reshape"(%20) <{shape = [1 : i32, 1 : i32, 7 : i32, 1024 : i32]}> : (tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.306_tm0_tm0_tm0"("broadcast.306_tm0_tm0"("broadcast.306_tm0"("broadcast.306"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%20) <{force = false}> : (tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.306_tm0_tm0_tm0"("broadcast.306_tm0_tm0"("broadcast.306_tm0"("broadcast.306"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %22 = "ttnn.typecast"(%21) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.162")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%21) <{force = false}> : (tensor<1x1x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.162")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %23 = "ttnn.reshape"(%9) <{shape = [1 : i32, 1 : i32, 7 : i32, 1024 : i32]}> : (tensor<7x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x1024xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.306_tm0_tm1_tm0_tm0"("broadcast.306_tm0_tm1_tm0"("broadcast.306_tm0_tm1"("broadcast.306_tm0"("broadcast.306")))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%9) <{force = false}> : (tensor<7x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.306_tm0_tm1_tm0_tm0"("broadcast.306_tm0_tm1_tm0"("broadcast.306_tm0_tm1"("broadcast.306_tm0"("broadcast.306")))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %24 = "ttnn.typecast"(%23) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x7x1024xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("compare.131_workaround"("compare.131"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%23) <{force = false}> : (tensor<1x1x7x1024xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("compare.131_workaround"("compare.131"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_3
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_3
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg12) <{force = false}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %4 = ttcore.load_cached(@main_const_eval_4, [%arg17]) : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_4
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_4
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_4
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_4
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg17) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %5 = ttcore.load_cached(@main_const_eval_5, [%arg5]) : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_5
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_5
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_5
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_5
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg5) <{force = false}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %6:3 = ttcore.load_cached(@main_const_eval_6, [%arg1]) : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_6
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_6
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.272_tm0_tm1_tm1_tm0_tm1"("reshape.272_tm0_tm1_tm1_tm0"("reshape.272_tm0_tm1_tm1"("reshape.272_tm0_tm1"("reshape.272_tm0"("reshape.272"))))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.272_tm0_tm1_tm1_tm0_tm1"("reshape.272_tm0_tm1_tm1_tm0"("reshape.272_tm0_tm1_tm1"("reshape.272_tm0_tm1"("reshape.272_tm0"("reshape.272"))))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.repeat"(%2) <{repeat_dims = #ttnn.shape<7x1>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.69")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.69")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 7 : i32]}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.65")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %5 = "ttnn.reshape"(%3) <{shape = [1 : i32, 7 : i32]}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.357")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %6 = "ttnn.reshape"(%3) <{shape = [1 : i32, 7 : i32]}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.421")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%3) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.421")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_6
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_6
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %7 = ttcore.load_cached(@main_const_eval_7, [%arg8]) : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_7
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_7
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.272_tm0_tm0_tm0"("reshape.272_tm0_tm0"("reshape.272_tm0"("reshape.272"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.272_tm0_tm0_tm0"("reshape.272_tm0_tm0"("reshape.272_tm0"("reshape.272"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.81")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.81")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_7
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_7
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg8) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %8 = ttcore.load_cached(@main_const_eval_8, [%arg18]) : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_8
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_8
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.386_tm0_tm0_tm0"("reshape.386_tm0_tm0"("reshape.386_tm0"("reshape.386"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.386_tm0_tm0_tm0"("reshape.386_tm0_tm0"("reshape.386_tm0"("reshape.386"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.373")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.373")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_8
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_8
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg18) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %9 = ttcore.load_cached(@main_const_eval_9, [%arg19]) : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_9
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_9
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_9
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_9
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg19) <{force = false}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %10 = ttcore.load_cached(@main_const_eval_10, [%arg2]) : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_10
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_10
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_10
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_10
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %11 = ttcore.load_cached(@main_const_eval_11, [%arg4]) : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_11
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_11
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_11
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_11
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %12 = ttcore.load_cached(@main_const_eval_12, [%arg3]) : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_12
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_12
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_12
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_12
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %13 = ttcore.load_cached(@main_const_eval_13, [%arg13]) : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_13
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_13
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.300")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.300")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.repeat"(%2) <{repeat_dims = #ttnn.shape<1x12x7x1024>}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.300")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.300")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %4 = "ttnn.reshape"(%3) <{shape = [12 : i32, 7 : i32, 1024 : i32]}> : (tensor<1x12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.299")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.299")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_13
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_13
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg13) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %14 = ttcore.load_cached(@main_const_eval_14, [%arg20]) : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_14
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_14
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.444_tm0_tm0_tm0"("reshape.444_tm0_tm0"("reshape.444_tm0"("reshape.444"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.444_tm0_tm0_tm0"("reshape.444_tm0_tm0"("reshape.444_tm0"("reshape.444"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.437")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.437")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_14
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_14
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg20) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %15 = ttcore.load_cached(@main_const_eval_15, []) : () -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_15
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_15
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_15
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_15
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %16 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %17 = "ttnn.full"(%16) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 3.25520843E-4 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x7>}> : (!ttnn.device) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %18 = "ttnn.mesh_shard"(%arg6, %16) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg6) <{force = false}> : (tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %19 = "ttnn.mesh_shard"(%arg9, %16) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg9) <{force = false}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %20 = "ttnn.mesh_shard"(%arg11, %16) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg11) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %21 = "ttnn.mesh_shard"(%arg14, %16) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg14) <{force = false}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %22 = "ttnn.mesh_shard"(%arg16, %16) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg16) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %23 = "ttnn.typecast"(%18) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.44")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%18) <{force = false}> : (tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.44")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %24 = "ttnn.reshape"(%23) <{shape = [7 : i32]}> : (tensor<1x7xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.44")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%23) <{force = false}> : (tensor<1x7xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.44")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %25 = "ttnn.from_device"(%24) : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>> loc("gather.45_workaround"("gather.45"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%24) <{force = false}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("gather.45_workaround"("gather.45"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %26 = "ttnn.to_layout"(%25) <{layout = #ttnn.layout<row_major>}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<system_memory>>>> loc("gather.45_workaround"("gather.45"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%25) <{force = false}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> () loc("gather.45_workaround"("gather.45"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %27 = "ttnn.to_device"(%26, %16) <{memory_config = #ttnn.memory_config<<dram>, <interleaved>>}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<dram>>, <interleaved>>> loc("gather.45_workaround"("gather.45"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%26) <{force = false}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<system_memory>>>>) -> () loc("gather.45_workaround"("gather.45"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %28 = "ttnn.embedding"(%27, %1) : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("gather.45")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%27) <{force = false}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("gather.45")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("gather.45")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %29 = "ttnn.typecast"(%28) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.47")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %30 = "ttnn.reshape"(%29) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.47")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %31 = "ttnn.pow"(%30, %15) : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("power.49")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%30) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("power.49")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %32 = "ttnn.sum"(%31) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reduce.56")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%31) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reduce.56")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %33 = "ttnn.multiply"(%32, %17) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.65")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%32) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.65")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %34 = "ttnn.add"(%33, %6#0) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.70")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%33) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.70")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%6#0) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.70")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %35 = "ttnn.rsqrt"(%34) : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("rsqrt.71")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%34) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("rsqrt.71")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %36 = "ttnn.reshape"(%35) <{shape = [7 : i32, 1 : i32]}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("rsqrt.71")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%35) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("rsqrt.71")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %37 = "ttnn.multiply"(%29, %36) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.74")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%36) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.74")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%29) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.74")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %38 = "ttnn.multiply"(%7, %37) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.83")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%37) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.83")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.83")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %39 = "ttnn.typecast"(%38) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.84")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%38) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.84")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %40 = "ttnn.matmul"(%39, %4) <{transpose_a = false, transpose_b = true}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.273")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%4) <{force = false}> : (tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.273")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %41 = "ttnn.reshape"(%40) <{shape = [1 : i32, 7 : i32, 12 : i32, 128 : i32]}> : (tensor<7x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x12x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.275")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%40) <{force = false}> : (tensor<7x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.275")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %42 = "ttnn.permute"(%41) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x12x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("transpose.276")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%41) <{force = false}> : (tensor<1x7x12x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("transpose.276")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %43 = "ttnn.typecast"(%42) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.287")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %44 = "ttnn.reshape"(%43) <{shape = [12 : i32, 7 : i32, 128 : i32]}> : (tensor<1x12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.287")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%43) <{force = false}> : (tensor<1x12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.287")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %45 = "ttnn.reshape"(%21) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.188")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%21) <{force = false}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.188")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %46 = "ttnn.typecast"(%19) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.180")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %47 = "ttnn.reshape"(%46) <{shape = [1 : i32, 1 : i32, 7 : i32]}> : (tensor<7xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.180")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%46) <{force = false}> : (tensor<7xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.180")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %48 = "ttnn.matmul"(%45, %47) <{transpose_a = false, transpose_b = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.191")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%47) <{force = false}> : (tensor<1x1x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.191")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%45) <{force = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.191")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %49 = "ttnn.permute"(%48) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("transpose.192")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%48) <{force = false}> : (tensor<1x64x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("transpose.192")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %50 = "ttnn.concat"(%49, %49) <{dim = 2 : si32}> : (tensor<1x7x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("concatenate.193")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%49) <{force = false}> : (tensor<1x7x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("concatenate.193")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %51 = "ttnn.cos"(%50) : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("cosine.220")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %52 = "ttnn.multiply"(%44, %51) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.290")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%44) <{force = false}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.290")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %53 = "ttnn.typecast"(%52) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.291")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%52) <{force = false}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.291")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %54 = "ttnn.slice_static"(%42) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 12 : i32, 7 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("slice.278")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %55 = "ttnn.neg"(%54) : (tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("negate.279")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%54) <{force = false}> : (tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("negate.279")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %56 = "ttnn.reshape"(%55) <{shape = [12 : i32, 7 : i32, 64 : i32]}> : (tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("negate.279")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%55) <{force = false}> : (tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("negate.279")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %57 = "ttnn.slice_static"(%42) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 12 : i32, 7 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("slice.277")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%42) <{force = false}> : (tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("slice.277")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %58 = "ttnn.reshape"(%57) <{shape = [12 : i32, 7 : i32, 64 : i32]}> : (tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("concatenate.280")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%57) <{force = false}> : (tensor<1x12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("concatenate.280")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %59 = "ttnn.concat"(%56, %58) <{dim = 2 : si32}> : (tensor<12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("concatenate.280")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%58) <{force = false}> : (tensor<12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("concatenate.280")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%56) <{force = false}> : (tensor<12x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("concatenate.280")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %60 = "ttnn.typecast"(%59) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.281")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%59) <{force = false}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.281")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %61 = "ttnn.sin"(%50) : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("sine.194")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%50) <{force = false}> : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("sine.194")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %62 = "ttnn.multiply"(%60, %61) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.284")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%60) <{force = false}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.284")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %63 = "ttnn.typecast"(%62) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.285")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%62) <{force = false}> : (tensor<12x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.285")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %64 = "ttnn.add"(%53, %63) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.294")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%63) <{force = false}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.294")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%53) <{force = false}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.294")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %65 = "ttnn.matmul"(%39, %2) <{transpose_a = false, transpose_b = true}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.206")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.206")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %66 = "ttnn.reshape"(%65) <{shape = [1 : i32, 7 : i32, 4 : i32, 128 : i32]}> : (tensor<7x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x4x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.208")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%65) <{force = false}> : (tensor<7x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.208")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %67 = "ttnn.permute"(%66) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x4x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("transpose.209")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%66) <{force = false}> : (tensor<1x7x4x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("transpose.209")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %68 = "ttnn.typecast"(%67) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.226")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %69 = "ttnn.reshape"(%51) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.228")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%51) <{force = false}> : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.228")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %70 = "ttnn.multiply"(%68, %69) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.229")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%69) <{force = false}> : (tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.229")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%68) <{force = false}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.229")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %71 = "ttnn.typecast"(%70) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.230")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%70) <{force = false}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.230")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %72 = "ttnn.slice_static"(%67) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 7 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("slice.211")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %73 = "ttnn.neg"(%72) : (tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("negate.212")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%72) <{force = false}> : (tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("negate.212")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %74 = "ttnn.slice_static"(%67) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 7 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("slice.210")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%67) <{force = false}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("slice.210")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %75 = "ttnn.concat"(%73, %74) <{dim = 3 : si32}> : (tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("concatenate.213")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%74) <{force = false}> : (tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("concatenate.213")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%73) <{force = false}> : (tensor<1x4x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("concatenate.213")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %76 = "ttnn.typecast"(%75) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.214")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%75) <{force = false}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.214")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %77 = "ttnn.reshape"(%61) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.216")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%61) <{force = false}> : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.216")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %78 = "ttnn.multiply"(%76, %77) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.217")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%77) <{force = false}> : (tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.217")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%76) <{force = false}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.217")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %79 = "ttnn.typecast"(%78) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.218")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%78) <{force = false}> : (tensor<1x4x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.218")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %80 = "ttnn.add"(%71, %79) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.233")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%79) <{force = false}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.233")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%71) <{force = false}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.233")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.fill_cache"(%22, %80) <{batch_offset = 0 : i32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("scatter.251")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%80) <{force = false}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("scatter.251")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %81 = "ttnn.reshape"(%22) <{shape = [1 : i32, 4 : i32, 1 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.260")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%22) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.260")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %82 = "ttnn.repeat"(%81) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.260")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%81) <{force = false}> : (tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.260")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %83 = "ttnn.reshape"(%82) <{shape = [1 : i32, 12 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12288 + d1 * 1024 + d2, d3), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.261")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%82) <{force = false}> : (tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.261")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %84 = "ttnn.permute"(%83) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12288 + d1 * 1024 + d2, d3), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("transpose.262")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%83) <{force = false}> : (tensor<1x12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12288 + d1 * 1024 + d2, d3), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("transpose.262")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %85 = "ttnn.reshape"(%84) <{shape = [12 : i32, 128 : i32, 1024 : i32]}> : (tensor<1x12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.264")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%84) <{force = false}> : (tensor<1x12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.264")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %86 = "ttnn.matmul"(%64, %85) <{transpose_a = false, transpose_b = false}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.297")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%85) <{force = false}> : (tensor<12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.297")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%64) <{force = false}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.297")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %87 = "ttnn.typecast"(%86) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.299")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%86) <{force = false}> : (tensor<12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.299")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %88 = "ttnn.multiply"(%87, %13) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.301")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%87) <{force = false}> : (tensor<12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.301")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%13) <{force = false}> : (tensor<12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.301")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %89 = "ttnn.typecast"(%88) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.302")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%88) <{force = false}> : (tensor<12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.302")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %90 = "ttnn.reshape"(%89) <{shape = [1 : i32, 12 : i32, 7 : i32, 1024 : i32]}> : (tensor<12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.302")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%89) <{force = false}> : (tensor<12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.302")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %91 = "ttnn.reshape"(%19) <{shape = [1 : i32, 1 : i32, 7 : i32, 1 : i32]}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.306_tm0_tm1_tm0_tm1"("broadcast.306_tm0_tm1_tm0"("broadcast.306_tm0_tm1"("broadcast.306_tm0"("broadcast.306")))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%19) <{force = false}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.306_tm0_tm1_tm0_tm1"("broadcast.306_tm0_tm1_tm0"("broadcast.306_tm0_tm1"("broadcast.306_tm0"("broadcast.306")))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %92 = "ttnn.typecast"(%91) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x7x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("compare.131_workaround"("compare.131"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%91) <{force = false}> : (tensor<1x1x7x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("compare.131_workaround"("compare.131"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %93 = "ttnn.gt"(%3#1, %92) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("compare.131")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%92) <{force = false}> : (tensor<1x1x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("compare.131")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%3#1) <{force = false}> : (tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("compare.131")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %94 = "ttnn.typecast"(%93) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("compare.131_workaround"("compare.131"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%93) <{force = false}> : (tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("compare.131_workaround"("compare.131"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %95 = "ttnn.typecast"(%94) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.132")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%94) <{force = false}> : (tensor<1x1x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.132")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %96 = "ttnn.multiply"(%3#0, %95) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.163")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%95) <{force = false}> : (tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.163")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%3#0) <{force = false}> : (tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.163")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %97 = "ttnn.typecast"(%96) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.164")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%96) <{force = false}> : (tensor<1x1x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.164")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %98 = "ttnn.add"(%90, %97) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.307")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%97) <{force = false}> : (tensor<1x1x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.307")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%90) <{force = false}> : (tensor<1x12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.307")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %99 = "ttnn.typecast"(%98) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.308")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%98) <{force = false}> : (tensor<1x12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.308")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %100 = "ttnn.softmax"(%99) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("divide.325")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%99) <{force = false}> : (tensor<1x12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("divide.325")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %101 = "ttnn.typecast"(%100) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.326")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%100) <{force = false}> : (tensor<1x12x7x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.326")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %102 = "ttnn.reshape"(%101) <{shape = [12 : i32, 7 : i32, 1024 : i32]}> : (tensor<1x12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.326")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%101) <{force = false}> : (tensor<1x12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.326")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %103 = "ttnn.matmul"(%39, %5) <{transpose_a = false, transpose_b = true}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.86")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%39) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.86")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%5) <{force = false}> : (tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.86")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %104 = "ttnn.reshape"(%103) <{shape = [1 : i32, 7 : i32, 4 : i32, 128 : i32]}> : (tensor<7x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x4x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.88")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%103) <{force = false}> : (tensor<7x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.88")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %105 = "ttnn.permute"(%104) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x4x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("transpose.89")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%104) <{force = false}> : (tensor<1x7x4x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("transpose.89")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.fill_cache"(%20, %105) <{batch_offset = 0 : i32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("scatter.112")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%105) <{force = false}> : (tensor<1x4x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("scatter.112")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %106 = "ttnn.reshape"(%20) <{shape = [1 : i32, 4 : i32, 1 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.121")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.121")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %107 = "ttnn.repeat"(%106) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.121")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%106) <{force = false}> : (tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.121")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %108 = "ttnn.reshape"(%107) <{shape = [12 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 1024 + d1, d2), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.124")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%107) <{force = false}> : (tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.124")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %109 = "ttnn.matmul"(%102, %108) <{transpose_a = false, transpose_b = false}> : (tensor<12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 1024 + d1, d2), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.329")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%108) <{force = false}> : (tensor<12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 1024 + d1, d2), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.329")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%102) <{force = false}> : (tensor<12x7x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.329")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %110 = "ttnn.reshape"(%109) <{shape = [1 : i32, 12 : i32, 7 : i32, 128 : i32]}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.330")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%109) <{force = false}> : (tensor<12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.330")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %111 = "ttnn.concatenate_heads"(%110) : (tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.333")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%110) <{force = false}> : (tensor<1x12x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.333")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %112 = "ttnn.reshape"(%111) <{shape = [7 : i32, 1536 : i32]}> : (tensor<1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.333")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%111) <{force = false}> : (tensor<1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.333")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %113 = "ttnn.matmul"(%112, %11) <{transpose_a = false, transpose_b = true}> : (tensor<7x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.334")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%112) <{force = false}> : (tensor<7x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.334")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%11) <{force = false}> : (tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.334")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %114 = "ttnn.reshape"(%113) <{shape = [1 : i32, 1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.334_reduceScatter_reshape_to_4d"("dot.334_reduceScatter"("dot.334")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%113) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.334_reduceScatter_reshape_to_4d"("dot.334_reduceScatter"("dot.334")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %115 = "ttnn.reduce_scatter"(%114, %16) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 3 : si32}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.334_reduceScatter_reduce_scatter_4d"("dot.334_reduceScatter"("dot.334")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%114) <{force = false}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.334_reduceScatter_reduce_scatter_4d"("dot.334_reduceScatter"("dot.334")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %116 = "ttnn.all_gather"(%115, %16) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.334_all_gather_4d"("dot.334"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%115) <{force = false}> : (tensor<1x1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.334_all_gather_4d"("dot.334"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %117 = "ttnn.reshape"(%116) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.334")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%116) <{force = false}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.334")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %118 = "ttnn.add"(%28, %117) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.338")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%117) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.338")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%28) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.338")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %119 = "ttnn.typecast"(%118) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.339")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %120 = "ttnn.reshape"(%119) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.339")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %121 = "ttnn.pow"(%120, %15) : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("power.341")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%120) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("power.341")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %122 = "ttnn.sum"(%121) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reduce.348")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%121) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reduce.348")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %123 = "ttnn.multiply"(%122, %17) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.357")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%122) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.357")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %124 = "ttnn.add"(%123, %6#1) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.362")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%123) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.362")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%6#1) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.362")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %125 = "ttnn.rsqrt"(%124) : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("rsqrt.363")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%124) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("rsqrt.363")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %126 = "ttnn.reshape"(%125) <{shape = [7 : i32, 1 : i32]}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("rsqrt.363")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%125) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("rsqrt.363")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %127 = "ttnn.multiply"(%119, %126) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.366")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%126) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.366")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%119) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.366")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %128 = "ttnn.multiply"(%8, %127) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.375")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%127) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.375")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.375")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %129 = "ttnn.typecast"(%128) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.376")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%128) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.376")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %130 = "ttnn.matmul"(%129, %9) <{transpose_a = false, transpose_b = true}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.387")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%9) <{force = false}> : (tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.387")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %131 = "ttnn.typecast"(%130) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.391")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %132 = "ttnn.sigmoid"(%130) : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("logistic.389")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%130) <{force = false}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("logistic.389")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %133 = "ttnn.typecast"(%132) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.390")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%132) <{force = false}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.390")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %134 = "ttnn.multiply"(%131, %133) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.392")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%133) <{force = false}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.392")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%131) <{force = false}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.392")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %135 = "ttnn.matmul"(%129, %12) <{transpose_a = false, transpose_b = true}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.378")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%129) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.378")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%12) <{force = false}> : (tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.378")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %136 = "ttnn.typecast"(%135) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.380")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%135) <{force = false}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.380")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %137 = "ttnn.multiply"(%134, %136) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.395")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%136) <{force = false}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.395")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%134) <{force = false}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.395")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %138 = "ttnn.typecast"(%137) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.396")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%137) <{force = false}> : (tensor<7x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.396")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %139 = "ttnn.matmul"(%138, %10) <{transpose_a = false, transpose_b = true}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.398")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%138) <{force = false}> : (tensor<7x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.398")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%10) <{force = false}> : (tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.398")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %140 = "ttnn.reshape"(%139) <{shape = [1 : i32, 1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.398_reduceScatter_reshape_to_4d"("dot.398_reduceScatter"("dot.398")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%139) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.398_reduceScatter_reshape_to_4d"("dot.398_reduceScatter"("dot.398")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %141 = "ttnn.reduce_scatter"(%140, %16) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 3 : si32}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.398_reduceScatter_reduce_scatter_4d"("dot.398_reduceScatter"("dot.398")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%140) <{force = false}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.398_reduceScatter_reduce_scatter_4d"("dot.398_reduceScatter"("dot.398")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %142 = "ttnn.all_gather"(%141, %16) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.398_all_gather_4d"("dot.398"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%141) <{force = false}> : (tensor<1x1x7x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.398_all_gather_4d"("dot.398"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %143 = "ttnn.reshape"(%142) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.398")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%142) <{force = false}> : (tensor<1x1x7x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.398")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %144 = "ttnn.add"(%118, %143) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.402")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%143) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.402")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%118) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.402")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %145 = "ttnn.typecast"(%144) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.403")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%144) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.403")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %146 = "ttnn.reshape"(%145) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.403")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %147 = "ttnn.pow"(%146, %15) : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("power.405")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%146) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("power.405")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("power.405")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %148 = "ttnn.sum"(%147) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reduce.412")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%147) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reduce.412")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %149 = "ttnn.multiply"(%148, %17) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.421")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%148) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.421")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%17) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.421")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %150 = "ttnn.add"(%149, %6#2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.426")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%149) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.426")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%6#2) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.426")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %151 = "ttnn.rsqrt"(%150) : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("rsqrt.427")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%150) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("rsqrt.427")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %152 = "ttnn.reshape"(%151) <{shape = [7 : i32, 1 : i32]}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("rsqrt.427")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%151) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("rsqrt.427")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %153 = "ttnn.multiply"(%145, %152) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.430")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%152) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.430")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%145) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.430")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %154 = "ttnn.multiply"(%14, %153) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.439")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%153) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.439")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.439")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %155 = "ttnn.typecast"(%154) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.440")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%154) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.440")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %156 = "ttnn.matmul"(%155, %0) <{transpose_a = false, transpose_b = true}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.445")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%155) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.445")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%0) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.445")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %157 = "ttnn.reshape"(%156) <{shape = [1 : i32, 7 : i32, 128256 : i32]}> : (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.446")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %158 = "ttnn.to_layout"(%156) <{layout = #ttnn.layout<row_major>}> : (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%156) <{force = false}> : (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %159 = "ttnn.from_device"(%158) : (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%158) <{force = false}> : (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %160 = "ttnn.mesh_shard"(%159, %16) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%159) <{force = false}> : (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %161 = "ttnn.to_layout"(%157) <{layout = #ttnn.layout<row_major>}> : (tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%157) <{force = false}> : (tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %162 = "ttnn.from_device"(%161) : (tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%161) <{force = false}> : (tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %163 = "ttnn.mesh_shard"(%162, %16) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%162) <{force = false}> : (tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main
2025-09-15 15:42:00.645 (  68.971s) [        F2FFD640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:00.645 (  68.971s) [        F2FFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:00.645 (  68.971s) [        F2FFD640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:00.645 (  68.971s) [        F2FFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:00.645 (  68.971s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:00.645 (  68.971s) [        F2FFD640]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:00.645 (  68.971s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:00.645 (  68.971s) [        F2FFD640]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:00.645 (  68.971s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:00.645 (  68.971s) [        F2FFD640]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:00.645 (  68.971s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:00.645 (  68.971s) [        F2FFD640]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:00.645 (  68.971s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:00.645 (  68.971s) [        F2FFD640]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:00.645 (  68.971s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:00.645 (  68.971s) [        F2FFD640]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:00.645 (  68.971s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:00.645 (  68.971s) [        F2FFD640]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:00.645 (  68.971s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:00.645 (  68.971s) [        F2FFD640]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:00.645 (  68.971s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:00.645 (  68.971s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:00.645 (  68.971s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:42:00.645 (  68.971s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:00.645 (  68.972s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:42:00.645 (  68.972s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User is requesting to copy the data from a runtime tensor with data type: Int32 into buffer with expected data type: Int64, the values will be casted, this may impact the throughput and the integrity of the data.
2025-09-15 15:42:00.646 (  68.972s) [        FA6C4640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:00.647 (  68.973s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:00.647 (  68.973s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:00.647 (  68.973s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:42:00.647 (  68.973s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:00.647 (  68.973s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:42:00.647 (  68.973s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User is requesting to copy the data from a runtime tensor with data type: Int32 into buffer with expected data type: Int64, the values will be casted, this may impact the throughput and the integrity of the data.
2025-09-15 15:42:00.647 (  68.974s) [        F1FFB640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:00.648 (  68.974s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:00.648 (  68.974s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:00.648 (  68.974s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:00.653 (  68.980s) [        F073E000]     client_instance.cc:428      1| ClientInstance::PJRT_Client_Compile
2025-09-15 15:42:00.653 (  68.980s) [        F073E000]      module_builder.cc:103      1| ModuleBuilder::buildModule
2025-09-15 15:42:00.654 (  68.980s) [        F073E000]      module_builder.cc:165      1| VHLO Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) {
    %0 = "vhlo.custom_call_v1"(%arg0) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"xla.sdy.FuncResultSharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<true>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {}, {}, {}]>]>">}>} : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>
    "vhlo.return_v1"(%0) : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>]>, res_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, sym_visibility = #vhlo.string_v1<"">}
}
2025-09-15 15:42:00.655 (  68.982s) [        F073E000]      module_builder.cc:185      1| Is using shardy? true
2025-09-15 15:42:00.658 (  68.984s) [        F073E000]      module_builder.cc:203      1| SHLO Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}) -> (tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>}) {
    return %arg0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:00.658 (  68.984s) [        F073E000]      module_builder.cc:212      1| SHLO Module after frontend StableHLO pipeline:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}) -> (tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>}) {
    return %arg0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:00.662 (  68.988s) [        F073E000]      module_builder.cc:486      1| SHLO Module after compiler StableHLO pipeline:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = sdy.manual_computation(%arg0) in_shardings=[<@mesh, [{}, {"_axis_0"}, {}, {}]>] out_shardings=[<@mesh, [{}, {}, {}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg1: tensor<1x4x1024x128xbf16>) {
      %1 = "stablehlo.all_gather"(%arg1) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> : (tensor<1x4x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
      sdy.return %1 : tensor<1x8x1024x128xbf16>
    } : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    return %0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:00.664 (  68.991s) [        F073E000]      module_builder.cc:510      1| TTIR Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = ttir.empty() : tensor<1x4x1024x128xbf16>
    %1 = "ttir.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16>, tensor<1x4x1024x128xbf16>) -> tensor<1x4x1024x128xbf16>
    %2 = ttir.empty() : tensor<1x8x1024x128xbf16>
    %3 = "ttir.all_gather"(%1, %2) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<1x4x1024x128xbf16>, tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %4 = ttir.empty() : tensor<1x8x1024x128xbf16>
    %5 = "ttir.mesh_shard"(%3, %4) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16>, tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    return %5 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:00.665 (  68.991s) [        F073E000]      module_builder.cc:561   WARN| `mhlo.num_partitions` attribute not found, assuming default number of partitions: 1
2025-09-15 15:42:00.665 (  68.991s) [        F073E000]      module_builder.cc:575   WARN| `mhlo.num_replicas` attribute not found, assuming default number of replicas: 1
2025-09-15 15:42:00.665 (  68.991s) [        F073E000]      module_builder.cc:585   WARN| Num replicas and num partitions are not set, inferring the number of devices from mesh shape
2025-09-15 15:42:00.696 (  69.023s) [        F073E000]      module_builder.cc:630      1| TTNN Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073185088, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073193408, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x16, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 2)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 2, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %2 = "ttnn.all_gather"(%1, %0) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.mesh_shard"(%2, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %3 : tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
    }
  }
}
2025-09-15 15:42:00.708 (  69.035s) [        F073E000]loaded_executable_insta:441      1| LoadedExecutableInstance::PJRT_LoadedExecutable_GetExecutable
2025-09-15 15:42:00.708 (  69.035s) [        F073E000]loaded_executable_insta:460      1| LoadedExecutableInstance::PJRT_LoadedExecutable_AddressableDevices
2025-09-15 15:42:00.709 (  69.035s) [        F073E000]              stubs.inc:76    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2025-09-15 15:42:00.709 (  69.035s) [        F073E000]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-09-15 15:42:00.709 (  69.035s) [        F073E000]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-09-15 15:42:00.709 (  69.035s) [        F073E000]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-09-15 15:42:00.709 (  69.035s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:00.709 (  69.035s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:00.712 (  69.039s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:00.712 (  69.039s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:00.714 (  69.041s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:00.714 (  69.041s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:00.714 (  69.041s) [        F073E000]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:00.714 (  69.041s) [        F073E000]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:00.714 (  69.041s) [        F073E000]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:00.714 (  69.041s) [        F073E000]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:00.714 (  69.041s) [        F073E000] executable_instance.cc:138      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-09-15 15:42:00.714 (  69.041s) [        F073E000]loaded_executable_insta:497      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-09-15 15:42:00.714 (  69.041s) [        F073E000]loaded_executable_insta:81       1| LoadedExecutableInstance::Execute
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Device grid size = { 8, 8 }
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.all_gather"(%1, %0) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("p0.1")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("p0.1")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.mesh_shard"(%2, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main
2025-09-15 15:42:01.003 (  69.329s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:01.003 (  69.329s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:01.003 (  69.330s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:01.003 (  69.330s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:01.003 (  69.330s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:01.003 (  69.330s) [        F073E000]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:01.003 (  69.330s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:01.003 (  69.330s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:01.003 (  69.330s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:01.003 (  69.330s) [        F073E000]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:01.003 (  69.330s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:01.003 (  69.330s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:01.003 (  69.330s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:01.003 (  69.330s) [        F073E000] executable_instance.cc:58       1| ExecutableInstance::PJRT_Executable_Destroy
2025-09-15 15:42:01.003 (  69.330s) [        F073E000]loaded_executable_insta:432      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Destroy
2025-09-15 15:42:01.003 (  69.330s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:42:01.003 (  69.330s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:01.004 (  69.330s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:01.004 (  69.330s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:42:01.004 (  69.330s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:01.004 (  69.330s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:01.005 (  69.331s) [        117FA640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:01.008 (  69.334s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:01.008 (  69.334s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:01.008 (  69.334s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:01.014 (  69.340s) [        F073E000]     client_instance.cc:428      1| ClientInstance::PJRT_Client_Compile
2025-09-15 15:42:01.014 (  69.340s) [        F073E000]      module_builder.cc:103      1| ModuleBuilder::buildModule
2025-09-15 15:42:01.014 (  69.341s) [        F073E000]      module_builder.cc:165      1| VHLO Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) {
    %0 = "vhlo.custom_call_v1"(%arg0) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"xla.sdy.FuncResultSharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<true>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {}, {}, {}]>]>">}>} : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>
    "vhlo.return_v1"(%0) : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>]>, res_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, sym_visibility = #vhlo.string_v1<"">}
}
2025-09-15 15:42:01.016 (  69.342s) [        F073E000]      module_builder.cc:185      1| Is using shardy? true
2025-09-15 15:42:01.018 (  69.344s) [        F073E000]      module_builder.cc:203      1| SHLO Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}) -> (tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>}) {
    return %arg0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:01.018 (  69.345s) [        F073E000]      module_builder.cc:212      1| SHLO Module after frontend StableHLO pipeline:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}) -> (tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>}) {
    return %arg0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:01.022 (  69.349s) [        F073E000]      module_builder.cc:486      1| SHLO Module after compiler StableHLO pipeline:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = sdy.manual_computation(%arg0) in_shardings=[<@mesh, [{}, {"_axis_0"}, {}, {}]>] out_shardings=[<@mesh, [{}, {}, {}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg1: tensor<1x4x1024x128xbf16>) {
      %1 = "stablehlo.all_gather"(%arg1) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> : (tensor<1x4x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
      sdy.return %1 : tensor<1x8x1024x128xbf16>
    } : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    return %0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:01.025 (  69.351s) [        F073E000]      module_builder.cc:510      1| TTIR Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = ttir.empty() : tensor<1x4x1024x128xbf16>
    %1 = "ttir.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16>, tensor<1x4x1024x128xbf16>) -> tensor<1x4x1024x128xbf16>
    %2 = ttir.empty() : tensor<1x8x1024x128xbf16>
    %3 = "ttir.all_gather"(%1, %2) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<1x4x1024x128xbf16>, tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %4 = ttir.empty() : tensor<1x8x1024x128xbf16>
    %5 = "ttir.mesh_shard"(%3, %4) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16>, tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    return %5 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:01.025 (  69.351s) [        F073E000]      module_builder.cc:561   WARN| `mhlo.num_partitions` attribute not found, assuming default number of partitions: 1
2025-09-15 15:42:01.025 (  69.351s) [        F073E000]      module_builder.cc:575   WARN| `mhlo.num_replicas` attribute not found, assuming default number of replicas: 1
2025-09-15 15:42:01.025 (  69.351s) [        F073E000]      module_builder.cc:585   WARN| Num replicas and num partitions are not set, inferring the number of devices from mesh shape
2025-09-15 15:42:01.056 (  69.382s) [        F073E000]      module_builder.cc:630      1| TTNN Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073185088, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073193408, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x16, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 2)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 2, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %2 = "ttnn.all_gather"(%1, %0) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.mesh_shard"(%2, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %3 : tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
    }
  }
}
2025-09-15 15:42:01.068 (  69.394s) [        F073E000]loaded_executable_insta:441      1| LoadedExecutableInstance::PJRT_LoadedExecutable_GetExecutable
2025-09-15 15:42:01.068 (  69.394s) [        F073E000]loaded_executable_insta:460      1| LoadedExecutableInstance::PJRT_LoadedExecutable_AddressableDevices
2025-09-15 15:42:01.069 (  69.395s) [        F073E000]              stubs.inc:76    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2025-09-15 15:42:01.069 (  69.395s) [        F073E000]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-09-15 15:42:01.069 (  69.395s) [        F073E000]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-09-15 15:42:01.069 (  69.395s) [        F073E000]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-09-15 15:42:01.069 (  69.395s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:01.069 (  69.395s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:01.072 (  69.398s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:01.072 (  69.398s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:01.073 (  69.400s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:01.074 (  69.400s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:01.074 (  69.400s) [        F073E000]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:01.074 (  69.400s) [        F073E000]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:01.074 (  69.400s) [        F073E000]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:01.074 (  69.400s) [        F073E000]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:01.074 (  69.400s) [        F073E000] executable_instance.cc:138      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-09-15 15:42:01.074 (  69.400s) [        F073E000]loaded_executable_insta:497      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-09-15 15:42:01.074 (  69.400s) [        F073E000]loaded_executable_insta:81       1| LoadedExecutableInstance::Execute
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Device grid size = { 8, 8 }
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.all_gather"(%1, %0) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("p0.1")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("p0.1")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.mesh_shard"(%2, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main
2025-09-15 15:42:01.443 (  69.770s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:01.443 (  69.770s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:01.444 (  69.770s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:01.444 (  69.770s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:01.444 (  69.770s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:01.444 (  69.770s) [        F073E000]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:01.444 (  69.770s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:01.444 (  69.770s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:01.444 (  69.770s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:01.444 (  69.770s) [        F073E000]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:01.444 (  69.770s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:01.444 (  69.770s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:01.444 (  69.770s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:01.444 (  69.770s) [        F073E000] executable_instance.cc:58       1| ExecutableInstance::PJRT_Executable_Destroy
2025-09-15 15:42:01.444 (  69.770s) [        F073E000]loaded_executable_insta:432      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Destroy
2025-09-15 15:42:01.444 (  69.770s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:42:01.444 (  69.770s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:01.444 (  69.770s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:01.444 (  69.771s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:42:01.444 (  69.771s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:01.444 (  69.771s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:01.445 (  69.771s) [        10FF9640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:01.448 (  69.775s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:01.448 (  69.775s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:01.448 (  69.775s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:42:01.448 (  69.775s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:01.448 (  69.775s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:01.448 (  69.775s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:42:01.449 (  69.775s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:01.449 (  69.775s) [        10FF9640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:01.451 (  69.777s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:01.451 (  69.777s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:01.451 (  69.777s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:42:01.451 (  69.777s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:01.451 (  69.777s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:42:01.451 (  69.778s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:01.452 (  69.778s) [        127FC640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:01.454 (  69.781s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:01.454 (  69.781s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:01.454 (  69.781s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:01.454 (  69.781s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:01.454 (  69.781s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:42:01.454 (  69.781s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:01.454 (  69.781s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:42:01.455 (  69.781s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User is requesting to copy the data from a runtime tensor with data type: Int32 into buffer with expected data type: Int64, the values will be casted, this may impact the throughput and the integrity of the data.
2025-09-15 15:42:01.455 (  69.781s) [        FA6C4640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:01.455 (  69.781s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:01.455 (  69.782s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:02.028 (  70.354s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:02.028 (  70.354s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:02.028 (  70.354s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:02.034 (  70.360s) [        F073E000]     client_instance.cc:428      1| ClientInstance::PJRT_Client_Compile
2025-09-15 15:42:02.034 (  70.360s) [        F073E000]      module_builder.cc:103      1| ModuleBuilder::buildModule
2025-09-15 15:42:02.034 (  70.361s) [        F073E000]      module_builder.cc:165      1| VHLO Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) {
    %0 = "vhlo.custom_call_v1"(%arg0) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"xla.sdy.FuncResultSharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<true>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {}, {}, {}]>]>">}>} : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>
    "vhlo.return_v1"(%0) : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>]>, res_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, sym_visibility = #vhlo.string_v1<"">}
}
2025-09-15 15:42:02.036 (  70.362s) [        F073E000]      module_builder.cc:185      1| Is using shardy? true
2025-09-15 15:42:02.038 (  70.364s) [        F073E000]      module_builder.cc:203      1| SHLO Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}) -> (tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>}) {
    return %arg0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:02.038 (  70.365s) [        F073E000]      module_builder.cc:212      1| SHLO Module after frontend StableHLO pipeline:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}) -> (tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>}) {
    return %arg0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:02.042 (  70.369s) [        F073E000]      module_builder.cc:486      1| SHLO Module after compiler StableHLO pipeline:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = sdy.manual_computation(%arg0) in_shardings=[<@mesh, [{}, {"_axis_0"}, {}, {}]>] out_shardings=[<@mesh, [{}, {}, {}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg1: tensor<1x4x1024x128xbf16>) {
      %1 = "stablehlo.all_gather"(%arg1) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> : (tensor<1x4x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
      sdy.return %1 : tensor<1x8x1024x128xbf16>
    } : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    return %0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:02.045 (  70.371s) [        F073E000]      module_builder.cc:510      1| TTIR Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = ttir.empty() : tensor<1x4x1024x128xbf16>
    %1 = "ttir.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16>, tensor<1x4x1024x128xbf16>) -> tensor<1x4x1024x128xbf16>
    %2 = ttir.empty() : tensor<1x8x1024x128xbf16>
    %3 = "ttir.all_gather"(%1, %2) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<1x4x1024x128xbf16>, tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %4 = ttir.empty() : tensor<1x8x1024x128xbf16>
    %5 = "ttir.mesh_shard"(%3, %4) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16>, tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    return %5 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:02.045 (  70.371s) [        F073E000]      module_builder.cc:561   WARN| `mhlo.num_partitions` attribute not found, assuming default number of partitions: 1
2025-09-15 15:42:02.045 (  70.371s) [        F073E000]      module_builder.cc:575   WARN| `mhlo.num_replicas` attribute not found, assuming default number of replicas: 1
2025-09-15 15:42:02.045 (  70.371s) [        F073E000]      module_builder.cc:585   WARN| Num replicas and num partitions are not set, inferring the number of devices from mesh shape
2025-09-15 15:42:02.076 (  70.402s) [        F073E000]      module_builder.cc:630      1| TTNN Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073185088, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073193408, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x16, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 2)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 2, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %2 = "ttnn.all_gather"(%1, %0) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.mesh_shard"(%2, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %3 : tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
    }
  }
}
2025-09-15 15:42:02.087 (  70.414s) [        F073E000]loaded_executable_insta:441      1| LoadedExecutableInstance::PJRT_LoadedExecutable_GetExecutable
2025-09-15 15:42:02.087 (  70.414s) [        F073E000]loaded_executable_insta:460      1| LoadedExecutableInstance::PJRT_LoadedExecutable_AddressableDevices
2025-09-15 15:42:02.088 (  70.414s) [        F073E000]              stubs.inc:76    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2025-09-15 15:42:02.088 (  70.414s) [        F073E000]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-09-15 15:42:02.088 (  70.414s) [        F073E000]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-09-15 15:42:02.088 (  70.414s) [        F073E000]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-09-15 15:42:02.088 (  70.414s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:02.088 (  70.414s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:02.091 (  70.417s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:02.091 (  70.417s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:02.093 (  70.419s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:02.093 (  70.419s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:02.093 (  70.419s) [        F073E000]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:02.093 (  70.419s) [        F073E000]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:02.093 (  70.419s) [        F073E000]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:02.093 (  70.419s) [        F073E000]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:02.093 (  70.419s) [        F073E000] executable_instance.cc:138      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-09-15 15:42:02.093 (  70.419s) [        F073E000]loaded_executable_insta:497      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-09-15 15:42:02.093 (  70.419s) [        F073E000]loaded_executable_insta:81       1| LoadedExecutableInstance::Execute
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Device grid size = { 8, 8 }
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.all_gather"(%1, %0) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("p0.1")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("p0.1")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.mesh_shard"(%2, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main
2025-09-15 15:42:02.375 (  70.701s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:02.375 (  70.701s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:02.375 (  70.702s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:02.375 (  70.702s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:02.375 (  70.702s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:02.375 (  70.702s) [        F073E000]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:02.375 (  70.702s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:02.375 (  70.702s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:02.375 (  70.702s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:02.375 (  70.702s) [        F073E000]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:02.375 (  70.702s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:02.375 (  70.702s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:02.375 (  70.702s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:02.375 (  70.702s) [        F073E000] executable_instance.cc:58       1| ExecutableInstance::PJRT_Executable_Destroy
2025-09-15 15:42:02.375 (  70.702s) [        F073E000]loaded_executable_insta:432      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Destroy
2025-09-15 15:42:02.375 (  70.702s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:42:02.375 (  70.702s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:02.375 (  70.702s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:02.376 (  70.702s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:42:02.376 (  70.702s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:02.376 (  70.702s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:02.376 (  70.703s) [        11FFB640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:02.380 (  70.706s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:02.380 (  70.706s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:02.380 (  70.707s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:02.386 (  70.712s) [        F073E000]     client_instance.cc:428      1| ClientInstance::PJRT_Client_Compile
2025-09-15 15:42:02.386 (  70.712s) [        F073E000]      module_builder.cc:103      1| ModuleBuilder::buildModule
2025-09-15 15:42:02.387 (  70.713s) [        F073E000]      module_builder.cc:165      1| VHLO Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) {
    %0 = "vhlo.custom_call_v1"(%arg0) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"xla.sdy.FuncResultSharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<true>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {}, {}, {}]>]>">}>} : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>
    "vhlo.return_v1"(%0) : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>]>, res_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, sym_visibility = #vhlo.string_v1<"">}
}
2025-09-15 15:42:02.388 (  70.714s) [        F073E000]      module_builder.cc:185      1| Is using shardy? true
2025-09-15 15:42:02.390 (  70.717s) [        F073E000]      module_builder.cc:203      1| SHLO Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}) -> (tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>}) {
    return %arg0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:02.391 (  70.717s) [        F073E000]      module_builder.cc:212      1| SHLO Module after frontend StableHLO pipeline:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}) -> (tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>}) {
    return %arg0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:02.395 (  70.721s) [        F073E000]      module_builder.cc:486      1| SHLO Module after compiler StableHLO pipeline:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = sdy.manual_computation(%arg0) in_shardings=[<@mesh, [{}, {"_axis_0"}, {}, {}]>] out_shardings=[<@mesh, [{}, {}, {}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg1: tensor<1x4x1024x128xbf16>) {
      %1 = "stablehlo.all_gather"(%arg1) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> : (tensor<1x4x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
      sdy.return %1 : tensor<1x8x1024x128xbf16>
    } : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    return %0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:02.398 (  70.724s) [        F073E000]      module_builder.cc:510      1| TTIR Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = ttir.empty() : tensor<1x4x1024x128xbf16>
    %1 = "ttir.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16>, tensor<1x4x1024x128xbf16>) -> tensor<1x4x1024x128xbf16>
    %2 = ttir.empty() : tensor<1x8x1024x128xbf16>
    %3 = "ttir.all_gather"(%1, %2) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<1x4x1024x128xbf16>, tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %4 = ttir.empty() : tensor<1x8x1024x128xbf16>
    %5 = "ttir.mesh_shard"(%3, %4) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16>, tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    return %5 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:02.398 (  70.724s) [        F073E000]      module_builder.cc:561   WARN| `mhlo.num_partitions` attribute not found, assuming default number of partitions: 1
2025-09-15 15:42:02.398 (  70.725s) [        F073E000]      module_builder.cc:575   WARN| `mhlo.num_replicas` attribute not found, assuming default number of replicas: 1
2025-09-15 15:42:02.398 (  70.725s) [        F073E000]      module_builder.cc:585   WARN| Num replicas and num partitions are not set, inferring the number of devices from mesh shape
2025-09-15 15:42:02.429 (  70.756s) [        F073E000]      module_builder.cc:630      1| TTNN Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073185088, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073193408, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x16, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 2)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 2, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %2 = "ttnn.all_gather"(%1, %0) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.mesh_shard"(%2, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %3 : tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
    }
  }
}
2025-09-15 15:42:02.442 (  70.769s) [        F073E000]loaded_executable_insta:441      1| LoadedExecutableInstance::PJRT_LoadedExecutable_GetExecutable
2025-09-15 15:42:02.442 (  70.769s) [        F073E000]loaded_executable_insta:460      1| LoadedExecutableInstance::PJRT_LoadedExecutable_AddressableDevices
2025-09-15 15:42:02.443 (  70.770s) [        F073E000]              stubs.inc:76    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2025-09-15 15:42:02.443 (  70.770s) [        F073E000]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-09-15 15:42:02.443 (  70.770s) [        F073E000]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-09-15 15:42:02.443 (  70.770s) [        F073E000]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-09-15 15:42:02.443 (  70.770s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:02.443 (  70.770s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:02.446 (  70.773s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:02.446 (  70.773s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:02.448 (  70.775s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:02.448 (  70.775s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:02.448 (  70.775s) [        F073E000]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:02.448 (  70.775s) [        F073E000]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:02.448 (  70.775s) [        F073E000]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:02.448 (  70.775s) [        F073E000]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:02.448 (  70.775s) [        F073E000] executable_instance.cc:138      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-09-15 15:42:02.448 (  70.775s) [        F073E000]loaded_executable_insta:497      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-09-15 15:42:02.448 (  70.775s) [        F073E000]loaded_executable_insta:81       1| LoadedExecutableInstance::Execute
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Device grid size = { 8, 8 }
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.all_gather"(%1, %0) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("p0.1")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("p0.1")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.mesh_shard"(%2, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main
2025-09-15 15:42:02.815 (  71.142s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:02.815 (  71.142s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:02.816 (  71.142s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:02.816 (  71.142s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:02.816 (  71.142s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:02.816 (  71.142s) [        F073E000]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:02.816 (  71.142s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:02.816 (  71.142s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:02.816 (  71.142s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:02.816 (  71.142s) [        F073E000]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:02.816 (  71.142s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:02.816 (  71.142s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:02.816 (  71.142s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:02.816 (  71.142s) [        F073E000] executable_instance.cc:58       1| ExecutableInstance::PJRT_Executable_Destroy
2025-09-15 15:42:02.816 (  71.142s) [        F073E000]loaded_executable_insta:432      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Destroy
2025-09-15 15:42:02.816 (  71.142s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:42:02.816 (  71.142s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:02.816 (  71.142s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:02.816 (  71.143s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:42:02.816 (  71.143s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:02.816 (  71.143s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:02.817 (  71.144s) [        117FA640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy

Args after sync (tensor([[128000,     40,   1093,   4737,  23291,    304,    279]],
       device='xla:0'), tensor([0, 1, 2, 3, 4, 5, 6], device='xla:0'), tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='xla:0',
       dtype=torch.bfloat16), tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='xla:0',
       dtype=torch.bfloat16))
CausalLMOutputWithPast(loss=None, logits=tensor([[[-0.8633, -2.3438, -0.5430,  ...,  0.8477,  0.8477,  0.8477],
         [-0.1650,  0.2227,  0.8438,  ..., -0.8867, -0.8867, -0.8867],
         [ 1.8047, -1.4062, -0.8906,  ..., -1.4062, -1.4062, -1.4062],
         ...,
         [ 0.0461, -0.8633, -1.4531,  ..., -0.6562, -0.6484, -0.6484],
         [ 0.0527, -1.0859, -1.1094,  ..., -2.4062, -2.4062, -2.4062],
         [-1.1250,  0.0242, -0.7617,  ..., -1.5078, -1.5156, -1.5078]]],
       device='xla:0', dtype=torch.bfloat16), past_key_values=<transformers.cache_utils.StaticCache object at 0x7f24ac6c3e50>, hidden_states=None, attentions=None)
Generated token:  the
Example inputs: [tensor([[279]], device='xla:0'), tensor([7], device='xla:0'), tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='xla:0',
       dtype=torch.bfloat16), tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='xla:0',
       dtype=torch.bfloat16)]
gm graph before export for training
opcode         name                                                                  target                                                                       args                                                                                                                                                   kwargs
-------------  --------------------------------------------------------------------  ---------------------------------------------------------------------------  -----------------------------------------------------------------------------------------------------------------------------------------------------  -------------------------------------------------------------------------------------------------------
placeholder    l_kwargs_input_ids_                                                   L_kwargs_input_ids_                                                          ()                                                                                                                                                     {}
placeholder    l_kwargs_cache_position_                                              L_kwargs_cache_position_                                                     ()                                                                                                                                                     {}
placeholder    l_kwargs_past_key_values_key_cache_0_                                 L_kwargs_past_key_values_key_cache_0_                                        ()                                                                                                                                                     {}
placeholder    l_kwargs_past_key_values_value_cache_0_                               L_kwargs_past_key_values_value_cache_0_                                      ()                                                                                                                                                     {}
call_module    inputs_embeds                                                         L__self___model_embed_tokens                                                 (l_kwargs_input_ids_,)                                                                                                                                 {}
call_method    position_ids                                                          unsqueeze                                                                    (l_kwargs_cache_position_, 0)                                                                                                                          {}
call_function  getitem                                                               <built-in function getitem>                                                  (l_kwargs_past_key_values_key_cache_0_, (0, 0))                                                                                                        {}
call_method    any_1                                                                 any                                                                          (getitem,)                                                                                                                                             {'dim': -1}
call_method    past_seen_tokens                                                      sum                                                                          (any_1,)                                                                                                                                               {}
call_function  causal_mask                                                           <built-in method full of type object at 0x7f26e82f6f80>                      ((1, 1024),)                                                                                                                                           {'fill_value': -3.3895313892515355e+38, 'dtype': torch.bfloat16, 'device': device(type='xla', index=0)}
call_function  arange                                                                <built-in method arange of type object at 0x7f26e82f6f80>                    (1024,)                                                                                                                                                {'device': device(type='xla', index=0)}
call_method    reshape                                                               reshape                                                                      (l_kwargs_cache_position_, -1, 1)                                                                                                                      {}
call_function  gt                                                                    <built-in function gt>                                                       (arange, reshape)                                                                                                                                      {}
call_function  causal_mask_1                                                         <built-in function imul>                                                     (causal_mask, gt)                                                                                                                                      {}
call_function  getitem_1                                                             <built-in function getitem>                                                  (causal_mask_1, (None, None, slice(None, None, None), slice(None, None, None)))                                                                        {}
call_method    causal_mask_2                                                         expand                                                                       (getitem_1, 1, 1, -1, -1)                                                                                                                              {}
get_attr       l__self___model_rotary_emb_inv_freq                                   L__self___model_rotary_emb_inv_freq                                          ()                                                                                                                                                     {}
call_function  getitem_2                                                             <built-in function getitem>                                                  (l__self___model_rotary_emb_inv_freq, (None, slice(None, None, None), None))                                                                           {}
call_method    float_1                                                               float                                                                        (getitem_2,)                                                                                                                                           {}
call_method    expand_1                                                              expand                                                                       (float_1, 1, -1, 1)                                                                                                                                    {}
call_method    inv_freq_expanded                                                     to                                                                           (expand_1, device(type='xla', index=0))                                                                                                                {}
call_function  getitem_3                                                             <built-in function getitem>                                                  (position_ids, (slice(None, None, None), None, slice(None, None, None)))                                                                               {}
call_method    position_ids_expanded                                                 float                                                                        (getitem_3,)                                                                                                                                           {}
call_function  _enter_autocast                                                       <function _enter_autocast at 0x7f26224fa160>                                 ('xla', None, False, None)                                                                                                                             {}
call_method    float_3                                                               float                                                                        (inv_freq_expanded,)                                                                                                                                   {}
call_method    float_4                                                               float                                                                        (position_ids_expanded,)                                                                                                                               {}
call_function  matmul                                                                <built-in function matmul>                                                   (float_3, float_4)                                                                                                                                     {}
call_method    freqs                                                                 transpose                                                                    (matmul, 1, 2)                                                                                                                                         {}
call_function  emb                                                                   <built-in method cat of type object at 0x7f26e82f6f80>                       ((freqs, freqs),)                                                                                                                                      {'dim': -1}
call_method    cos                                                                   cos                                                                          (emb,)                                                                                                                                                 {}
call_function  cos_1                                                                 <built-in function mul>                                                      (cos, 1.0)                                                                                                                                             {}
call_method    sin                                                                   sin                                                                          (emb,)                                                                                                                                                 {}
call_function  sin_1                                                                 <built-in function mul>                                                      (sin, 1.0)                                                                                                                                             {}
call_function  _exit_autocast                                                        <function _exit_autocast at 0x7f26224fa480>                                  (_enter_autocast,)                                                                                                                                     {}
call_method    cos_2                                                                 to                                                                           (cos_1,)                                                                                                                                               {'dtype': torch.bfloat16}
call_method    sin_2                                                                 to                                                                           (sin_1,)                                                                                                                                               {'dtype': torch.bfloat16}
call_function  _log_api_usage_once                                                   <built-in method _log_api_usage_once of PyCapsule object at 0x7f2655575740>  ('python.nn_module',)                                                                                                                                  {}
call_method    hidden_states                                                         to                                                                           (inputs_embeds, torch.float32)                                                                                                                         {}
call_method    pow_1                                                                 pow                                                                          (hidden_states, 2)                                                                                                                                     {}
call_method    variance                                                              mean                                                                         (pow_1, -1)                                                                                                                                            {'keepdim': True}
call_function  add                                                                   <built-in function add>                                                      (variance, 1e-05)                                                                                                                                      {}
call_function  rsqrt                                                                 <built-in method rsqrt of type object at 0x7f26e82f6f80>                     (add,)                                                                                                                                                 {}
call_function  hidden_states_1                                                       <built-in function mul>                                                      (hidden_states, rsqrt)                                                                                                                                 {}
get_attr       l__self___model_layers__modules__0___input_layernorm_weight           L__self___model_layers__modules__0___input_layernorm_weight                  ()                                                                                                                                                     {}
call_method    to_4                                                                  to                                                                           (hidden_states_1, torch.bfloat16)                                                                                                                      {}
call_function  hidden_states_2                                                       <built-in function mul>                                                      (l__self___model_layers__modules__0___input_layernorm_weight, to_4)                                                                                    {}
call_module    l__self___model_layers__modules__0___self_attn_q_proj                 L__self___model_layers__modules__0___self_attn_q_proj                        (hidden_states_2,)                                                                                                                                     {}
call_method    view                                                                  view                                                                         (l__self___model_layers__modules__0___self_attn_q_proj, (1, 1, -1, 128))                                                                               {}
call_method    query_states                                                          transpose                                                                    (view, 1, 2)                                                                                                                                           {}
call_module    l__self___model_layers__modules__0___self_attn_k_proj                 L__self___model_layers__modules__0___self_attn_k_proj                        (hidden_states_2,)                                                                                                                                     {}
call_method    view_1                                                                view                                                                         (l__self___model_layers__modules__0___self_attn_k_proj, (1, 1, -1, 128))                                                                               {}
call_method    key_states                                                            transpose                                                                    (view_1, 1, 2)                                                                                                                                         {}
call_module    l__self___model_layers__modules__0___self_attn_v_proj                 L__self___model_layers__modules__0___self_attn_v_proj                        (hidden_states_2,)                                                                                                                                     {}
call_method    view_2                                                                view                                                                         (l__self___model_layers__modules__0___self_attn_v_proj, (1, 1, -1, 128))                                                                               {}
call_method    value_states                                                          transpose                                                                    (view_2, 1, 2)                                                                                                                                         {}
call_method    cos_3                                                                 unsqueeze                                                                    (cos_2, 1)                                                                                                                                             {}
call_method    sin_3                                                                 unsqueeze                                                                    (sin_2, 1)                                                                                                                                             {}
call_function  mul_4                                                                 <built-in function mul>                                                      (query_states, cos_3)                                                                                                                                  {}
call_function  x1                                                                    <built-in function getitem>                                                  (query_states, (Ellipsis, slice(None, 64, None)))                                                                                                      {}
call_function  x2                                                                    <built-in function getitem>                                                  (query_states, (Ellipsis, slice(64, None, None)))                                                                                                      {}
call_function  neg                                                                   <built-in function neg>                                                      (x2,)                                                                                                                                                  {}
call_function  cat_1                                                                 <built-in method cat of type object at 0x7f26e82f6f80>                       ((neg, x1),)                                                                                                                                           {'dim': -1}
call_function  mul_5                                                                 <built-in function mul>                                                      (cat_1, sin_3)                                                                                                                                         {}
call_function  q_embed                                                               <built-in function add>                                                      (mul_4, mul_5)                                                                                                                                         {}
call_function  mul_6                                                                 <built-in function mul>                                                      (key_states, cos_3)                                                                                                                                    {}
call_function  x1_1                                                                  <built-in function getitem>                                                  (key_states, (Ellipsis, slice(None, 64, None)))                                                                                                        {}
call_function  x2_1                                                                  <built-in function getitem>                                                  (key_states, (Ellipsis, slice(64, None, None)))                                                                                                        {}
call_function  neg_1                                                                 <built-in function neg>                                                      (x2_1,)                                                                                                                                                {}
call_function  cat_2                                                                 <built-in method cat of type object at 0x7f26e82f6f80>                       ((neg_1, x1_1),)                                                                                                                                       {'dim': -1}
call_function  mul_7                                                                 <built-in function mul>                                                      (cat_2, sin_3)                                                                                                                                         {}
call_function  k_embed                                                               <built-in function add>                                                      (mul_6, mul_7)                                                                                                                                         {}
call_method    key_states_1                                                          to                                                                           (k_embed, torch.bfloat16)                                                                                                                              {}
call_method    value_states_1                                                        to                                                                           (value_states, torch.bfloat16)                                                                                                                         {}
call_method    index_copy_                                                           index_copy_                                                                  (l_kwargs_past_key_values_key_cache_0_, 2, l_kwargs_cache_position_, key_states_1)                                                                     {}
call_method    index_copy__1                                                         index_copy_                                                                  (l_kwargs_past_key_values_value_cache_0_, 2, l_kwargs_cache_position_, value_states_1)                                                                 {}
call_function  getitem_8                                                             <built-in function getitem>                                                  (l_kwargs_past_key_values_key_cache_0_, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))    {}
call_method    hidden_states_3                                                       expand                                                                       (getitem_8, 1, 8, 3, 1024, 128)                                                                                                                        {}
call_method    key_states_2                                                          reshape                                                                      (hidden_states_3, 1, 24, 1024, 128)                                                                                                                    {}
call_function  getitem_9                                                             <built-in function getitem>                                                  (l_kwargs_past_key_values_value_cache_0_, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))  {}
call_method    hidden_states_4                                                       expand                                                                       (getitem_9, 1, 8, 3, 1024, 128)                                                                                                                        {}
call_method    value_states_2                                                        reshape                                                                      (hidden_states_4, 1, 24, 1024, 128)                                                                                                                    {}
call_method    transpose_4                                                           transpose                                                                    (key_states_2, 2, 3)                                                                                                                                   {}
call_function  matmul_1                                                              <built-in method matmul of type object at 0x7f26e82f6f80>                    (q_embed, transpose_4)                                                                                                                                 {}
call_function  attn_weights                                                          <built-in function mul>                                                      (matmul_1, 0.08838834764831845)                                                                                                                        {}
call_function  causal_mask_3                                                         <built-in function getitem>                                                  (causal_mask_2, (slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 1024, None)))                                  {}
call_function  attn_weights_1                                                        <built-in function add>                                                      (attn_weights, causal_mask_3)                                                                                                                          {}
call_function  softmax                                                               <function softmax at 0x7f25835fb600>                                         (attn_weights_1,)                                                                                                                                      {'dim': -1, 'dtype': torch.float32}
call_method    attn_weights_2                                                        to                                                                           (softmax, torch.bfloat16)                                                                                                                              {}
call_function  attn_weights_3                                                        <function dropout at 0x7f25835fa980>                                         (attn_weights_2,)                                                                                                                                      {'p': 0.0, 'training': False}
call_function  attn_output                                                           <built-in method matmul of type object at 0x7f26e82f6f80>                    (attn_weights_3, value_states_2)                                                                                                                       {}
call_method    transpose_5                                                           transpose                                                                    (attn_output, 1, 2)                                                                                                                                    {}
call_method    attn_output_1                                                         contiguous                                                                   (transpose_5,)                                                                                                                                         {}
call_method    reshape_3                                                             reshape                                                                      (attn_output_1, 1, 1, -1)                                                                                                                              {}
call_method    attn_output_2                                                         contiguous                                                                   (reshape_3,)                                                                                                                                           {}
call_module    attn_output_3                                                         L__self___model_layers__modules__0___self_attn_o_proj                        (attn_output_2,)                                                                                                                                       {}
call_function  hidden_states_5                                                       <built-in function add>                                                      (inputs_embeds, attn_output_3)                                                                                                                         {}
call_method    hidden_states_6                                                       to                                                                           (hidden_states_5, torch.float32)                                                                                                                       {}
call_method    pow_2                                                                 pow                                                                          (hidden_states_6, 2)                                                                                                                                   {}
call_method    variance_1                                                            mean                                                                         (pow_2, -1)                                                                                                                                            {'keepdim': True}
call_function  add_5                                                                 <built-in function add>                                                      (variance_1, 1e-05)                                                                                                                                    {}
call_function  rsqrt_1                                                               <built-in method rsqrt of type object at 0x7f26e82f6f80>                     (add_5,)                                                                                                                                               {}
call_function  hidden_states_7                                                       <built-in function mul>                                                      (hidden_states_6, rsqrt_1)                                                                                                                             {}
get_attr       l__self___model_layers__modules__0___post_attention_layernorm_weight  L__self___model_layers__modules__0___post_attention_layernorm_weight         ()                                                                                                                                                     {}
call_method    to_9                                                                  to                                                                           (hidden_states_7, torch.bfloat16)                                                                                                                      {}
call_function  hidden_states_8                                                       <built-in function mul>                                                      (l__self___model_layers__modules__0___post_attention_layernorm_weight, to_9)                                                                           {}
call_module    l__self___model_layers__modules__0___mlp_gate_proj                    L__self___model_layers__modules__0___mlp_gate_proj                           (hidden_states_8,)                                                                                                                                     {}
call_module    l__self___model_layers__modules__0___mlp_act_fn                       L__self___model_layers__modules__0___mlp_act_fn                              (l__self___model_layers__modules__0___mlp_gate_proj,)                                                                                                  {}
call_module    l__self___model_layers__modules__0___mlp_up_proj                      L__self___model_layers__modules__0___mlp_up_proj                             (hidden_states_8,)                                                                                                                                     {}
call_function  mul_11                                                                <built-in function mul>                                                      (l__self___model_layers__modules__0___mlp_act_fn, l__self___model_layers__modules__0___mlp_up_proj)                                                    {}
call_module    down_proj                                                             L__self___model_layers__modules__0___mlp_down_proj                           (mul_11,)                                                                                                                                              {}
call_function  hidden_states_9                                                       <built-in function add>                                                      (hidden_states_5, down_proj)                                                                                                                           {}
call_method    hidden_states_10                                                      to                                                                           (hidden_states_9, torch.float32)                                                                                                                       {}
call_method    pow_3                                                                 pow                                                                          (hidden_states_10, 2)                                                                                                                                  {}
call_method    variance_2                                                            mean                                                                         (pow_3, -1)                                                                                                                                            {'keepdim': True}
call_function  add_7                                                                 <built-in function add>                                                      (variance_2, 1e-05)                                                                                                                                    {}
call_function  rsqrt_2                                                               <built-in method rsqrt of type object at 0x7f26e82f6f80>                     (add_7,)                                                                                                                                               {}
call_function  hidden_states_11                                                      <built-in function mul>                                                      (hidden_states_10, rsqrt_2)                                                                                                                            {}
get_attr       l__self___model_norm_weight                                           L__self___model_norm_weight                                                  ()                                                                                                                                                     {}
call_method    to_11                                                                 to                                                                           (hidden_states_11, torch.bfloat16)                                                                                                                     {}
call_function  hidden_states_12                                                      <built-in function mul>                                                      (l__self___model_norm_weight, to_11)                                                                                                                   {}
call_function  getitem_11                                                            <built-in function getitem>                                                  (hidden_states_12, (slice(None, None, None), slice(0, None, None), slice(None, None, None)))                                                           {}
call_module    logits                                                                L__self___lm_head                                                            (getitem_11,)                                                                                                                                          {}
output         output                                                                output                                                                       ((logits,),)                                                                                                                                           {}
[james] override use torch.export.export
program.graph_signature: ExportGraphSignature(input_specs=[InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_layers__modules__0___input_layernorm_weight'), target='L__self___model_layers__modules__0___input_layernorm_weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_layers__modules__0___post_attention_layernorm_weight'), target='L__self___model_layers__modules__0___post_attention_layernorm_weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_norm_weight'), target='L__self___model_norm_weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_embed_tokens_weight'), target='L__self___model_embed_tokens.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_layers__modules__0___self_attn_q_proj_weight'), target='L__self___model_layers__modules__0___self_attn_q_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_layers__modules__0___self_attn_k_proj_weight'), target='L__self___model_layers__modules__0___self_attn_k_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_layers__modules__0___self_attn_v_proj_weight'), target='L__self___model_layers__modules__0___self_attn_v_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_layers__modules__0___self_attn_o_proj_weight'), target='L__self___model_layers__modules__0___self_attn_o_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_layers__modules__0___mlp_gate_proj_weight'), target='L__self___model_layers__modules__0___mlp_gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_layers__modules__0___mlp_up_proj_weight'), target='L__self___model_layers__modules__0___mlp_up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_layers__modules__0___mlp_down_proj_weight'), target='L__self___model_layers__modules__0___mlp_down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_lm_head_weight'), target='L__self___lm_head.weight', persistent=None), InputSpec(kind=<InputKind.BUFFER: 3>, arg=TensorArgument(name='b_model_rotary_emb_inv_freq'), target='L__self___model_rotary_emb_inv_freq', persistent=True), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='args_0'), target=None, persistent=None), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='args_1'), target=None, persistent=None), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='args_2'), target=None, persistent=None), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='args_3'), target=None, persistent=None)], output_specs=[OutputSpec(kind=<OutputKind.USER_INPUT_MUTATION: 6>, arg=TensorArgument(name='index_put'), target='args_2'), OutputSpec(kind=<OutputKind.USER_INPUT_MUTATION: 6>, arg=TensorArgument(name='index_put_1'), target='args_3'), OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='view_31'), target=None)])
opcode         name                                                                  target                                                                args                                                                                 kwargs
-------------  --------------------------------------------------------------------  --------------------------------------------------------------------  -----------------------------------------------------------------------------------  --------------------------------------------------------------------------------------------------------------
get_attr       l__self___model_layers__modules__0___input_layernorm_weight           L__self___model_layers__modules__0___input_layernorm_weight           ()                                                                                   {}
call_function  mark_argument_attributes                                              tt.mark_argument_attributes                                           (l__self___model_layers__modules__0___input_layernorm_weight,)                       {'argument_type': 'parameter', 'name': 'l__self___model_layers__modules__0___input_layernorm_weight'}
get_attr       l__self___model_layers__modules__0___post_attention_layernorm_weight  L__self___model_layers__modules__0___post_attention_layernorm_weight  ()                                                                                   {}
call_function  mark_argument_attributes_1                                            tt.mark_argument_attributes                                           (l__self___model_layers__modules__0___post_attention_layernorm_weight,)              {'argument_type': 'parameter', 'name': 'l__self___model_layers__modules__0___post_attention_layernorm_weight'}
get_attr       l__self___model_norm_weight                                           L__self___model_norm_weight                                           ()                                                                                   {}
call_function  mark_argument_attributes_2                                            tt.mark_argument_attributes                                           (l__self___model_norm_weight,)                                                       {'argument_type': 'parameter', 'name': 'l__self___model_norm_weight'}
get_attr       l__self___model_embed_tokens_weight                                   L__self___model_embed_tokens.weight                                   ()                                                                                   {}
call_function  mark_argument_attributes_3                                            tt.mark_argument_attributes                                           (l__self___model_embed_tokens_weight,)                                               {'argument_type': 'parameter', 'name': 'l__self___model_embed_tokens_weight'}
get_attr       l__self___model_layers__modules__0___self_attn_q_proj_weight          L__self___model_layers__modules__0___self_attn_q_proj.weight          ()                                                                                   {}
call_function  mark_argument_attributes_4                                            tt.mark_argument_attributes                                           (l__self___model_layers__modules__0___self_attn_q_proj_weight,)                      {'argument_type': 'parameter', 'name': 'l__self___model_layers__modules__0___self_attn_q_proj_weight'}
get_attr       l__self___model_layers__modules__0___self_attn_k_proj_weight          L__self___model_layers__modules__0___self_attn_k_proj.weight          ()                                                                                   {}
call_function  mark_argument_attributes_5                                            tt.mark_argument_attributes                                           (l__self___model_layers__modules__0___self_attn_k_proj_weight,)                      {'argument_type': 'parameter', 'name': 'l__self___model_layers__modules__0___self_attn_k_proj_weight'}
get_attr       l__self___model_layers__modules__0___self_attn_v_proj_weight          L__self___model_layers__modules__0___self_attn_v_proj.weight          ()                                                                                   {}
call_function  mark_argument_attributes_6                                            tt.mark_argument_attributes                                           (l__self___model_layers__modules__0___self_attn_v_proj_weight,)                      {'argument_type': 'parameter', 'name': 'l__self___model_layers__modules__0___self_attn_v_proj_weight'}
get_attr       l__self___model_layers__modules__0___self_attn_o_proj_weight          L__self___model_layers__modules__0___self_attn_o_proj.weight          ()                                                                                   {}
call_function  mark_argument_attributes_7                                            tt.mark_argument_attributes                                           (l__self___model_layers__modules__0___self_attn_o_proj_weight,)                      {'argument_type': 'parameter', 'name': 'l__self___model_layers__modules__0___self_attn_o_proj_weight'}
get_attr       l__self___model_layers__modules__0___mlp_gate_proj_weight             L__self___model_layers__modules__0___mlp_gate_proj.weight             ()                                                                                   {}
call_function  mark_argument_attributes_8                                            tt.mark_argument_attributes                                           (l__self___model_layers__modules__0___mlp_gate_proj_weight,)                         {'argument_type': 'parameter', 'name': 'l__self___model_layers__modules__0___mlp_gate_proj_weight'}
get_attr       l__self___model_layers__modules__0___mlp_up_proj_weight               L__self___model_layers__modules__0___mlp_up_proj.weight               ()                                                                                   {}
call_function  mark_argument_attributes_9                                            tt.mark_argument_attributes                                           (l__self___model_layers__modules__0___mlp_up_proj_weight,)                           {'argument_type': 'parameter', 'name': 'l__self___model_layers__modules__0___mlp_up_proj_weight'}
get_attr       l__self___model_layers__modules__0___mlp_down_proj_weight             L__self___model_layers__modules__0___mlp_down_proj.weight             ()                                                                                   {}
call_function  mark_argument_attributes_10                                           tt.mark_argument_attributes                                           (l__self___model_layers__modules__0___mlp_down_proj_weight,)                         {'argument_type': 'parameter', 'name': 'l__self___model_layers__modules__0___mlp_down_proj_weight'}
get_attr       l__self___lm_head_weight                                              L__self___lm_head.weight                                              ()                                                                                   {}
call_function  mark_argument_attributes_11                                           tt.mark_argument_attributes                                           (l__self___lm_head_weight,)                                                          {'argument_type': 'parameter', 'name': 'l__self___lm_head_weight'}
get_attr       l__self___model_rotary_emb_inv_freq                                   L__self___model_rotary_emb_inv_freq                                   ()                                                                                   {}
call_function  mark_argument_attributes_12                                           tt.mark_argument_attributes                                           (l__self___model_rotary_emb_inv_freq,)                                               {'argument_type': 'input', 'name': 'l__self___model_rotary_emb_inv_freq'}
placeholder    args_0                                                                args_0                                                                ()                                                                                   {}
call_function  mark_argument_attributes_13                                           tt.mark_argument_attributes                                           (args_0,)                                                                            {'argument_type': 'input', 'name': 'args_0'}
placeholder    args_1                                                                args_1                                                                ()                                                                                   {}
call_function  mark_argument_attributes_14                                           tt.mark_argument_attributes                                           (args_1,)                                                                            {'argument_type': 'input', 'name': 'args_1'}
placeholder    args_2                                                                args_2                                                                ()                                                                                   {}
call_function  mark_argument_attributes_15                                           tt.mark_argument_attributes                                           (args_2,)                                                                            {'argument_type': 'input', 'name': 'args_2'}
placeholder    args_3                                                                args_3                                                                ()                                                                                   {}
call_function  mark_argument_attributes_16                                           tt.mark_argument_attributes                                           (args_3,)                                                                            {'argument_type': 'input', 'name': 'args_3'}
call_function  embedding                                                             aten.embedding.default                                                (mark_argument_attributes_3, mark_argument_attributes_13)                            {}
call_function  unsqueeze                                                             aten.unsqueeze.default                                                (mark_argument_attributes_14, 0)                                                     {}
call_function  empty_strided                                                         aten.empty_strided.default                                            ([1, 1024], [1024, 1])                                                               {'dtype': torch.bfloat16, 'layout': torch.strided, 'device': device(type='xla', index=0), 'pin_memory': False}
call_function  full_like                                                             aten.full_like.default                                                (empty_strided, -3.3895313892515355e+38)                                             {'pin_memory': False}
call_function  arange                                                                aten.arange.start_step                                                (0, 1024)                                                                            {'layout': torch.strided, 'device': device(type='xla', index=0), 'pin_memory': False}
call_function  view                                                                  aten.view.default                                                     (mark_argument_attributes_14, [-1, 1])                                               {}
call_function  gt                                                                    aten.gt.Tensor                                                        (arange, view)                                                                       {}
call_function  mul                                                                   aten.mul.Tensor                                                       (full_like, gt)                                                                      {}
call_function  unsqueeze_3                                                           aten.unsqueeze.default                                                (mark_argument_attributes_12, 0)                                                     {}
call_function  slice_3                                                               aten.slice.Tensor                                                     (unsqueeze_3, 1, 0, 9223372036854775807)                                             {}
call_function  unsqueeze_4                                                           aten.unsqueeze.default                                                (slice_3, 2)                                                                         {}
call_function  convert_element_type                                                  prims.convert_element_type.default                                    (unsqueeze_4, torch.float32)                                                         {}
call_function  expand_1                                                              aten.expand.default                                                   (unsqueeze_4, [1, -1, 1])                                                            {}
call_function  convert_element_type_1                                                prims.convert_element_type.default                                    (expand_1, torch.float32)                                                            {}
call_function  slice_4                                                               aten.slice.Tensor                                                     (unsqueeze, 0, 0, 9223372036854775807)                                               {}
call_function  unsqueeze_5                                                           aten.unsqueeze.default                                                (slice_4, 1)                                                                         {}
call_function  slice_5                                                               aten.slice.Tensor                                                     (unsqueeze_5, 2, 0, 9223372036854775807)                                             {}
call_function  convert_element_type_2                                                prims.convert_element_type.default                                    (slice_5, torch.float32)                                                             {}
call_function  convert_element_type_3                                                prims.convert_element_type.default                                    (expand_1, torch.float32)                                                            {}
call_function  convert_element_type_4                                                prims.convert_element_type.default                                    (convert_element_type_2, torch.float32)                                              {}
call_function  expand_2                                                              aten.expand.default                                                   (expand_1, [1, 64, 1])                                                               {}
call_function  view_1                                                                aten.view.default                                                     (expand_2, [1, 64, 1])                                                               {}
call_function  expand_3                                                              aten.expand.default                                                   (convert_element_type_2, [1, 1, 1])                                                  {}
call_function  view_2                                                                aten.view.default                                                     (expand_3, [1, 1, 1])                                                                {}
call_function  bmm                                                                   aten.bmm.default                                                      (view_1, view_2)                                                                     {}
call_function  view_3                                                                aten.view.default                                                     (bmm, [1, 64, 1])                                                                    {}
call_function  permute                                                               aten.permute.default                                                  (view_3, [0, 2, 1])                                                                  {}
call_function  cat                                                                   aten.cat.default                                                      ([permute, permute], -1)                                                             {}
call_function  cos                                                                   aten.cos.default                                                      (cat,)                                                                               {}
call_function  mul_1                                                                 aten.mul.Tensor                                                       (cos, 1.0)                                                                           {}
call_function  sin                                                                   aten.sin.default                                                      (cat,)                                                                               {}
call_function  mul_2                                                                 aten.mul.Tensor                                                       (sin, 1.0)                                                                           {}
call_function  convert_element_type_5                                                prims.convert_element_type.default                                    (mul_1, torch.bfloat16)                                                              {}
call_function  convert_element_type_6                                                prims.convert_element_type.default                                    (mul_2, torch.bfloat16)                                                              {}
call_function  convert_element_type_7                                                prims.convert_element_type.default                                    (embedding, torch.float32)                                                           {}
call_function  pow_1                                                                 aten.pow.Tensor_Scalar                                                (convert_element_type_7, 2)                                                          {}
call_function  mean                                                                  aten.mean.dim                                                         (pow_1, [-1], True)                                                                  {}
call_function  add                                                                   aten.add.Tensor                                                       (mean, 1e-05)                                                                        {}
call_function  rsqrt                                                                 aten.rsqrt.default                                                    (add,)                                                                               {}
call_function  mul_3                                                                 aten.mul.Tensor                                                       (convert_element_type_7, rsqrt)                                                      {}
call_function  convert_element_type_8                                                prims.convert_element_type.default                                    (mul_3, torch.bfloat16)                                                              {}
call_function  mul_4                                                                 aten.mul.Tensor                                                       (mark_argument_attributes, convert_element_type_8)                                   {}
call_function  permute_1                                                             aten.permute.default                                                  (mark_argument_attributes_4, [1, 0])                                                 {}
call_function  view_4                                                                aten.view.default                                                     (mul_4, [1, 3072])                                                                   {}
call_function  mm                                                                    aten.mm.default                                                       (view_4, permute_1)                                                                  {}
call_function  view_5                                                                aten.view.default                                                     (mm, [1, 1, 3072])                                                                   {}
call_function  view_6                                                                aten.view.default                                                     (view_5, [1, 1, -1, 128])                                                            {}
call_function  permute_2                                                             aten.permute.default                                                  (view_6, [0, 2, 1, 3])                                                               {}
call_function  permute_3                                                             aten.permute.default                                                  (mark_argument_attributes_5, [1, 0])                                                 {}
call_function  view_7                                                                aten.view.default                                                     (mul_4, [1, 3072])                                                                   {}
call_function  mm_1                                                                  aten.mm.default                                                       (view_7, permute_3)                                                                  {}
call_function  view_8                                                                aten.view.default                                                     (mm_1, [1, 1, 1024])                                                                 {}
call_function  view_9                                                                aten.view.default                                                     (view_8, [1, 1, -1, 128])                                                            {}
call_function  permute_4                                                             aten.permute.default                                                  (view_9, [0, 2, 1, 3])                                                               {}
call_function  permute_5                                                             aten.permute.default                                                  (mark_argument_attributes_6, [1, 0])                                                 {}
call_function  view_10                                                               aten.view.default                                                     (mul_4, [1, 3072])                                                                   {}
call_function  mm_2                                                                  aten.mm.default                                                       (view_10, permute_5)                                                                 {}
call_function  view_11                                                               aten.view.default                                                     (mm_2, [1, 1, 1024])                                                                 {}
call_function  view_12                                                               aten.view.default                                                     (view_11, [1, 1, -1, 128])                                                           {}
call_function  permute_6                                                             aten.permute.default                                                  (view_12, [0, 2, 1, 3])                                                              {}
call_function  unsqueeze_6                                                           aten.unsqueeze.default                                                (convert_element_type_5, 1)                                                          {}
call_function  unsqueeze_7                                                           aten.unsqueeze.default                                                (convert_element_type_6, 1)                                                          {}
call_function  mul_5                                                                 aten.mul.Tensor                                                       (permute_2, unsqueeze_6)                                                             {}
call_function  slice_6                                                               aten.slice.Tensor                                                     (permute_2, 3, 0, 64)                                                                {}
call_function  slice_7                                                               aten.slice.Tensor                                                     (permute_2, 3, 64, 9223372036854775807)                                              {}
call_function  neg                                                                   aten.neg.default                                                      (slice_7,)                                                                           {}
call_function  cat_1                                                                 aten.cat.default                                                      ([neg, slice_6], -1)                                                                 {}
call_function  mul_6                                                                 aten.mul.Tensor                                                       (cat_1, unsqueeze_7)                                                                 {}
call_function  add_1                                                                 aten.add.Tensor                                                       (mul_5, mul_6)                                                                       {}
call_function  mul_7                                                                 aten.mul.Tensor                                                       (permute_4, unsqueeze_6)                                                             {}
call_function  slice_8                                                               aten.slice.Tensor                                                     (permute_4, 3, 0, 64)                                                                {}
call_function  slice_9                                                               aten.slice.Tensor                                                     (permute_4, 3, 64, 9223372036854775807)                                              {}
call_function  neg_1                                                                 aten.neg.default                                                      (slice_9,)                                                                           {}
call_function  cat_2                                                                 aten.cat.default                                                      ([neg_1, slice_8], -1)                                                               {}
call_function  mul_8                                                                 aten.mul.Tensor                                                       (cat_2, unsqueeze_7)                                                                 {}
call_function  add_2                                                                 aten.add.Tensor                                                       (mul_7, mul_8)                                                                       {}
call_function  convert_element_type_9                                                prims.convert_element_type.default                                    (add_2, torch.bfloat16)                                                              {}
call_function  convert_element_type_10                                               prims.convert_element_type.default                                    (permute_6, torch.bfloat16)                                                          {}
call_function  index_put                                                             aten.index_put.default                                                (mark_argument_attributes_15, [None, None, mark_argument_attributes_14], add_2)      {}
call_function  index_put_1                                                           aten.index_put.default                                                (mark_argument_attributes_16, [None, None, mark_argument_attributes_14], permute_6)  {}
call_function  slice_14                                                              aten.slice.Tensor                                                     (index_put, 0, 0, 9223372036854775807)                                               {}
call_function  slice_15                                                              aten.slice.Tensor                                                     (slice_14, 1, 0, 9223372036854775807)                                                {}
call_function  unsqueeze_9                                                           aten.unsqueeze.default                                                (slice_15, 2)                                                                        {}
call_function  slice_16                                                              aten.slice.Tensor                                                     (unsqueeze_9, 3, 0, 9223372036854775807)                                             {}
call_function  slice_17                                                              aten.slice.Tensor                                                     (slice_16, 4, 0, 9223372036854775807)                                                {}
call_function  expand_5                                                              aten.expand.default                                                   (slice_17, [1, 8, 3, 1024, 128])                                                     {}
call_function  clone                                                                 aten.clone.default                                                    (expand_5,)                                                                          {'memory_format': torch.contiguous_format}
call_function  view_13                                                               aten.view.default                                                     (clone, [1, 24, 1024, 128])                                                          {}
call_function  slice_22                                                              aten.slice.Tensor                                                     (index_put_1, 0, 0, 9223372036854775807)                                             {}
call_function  slice_23                                                              aten.slice.Tensor                                                     (slice_22, 1, 0, 9223372036854775807)                                                {}
call_function  unsqueeze_11                                                          aten.unsqueeze.default                                                (slice_23, 2)                                                                        {}
call_function  slice_24                                                              aten.slice.Tensor                                                     (unsqueeze_11, 3, 0, 9223372036854775807)                                            {}
call_function  slice_25                                                              aten.slice.Tensor                                                     (slice_24, 4, 0, 9223372036854775807)                                                {}
call_function  expand_7                                                              aten.expand.default                                                   (slice_25, [1, 8, 3, 1024, 128])                                                     {}
call_function  clone_1                                                               aten.clone.default                                                    (expand_7,)                                                                          {'memory_format': torch.contiguous_format}
call_function  view_14                                                               aten.view.default                                                     (clone_1, [1, 24, 1024, 128])                                                        {}
call_function  permute_7                                                             aten.permute.default                                                  (view_13, [0, 1, 3, 2])                                                              {}
call_function  expand_8                                                              aten.expand.default                                                   (add_1, [1, 24, 1, 128])                                                             {}
call_function  view_15                                                               aten.view.default                                                     (expand_8, [24, 1, 128])                                                             {}
call_function  expand_9                                                              aten.expand.default                                                   (permute_7, [1, 24, 128, 1024])                                                      {}
call_function  view_16                                                               aten.view.default                                                     (expand_9, [24, 128, 1024])                                                          {}
call_function  bmm_1                                                                 aten.bmm.default                                                      (view_15, view_16)                                                                   {}
call_function  view_17                                                               aten.view.default                                                     (bmm_1, [1, 24, 1, 1024])                                                            {}
call_function  mul_9                                                                 aten.mul.Tensor                                                       (view_17, 0.08838834764831845)                                                       {}
call_function  unsqueeze_12                                                          aten.unsqueeze.default                                                (mul, 0)                                                                             {}
call_function  unsqueeze_13                                                          aten.unsqueeze.default                                                (unsqueeze_12, 1)                                                                    {}
call_function  slice_29                                                              aten.slice.Tensor                                                     (unsqueeze_13, 2, 0, 9223372036854775807)                                            {}
call_function  slice_30                                                              aten.slice.Tensor                                                     (slice_29, 3, 0, 9223372036854775807)                                                {}
call_function  expand_10                                                             aten.expand.default                                                   (slice_30, [1, 1, -1, -1])                                                           {}
call_function  slice_31                                                              aten.slice.Tensor                                                     (expand_10, 0, 0, 9223372036854775807)                                               {}
call_function  slice_32                                                              aten.slice.Tensor                                                     (slice_31, 1, 0, 9223372036854775807)                                                {}
call_function  slice_33                                                              aten.slice.Tensor                                                     (slice_32, 2, 0, 9223372036854775807)                                                {}
call_function  add_3                                                                 aten.add.Tensor                                                       (mul_9, slice_33)                                                                    {}
call_function  convert_element_type_11                                               prims.convert_element_type.default                                    (add_3, torch.float32)                                                               {}
call_function  _softmax                                                              aten._softmax.default                                                 (convert_element_type_11, -1, False)                                                 {}
call_function  convert_element_type_12                                               prims.convert_element_type.default                                    (_softmax, torch.bfloat16)                                                           {}
call_function  clone_2                                                               aten.clone.default                                                    (convert_element_type_12,)                                                           {}
call_function  expand_11                                                             aten.expand.default                                                   (clone_2, [1, 24, 1, 1024])                                                          {}
call_function  view_18                                                               aten.view.default                                                     (expand_11, [24, 1, 1024])                                                           {}
call_function  expand_12                                                             aten.expand.default                                                   (view_14, [1, 24, 1024, 128])                                                        {}
call_function  view_19                                                               aten.view.default                                                     (expand_12, [24, 1024, 128])                                                         {}
call_function  bmm_2                                                                 aten.bmm.default                                                      (view_18, view_19)                                                                   {}
call_function  view_20                                                               aten.view.default                                                     (bmm_2, [1, 24, 1, 128])                                                             {}
call_function  permute_8                                                             aten.permute.default                                                  (view_20, [0, 2, 1, 3])                                                              {}
call_function  view_21                                                               aten.view.default                                                     (permute_8, [1, 1, -1])                                                              {}
call_function  permute_9                                                             aten.permute.default                                                  (mark_argument_attributes_7, [1, 0])                                                 {}
call_function  view_22                                                               aten.view.default                                                     (view_21, [1, 3072])                                                                 {}
call_function  mm_3                                                                  aten.mm.default                                                       (view_22, permute_9)                                                                 {}
call_function  view_23                                                               aten.view.default                                                     (mm_3, [1, 1, 3072])                                                                 {}
call_function  add_4                                                                 aten.add.Tensor                                                       (embedding, view_23)                                                                 {}
call_function  convert_element_type_13                                               prims.convert_element_type.default                                    (add_4, torch.float32)                                                               {}
call_function  pow_2                                                                 aten.pow.Tensor_Scalar                                                (convert_element_type_13, 2)                                                         {}
call_function  mean_1                                                                aten.mean.dim                                                         (pow_2, [-1], True)                                                                  {}
call_function  add_5                                                                 aten.add.Tensor                                                       (mean_1, 1e-05)                                                                      {}
call_function  rsqrt_1                                                               aten.rsqrt.default                                                    (add_5,)                                                                             {}
call_function  mul_10                                                                aten.mul.Tensor                                                       (convert_element_type_13, rsqrt_1)                                                   {}
call_function  convert_element_type_14                                               prims.convert_element_type.default                                    (mul_10, torch.bfloat16)                                                             {}
call_function  mul_11                                                                aten.mul.Tensor                                                       (mark_argument_attributes_1, convert_element_type_14)                                {}
call_function  permute_10                                                            aten.permute.default                                                  (mark_argument_attributes_8, [1, 0])                                                 {}
call_function  view_24                                                               aten.view.default                                                     (mul_11, [1, 3072])                                                                  {}
call_function  mm_4                                                                  aten.mm.default                                                       (view_24, permute_10)                                                                {}
call_function  view_25                                                               aten.view.default                                                     (mm_4, [1, 1, 8192])                                                                 {}
call_function  convert_element_type_15                                               prims.convert_element_type.default                                    (view_25, torch.float32)                                                             {}
call_function  sigmoid                                                               aten.sigmoid.default                                                  (view_25,)                                                                           {}
call_function  mul_12                                                                aten.mul.Tensor                                                       (view_25, sigmoid)                                                                   {}
call_function  convert_element_type_16                                               prims.convert_element_type.default                                    (mul_12, torch.bfloat16)                                                             {}
call_function  permute_11                                                            aten.permute.default                                                  (mark_argument_attributes_9, [1, 0])                                                 {}
call_function  view_26                                                               aten.view.default                                                     (mul_11, [1, 3072])                                                                  {}
call_function  mm_5                                                                  aten.mm.default                                                       (view_26, permute_11)                                                                {}
call_function  view_27                                                               aten.view.default                                                     (mm_5, [1, 1, 8192])                                                                 {}
call_function  mul_13                                                                aten.mul.Tensor                                                       (convert_element_type_16, view_27)                                                   {}
call_function  permute_12                                                            aten.permute.default                                                  (mark_argument_attributes_10, [1, 0])                                                {}
call_function  view_28                                                               aten.view.default                                                     (mul_13, [1, 8192])                                                                  {}
call_function  mm_6                                                                  aten.mm.default                                                       (view_28, permute_12)                                                                {}
call_function  view_29                                                               aten.view.default                                                     (mm_6, [1, 1, 3072])                                                                 {}
call_function  add_6                                                                 aten.add.Tensor                                                       (add_4, view_29)                                                                     {}
call_function  convert_element_type_17                                               prims.convert_element_type.default                                    (add_6, torch.float32)                                                               {}
call_function  pow_3                                                                 aten.pow.Tensor_Scalar                                                (convert_element_type_17, 2)                                                         {}
call_function  mean_2                                                                aten.mean.dim                                                         (pow_3, [-1], True)                                                                  {}
call_function  add_7                                                                 aten.add.Tensor                                                       (mean_2, 1e-05)                                                                      {}
call_function  rsqrt_2                                                               aten.rsqrt.default                                                    (add_7,)                                                                             {}
call_function  mul_14                                                                aten.mul.Tensor                                                       (convert_element_type_17, rsqrt_2)                                                   {}
call_function  convert_element_type_18                                               prims.convert_element_type.default                                    (mul_14, torch.bfloat16)                                                             {}
call_function  mul_15                                                                aten.mul.Tensor                                                       (mark_argument_attributes_2, convert_element_type_18)                                {}
call_function  slice_34                                                              aten.slice.Tensor                                                     (mul_15, 0, 0, 9223372036854775807)                                                  {}
call_function  slice_35                                                              aten.slice.Tensor                                                     (slice_34, 1, 0, 9223372036854775807)                                                {}
call_function  slice_36                                                              aten.slice.Tensor                                                     (slice_35, 2, 0, 9223372036854775807)                                                {}
call_function  permute_13                                                            aten.permute.default                                                  (mark_argument_attributes_11, [1, 0])                                                {}
call_function  view_30                                                               aten.view.default                                                     (slice_36, [1, 3072])                                                                {}
call_function  mm_7                                                                  aten.mm.default                                                       (view_30, permute_13)                                                                {}
call_function  view_31                                                               aten.view.default                                                     (mm_7, [1, 1, 128256])                                                               {}
call_function  copy__default                                                         aten.copy_.default                                                    (mark_argument_attributes_15, index_put)                                             {}
call_function  copy__default_1                                                       aten.copy_.default                                                    (mark_argument_attributes_16, index_put_1)                                           {}
output         output                                                                output                                                                ((view_31,),)                                                                        {}
readable graph module
class GraphModule(torch.nn.Module):
    def forward(self, args_0, args_1, args_2, args_3):
        args_0: "i64[1, 1]"; args_1: "i64[1]"; args_2: "bf16[1, 8, 1024, 128]"; args_3: "bf16[1, 8, 1024, 128]"; 
    
        args_0, args_1, args_2, args_3, = fx_pytree.tree_flatten_spec(([args_0, args_1, args_2, args_3], {}), self._in_spec)
        # No stacktrace found for following nodes
        l__self___model_layers__modules__0___input_layernorm_weight: "bf16[3072]" = self.L__self___model_layers__modules__0___input_layernorm_weight
        mark_argument_attributes = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___input_layernorm_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___input_layernorm_weight');  l__self___model_layers__modules__0___input_layernorm_weight = None
        l__self___model_layers__modules__0___post_attention_layernorm_weight: "bf16[3072]" = self.L__self___model_layers__modules__0___post_attention_layernorm_weight
        mark_argument_attributes_1 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___post_attention_layernorm_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___post_attention_layernorm_weight');  l__self___model_layers__modules__0___post_attention_layernorm_weight = None
        l__self___model_norm_weight: "bf16[3072]" = self.L__self___model_norm_weight
        mark_argument_attributes_2 = torch.ops.tt.mark_argument_attributes(l__self___model_norm_weight, argument_type = 'parameter', name = 'l__self___model_norm_weight');  l__self___model_norm_weight = None
        l__self___model_embed_tokens_weight: "bf16[128256, 3072]" = self.L__self___model_embed_tokens.weight
        mark_argument_attributes_3 = torch.ops.tt.mark_argument_attributes(l__self___model_embed_tokens_weight, argument_type = 'parameter', name = 'l__self___model_embed_tokens_weight');  l__self___model_embed_tokens_weight = None
        l__self___model_layers__modules__0___self_attn_q_proj_weight: "bf16[3072, 3072]" = self.L__self___model_layers__modules__0___self_attn_q_proj.weight
        mark_argument_attributes_4 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___self_attn_q_proj_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___self_attn_q_proj_weight');  l__self___model_layers__modules__0___self_attn_q_proj_weight = None
        l__self___model_layers__modules__0___self_attn_k_proj_weight: "bf16[1024, 3072]" = self.L__self___model_layers__modules__0___self_attn_k_proj.weight
        mark_argument_attributes_5 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___self_attn_k_proj_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___self_attn_k_proj_weight');  l__self___model_layers__modules__0___self_attn_k_proj_weight = None
        l__self___model_layers__modules__0___self_attn_v_proj_weight: "bf16[1024, 3072]" = self.L__self___model_layers__modules__0___self_attn_v_proj.weight
        mark_argument_attributes_6 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___self_attn_v_proj_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___self_attn_v_proj_weight');  l__self___model_layers__modules__0___self_attn_v_proj_weight = None
        l__self___model_layers__modules__0___self_attn_o_proj_weight: "bf16[3072, 3072]" = self.L__self___model_layers__modules__0___self_attn_o_proj.weight
        mark_argument_attributes_7 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___self_attn_o_proj_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___self_attn_o_proj_weight');  l__self___model_layers__modules__0___self_attn_o_proj_weight = None
        l__self___model_layers__modules__0___mlp_gate_proj_weight: "bf16[8192, 3072]" = self.L__self___model_layers__modules__0___mlp_gate_proj.weight
        mark_argument_attributes_8 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___mlp_gate_proj_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___mlp_gate_proj_weight');  l__self___model_layers__modules__0___mlp_gate_proj_weight = None
        l__self___model_layers__modules__0___mlp_up_proj_weight: "bf16[8192, 3072]" = self.L__self___model_layers__modules__0___mlp_up_proj.weight
        mark_argument_attributes_9 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___mlp_up_proj_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___mlp_up_proj_weight');  l__self___model_layers__modules__0___mlp_up_proj_weight = None
        l__self___model_layers__modules__0___mlp_down_proj_weight: "bf16[3072, 8192]" = self.L__self___model_layers__modules__0___mlp_down_proj.weight
        mark_argument_attributes_10 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___mlp_down_proj_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___mlp_down_proj_weight');  l__self___model_layers__modules__0___mlp_down_proj_weight = None
        l__self___lm_head_weight: "bf16[128256, 3072]" = self.L__self___lm_head.weight
        mark_argument_attributes_11 = torch.ops.tt.mark_argument_attributes(l__self___lm_head_weight, argument_type = 'parameter', name = 'l__self___lm_head_weight');  l__self___lm_head_weight = None
        l__self___model_rotary_emb_inv_freq: "f32[64]" = self.L__self___model_rotary_emb_inv_freq
        mark_argument_attributes_12 = torch.ops.tt.mark_argument_attributes(l__self___model_rotary_emb_inv_freq, argument_type = 'input', name = 'l__self___model_rotary_emb_inv_freq');  l__self___model_rotary_emb_inv_freq = None
        mark_argument_attributes_13 = torch.ops.tt.mark_argument_attributes(args_0, argument_type = 'input', name = 'args_0');  args_0 = None
        mark_argument_attributes_14 = torch.ops.tt.mark_argument_attributes(args_1, argument_type = 'input', name = 'args_1');  args_1 = None
        mark_argument_attributes_15 = torch.ops.tt.mark_argument_attributes(args_2, argument_type = 'input', name = 'args_2');  args_2 = None
        mark_argument_attributes_16 = torch.ops.tt.mark_argument_attributes(args_3, argument_type = 'input', name = 'args_3');  args_3 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:422 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        embedding: "bf16[1, 1, 3072]" = torch.ops.aten.embedding.default(mark_argument_attributes_3, mark_argument_attributes_13);  mark_argument_attributes_3 = mark_argument_attributes_13 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:434 in forward, code: position_ids = cache_position.unsqueeze(0)
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(mark_argument_attributes_14, 0)
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:586 in _prepare_4d_causal_attention_mask_with_cache_position, code: causal_mask = torch.full(
        empty_strided: "bf16[1, 1024]" = torch.ops.aten.empty_strided.default([1, 1024], [1024, 1], dtype = torch.bfloat16, layout = torch.strided, device = device(type='xla', index=0), pin_memory = False)
        full_like: "bf16[1, 1024]" = torch.ops.aten.full_like.default(empty_strided, -3.3895313892515355e+38, pin_memory = False);  empty_strided = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:591 in _prepare_4d_causal_attention_mask_with_cache_position, code: causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)
        arange: "i64[1024]" = torch.ops.aten.arange.start_step(0, 1024, layout = torch.strided, device = device(type='xla', index=0), pin_memory = False)
        view: "i64[1, 1]" = torch.ops.aten.view.default(mark_argument_attributes_14, [-1, 1])
        gt: "b8[1, 1024]" = torch.ops.aten.gt.Tensor(arange, view);  arange = view = None
        mul: "bf16[1, 1024]" = torch.ops.aten.mul.Tensor(full_like, gt);  full_like = gt = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:103 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        unsqueeze_3: "f32[1, 64]" = torch.ops.aten.unsqueeze.default(mark_argument_attributes_12, 0);  mark_argument_attributes_12 = None
        slice_3: "f32[1, 64]" = torch.ops.aten.slice.Tensor(unsqueeze_3, 1, 0, 9223372036854775807);  unsqueeze_3 = None
        unsqueeze_4: "f32[1, 64, 1]" = torch.ops.aten.unsqueeze.default(slice_3, 2);  slice_3 = None
        convert_element_type: "f32[1, 64, 1]" = torch.ops.prims.convert_element_type.default(unsqueeze_4, torch.float32);  convert_element_type = None
        expand_1: "f32[1, 64, 1]" = torch.ops.aten.expand.default(unsqueeze_4, [1, -1, 1]);  unsqueeze_4 = None
        convert_element_type_1: "f32[1, 64, 1]" = torch.ops.prims.convert_element_type.default(expand_1, torch.float32);  convert_element_type_1 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:104 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        slice_4: "i64[1, 1]" = torch.ops.aten.slice.Tensor(unsqueeze, 0, 0, 9223372036854775807);  unsqueeze = None
        unsqueeze_5: "i64[1, 1, 1]" = torch.ops.aten.unsqueeze.default(slice_4, 1);  slice_4 = None
        slice_5: "i64[1, 1, 1]" = torch.ops.aten.slice.Tensor(unsqueeze_5, 2, 0, 9223372036854775807);  unsqueeze_5 = None
        convert_element_type_2: "f32[1, 1, 1]" = torch.ops.prims.convert_element_type.default(slice_5, torch.float32);  slice_5 = None
        
         # File: <eval_with_key>.32:5 in forward, code: to_3 = torch.ops.aten.to.dtype(to_1, torch.float32);  to_1 = None
        convert_element_type_3: "f32[1, 64, 1]" = torch.ops.prims.convert_element_type.default(expand_1, torch.float32);  convert_element_type_3 = None
        
         # File: <eval_with_key>.32:6 in forward, code: to_4 = torch.ops.aten.to.dtype(to_2, torch.float32);  to_2 = None
        convert_element_type_4: "f32[1, 1, 1]" = torch.ops.prims.convert_element_type.default(convert_element_type_2, torch.float32);  convert_element_type_4 = None
        
         # File: <eval_with_key>.32:7 in forward, code: matmul = torch.ops.aten.matmul.default(to_3, to_4);  to_3 = to_4 = None
        expand_2: "f32[1, 64, 1]" = torch.ops.aten.expand.default(expand_1, [1, 64, 1]);  expand_1 = None
        view_1: "f32[1, 64, 1]" = torch.ops.aten.view.default(expand_2, [1, 64, 1]);  expand_2 = None
        expand_3: "f32[1, 1, 1]" = torch.ops.aten.expand.default(convert_element_type_2, [1, 1, 1]);  convert_element_type_2 = None
        view_2: "f32[1, 1, 1]" = torch.ops.aten.view.default(expand_3, [1, 1, 1]);  expand_3 = None
        bmm: "f32[1, 64, 1]" = torch.ops.aten.bmm.default(view_1, view_2);  view_1 = view_2 = None
        view_3: "f32[1, 64, 1]" = torch.ops.aten.view.default(bmm, [1, 64, 1]);  bmm = None
        
         # File: <eval_with_key>.32:8 in forward, code: transpose = torch.ops.aten.transpose.int(matmul, 1, 2);  matmul = None
        permute: "f32[1, 1, 64]" = torch.ops.aten.permute.default(view_3, [0, 2, 1]);  view_3 = None
        
         # File: <eval_with_key>.32:9 in forward, code: cat = torch.ops.aten.cat.default([transpose, transpose], -1);  transpose = None
        cat: "f32[1, 1, 128]" = torch.ops.aten.cat.default([permute, permute], -1);  permute = None
        
         # File: <eval_with_key>.32:10 in forward, code: cos = torch.ops.aten.cos.default(cat)
        cos: "f32[1, 1, 128]" = torch.ops.aten.cos.default(cat)
        
         # File: <eval_with_key>.32:11 in forward, code: mul = torch.ops.aten.mul.Tensor(cos, 1.0);  cos = None
        mul_1: "f32[1, 1, 128]" = torch.ops.aten.mul.Tensor(cos, 1.0);  cos = None
        
         # File: <eval_with_key>.32:12 in forward, code: sin = torch.ops.aten.sin.default(cat);  cat = None
        sin: "f32[1, 1, 128]" = torch.ops.aten.sin.default(cat);  cat = None
        
         # File: <eval_with_key>.32:13 in forward, code: mul_1 = torch.ops.aten.mul.Tensor(sin, 1.0);  sin = None
        mul_2: "f32[1, 1, 128]" = torch.ops.aten.mul.Tensor(sin, 1.0);  sin = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        convert_element_type_5: "bf16[1, 1, 128]" = torch.ops.prims.convert_element_type.default(mul_1, torch.bfloat16);  mul_1 = None
        convert_element_type_6: "bf16[1, 1, 128]" = torch.ops.prims.convert_element_type.default(mul_2, torch.bfloat16);  mul_2 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:70 in forward, code: hidden_states = hidden_states.to(torch.float32)
        convert_element_type_7: "f32[1, 1, 3072]" = torch.ops.prims.convert_element_type.default(embedding, torch.float32)
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:71 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 1, 3072]" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_7, 2)
        mean: "f32[1, 1, 1]" = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:72 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 1, 1]" = torch.ops.aten.add.Tensor(mean, 1e-05);  mean = None
        rsqrt: "f32[1, 1, 1]" = torch.ops.aten.rsqrt.default(add);  add = None
        mul_3: "f32[1, 1, 3072]" = torch.ops.aten.mul.Tensor(convert_element_type_7, rsqrt);  convert_element_type_7 = rsqrt = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:73 in forward, code: return self.weight * hidden_states.to(input_dtype)
        convert_element_type_8: "bf16[1, 1, 3072]" = torch.ops.prims.convert_element_type.default(mul_3, torch.bfloat16);  mul_3 = None
        mul_4: "bf16[1, 1, 3072]" = torch.ops.aten.mul.Tensor(mark_argument_attributes, convert_element_type_8);  mark_argument_attributes = convert_element_type_8 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:242 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_1: "bf16[3072, 3072]" = torch.ops.aten.permute.default(mark_argument_attributes_4, [1, 0]);  mark_argument_attributes_4 = None
        view_4: "bf16[1, 3072]" = torch.ops.aten.view.default(mul_4, [1, 3072])
        mm: "bf16[1, 3072]" = torch.ops.aten.mm.default(view_4, permute_1);  view_4 = permute_1 = None
        view_5: "bf16[1, 1, 3072]" = torch.ops.aten.view.default(mm, [1, 1, 3072]);  mm = None
        view_6: "bf16[1, 1, 24, 128]" = torch.ops.aten.view.default(view_5, [1, 1, -1, 128]);  view_5 = None
        permute_2: "bf16[1, 24, 1, 128]" = torch.ops.aten.permute.default(view_6, [0, 2, 1, 3]);  view_6 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:243 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_3: "bf16[3072, 1024]" = torch.ops.aten.permute.default(mark_argument_attributes_5, [1, 0]);  mark_argument_attributes_5 = None
        view_7: "bf16[1, 3072]" = torch.ops.aten.view.default(mul_4, [1, 3072])
        mm_1: "bf16[1, 1024]" = torch.ops.aten.mm.default(view_7, permute_3);  view_7 = permute_3 = None
        view_8: "bf16[1, 1, 1024]" = torch.ops.aten.view.default(mm_1, [1, 1, 1024]);  mm_1 = None
        view_9: "bf16[1, 1, 8, 128]" = torch.ops.aten.view.default(view_8, [1, 1, -1, 128]);  view_8 = None
        permute_4: "bf16[1, 8, 1, 128]" = torch.ops.aten.permute.default(view_9, [0, 2, 1, 3]);  view_9 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:244 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_5: "bf16[3072, 1024]" = torch.ops.aten.permute.default(mark_argument_attributes_6, [1, 0]);  mark_argument_attributes_6 = None
        view_10: "bf16[1, 3072]" = torch.ops.aten.view.default(mul_4, [1, 3072]);  mul_4 = None
        mm_2: "bf16[1, 1024]" = torch.ops.aten.mm.default(view_10, permute_5);  view_10 = permute_5 = None
        view_11: "bf16[1, 1, 1024]" = torch.ops.aten.view.default(mm_2, [1, 1, 1024]);  mm_2 = None
        view_12: "bf16[1, 1, 8, 128]" = torch.ops.aten.view.default(view_11, [1, 1, -1, 128]);  view_11 = None
        permute_6: "bf16[1, 8, 1, 128]" = torch.ops.aten.permute.default(view_12, [0, 2, 1, 3]);  view_12 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:143 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_6: "bf16[1, 1, 1, 128]" = torch.ops.aten.unsqueeze.default(convert_element_type_5, 1);  convert_element_type_5 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:144 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_7: "bf16[1, 1, 1, 128]" = torch.ops.aten.unsqueeze.default(convert_element_type_6, 1);  convert_element_type_6 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:145 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 24, 1, 128]" = torch.ops.aten.mul.Tensor(permute_2, unsqueeze_6)
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_6: "bf16[1, 24, 1, 64]" = torch.ops.aten.slice.Tensor(permute_2, 3, 0, 64)
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_7: "bf16[1, 24, 1, 64]" = torch.ops.aten.slice.Tensor(permute_2, 3, 64, 9223372036854775807);  permute_2 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 24, 1, 64]" = torch.ops.aten.neg.default(slice_7);  slice_7 = None
        cat_1: "bf16[1, 24, 1, 128]" = torch.ops.aten.cat.default([neg, slice_6], -1);  neg = slice_6 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:145 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_6: "bf16[1, 24, 1, 128]" = torch.ops.aten.mul.Tensor(cat_1, unsqueeze_7);  cat_1 = None
        add_1: "bf16[1, 24, 1, 128]" = torch.ops.aten.add.Tensor(mul_5, mul_6);  mul_5 = mul_6 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:146 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 1, 128]" = torch.ops.aten.mul.Tensor(permute_4, unsqueeze_6);  unsqueeze_6 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_8: "bf16[1, 8, 1, 64]" = torch.ops.aten.slice.Tensor(permute_4, 3, 0, 64)
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_9: "bf16[1, 8, 1, 64]" = torch.ops.aten.slice.Tensor(permute_4, 3, 64, 9223372036854775807);  permute_4 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 1, 64]" = torch.ops.aten.neg.default(slice_9);  slice_9 = None
        cat_2: "bf16[1, 8, 1, 128]" = torch.ops.aten.cat.default([neg_1, slice_8], -1);  neg_1 = slice_8 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:146 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_8: "bf16[1, 8, 1, 128]" = torch.ops.aten.mul.Tensor(cat_2, unsqueeze_7);  cat_2 = unsqueeze_7 = None
        add_2: "bf16[1, 8, 1, 128]" = torch.ops.aten.add.Tensor(mul_7, mul_8);  mul_7 = mul_8 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/cache_utils.py:1368 in update, code: key_states = key_states.to(self.key_cache[layer_idx].dtype)
        convert_element_type_9: "bf16[1, 8, 1, 128]" = torch.ops.prims.convert_element_type.default(add_2, torch.bfloat16);  convert_element_type_9 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/cache_utils.py:1369 in update, code: value_states = value_states.to(self.value_cache[layer_idx].dtype)
        convert_element_type_10: "bf16[1, 8, 1, 128]" = torch.ops.prims.convert_element_type.default(permute_6, torch.bfloat16);  convert_element_type_10 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/cache_utils.py:54 in _static_cache_update, code: k_cache.index_copy_(2, cache_position, key_states)
        index_put: "bf16[1, 8, 1024, 128]" = torch.ops.aten.index_put.default(mark_argument_attributes_15, [None, None, mark_argument_attributes_14], add_2);  add_2 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/cache_utils.py:55 in _static_cache_update, code: v_cache.index_copy_(2, cache_position, value_states)
        index_put_1: "bf16[1, 8, 1024, 128]" = torch.ops.aten.index_put.default(mark_argument_attributes_16, [None, None, mark_argument_attributes_14], permute_6);  mark_argument_attributes_14 = permute_6 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        slice_14: "bf16[1, 8, 1024, 128]" = torch.ops.aten.slice.Tensor(index_put, 0, 0, 9223372036854775807)
        slice_15: "bf16[1, 8, 1024, 128]" = torch.ops.aten.slice.Tensor(slice_14, 1, 0, 9223372036854775807);  slice_14 = None
        unsqueeze_9: "bf16[1, 8, 1, 1024, 128]" = torch.ops.aten.unsqueeze.default(slice_15, 2);  slice_15 = None
        slice_16: "bf16[1, 8, 1, 1024, 128]" = torch.ops.aten.slice.Tensor(unsqueeze_9, 3, 0, 9223372036854775807);  unsqueeze_9 = None
        slice_17: "bf16[1, 8, 1, 1024, 128]" = torch.ops.aten.slice.Tensor(slice_16, 4, 0, 9223372036854775807);  slice_16 = None
        expand_5: "bf16[1, 8, 3, 1024, 128]" = torch.ops.aten.expand.default(slice_17, [1, 8, 3, 1024, 128]);  slice_17 = None
        clone: "bf16[1, 8, 3, 1024, 128]" = torch.ops.aten.clone.default(expand_5, memory_format = torch.contiguous_format);  expand_5 = None
        view_13: "bf16[1, 24, 1024, 128]" = torch.ops.aten.view.default(clone, [1, 24, 1024, 128]);  clone = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        slice_22: "bf16[1, 8, 1024, 128]" = torch.ops.aten.slice.Tensor(index_put_1, 0, 0, 9223372036854775807)
        slice_23: "bf16[1, 8, 1024, 128]" = torch.ops.aten.slice.Tensor(slice_22, 1, 0, 9223372036854775807);  slice_22 = None
        unsqueeze_11: "bf16[1, 8, 1, 1024, 128]" = torch.ops.aten.unsqueeze.default(slice_23, 2);  slice_23 = None
        slice_24: "bf16[1, 8, 1, 1024, 128]" = torch.ops.aten.slice.Tensor(unsqueeze_11, 3, 0, 9223372036854775807);  unsqueeze_11 = None
        slice_25: "bf16[1, 8, 1, 1024, 128]" = torch.ops.aten.slice.Tensor(slice_24, 4, 0, 9223372036854775807);  slice_24 = None
        expand_7: "bf16[1, 8, 3, 1024, 128]" = torch.ops.aten.expand.default(slice_25, [1, 8, 3, 1024, 128]);  slice_25 = None
        clone_1: "bf16[1, 8, 3, 1024, 128]" = torch.ops.aten.clone.default(expand_7, memory_format = torch.contiguous_format);  expand_7 = None
        view_14: "bf16[1, 24, 1024, 128]" = torch.ops.aten.view.default(clone_1, [1, 24, 1024, 128]);  clone_1 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:191 in eager_attention_forward, code: attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling
        permute_7: "bf16[1, 24, 128, 1024]" = torch.ops.aten.permute.default(view_13, [0, 1, 3, 2]);  view_13 = None
        expand_8: "bf16[1, 24, 1, 128]" = torch.ops.aten.expand.default(add_1, [1, 24, 1, 128]);  add_1 = None
        view_15: "bf16[24, 1, 128]" = torch.ops.aten.view.default(expand_8, [24, 1, 128]);  expand_8 = None
        expand_9: "bf16[1, 24, 128, 1024]" = torch.ops.aten.expand.default(permute_7, [1, 24, 128, 1024]);  permute_7 = None
        view_16: "bf16[24, 128, 1024]" = torch.ops.aten.view.default(expand_9, [24, 128, 1024]);  expand_9 = None
        bmm_1: "bf16[24, 1, 1024]" = torch.ops.aten.bmm.default(view_15, view_16);  view_15 = view_16 = None
        view_17: "bf16[1, 24, 1, 1024]" = torch.ops.aten.view.default(bmm_1, [1, 24, 1, 1024]);  bmm_1 = None
        mul_9: "bf16[1, 24, 1, 1024]" = torch.ops.aten.mul.Tensor(view_17, 0.08838834764831845);  view_17 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:194 in eager_attention_forward, code: attn_weights = attn_weights + causal_mask
        unsqueeze_12: "bf16[1, 1, 1024]" = torch.ops.aten.unsqueeze.default(mul, 0);  mul = None
        unsqueeze_13: "bf16[1, 1, 1, 1024]" = torch.ops.aten.unsqueeze.default(unsqueeze_12, 1);  unsqueeze_12 = None
        slice_29: "bf16[1, 1, 1, 1024]" = torch.ops.aten.slice.Tensor(unsqueeze_13, 2, 0, 9223372036854775807);  unsqueeze_13 = None
        slice_30: "bf16[1, 1, 1, 1024]" = torch.ops.aten.slice.Tensor(slice_29, 3, 0, 9223372036854775807);  slice_29 = None
        expand_10: "bf16[1, 1, 1, 1024]" = torch.ops.aten.expand.default(slice_30, [1, 1, -1, -1]);  slice_30 = None
        slice_31: "bf16[1, 1, 1, 1024]" = torch.ops.aten.slice.Tensor(expand_10, 0, 0, 9223372036854775807);  expand_10 = None
        slice_32: "bf16[1, 1, 1, 1024]" = torch.ops.aten.slice.Tensor(slice_31, 1, 0, 9223372036854775807);  slice_31 = None
        slice_33: "bf16[1, 1, 1, 1024]" = torch.ops.aten.slice.Tensor(slice_32, 2, 0, 9223372036854775807);  slice_32 = None
        add_3: "bf16[1, 24, 1, 1024]" = torch.ops.aten.add.Tensor(mul_9, slice_33);  mul_9 = slice_33 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:196 in eager_attention_forward, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
        convert_element_type_11: "f32[1, 24, 1, 1024]" = torch.ops.prims.convert_element_type.default(add_3, torch.float32);  add_3 = None
        _softmax: "f32[1, 24, 1, 1024]" = torch.ops.aten._softmax.default(convert_element_type_11, -1, False);  convert_element_type_11 = None
        convert_element_type_12: "bf16[1, 24, 1, 1024]" = torch.ops.prims.convert_element_type.default(_softmax, torch.bfloat16);  _softmax = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:197 in eager_attention_forward, code: attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
        clone_2: "bf16[1, 24, 1, 1024]" = torch.ops.aten.clone.default(convert_element_type_12);  convert_element_type_12 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:198 in eager_attention_forward, code: attn_output = torch.matmul(attn_weights, value_states)
        expand_11: "bf16[1, 24, 1, 1024]" = torch.ops.aten.expand.default(clone_2, [1, 24, 1, 1024]);  clone_2 = None
        view_18: "bf16[24, 1, 1024]" = torch.ops.aten.view.default(expand_11, [24, 1, 1024]);  expand_11 = None
        expand_12: "bf16[1, 24, 1024, 128]" = torch.ops.aten.expand.default(view_14, [1, 24, 1024, 128]);  view_14 = None
        view_19: "bf16[24, 1024, 128]" = torch.ops.aten.view.default(expand_12, [24, 1024, 128]);  expand_12 = None
        bmm_2: "bf16[24, 1, 128]" = torch.ops.aten.bmm.default(view_18, view_19);  view_18 = view_19 = None
        view_20: "bf16[1, 24, 1, 128]" = torch.ops.aten.view.default(bmm_2, [1, 24, 1, 128]);  bmm_2 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:199 in eager_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_8: "bf16[1, 1, 24, 128]" = torch.ops.aten.permute.default(view_20, [0, 2, 1, 3]);  view_20 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:276 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_21: "bf16[1, 1, 3072]" = torch.ops.aten.view.default(permute_8, [1, 1, -1]);  permute_8 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:277 in forward, code: attn_output = self.o_proj(attn_output)
        permute_9: "bf16[3072, 3072]" = torch.ops.aten.permute.default(mark_argument_attributes_7, [1, 0]);  mark_argument_attributes_7 = None
        view_22: "bf16[1, 3072]" = torch.ops.aten.view.default(view_21, [1, 3072]);  view_21 = None
        mm_3: "bf16[1, 3072]" = torch.ops.aten.mm.default(view_22, permute_9);  view_22 = permute_9 = None
        view_23: "bf16[1, 1, 3072]" = torch.ops.aten.view.default(mm_3, [1, 1, 3072]);  mm_3 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:319 in forward, code: hidden_states = residual + hidden_states
        add_4: "bf16[1, 1, 3072]" = torch.ops.aten.add.Tensor(embedding, view_23);  embedding = view_23 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:70 in forward, code: hidden_states = hidden_states.to(torch.float32)
        convert_element_type_13: "f32[1, 1, 3072]" = torch.ops.prims.convert_element_type.default(add_4, torch.float32)
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:71 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_2: "f32[1, 1, 3072]" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_13, 2)
        mean_1: "f32[1, 1, 1]" = torch.ops.aten.mean.dim(pow_2, [-1], True);  pow_2 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:72 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add_5: "f32[1, 1, 1]" = torch.ops.aten.add.Tensor(mean_1, 1e-05);  mean_1 = None
        rsqrt_1: "f32[1, 1, 1]" = torch.ops.aten.rsqrt.default(add_5);  add_5 = None
        mul_10: "f32[1, 1, 3072]" = torch.ops.aten.mul.Tensor(convert_element_type_13, rsqrt_1);  convert_element_type_13 = rsqrt_1 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:73 in forward, code: return self.weight * hidden_states.to(input_dtype)
        convert_element_type_14: "bf16[1, 1, 3072]" = torch.ops.prims.convert_element_type.default(mul_10, torch.bfloat16);  mul_10 = None
        mul_11: "bf16[1, 1, 3072]" = torch.ops.aten.mul.Tensor(mark_argument_attributes_1, convert_element_type_14);  mark_argument_attributes_1 = convert_element_type_14 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:162 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_10: "bf16[3072, 8192]" = torch.ops.aten.permute.default(mark_argument_attributes_8, [1, 0]);  mark_argument_attributes_8 = None
        view_24: "bf16[1, 3072]" = torch.ops.aten.view.default(mul_11, [1, 3072])
        mm_4: "bf16[1, 8192]" = torch.ops.aten.mm.default(view_24, permute_10);  view_24 = permute_10 = None
        view_25: "bf16[1, 1, 8192]" = torch.ops.aten.view.default(mm_4, [1, 1, 8192]);  mm_4 = None
        convert_element_type_15: "f32[1, 1, 8192]" = torch.ops.prims.convert_element_type.default(view_25, torch.float32);  convert_element_type_15 = None
        sigmoid: "f32[1, 1, 8192]" = torch.ops.aten.sigmoid.default(view_25)
        mul_12: "f32[1, 1, 8192]" = torch.ops.aten.mul.Tensor(view_25, sigmoid);  view_25 = sigmoid = None
        convert_element_type_16: "bf16[1, 1, 8192]" = torch.ops.prims.convert_element_type.default(mul_12, torch.bfloat16);  mul_12 = None
        permute_11: "bf16[3072, 8192]" = torch.ops.aten.permute.default(mark_argument_attributes_9, [1, 0]);  mark_argument_attributes_9 = None
        view_26: "bf16[1, 3072]" = torch.ops.aten.view.default(mul_11, [1, 3072]);  mul_11 = None
        mm_5: "bf16[1, 8192]" = torch.ops.aten.mm.default(view_26, permute_11);  view_26 = permute_11 = None
        view_27: "bf16[1, 1, 8192]" = torch.ops.aten.view.default(mm_5, [1, 1, 8192]);  mm_5 = None
        mul_13: "bf16[1, 1, 8192]" = torch.ops.aten.mul.Tensor(convert_element_type_16, view_27);  convert_element_type_16 = view_27 = None
        permute_12: "bf16[8192, 3072]" = torch.ops.aten.permute.default(mark_argument_attributes_10, [1, 0]);  mark_argument_attributes_10 = None
        view_28: "bf16[1, 8192]" = torch.ops.aten.view.default(mul_13, [1, 8192]);  mul_13 = None
        mm_6: "bf16[1, 3072]" = torch.ops.aten.mm.default(view_28, permute_12);  view_28 = permute_12 = None
        view_29: "bf16[1, 1, 3072]" = torch.ops.aten.view.default(mm_6, [1, 1, 3072]);  mm_6 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325 in forward, code: hidden_states = residual + hidden_states
        add_6: "bf16[1, 1, 3072]" = torch.ops.aten.add.Tensor(add_4, view_29);  add_4 = view_29 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:70 in forward, code: hidden_states = hidden_states.to(torch.float32)
        convert_element_type_17: "f32[1, 1, 3072]" = torch.ops.prims.convert_element_type.default(add_6, torch.float32);  add_6 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:71 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_3: "f32[1, 1, 3072]" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_17, 2)
        mean_2: "f32[1, 1, 1]" = torch.ops.aten.mean.dim(pow_3, [-1], True);  pow_3 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:72 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add_7: "f32[1, 1, 1]" = torch.ops.aten.add.Tensor(mean_2, 1e-05);  mean_2 = None
        rsqrt_2: "f32[1, 1, 1]" = torch.ops.aten.rsqrt.default(add_7);  add_7 = None
        mul_14: "f32[1, 1, 3072]" = torch.ops.aten.mul.Tensor(convert_element_type_17, rsqrt_2);  convert_element_type_17 = rsqrt_2 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:73 in forward, code: return self.weight * hidden_states.to(input_dtype)
        convert_element_type_18: "bf16[1, 1, 3072]" = torch.ops.prims.convert_element_type.default(mul_14, torch.bfloat16);  mul_14 = None
        mul_15: "bf16[1, 1, 3072]" = torch.ops.aten.mul.Tensor(mark_argument_attributes_2, convert_element_type_18);  mark_argument_attributes_2 = convert_element_type_18 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:704 in forward, code: logits = self.lm_head(hidden_states[:, slice_indices, :])
        slice_34: "bf16[1, 1, 3072]" = torch.ops.aten.slice.Tensor(mul_15, 0, 0, 9223372036854775807);  mul_15 = None
        slice_35: "bf16[1, 1, 3072]" = torch.ops.aten.slice.Tensor(slice_34, 1, 0, 9223372036854775807);  slice_34 = None
        slice_36: "bf16[1, 1, 3072]" = torch.ops.aten.slice.Tensor(slice_35, 2, 0, 9223372036854775807);  slice_35 = None
        permute_13: "bf16[3072, 128256]" = torch.ops.aten.permute.default(mark_argument_attributes_11, [1, 0]);  mark_argument_attributes_11 = None
        view_30: "bf16[1, 3072]" = torch.ops.aten.view.default(slice_36, [1, 3072]);  slice_36 = None
        mm_7: "bf16[1, 128256]" = torch.ops.aten.mm.default(view_30, permute_13);  view_30 = permute_13 = None
        view_31: "bf16[1, 1, 128256]" = torch.ops.aten.view.default(mm_7, [1, 1, 128256]);  mm_7 = None
        
        # No stacktrace found for following nodes
        copy__default = torch.ops.aten.copy_.default(mark_argument_attributes_15, index_put);  mark_argument_attributes_15 = index_put = copy__default = None
        copy__default_1 = torch.ops.aten.copy_.default(mark_argument_attributes_16, index_put_1);  mark_argument_attributes_16 = index_put_1 = copy__default_1 = None
        return pytree.tree_unflatten((view_31,), self._out_spec)
        2025-09-15 15:42:03.577 (  71.904s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:03.578 (  71.904s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:03.578 (  71.904s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:42:03.578 (  71.904s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:42:03.578 (  71.904s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-09-15 15:42:03.578 (  71.904s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:03.578 (  71.904s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:03.578 (  71.905s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:42:03.578 (  71.905s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:42:03.578 (  71.905s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-09-15 15:42:03.578 (  71.905s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:03.578 (  71.905s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:03.578 (  71.905s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:03.578 (  71.905s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:03.578 (  71.905s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:42:03.578 (  71.905s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:42:03.578 (  71.905s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-09-15 15:42:03.579 (  71.905s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:03.579 (  71.905s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:03.579 (  71.905s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:42:03.579 (  71.905s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:42:03.579 (  71.905s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-09-15 15:42:03.579 (  71.905s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:03.579 (  71.905s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.914s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.915s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.915s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.915s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.915s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.588 (  71.915s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.589 (  71.915s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.589 (  71.915s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.589 (  71.915s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.589 (  71.915s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.589 (  71.915s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.589 (  71.915s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.589 (  71.916s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.590 (  71.916s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.590 (  71.916s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.590 (  71.916s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:03.619 (  71.946s) [        F073E000]     client_instance.cc:428      1| ClientInstance::PJRT_Client_Compile
2025-09-15 15:42:03.619 (  71.946s) [        F073E000]      module_builder.cc:103      1| ModuleBuilder::buildModule
2025-09-15 15:42:03.622 (  71.948s) [        F073E000]      module_builder.cc:165      1| VHLO Module:
module @SyncTensorsGraph.420 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, %arg1: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg2: !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>, %arg4: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg5: !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>, %arg6: !vhlo.tensor_v1<1x1x!vhlo.i64_v1>, %arg7: !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, %arg8: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg9: !vhlo.tensor_v1<1x!vhlo.i64_v1>, %arg10: !vhlo.tensor_v1<!vhlo.i64_v1>, %arg11: !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg13: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg14: !vhlo.tensor_v1<64x!vhlo.f32_v1>, %arg15: !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>, %arg16: !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>, %arg17: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg18: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg19: !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>, %arg20: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<1x128256x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x128256x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF800000> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<"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tensor<1x1024xi64>>}> : () -> !vhlo.tensor_v1<1x1024x!vhlo.i64_v1>
    %3 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : () -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %4 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<3.25520843E-4> : tensor<1x1xf32>>}> : () -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %5 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %7 = "vhlo.reshape_v1"(%arg20) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %8 = "vhlo.custom_call_v1"(%7) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_norm_weight">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %9 = "vhlo.reshape_v1"(%8) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1>
    %10 = "vhlo.convert_v1"(%9) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %12 = "vhlo.reshape_v1"(%arg7) : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>
    %13 = "vhlo.custom_call_v1"(%12) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_embed_tokens_weight">}>} : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>
    %15 = "vhlo.reshape_v1"(%arg6) : (!vhlo.tensor_v1<1x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.i64_v1>
    %16 = "vhlo.custom_call_v1"(%15) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_0">}>} : (!vhlo.tensor_v1<1x1x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.i64_v1>
    %17 = "vhlo.reshape_v1"(%16) : (!vhlo.tensor_v1<1x1x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %18 = "vhlo.convert_v1"(%17) : (!vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.ui32_v1>
    %19 = "vhlo.gather_v2"(%14, %18) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 3072]> : tensor<2xi64>>, start_index_map = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x!vhlo.ui32_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %20 = "vhlo.reshape_v1"(%19) : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %21 = "vhlo.reshape_v1"(%arg8) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %22 = "vhlo.custom_call_v1"(%21) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___input_layernorm_weight">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %23 = "vhlo.reshape_v1"(%22) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1>
    %24 = "vhlo.convert_v1"(%23) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %25 = "vhlo.reshape_v1"(%24) : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %26 = "vhlo.convert_v1"(%20) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %27 = "vhlo.power_v1"(%26, %6) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %28 = "vhlo.reduce_v1"(%27, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %228 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%228) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %29 = "vhlo.multiply_v1"(%28, %4) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %30 = "vhlo.reshape_v1"(%29) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %31 = "vhlo.reshape_v1"(%arg1) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %32 = "vhlo.add_v1"(%30, %31) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %33 = "vhlo.rsqrt_v2"(%32) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %34 = "vhlo.reshape_v1"(%33) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %35 = "vhlo.broadcast_in_dim_v1"(%34) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %36 = "vhlo.multiply_v1"(%26, %35) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %37 = "vhlo.convert_v1"(%36) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %38 = "vhlo.convert_v1"(%37) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %39 = "vhlo.multiply_v1"(%25, %38) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %40 = "vhlo.convert_v1"(%39) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %41 = "vhlo.reshape_v1"(%40) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %42 = "vhlo.reshape_v1"(%arg17) : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>
    %43 = "vhlo.custom_call_v1"(%42) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>
    %44 = "vhlo.reshape_v1"(%43) : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %45 = "vhlo.transpose_v1"(%44) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,3072]{0,1}">} : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %46 = "vhlo.dot_general_v2"(%41, %45) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %47 = "vhlo.reshape_v1"(%46) : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %48 = "vhlo.convert_v1"(%47) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,24,1,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %49 = "vhlo.reshape_v1"(%arg14) : (!vhlo.tensor_v1<64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>
    %50 = "vhlo.custom_call_v1"(%49) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_rotary_emb_inv_freq">}>} : (!vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>
    %51 = "vhlo.reshape_v1"(%50) : (!vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>
    %52 = "vhlo.reshape_v1"(%arg9) : (!vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.i64_v1>
    %53 = "vhlo.custom_call_v1"(%52) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_1">}>} : (!vhlo.tensor_v1<1x1x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.i64_v1>
    %54 = "vhlo.reshape_v1"(%53) : (!vhlo.tensor_v1<1x1x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %55 = "vhlo.convert_v1"(%53) : (!vhlo.tensor_v1<1x1x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %56 = "vhlo.dot_general_v2"(%51, %55) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>
    %57 = "vhlo.reshape_v1"(%56) : (!vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>
    %58 = "vhlo.concatenate_v1"(%57, %57) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>
    %59 = "vhlo.cosine_v2"(%58) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>
    %60 = "vhlo.convert_v1"(%59) : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>
    %61 = "vhlo.reshape_v1"(%60) : (!vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1x128x!vhlo.bf16_v1>
    %62 = "vhlo.convert_v1"(%61) : (!vhlo.tensor_v1<1x1x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1x128x!vhlo.f32_v1>
    %63 = "vhlo.reshape_v1"(%62) : (!vhlo.tensor_v1<1x1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>
    %64 = "vhlo.broadcast_in_dim_v1"(%63) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %65 = "vhlo.multiply_v1"(%48, %64) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %66 = "vhlo.convert_v1"(%65) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %67 = "vhlo.slice_v1"(%47) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 1, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>
    %68 = "vhlo.negate_v1"(%67) : (!vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>
    %69 = "vhlo.slice_v1"(%47) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 1, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>
    %70 = "vhlo.concatenate_v1"(%68, %69) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %71 = "vhlo.convert_v1"(%70) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %72 = "vhlo.sine_v2"(%58) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>
    %73 = "vhlo.convert_v1"(%72) : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>
    %74 = "vhlo.reshape_v1"(%73) : (!vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1x128x!vhlo.bf16_v1>
    %75 = "vhlo.convert_v1"(%74) : (!vhlo.tensor_v1<1x1x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1x128x!vhlo.f32_v1>
    %76 = "vhlo.reshape_v1"(%75) : (!vhlo.tensor_v1<1x1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>
    %77 = "vhlo.broadcast_in_dim_v1"(%76) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %78 = "vhlo.multiply_v1"(%71, %77) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %79 = "vhlo.convert_v1"(%78) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %80 = "vhlo.add_v1"(%66, %79) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %81 = "vhlo.reshape_v1"(%80) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>
    %82 = "vhlo.custom_call_v1"(%arg16) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_2">}>} : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>
    %83 = "vhlo.compare_v1"(%54, %3) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 LT>}> : (!vhlo.tensor_v1<1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.bool_v1>
    %84 = "vhlo.reshape_v1"(%arg10) : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %85 = "vhlo.add_v1"(%54, %84) : (!vhlo.tensor_v1<1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %86 = "vhlo.select_v1"(%83, %85, %54) : (!vhlo.tensor_v1<1x!vhlo.bool_v1>, !vhlo.tensor_v1<1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %87 = "vhlo.reshape_v1"(%86) : (!vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.i64_v1>
    %88 = "vhlo.reshape_v1"(%arg15) : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>
    %89 = "vhlo.custom_call_v1"(%88) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>
    %90 = "vhlo.reshape_v1"(%89) : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>
    %91 = "vhlo.transpose_v1"(%90) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,1024]{0,1}">} : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>
    %92 = "vhlo.dot_general_v2"(%41, %91) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>
    %93 = "vhlo.reshape_v1"(%92) : (!vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %94 = "vhlo.convert_v1"(%93) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,8,1,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %95 = "vhlo.broadcast_in_dim_v1"(%63) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %96 = "vhlo.multiply_v1"(%94, %95) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %97 = "vhlo.convert_v1"(%96) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %98 = "vhlo.slice_v1"(%93) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 1, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>
    %99 = "vhlo.negate_v1"(%98) : (!vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>
    %100 = "vhlo.slice_v1"(%93) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 1, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>
    %101 = "vhlo.concatenate_v1"(%99, %100) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %102 = "vhlo.convert_v1"(%101) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %103 = "vhlo.broadcast_in_dim_v1"(%76) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %104 = "vhlo.multiply_v1"(%102, %103) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %105 = "vhlo.convert_v1"(%104) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %106 = "vhlo.add_v1"(%97, %105) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %107 = "vhlo.scatter_v2"(%82, %87, %106) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg22: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg22) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>
    %108 = "vhlo.broadcast_in_dim_v1"(%107) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x1024x128x!vhlo.bf16_v1>
    %109 = "vhlo.reshape_v1"(%108) : (!vhlo.tensor_v1<1x8x3x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1024x128x!vhlo.bf16_v1>
    %110 = "vhlo.transpose_v1"(%109) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 3, 1, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,24,128,1024]{2,3,1,0}">} : (!vhlo.tensor_v1<1x24x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x128x1024x!vhlo.bf16_v1>
    %111 = "vhlo.reshape_v1"(%110) : (!vhlo.tensor_v1<1x24x128x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x128x1024x!vhlo.bf16_v1>
    %112 = "vhlo.dot_general_v2"(%81, %111) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x128x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x1x1024x!vhlo.bf16_v1>
    %113 = "vhlo.reshape_v1"(%112) : (!vhlo.tensor_v1<24x1x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x1024x!vhlo.bf16_v1>
    %114 = "vhlo.convert_v1"(%113) : (!vhlo.tensor_v1<1x24x1x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x1024x!vhlo.f32_v1>
    %115 = "vhlo.broadcast_in_dim_v1"(%arg13) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x1024x!vhlo.f32_v1>
    %116 = "vhlo.multiply_v1"(%114, %115) : (!vhlo.tensor_v1<1x24x1x1024x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x1024x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x1024x!vhlo.f32_v1>
    %117 = "vhlo.convert_v1"(%116) : (!vhlo.tensor_v1<1x24x1x1024x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x1024x!vhlo.bf16_v1>
    %118 = "vhlo.reshape_v1"(%arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x!vhlo.bf16_v1>
    %119 = "vhlo.broadcast_in_dim_v1"(%118) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>
    %120 = "vhlo.convert_v1"(%119) : (!vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x!vhlo.f32_v1>
    %121 = "vhlo.broadcast_in_dim_v1"(%54) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1024x!vhlo.i64_v1>
    %122 = "vhlo.compare_v1"(%2, %121) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<1x1024x!vhlo.i64_v1>, !vhlo.tensor_v1<1x1024x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1024x!vhlo.bool_v1>
    %123 = "vhlo.convert_v1"(%122) : (!vhlo.tensor_v1<1x1024x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x1024x!vhlo.f32_v1>
    %124 = "vhlo.multiply_v1"(%120, %123) : (!vhlo.tensor_v1<1x1024x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1024x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1024x!vhlo.f32_v1>
    %125 = "vhlo.convert_v1"(%124) : (!vhlo.tensor_v1<1x1024x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>
    %126 = "vhlo.reshape_v1"(%125) : (!vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1024x!vhlo.bf16_v1>
    %127 = "vhlo.broadcast_in_dim_v1"(%126) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x1024x!vhlo.bf16_v1>
    %128 = "vhlo.add_v1"(%117, %127) : (!vhlo.tensor_v1<1x24x1x1024x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x1x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x1024x!vhlo.bf16_v1>
    %129 = "vhlo.convert_v1"(%128) : (!vhlo.tensor_v1<1x24x1x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x1024x!vhlo.f32_v1>
    %130 = "vhlo.reduce_v1"(%129, %1) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %228 = "vhlo.maximum_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%228) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x1x1024x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x!vhlo.f32_v1>
    %131 = "vhlo.broadcast_in_dim_v1"(%130) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x1024x!vhlo.f32_v1>
    %132 = "vhlo.subtract_v1"(%129, %131) : (!vhlo.tensor_v1<1x24x1x1024x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x1024x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x1024x!vhlo.f32_v1>
    %133 = "vhlo.exponential_v2"(%132) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x24x1x1024x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x1024x!vhlo.f32_v1>
    %134 = "vhlo.reduce_v1"(%133, %0) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %228 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%228) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x1x1024x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x!vhlo.f32_v1>
    %135 = "vhlo.broadcast_in_dim_v1"(%134) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x1024x!vhlo.f32_v1>
    %136 = "vhlo.divide_v1"(%133, %135) : (!vhlo.tensor_v1<1x24x1x1024x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x1024x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x1024x!vhlo.f32_v1>
    %137 = "vhlo.convert_v1"(%136) : (!vhlo.tensor_v1<1x24x1x1024x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x1024x!vhlo.bf16_v1>
    %138 = "vhlo.reshape_v1"(%137) : (!vhlo.tensor_v1<1x24x1x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x1x1024x!vhlo.bf16_v1>
    %139 = "vhlo.custom_call_v1"(%arg11) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_3">}>} : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>
    %140 = "vhlo.reshape_v1"(%arg5) : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>
    %141 = "vhlo.custom_call_v1"(%140) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>
    %142 = "vhlo.reshape_v1"(%141) : (!vhlo.tensor_v1<1x1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>
    %143 = "vhlo.transpose_v1"(%142) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,1024]{0,1}">} : (!vhlo.tensor_v1<1024x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>
    %144 = "vhlo.dot_general_v2"(%41, %143) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>
    %145 = "vhlo.reshape_v1"(%144) : (!vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %146 = "vhlo.scatter_v2"(%139, %87, %145) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg22: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg22) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>
    %147 = "vhlo.broadcast_in_dim_v1"(%146) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x1024x128x!vhlo.bf16_v1>
    %148 = "vhlo.reshape_v1"(%147) : (!vhlo.tensor_v1<1x8x3x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x1024x128x!vhlo.bf16_v1>
    %149 = "vhlo.dot_general_v2"(%138, %148) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x1x1024x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>
    %150 = "vhlo.reshape_v1"(%149) : (!vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %151 = "vhlo.reshape_v1"(%arg4) : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>
    %152 = "vhlo.custom_call_v1"(%151) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_o_proj_weight">}>} : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>
    %153 = "vhlo.reshape_v1"(%152) : (!vhlo.tensor_v1<1x3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %154 = "vhlo.transpose_v1"(%153) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,3072]{0,1}">} : (!vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>
    %155 = "vhlo.dot_general_v2"(%150, %154) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %156 = "vhlo.reshape_v1"(%155) : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %157 = "vhlo.add_v1"(%20, %156) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %158 = "vhlo.reshape_v1"(%arg18) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %159 = "vhlo.custom_call_v1"(%158) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___post_attention_layernorm_weight">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %160 = "vhlo.reshape_v1"(%159) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1>
    %161 = "vhlo.convert_v1"(%160) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %162 = "vhlo.reshape_v1"(%161) : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %163 = "vhlo.convert_v1"(%157) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %164 = "vhlo.power_v1"(%163, %6) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %165 = "vhlo.reduce_v1"(%164, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %228 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%228) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %166 = "vhlo.multiply_v1"(%165, %4) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %167 = "vhlo.reshape_v1"(%166) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %168 = "vhlo.add_v1"(%167, %31) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %169 = "vhlo.rsqrt_v2"(%168) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %170 = "vhlo.reshape_v1"(%169) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %171 = "vhlo.broadcast_in_dim_v1"(%170) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %172 = "vhlo.multiply_v1"(%163, %171) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %173 = "vhlo.convert_v1"(%172) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %174 = "vhlo.convert_v1"(%173) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %175 = "vhlo.multiply_v1"(%162, %174) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %176 = "vhlo.convert_v1"(%175) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %177 = "vhlo.reshape_v1"(%176) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %178 = "vhlo.reshape_v1"(%arg19) : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>
    %179 = "vhlo.custom_call_v1"(%178) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_gate_proj_weight">}>} : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>
    %180 = "vhlo.reshape_v1"(%179) : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>
    %181 = "vhlo.transpose_v1"(%180) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,8192]{0,1}">} : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>
    %182 = "vhlo.dot_general_v2"(%177, %181) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>
    %183 = "vhlo.reshape_v1"(%182) : (!vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %184 = "vhlo.convert_v1"(%183) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %185 = "vhlo.logistic_v2"(%183) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %186 = "vhlo.convert_v1"(%185) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %187 = "vhlo.multiply_v1"(%184, %186) : (!vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %188 = "vhlo.convert_v1"(%187) : (!vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %189 = "vhlo.convert_v1"(%188) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %190 = "vhlo.reshape_v1"(%arg3) : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>
    %191 = "vhlo.custom_call_v1"(%190) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_up_proj_weight">}>} : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>
    %192 = "vhlo.reshape_v1"(%191) : (!vhlo.tensor_v1<1x8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>
    %193 = "vhlo.transpose_v1"(%192) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,8192]{0,1}">} : (!vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>
    %194 = "vhlo.dot_general_v2"(%177, %193) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>
    %195 = "vhlo.reshape_v1"(%194) : (!vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %196 = "vhlo.convert_v1"(%195) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %197 = "vhlo.multiply_v1"(%189, %196) : (!vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %198 = "vhlo.convert_v1"(%197) : (!vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %199 = "vhlo.reshape_v1"(%198) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>
    %200 = "vhlo.reshape_v1"(%arg2) : (!vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1>
    %201 = "vhlo.custom_call_v1"(%200) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_down_proj_weight">}>} : (!vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1>
    %202 = "vhlo.reshape_v1"(%201) : (!vhlo.tensor_v1<1x3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>
    %203 = "vhlo.transpose_v1"(%202) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,3072]{0,1}">} : (!vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>
    %204 = "vhlo.dot_general_v2"(%199, %203) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %205 = "vhlo.reshape_v1"(%204) : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %206 = "vhlo.add_v1"(%157, %205) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %207 = "vhlo.convert_v1"(%206) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %208 = "vhlo.power_v1"(%207, %6) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %209 = "vhlo.reduce_v1"(%208, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg21: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg22: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %228 = "vhlo.add_v1"(%arg21, %arg22) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%228) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %210 = "vhlo.multiply_v1"(%209, %4) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %211 = "vhlo.reshape_v1"(%210) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %212 = "vhlo.add_v1"(%211, %31) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %213 = "vhlo.rsqrt_v2"(%212) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %214 = "vhlo.reshape_v1"(%213) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %215 = "vhlo.broadcast_in_dim_v1"(%214) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %216 = "vhlo.multiply_v1"(%207, %215) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %217 = "vhlo.convert_v1"(%216) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %218 = "vhlo.convert_v1"(%217) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %219 = "vhlo.multiply_v1"(%11, %218) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %220 = "vhlo.convert_v1"(%219) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %221 = "vhlo.reshape_v1"(%220) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %222 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>
    %223 = "vhlo.custom_call_v1"(%222) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___lm_head_weight">}>} : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>
    %224 = "vhlo.reshape_v1"(%223) : (!vhlo.tensor_v1<1x128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>
    %225 = "vhlo.transpose_v1"(%224) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,128256]{0,1}">} : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x128256x!vhlo.bf16_v1>
    %226 = "vhlo.dot_general_v2"(%221, %225) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x!vhlo.bf16_v1>
    %227 = "vhlo.reshape_v1"(%226) : (!vhlo.tensor_v1<1x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x128256x!vhlo.bf16_v1>
    "vhlo.return_v1"(%226, %227) : (!vhlo.tensor_v1<1x128256x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x128256x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}
2025-09-15 15:42:03.651 (  71.977s) [        F073E000]      module_builder.cc:185      1| Is using shardy? true
2025-09-15 15:42:03.670 (  71.996s) [        F073E000]      module_builder.cc:203      1| SHLO Module:
module @SyncTensorsGraph.420 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg1: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg2: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>}, %arg3: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg4: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>}, %arg5: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg6: tensor<1x1xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg7: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg8: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg9: tensor<1xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg10: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg11: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}, %arg12: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg13: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg14: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg15: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg16: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}) -> (tensor<1x128256xbf16>, tensor<1x1x128256xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c = stablehlo.constant dense<"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tensor<1x1024xi64>
    %c_1 = stablehlo.constant dense<0> : tensor<1xi64>
    %cst_2 = stablehlo.constant dense<3.25520843E-4> : tensor<1x1xf32>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %0 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x1x3072xf32>
    %1 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %2 = stablehlo.custom_call @tt.mark_argument(%1) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_norm_weight"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %3 = stablehlo.convert %2 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %4 = stablehlo.reshape %arg7 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %5 = stablehlo.custom_call @tt.mark_argument(%4) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_embed_tokens_weight"}} : (tensor<1x128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %6 = stablehlo.reshape %5 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %7 = stablehlo.reshape %arg6 : (tensor<1x1xi64>) -> tensor<1x1x1xi64>
    %8 = stablehlo.custom_call @tt.mark_argument(%7) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_0"}} : (tensor<1x1x1xi64>) -> tensor<1x1x1xi64>
    %9 = stablehlo.convert %8 : (tensor<1x1x1xi64>) -> tensor<1x1x1xui32>
    %10 = stablehlo.reshape %9 : (tensor<1x1x1xui32>) -> tensor<1xui32>
    %11 = "stablehlo.gather"(%6, %10) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<1xui32>) -> tensor<1x3072xbf16>
    %12 = stablehlo.reshape %11 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %13 = stablehlo.reshape %arg8 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %14 = stablehlo.custom_call @tt.mark_argument(%13) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %15 = stablehlo.convert %14 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %16 = stablehlo.convert %12 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %17 = stablehlo.power %16, %0 : tensor<1x1x3072xf32>
    %18 = stablehlo.reduce(%17 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %19 = stablehlo.multiply %18, %cst_2 : tensor<1x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %21 = stablehlo.reshape %arg1 : (tensor<f32>) -> tensor<1x1x1xf32>
    %22 = stablehlo.add %20, %21 : tensor<1x1x1xf32>
    %23 = stablehlo.rsqrt %22 : tensor<1x1x1xf32>
    %24 = stablehlo.reshape %23 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %25 = stablehlo.broadcast_in_dim %24, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %26 = stablehlo.multiply %16, %25 : tensor<1x1x3072xf32>
    %27 = stablehlo.convert %26 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %28 = stablehlo.convert %27 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %29 = stablehlo.multiply %15, %28 : tensor<1x1x3072xf32>
    %30 = stablehlo.convert %29 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %32 = stablehlo.reshape %arg17 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %33 = stablehlo.custom_call @tt.mark_argument(%32) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}} : (tensor<1x3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %34 = stablehlo.reshape %33 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %35 = stablehlo.transpose %34, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %36 = stablehlo.dot_general %31, %35, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %37 = stablehlo.reshape %36 : (tensor<1x3072xbf16>) -> tensor<1x24x1x128xbf16>
    %38 = stablehlo.convert %37 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,1,128]{3,1,2,0}"} : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %39 = stablehlo.reshape %arg14 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %40 = stablehlo.custom_call @tt.mark_argument(%39) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "l__self___model_rotary_emb_inv_freq"}} : (tensor<1x1x64xf32>) -> tensor<1x1x64xf32>
    %41 = stablehlo.reshape %40 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %42 = stablehlo.reshape %arg9 : (tensor<1xi64>) -> tensor<1x1x1xi64>
    %43 = stablehlo.custom_call @tt.mark_argument(%42) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_1"}} : (tensor<1x1x1xi64>) -> tensor<1x1x1xi64>
    %44 = stablehlo.reshape %43 : (tensor<1x1x1xi64>) -> tensor<1xi64>
    %45 = stablehlo.convert %43 : (tensor<1x1x1xi64>) -> tensor<1x1x1xf32>
    %46 = stablehlo.dot_general %41, %45, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x1xf32>) -> tensor<1x64x1xf32>
    %47 = stablehlo.reshape %46 : (tensor<1x64x1xf32>) -> tensor<1x1x64xf32>
    %48 = stablehlo.concatenate %47, %47, dim = 2 : (tensor<1x1x64xf32>, tensor<1x1x64xf32>) -> tensor<1x1x128xf32>
    %49 = stablehlo.cosine %48 : tensor<1x1x128xf32>
    %50 = stablehlo.convert %49 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
    %51 = stablehlo.convert %50 : (tensor<1x1x128xbf16>) -> tensor<1x1x128xf32>
    %52 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x24x1x128xf32>
    %53 = stablehlo.multiply %38, %52 : tensor<1x24x1x128xf32>
    %54 = stablehlo.convert %53 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %55 = stablehlo.slice %37 [0:1, 0:24, 0:1, 64:128] : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x64xbf16>
    %56 = stablehlo.negate %55 : tensor<1x24x1x64xbf16>
    %57 = stablehlo.slice %37 [0:1, 0:24, 0:1, 0:64] : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x64xbf16>
    %58 = stablehlo.concatenate %56, %57, dim = 3 : (tensor<1x24x1x64xbf16>, tensor<1x24x1x64xbf16>) -> tensor<1x24x1x128xbf16>
    %59 = stablehlo.convert %58 : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %60 = stablehlo.sine %48 : tensor<1x1x128xf32>
    %61 = stablehlo.convert %60 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
    %62 = stablehlo.convert %61 : (tensor<1x1x128xbf16>) -> tensor<1x1x128xf32>
    %63 = stablehlo.broadcast_in_dim %62, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x24x1x128xf32>
    %64 = stablehlo.multiply %59, %63 : tensor<1x24x1x128xf32>
    %65 = stablehlo.convert %64 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %66 = stablehlo.add %54, %65 : tensor<1x24x1x128xbf16>
    %67 = stablehlo.reshape %66 : (tensor<1x24x1x128xbf16>) -> tensor<24x1x128xbf16>
    %68 = stablehlo.custom_call @tt.mark_argument(%arg16) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_2"}} : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %69 = stablehlo.compare  LT, %44, %c_1 : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
    %70 = stablehlo.reshape %arg10 : (tensor<i64>) -> tensor<1xi64>
    %71 = stablehlo.add %44, %70 : tensor<1xi64>
    %72 = stablehlo.select %69, %71, %44 : tensor<1xi1>, tensor<1xi64>
    %73 = stablehlo.reshape %72 : (tensor<1xi64>) -> tensor<1x1xi64>
    %74 = stablehlo.reshape %arg15 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %75 = stablehlo.custom_call @tt.mark_argument(%74) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}} : (tensor<1x1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %76 = stablehlo.reshape %75 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %77 = stablehlo.transpose %76, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %78 = stablehlo.dot_general %31, %77, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %79 = stablehlo.reshape %78 : (tensor<1x1024xbf16>) -> tensor<1x8x1x128xbf16>
    %80 = stablehlo.convert %79 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,1,128]{3,1,2,0}"} : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xf32>
    %81 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x8x1x128xf32>
    %82 = stablehlo.multiply %80, %81 : tensor<1x8x1x128xf32>
    %83 = stablehlo.convert %82 : (tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xbf16>
    %84 = stablehlo.slice %79 [0:1, 0:8, 0:1, 64:128] : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x64xbf16>
    %85 = stablehlo.negate %84 : tensor<1x8x1x64xbf16>
    %86 = stablehlo.slice %79 [0:1, 0:8, 0:1, 0:64] : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x64xbf16>
    %87 = stablehlo.concatenate %85, %86, dim = 3 : (tensor<1x8x1x64xbf16>, tensor<1x8x1x64xbf16>) -> tensor<1x8x1x128xbf16>
    %88 = stablehlo.convert %87 : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xf32>
    %89 = stablehlo.broadcast_in_dim %62, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x8x1x128xf32>
    %90 = stablehlo.multiply %88, %89 : tensor<1x8x1x128xf32>
    %91 = stablehlo.convert %90 : (tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xbf16>
    %92 = stablehlo.add %83, %91 : tensor<1x8x1x128xbf16>
    %93 = "stablehlo.scatter"(%68, %73, %92) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x1024x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %94 = stablehlo.broadcast_in_dim %93, dims = [0, 1, 3, 4] : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x3x1024x128xbf16>
    %95 = stablehlo.reshape %94 : (tensor<1x8x3x1024x128xbf16>) -> tensor<1x24x1024x128xbf16>
    %96 = stablehlo.transpose %95, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,1024]{2,3,1,0}"} : (tensor<1x24x1024x128xbf16>) -> tensor<1x24x128x1024xbf16>
    %97 = stablehlo.reshape %96 : (tensor<1x24x128x1024xbf16>) -> tensor<24x128x1024xbf16>
    %98 = stablehlo.dot_general %67, %97, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x1x128xbf16>, tensor<24x128x1024xbf16>) -> tensor<24x1x1024xbf16>
    %99 = stablehlo.convert %98 : (tensor<24x1x1024xbf16>) -> tensor<24x1x1024xf32>
    %100 = stablehlo.reshape %99 : (tensor<24x1x1024xf32>) -> tensor<1x24x1x1024xf32>
    %101 = stablehlo.broadcast_in_dim %arg13, dims = [] : (tensor<f32>) -> tensor<1x24x1x1024xf32>
    %102 = stablehlo.multiply %100, %101 : tensor<1x24x1x1024xf32>
    %103 = stablehlo.convert %102 : (tensor<1x24x1x1024xf32>) -> tensor<1x24x1x1024xbf16>
    %104 = stablehlo.reshape %arg12 : (tensor<bf16>) -> tensor<1xbf16>
    %105 = stablehlo.broadcast_in_dim %104, dims = [0] : (tensor<1xbf16>) -> tensor<1x1024xbf16>
    %106 = stablehlo.convert %105 : (tensor<1x1024xbf16>) -> tensor<1x1024xf32>
    %107 = stablehlo.broadcast_in_dim %44, dims = [0] : (tensor<1xi64>) -> tensor<1x1024xi64>
    %108 = stablehlo.compare  GT, %c, %107 : (tensor<1x1024xi64>, tensor<1x1024xi64>) -> tensor<1x1024xi1>
    %109 = stablehlo.convert %108 : (tensor<1x1024xi1>) -> tensor<1x1024xf32>
    %110 = stablehlo.multiply %106, %109 : tensor<1x1024xf32>
    %111 = stablehlo.convert %110 : (tensor<1x1024xf32>) -> tensor<1x1024xbf16>
    %112 = stablehlo.reshape %111 : (tensor<1x1024xbf16>) -> tensor<1x1x1024xbf16>
    %113 = stablehlo.broadcast_in_dim %112, dims = [0, 2, 3] : (tensor<1x1x1024xbf16>) -> tensor<1x24x1x1024xbf16>
    %114 = stablehlo.add %103, %113 : tensor<1x24x1x1024xbf16>
    %115 = stablehlo.convert %114 : (tensor<1x24x1x1024xbf16>) -> tensor<1x24x1x1024xf32>
    %116 = stablehlo.reduce(%115 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x1x1024xf32>, tensor<f32>) -> tensor<1x24x1xf32>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1, 2] : (tensor<1x24x1xf32>) -> tensor<1x24x1x1024xf32>
    %118 = stablehlo.subtract %115, %117 : tensor<1x24x1x1024xf32>
    %119 = stablehlo.exponential %118 : tensor<1x24x1x1024xf32>
    %120 = stablehlo.reduce(%119 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x1x1024xf32>, tensor<f32>) -> tensor<1x24x1xf32>
    %121 = stablehlo.broadcast_in_dim %120, dims = [0, 1, 2] : (tensor<1x24x1xf32>) -> tensor<1x24x1x1024xf32>
    %122 = stablehlo.divide %119, %121 : tensor<1x24x1x1024xf32>
    %123 = stablehlo.convert %122 : (tensor<1x24x1x1024xf32>) -> tensor<1x24x1x1024xbf16>
    %124 = stablehlo.reshape %123 : (tensor<1x24x1x1024xbf16>) -> tensor<24x1x1024xbf16>
    %125 = stablehlo.custom_call @tt.mark_argument(%arg11) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_3"}} : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %126 = stablehlo.reshape %arg5 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %127 = stablehlo.custom_call @tt.mark_argument(%126) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}} : (tensor<1x1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %128 = stablehlo.reshape %127 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %129 = stablehlo.transpose %128, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %130 = stablehlo.dot_general %31, %129, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %131 = stablehlo.reshape %130 : (tensor<1x1024xbf16>) -> tensor<1x8x1x128xbf16>
    %132 = "stablehlo.scatter"(%125, %73, %131) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x1024x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %133 = stablehlo.broadcast_in_dim %132, dims = [0, 1, 3, 4] : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x3x1024x128xbf16>
    %134 = stablehlo.reshape %133 : (tensor<1x8x3x1024x128xbf16>) -> tensor<24x1024x128xbf16>
    %135 = stablehlo.dot_general %124, %134, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x1x1024xbf16>, tensor<24x1024x128xbf16>) -> tensor<24x1x128xbf16>
    %136 = stablehlo.reshape %135 : (tensor<24x1x128xbf16>) -> tensor<1x3072xbf16>
    %137 = stablehlo.reshape %arg4 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %138 = stablehlo.custom_call @tt.mark_argument(%137) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}} : (tensor<1x3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %139 = stablehlo.reshape %138 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %140 = stablehlo.transpose %139, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %141 = stablehlo.dot_general %136, %140, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %142 = stablehlo.reshape %141 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %143 = stablehlo.add %12, %142 : tensor<1x1x3072xbf16>
    %144 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %145 = stablehlo.custom_call @tt.mark_argument(%144) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %146 = stablehlo.convert %145 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %147 = stablehlo.convert %143 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %148 = stablehlo.power %147, %0 : tensor<1x1x3072xf32>
    %149 = stablehlo.reduce(%148 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %150 = stablehlo.multiply %149, %cst_2 : tensor<1x1xf32>
    %151 = stablehlo.reshape %150 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %152 = stablehlo.add %151, %21 : tensor<1x1x1xf32>
    %153 = stablehlo.rsqrt %152 : tensor<1x1x1xf32>
    %154 = stablehlo.reshape %153 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %155 = stablehlo.broadcast_in_dim %154, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %156 = stablehlo.multiply %147, %155 : tensor<1x1x3072xf32>
    %157 = stablehlo.convert %156 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %158 = stablehlo.convert %157 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %159 = stablehlo.multiply %146, %158 : tensor<1x1x3072xf32>
    %160 = stablehlo.convert %159 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %161 = stablehlo.reshape %160 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %162 = stablehlo.reshape %arg19 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %163 = stablehlo.custom_call @tt.mark_argument(%162) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}} : (tensor<1x8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %164 = stablehlo.reshape %163 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %165 = stablehlo.transpose %164, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %166 = stablehlo.dot_general %161, %165, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %167 = stablehlo.reshape %166 : (tensor<1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %168 = stablehlo.convert %167 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %169 = stablehlo.logistic %167 : tensor<1x1x8192xbf16>
    %170 = stablehlo.convert %169 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %171 = stablehlo.multiply %168, %170 : tensor<1x1x8192xf32>
    %172 = stablehlo.convert %171 : (tensor<1x1x8192xf32>) -> tensor<1x1x8192xbf16>
    %173 = stablehlo.convert %172 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %174 = stablehlo.reshape %arg3 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %175 = stablehlo.custom_call @tt.mark_argument(%174) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}} : (tensor<1x8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %176 = stablehlo.reshape %175 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %177 = stablehlo.transpose %176, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %178 = stablehlo.dot_general %161, %177, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %179 = stablehlo.convert %178 : (tensor<1x8192xbf16>) -> tensor<1x8192xf32>
    %180 = stablehlo.reshape %179 : (tensor<1x8192xf32>) -> tensor<1x1x8192xf32>
    %181 = stablehlo.multiply %173, %180 : tensor<1x1x8192xf32>
    %182 = stablehlo.convert %181 : (tensor<1x1x8192xf32>) -> tensor<1x1x8192xbf16>
    %183 = stablehlo.reshape %182 : (tensor<1x1x8192xbf16>) -> tensor<1x8192xbf16>
    %184 = stablehlo.reshape %arg2 : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %185 = stablehlo.custom_call @tt.mark_argument(%184) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}} : (tensor<1x3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %186 = stablehlo.reshape %185 : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %187 = stablehlo.transpose %186, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %188 = stablehlo.dot_general %183, %187, contracting_dims = [1] x [0] : (tensor<1x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<1x3072xbf16>
    %189 = stablehlo.reshape %188 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %190 = stablehlo.add %143, %189 : tensor<1x1x3072xbf16>
    %191 = stablehlo.convert %190 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %192 = stablehlo.power %191, %0 : tensor<1x1x3072xf32>
    %193 = stablehlo.reduce(%192 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %194 = stablehlo.multiply %193, %cst_2 : tensor<1x1xf32>
    %195 = stablehlo.reshape %194 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %196 = stablehlo.add %195, %21 : tensor<1x1x1xf32>
    %197 = stablehlo.rsqrt %196 : tensor<1x1x1xf32>
    %198 = stablehlo.reshape %197 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %199 = stablehlo.broadcast_in_dim %198, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %200 = stablehlo.multiply %191, %199 : tensor<1x1x3072xf32>
    %201 = stablehlo.convert %200 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %202 = stablehlo.convert %201 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %203 = stablehlo.multiply %3, %202 : tensor<1x1x3072xf32>
    %204 = stablehlo.convert %203 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %205 = stablehlo.reshape %204 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %206 = stablehlo.reshape %arg0 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %207 = stablehlo.custom_call @tt.mark_argument(%206) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___lm_head_weight"}} : (tensor<1x128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %208 = stablehlo.reshape %207 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %209 = stablehlo.transpose %208, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %210 = stablehlo.dot_general %205, %209, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<1x128256xbf16>
    %211 = stablehlo.reshape %210 : (tensor<1x128256xbf16>) -> tensor<1x1x128256xbf16>
    return %210, %211 : tensor<1x128256xbf16>, tensor<1x1x128256xbf16>
  }
}
2025-09-15 15:42:03.680 (  72.007s) [        F073E000]      module_builder.cc:212      1| SHLO Module after frontend StableHLO pipeline:
module @SyncTensorsGraph.420 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___lm_head_weight"}, %arg1: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_0"}, %arg2: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg3: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg4: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg5: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg6: tensor<1x1xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"}, %arg7: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg8: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg9: tensor<1xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_1"}, %arg10: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_1"}, %arg11: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_3"}, %arg12: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_2"}, %arg13: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "auto_annotated_const_3"}, %arg14: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg15: tensor<1024x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg16: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_2"}, %arg17: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x128256xbf16>, tensor<1x1x128256xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %c = stablehlo.constant dense<"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tensor<1x1024xi64>
    %c_1 = stablehlo.constant dense<0> : tensor<1xi64>
    %cst_2 = stablehlo.constant dense<3.25520843E-4> : tensor<1x1xf32>
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %0 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x1x3072xf32>
    %1 = stablehlo.reshape %arg20 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %2 = stablehlo.convert %1 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %3 = stablehlo.reshape %arg7 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %4 = stablehlo.reshape %3 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %5 = stablehlo.reshape %arg6 : (tensor<1x1xi64>) -> tensor<1x1x1xi64>
    %6 = stablehlo.convert %5 : (tensor<1x1x1xi64>) -> tensor<1x1x1xui32>
    %7 = stablehlo.reshape %6 : (tensor<1x1x1xui32>) -> tensor<1xui32>
    %8 = "stablehlo.gather"(%4, %7) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<1xui32>) -> tensor<1x3072xbf16>
    %9 = stablehlo.reshape %8 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %10 = stablehlo.reshape %arg8 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %11 = stablehlo.convert %10 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %12 = stablehlo.convert %9 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %13 = stablehlo.power %12, %0 : tensor<1x1x3072xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %15 = stablehlo.multiply %14, %cst_2 : tensor<1x1xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %17 = stablehlo.reshape %arg1 : (tensor<f32>) -> tensor<1x1x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x1x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x1x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x1x3072xf32>
    %23 = stablehlo.convert %22 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %24 = stablehlo.convert %23 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %25 = stablehlo.multiply %11, %24 : tensor<1x1x3072xf32>
    %26 = stablehlo.convert %25 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %27 = stablehlo.reshape %26 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %28 = stablehlo.reshape %arg17 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %29 = stablehlo.reshape %28 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %30 = stablehlo.transpose %29, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %31 = stablehlo.dot_general %27, %30, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %32 = stablehlo.reshape %31 : (tensor<1x3072xbf16>) -> tensor<1x24x1x128xbf16>
    %33 = stablehlo.convert %32 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,1,128]{3,1,2,0}"} : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %34 = stablehlo.reshape %arg14 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %35 = stablehlo.reshape %34 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %36 = stablehlo.reshape %arg9 : (tensor<1xi64>) -> tensor<1x1x1xi64>
    %37 = stablehlo.reshape %36 : (tensor<1x1x1xi64>) -> tensor<1xi64>
    %38 = stablehlo.convert %36 : (tensor<1x1x1xi64>) -> tensor<1x1x1xf32>
    %39 = stablehlo.dot_general %35, %38, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x1xf32>) -> tensor<1x64x1xf32>
    %40 = stablehlo.reshape %39 : (tensor<1x64x1xf32>) -> tensor<1x1x64xf32>
    %41 = stablehlo.concatenate %40, %40, dim = 2 : (tensor<1x1x64xf32>, tensor<1x1x64xf32>) -> tensor<1x1x128xf32>
    %42 = stablehlo.cosine %41 : tensor<1x1x128xf32>
    %43 = stablehlo.convert %42 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
    %44 = stablehlo.convert %43 : (tensor<1x1x128xbf16>) -> tensor<1x1x128xf32>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x24x1x128xf32>
    %46 = stablehlo.multiply %33, %45 : tensor<1x24x1x128xf32>
    %47 = stablehlo.convert %46 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %48 = stablehlo.slice %32 [0:1, 0:24, 0:1, 64:128] : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x64xbf16>
    %49 = stablehlo.negate %48 : tensor<1x24x1x64xbf16>
    %50 = stablehlo.slice %32 [0:1, 0:24, 0:1, 0:64] : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x64xbf16>
    %51 = stablehlo.concatenate %49, %50, dim = 3 : (tensor<1x24x1x64xbf16>, tensor<1x24x1x64xbf16>) -> tensor<1x24x1x128xbf16>
    %52 = stablehlo.convert %51 : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %53 = stablehlo.sine %41 : tensor<1x1x128xf32>
    %54 = stablehlo.convert %53 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
    %55 = stablehlo.convert %54 : (tensor<1x1x128xbf16>) -> tensor<1x1x128xf32>
    %56 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x24x1x128xf32>
    %57 = stablehlo.multiply %52, %56 : tensor<1x24x1x128xf32>
    %58 = stablehlo.convert %57 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %59 = stablehlo.add %47, %58 : tensor<1x24x1x128xbf16>
    %60 = stablehlo.reshape %59 : (tensor<1x24x1x128xbf16>) -> tensor<24x1x128xbf16>
    %61 = stablehlo.compare  LT, %37, %c_1 : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
    %62 = stablehlo.reshape %arg10 : (tensor<i64>) -> tensor<1xi64>
    %63 = stablehlo.add %37, %62 : tensor<1xi64>
    %64 = stablehlo.select %61, %63, %37 : tensor<1xi1>, tensor<1xi64>
    %65 = stablehlo.reshape %64 : (tensor<1xi64>) -> tensor<1x1xi64>
    %66 = stablehlo.reshape %arg15 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %67 = stablehlo.reshape %66 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %68 = stablehlo.transpose %67, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %69 = stablehlo.dot_general %27, %68, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %70 = stablehlo.reshape %69 : (tensor<1x1024xbf16>) -> tensor<1x8x1x128xbf16>
    %71 = stablehlo.convert %70 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,1,128]{3,1,2,0}"} : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xf32>
    %72 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x8x1x128xf32>
    %73 = stablehlo.multiply %71, %72 : tensor<1x8x1x128xf32>
    %74 = stablehlo.convert %73 : (tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xbf16>
    %75 = stablehlo.slice %70 [0:1, 0:8, 0:1, 64:128] : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x64xbf16>
    %76 = stablehlo.negate %75 : tensor<1x8x1x64xbf16>
    %77 = stablehlo.slice %70 [0:1, 0:8, 0:1, 0:64] : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x64xbf16>
    %78 = stablehlo.concatenate %76, %77, dim = 3 : (tensor<1x8x1x64xbf16>, tensor<1x8x1x64xbf16>) -> tensor<1x8x1x128xbf16>
    %79 = stablehlo.convert %78 : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xf32>
    %80 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x8x1x128xf32>
    %81 = stablehlo.multiply %79, %80 : tensor<1x8x1x128xf32>
    %82 = stablehlo.convert %81 : (tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xbf16>
    %83 = stablehlo.add %74, %82 : tensor<1x8x1x128xbf16>
    %84 = "stablehlo.scatter"(%arg16, %65, %83) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x1024x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %85 = stablehlo.broadcast_in_dim %84, dims = [0, 1, 3, 4] : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x3x1024x128xbf16>
    %86 = stablehlo.reshape %85 : (tensor<1x8x3x1024x128xbf16>) -> tensor<1x24x1024x128xbf16>
    %87 = stablehlo.transpose %86, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,1024]{2,3,1,0}"} : (tensor<1x24x1024x128xbf16>) -> tensor<1x24x128x1024xbf16>
    %88 = stablehlo.reshape %87 : (tensor<1x24x128x1024xbf16>) -> tensor<24x128x1024xbf16>
    %89 = stablehlo.dot_general %60, %88, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x1x128xbf16>, tensor<24x128x1024xbf16>) -> tensor<24x1x1024xbf16>
    %90 = stablehlo.convert %89 : (tensor<24x1x1024xbf16>) -> tensor<24x1x1024xf32>
    %91 = stablehlo.reshape %90 : (tensor<24x1x1024xf32>) -> tensor<1x24x1x1024xf32>
    %92 = stablehlo.broadcast_in_dim %arg13, dims = [] : (tensor<f32>) -> tensor<1x24x1x1024xf32>
    %93 = stablehlo.multiply %91, %92 : tensor<1x24x1x1024xf32>
    %94 = stablehlo.convert %93 : (tensor<1x24x1x1024xf32>) -> tensor<1x24x1x1024xbf16>
    %95 = stablehlo.reshape %arg12 : (tensor<bf16>) -> tensor<1xbf16>
    %96 = stablehlo.broadcast_in_dim %95, dims = [0] : (tensor<1xbf16>) -> tensor<1x1024xbf16>
    %97 = stablehlo.convert %96 : (tensor<1x1024xbf16>) -> tensor<1x1024xf32>
    %98 = stablehlo.broadcast_in_dim %37, dims = [0] : (tensor<1xi64>) -> tensor<1x1024xi64>
    %99 = stablehlo.compare  GT, %c, %98 : (tensor<1x1024xi64>, tensor<1x1024xi64>) -> tensor<1x1024xi1>
    %100 = stablehlo.convert %99 : (tensor<1x1024xi1>) -> tensor<1x1024xf32>
    %101 = stablehlo.multiply %97, %100 : tensor<1x1024xf32>
    %102 = stablehlo.convert %101 : (tensor<1x1024xf32>) -> tensor<1x1024xbf16>
    %103 = stablehlo.reshape %102 : (tensor<1x1024xbf16>) -> tensor<1x1x1024xbf16>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0, 2, 3] : (tensor<1x1x1024xbf16>) -> tensor<1x24x1x1024xbf16>
    %105 = stablehlo.add %94, %104 : tensor<1x24x1x1024xbf16>
    %106 = stablehlo.convert %105 : (tensor<1x24x1x1024xbf16>) -> tensor<1x24x1x1024xf32>
    %107 = stablehlo.reduce(%106 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x1x1024xf32>, tensor<f32>) -> tensor<1x24x1xf32>
    %108 = stablehlo.broadcast_in_dim %107, dims = [0, 1, 2] : (tensor<1x24x1xf32>) -> tensor<1x24x1x1024xf32>
    %109 = stablehlo.subtract %106, %108 : tensor<1x24x1x1024xf32>
    %110 = stablehlo.exponential %109 : tensor<1x24x1x1024xf32>
    %111 = stablehlo.reduce(%110 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x1x1024xf32>, tensor<f32>) -> tensor<1x24x1xf32>
    %112 = stablehlo.broadcast_in_dim %111, dims = [0, 1, 2] : (tensor<1x24x1xf32>) -> tensor<1x24x1x1024xf32>
    %113 = stablehlo.divide %110, %112 : tensor<1x24x1x1024xf32>
    %114 = stablehlo.convert %113 : (tensor<1x24x1x1024xf32>) -> tensor<1x24x1x1024xbf16>
    %115 = stablehlo.reshape %114 : (tensor<1x24x1x1024xbf16>) -> tensor<24x1x1024xbf16>
    %116 = stablehlo.reshape %arg5 : (tensor<1024x3072xbf16>) -> tensor<1x1024x3072xbf16>
    %117 = stablehlo.reshape %116 : (tensor<1x1024x3072xbf16>) -> tensor<1024x3072xbf16>
    %118 = stablehlo.transpose %117, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<1024x3072xbf16>) -> tensor<3072x1024xbf16>
    %119 = stablehlo.dot_general %27, %118, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %120 = stablehlo.reshape %119 : (tensor<1x1024xbf16>) -> tensor<1x8x1x128xbf16>
    %121 = "stablehlo.scatter"(%arg11, %65, %120) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg21: tensor<bf16>, %arg22: tensor<bf16>):
      stablehlo.return %arg22 : tensor<bf16>
    }) : (tensor<1x8x1024x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %122 = stablehlo.broadcast_in_dim %121, dims = [0, 1, 3, 4] : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x3x1024x128xbf16>
    %123 = stablehlo.reshape %122 : (tensor<1x8x3x1024x128xbf16>) -> tensor<24x1024x128xbf16>
    %124 = stablehlo.dot_general %115, %123, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x1x1024xbf16>, tensor<24x1024x128xbf16>) -> tensor<24x1x128xbf16>
    %125 = stablehlo.reshape %124 : (tensor<24x1x128xbf16>) -> tensor<1x3072xbf16>
    %126 = stablehlo.reshape %arg4 : (tensor<3072x3072xbf16>) -> tensor<1x3072x3072xbf16>
    %127 = stablehlo.reshape %126 : (tensor<1x3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %128 = stablehlo.transpose %127, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %129 = stablehlo.dot_general %125, %128, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %131 = stablehlo.add %9, %130 : tensor<1x1x3072xbf16>
    %132 = stablehlo.reshape %arg18 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %133 = stablehlo.convert %132 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %134 = stablehlo.convert %131 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %135 = stablehlo.power %134, %0 : tensor<1x1x3072xf32>
    %136 = stablehlo.reduce(%135 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %137 = stablehlo.multiply %136, %cst_2 : tensor<1x1xf32>
    %138 = stablehlo.reshape %137 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %139 = stablehlo.add %138, %17 : tensor<1x1x1xf32>
    %140 = stablehlo.rsqrt %139 : tensor<1x1x1xf32>
    %141 = stablehlo.reshape %140 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %143 = stablehlo.multiply %134, %142 : tensor<1x1x3072xf32>
    %144 = stablehlo.convert %143 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %145 = stablehlo.convert %144 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %146 = stablehlo.multiply %133, %145 : tensor<1x1x3072xf32>
    %147 = stablehlo.convert %146 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %148 = stablehlo.reshape %147 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %149 = stablehlo.reshape %arg19 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %150 = stablehlo.reshape %149 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %151 = stablehlo.transpose %150, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %152 = stablehlo.dot_general %148, %151, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %153 = stablehlo.reshape %152 : (tensor<1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %154 = stablehlo.convert %153 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %155 = stablehlo.logistic %153 : tensor<1x1x8192xbf16>
    %156 = stablehlo.convert %155 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %157 = stablehlo.multiply %154, %156 : tensor<1x1x8192xf32>
    %158 = stablehlo.convert %157 : (tensor<1x1x8192xf32>) -> tensor<1x1x8192xbf16>
    %159 = stablehlo.convert %158 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %160 = stablehlo.reshape %arg3 : (tensor<8192x3072xbf16>) -> tensor<1x8192x3072xbf16>
    %161 = stablehlo.reshape %160 : (tensor<1x8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %162 = stablehlo.transpose %161, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<8192x3072xbf16>) -> tensor<3072x8192xbf16>
    %163 = stablehlo.dot_general %148, %162, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %164 = stablehlo.convert %163 : (tensor<1x8192xbf16>) -> tensor<1x8192xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x8192xf32>) -> tensor<1x1x8192xf32>
    %166 = stablehlo.multiply %159, %165 : tensor<1x1x8192xf32>
    %167 = stablehlo.convert %166 : (tensor<1x1x8192xf32>) -> tensor<1x1x8192xbf16>
    %168 = stablehlo.reshape %167 : (tensor<1x1x8192xbf16>) -> tensor<1x8192xbf16>
    %169 = stablehlo.reshape %arg2 : (tensor<3072x8192xbf16>) -> tensor<1x3072x8192xbf16>
    %170 = stablehlo.reshape %169 : (tensor<1x3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %171 = stablehlo.transpose %170, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x8192xbf16>) -> tensor<8192x3072xbf16>
    %172 = stablehlo.dot_general %168, %171, contracting_dims = [1] x [0] : (tensor<1x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<1x3072xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %174 = stablehlo.add %131, %173 : tensor<1x1x3072xbf16>
    %175 = stablehlo.convert %174 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %176 = stablehlo.power %175, %0 : tensor<1x1x3072xf32>
    %177 = stablehlo.reduce(%176 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %178 = stablehlo.multiply %177, %cst_2 : tensor<1x1xf32>
    %179 = stablehlo.reshape %178 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %180 = stablehlo.add %179, %17 : tensor<1x1x1xf32>
    %181 = stablehlo.rsqrt %180 : tensor<1x1x1xf32>
    %182 = stablehlo.reshape %181 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %183 = stablehlo.broadcast_in_dim %182, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %184 = stablehlo.multiply %175, %183 : tensor<1x1x3072xf32>
    %185 = stablehlo.convert %184 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %186 = stablehlo.convert %185 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %187 = stablehlo.multiply %2, %186 : tensor<1x1x3072xf32>
    %188 = stablehlo.convert %187 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %189 = stablehlo.reshape %188 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %190 = stablehlo.reshape %arg0 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %191 = stablehlo.reshape %190 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %192 = stablehlo.transpose %191, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
    %193 = stablehlo.dot_general %189, %192, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<1x128256xbf16>
    %194 = stablehlo.reshape %193 : (tensor<1x128256xbf16>) -> tensor<1x1x128256xbf16>
    return %193, %194 : tensor<1x128256xbf16>, tensor<1x1x128256xbf16>
  }
}
2025-09-15 15:42:03.807 (  72.134s) [        F073E000]      module_builder.cc:486      1| SHLO Module after compiler StableHLO pipeline:
module @SyncTensorsGraph.420 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<128256x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg1: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg2: tensor<3072x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg3: tensor<8192x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg4: tensor<3072x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg5: tensor<1024x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg6: tensor<1x1xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg7: tensor<128256x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg8: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg9: tensor<1xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg10: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg11: tensor<1x8x1024x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg12: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg13: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg14: tensor<64xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg15: tensor<1024x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg16: tensor<1x8x1024x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg17: tensor<3072x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x1x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:2 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %arg16, %arg17, %arg18, %arg19, %arg20) in_shardings=[<@mesh, [{}, {}]>, <@mesh, []>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, []>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, []>, <@mesh, []>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>] out_shardings=[<@mesh, [{}, {}]>, <@mesh, [{}, {}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg21: tensor<128256x3072xbf16>, %arg22: tensor<f32>, %arg23: tensor<3072x4096xbf16>, %arg24: tensor<4096x3072xbf16>, %arg25: tensor<3072x1536xbf16>, %arg26: tensor<512x3072xbf16>, %arg27: tensor<1x1xi64>, %arg28: tensor<128256x3072xbf16>, %arg29: tensor<3072xbf16>, %arg30: tensor<1xi64>, %arg31: tensor<i64>, %arg32: tensor<1x4x1024x128xbf16>, %arg33: tensor<bf16>, %arg34: tensor<f32>, %arg35: tensor<64xf32>, %arg36: tensor<512x3072xbf16>, %arg37: tensor<1x4x1024x128xbf16>, %arg38: tensor<1536x3072xbf16>, %arg39: tensor<3072xbf16>, %arg40: tensor<4096x3072xbf16>, %arg41: tensor<3072xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
      %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
      %c = stablehlo.constant dense<"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tensor<1x1024xi64>
      %c_1 = stablehlo.constant dense<0> : tensor<1xi64>
      %cst_2 = stablehlo.constant dense<3.25520843E-4> : tensor<1x1xf32>
      %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %1 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x1x3072xf32>
      %2 = stablehlo.reshape %arg41 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
      %3 = stablehlo.convert %2 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %4 = stablehlo.reshape %arg28 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
      %5 = stablehlo.reshape %4 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
      %6 = stablehlo.reshape %arg27 : (tensor<1x1xi64>) -> tensor<1x1x1xi64>
      %7 = stablehlo.convert %6 : (tensor<1x1x1xi64>) -> tensor<1x1x1xui32>
      %8 = stablehlo.reshape %7 : (tensor<1x1x1xui32>) -> tensor<1xui32>
      %9 = "stablehlo.gather"(%5, %8) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<1xui32>) -> tensor<1x3072xbf16>
      %10 = stablehlo.reshape %9 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
      %11 = stablehlo.reshape %arg29 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
      %12 = stablehlo.convert %11 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %13 = stablehlo.convert %10 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %14 = stablehlo.power %13, %1 : tensor<1x1x3072xf32>
      %15 = stablehlo.reduce(%14 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
      %16 = stablehlo.multiply %15, %cst_2 : tensor<1x1xf32>
      %17 = stablehlo.reshape %16 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
      %18 = stablehlo.reshape %arg22 : (tensor<f32>) -> tensor<1x1x1xf32>
      %19 = stablehlo.add %17, %18 : tensor<1x1x1xf32>
      %20 = stablehlo.rsqrt %19 : tensor<1x1x1xf32>
      %21 = stablehlo.reshape %20 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
      %22 = stablehlo.broadcast_in_dim %21, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
      %23 = stablehlo.multiply %13, %22 : tensor<1x1x3072xf32>
      %24 = stablehlo.convert %23 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
      %25 = stablehlo.convert %24 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %26 = stablehlo.multiply %12, %25 : tensor<1x1x3072xf32>
      %27 = stablehlo.convert %26 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
      %28 = stablehlo.reshape %27 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
      %29 = stablehlo.reshape %arg38 : (tensor<1536x3072xbf16>) -> tensor<1x1536x3072xbf16>
      %30 = stablehlo.reshape %29 : (tensor<1x1536x3072xbf16>) -> tensor<1536x3072xbf16>
      %31 = stablehlo.transpose %30, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<1536x3072xbf16>) -> tensor<3072x1536xbf16>
      %32 = stablehlo.dot_general %28, %31, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1536xbf16>) -> tensor<1x1536xbf16>
      %33 = stablehlo.reshape %32 : (tensor<1x1536xbf16>) -> tensor<1x12x1x128xbf16>
      %34 = stablehlo.convert %33 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,1,128]{3,1,2,0}"} : (tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xf32>
      %35 = stablehlo.reshape %arg35 : (tensor<64xf32>) -> tensor<1x1x64xf32>
      %36 = stablehlo.reshape %35 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
      %37 = stablehlo.reshape %arg30 : (tensor<1xi64>) -> tensor<1x1x1xi64>
      %38 = stablehlo.reshape %37 : (tensor<1x1x1xi64>) -> tensor<1xi64>
      %39 = stablehlo.convert %37 : (tensor<1x1x1xi64>) -> tensor<1x1x1xf32>
      %40 = stablehlo.dot_general %36, %39, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x1xf32>) -> tensor<1x64x1xf32>
      %41 = stablehlo.reshape %40 : (tensor<1x64x1xf32>) -> tensor<1x1x64xf32>
      %42 = stablehlo.concatenate %41, %41, dim = 2 : (tensor<1x1x64xf32>, tensor<1x1x64xf32>) -> tensor<1x1x128xf32>
      %43 = stablehlo.cosine %42 : tensor<1x1x128xf32>
      %44 = stablehlo.convert %43 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
      %45 = stablehlo.convert %44 : (tensor<1x1x128xbf16>) -> tensor<1x1x128xf32>
      %46 = stablehlo.broadcast_in_dim %45, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x12x1x128xf32>
      %47 = stablehlo.multiply %34, %46 : tensor<1x12x1x128xf32>
      %48 = stablehlo.convert %47 : (tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xbf16>
      %49 = stablehlo.slice %33 [0:1, 0:12, 0:1, 64:128] : (tensor<1x12x1x128xbf16>) -> tensor<1x12x1x64xbf16>
      %50 = stablehlo.negate %49 : tensor<1x12x1x64xbf16>
      %51 = stablehlo.slice %33 [0:1, 0:12, 0:1, 0:64] : (tensor<1x12x1x128xbf16>) -> tensor<1x12x1x64xbf16>
      %52 = stablehlo.concatenate %50, %51, dim = 3 : (tensor<1x12x1x64xbf16>, tensor<1x12x1x64xbf16>) -> tensor<1x12x1x128xbf16>
      %53 = stablehlo.convert %52 : (tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xf32>
      %54 = stablehlo.sine %42 : tensor<1x1x128xf32>
      %55 = stablehlo.convert %54 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
      %56 = stablehlo.convert %55 : (tensor<1x1x128xbf16>) -> tensor<1x1x128xf32>
      %57 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x12x1x128xf32>
      %58 = stablehlo.multiply %53, %57 : tensor<1x12x1x128xf32>
      %59 = stablehlo.convert %58 : (tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xbf16>
      %60 = stablehlo.add %48, %59 : tensor<1x12x1x128xbf16>
      %61 = stablehlo.reshape %60 : (tensor<1x12x1x128xbf16>) -> tensor<12x1x128xbf16>
      %62 = stablehlo.compare  LT, %38, %c_1 : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
      %63 = stablehlo.reshape %arg31 : (tensor<i64>) -> tensor<1xi64>
      %64 = stablehlo.add %38, %63 : tensor<1xi64>
      %65 = stablehlo.select %62, %64, %38 : tensor<1xi1>, tensor<1xi64>
      %66 = stablehlo.reshape %65 : (tensor<1xi64>) -> tensor<1x1xi64>
      %67 = stablehlo.reshape %arg36 : (tensor<512x3072xbf16>) -> tensor<1x512x3072xbf16>
      %68 = stablehlo.reshape %67 : (tensor<1x512x3072xbf16>) -> tensor<512x3072xbf16>
      %69 = stablehlo.transpose %68, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<512x3072xbf16>) -> tensor<3072x512xbf16>
      %70 = stablehlo.dot_general %28, %69, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x512xbf16>) -> tensor<1x512xbf16>
      %71 = stablehlo.reshape %70 : (tensor<1x512xbf16>) -> tensor<1x4x1x128xbf16>
      %72 = stablehlo.convert %71 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,1,128]{3,1,2,0}"} : (tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xf32>
      %73 = stablehlo.broadcast_in_dim %45, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x4x1x128xf32>
      %74 = stablehlo.multiply %72, %73 : tensor<1x4x1x128xf32>
      %75 = stablehlo.convert %74 : (tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xbf16>
      %76 = stablehlo.slice %71 [0:1, 0:4, 0:1, 64:128] : (tensor<1x4x1x128xbf16>) -> tensor<1x4x1x64xbf16>
      %77 = stablehlo.negate %76 : tensor<1x4x1x64xbf16>
      %78 = stablehlo.slice %71 [0:1, 0:4, 0:1, 0:64] : (tensor<1x4x1x128xbf16>) -> tensor<1x4x1x64xbf16>
      %79 = stablehlo.concatenate %77, %78, dim = 3 : (tensor<1x4x1x64xbf16>, tensor<1x4x1x64xbf16>) -> tensor<1x4x1x128xbf16>
      %80 = stablehlo.convert %79 : (tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xf32>
      %81 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x4x1x128xf32>
      %82 = stablehlo.multiply %80, %81 : tensor<1x4x1x128xf32>
      %83 = stablehlo.convert %82 : (tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xbf16>
      %84 = stablehlo.add %75, %83 : tensor<1x4x1x128xbf16>
      %85 = "stablehlo.scatter"(%arg37, %66, %84) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg42: tensor<bf16>, %arg43: tensor<bf16>):
        stablehlo.return %arg43 : tensor<bf16>
      }) : (tensor<1x4x1024x128xbf16>, tensor<1x1xi64>, tensor<1x4x1x128xbf16>) -> tensor<1x4x1024x128xbf16>
      %86 = stablehlo.broadcast_in_dim %85, dims = [0, 1, 3, 4] : (tensor<1x4x1024x128xbf16>) -> tensor<1x4x3x1024x128xbf16>
      %87 = stablehlo.reshape %86 : (tensor<1x4x3x1024x128xbf16>) -> tensor<1x12x1024x128xbf16>
      %88 = stablehlo.transpose %87, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,1024]{2,3,1,0}"} : (tensor<1x12x1024x128xbf16>) -> tensor<1x12x128x1024xbf16>
      %89 = stablehlo.reshape %88 : (tensor<1x12x128x1024xbf16>) -> tensor<12x128x1024xbf16>
      %90 = stablehlo.dot_general %61, %89, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<12x1x128xbf16>, tensor<12x128x1024xbf16>) -> tensor<12x1x1024xbf16>
      %91 = stablehlo.convert %90 : (tensor<12x1x1024xbf16>) -> tensor<12x1x1024xf32>
      %92 = stablehlo.reshape %91 : (tensor<12x1x1024xf32>) -> tensor<1x12x1x1024xf32>
      %93 = stablehlo.broadcast_in_dim %arg34, dims = [] : (tensor<f32>) -> tensor<1x12x1x1024xf32>
      %94 = stablehlo.multiply %92, %93 : tensor<1x12x1x1024xf32>
      %95 = stablehlo.convert %94 : (tensor<1x12x1x1024xf32>) -> tensor<1x12x1x1024xbf16>
      %96 = stablehlo.reshape %arg33 : (tensor<bf16>) -> tensor<1xbf16>
      %97 = stablehlo.broadcast_in_dim %96, dims = [0] : (tensor<1xbf16>) -> tensor<1x1024xbf16>
      %98 = stablehlo.convert %97 : (tensor<1x1024xbf16>) -> tensor<1x1024xf32>
      %99 = stablehlo.broadcast_in_dim %38, dims = [0] : (tensor<1xi64>) -> tensor<1x1024xi64>
      %100 = stablehlo.compare  GT, %c, %99 : (tensor<1x1024xi64>, tensor<1x1024xi64>) -> tensor<1x1024xi1>
      %101 = stablehlo.convert %100 : (tensor<1x1024xi1>) -> tensor<1x1024xf32>
      %102 = stablehlo.multiply %98, %101 : tensor<1x1024xf32>
      %103 = stablehlo.convert %102 : (tensor<1x1024xf32>) -> tensor<1x1024xbf16>
      %104 = stablehlo.reshape %103 : (tensor<1x1024xbf16>) -> tensor<1x1x1024xbf16>
      %105 = stablehlo.broadcast_in_dim %104, dims = [0, 2, 3] : (tensor<1x1x1024xbf16>) -> tensor<1x12x1x1024xbf16>
      %106 = stablehlo.add %95, %105 : tensor<1x12x1x1024xbf16>
      %107 = stablehlo.convert %106 : (tensor<1x12x1x1024xbf16>) -> tensor<1x12x1x1024xf32>
      %108 = stablehlo.reduce(%107 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x12x1x1024xf32>, tensor<f32>) -> tensor<1x12x1xf32>
      %109 = stablehlo.broadcast_in_dim %108, dims = [0, 1, 2] : (tensor<1x12x1xf32>) -> tensor<1x12x1x1024xf32>
      %110 = stablehlo.subtract %107, %109 : tensor<1x12x1x1024xf32>
      %111 = stablehlo.exponential %110 : tensor<1x12x1x1024xf32>
      %112 = stablehlo.reduce(%111 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x12x1x1024xf32>, tensor<f32>) -> tensor<1x12x1xf32>
      %113 = stablehlo.broadcast_in_dim %112, dims = [0, 1, 2] : (tensor<1x12x1xf32>) -> tensor<1x12x1x1024xf32>
      %114 = stablehlo.divide %111, %113 : tensor<1x12x1x1024xf32>
      %115 = stablehlo.convert %114 : (tensor<1x12x1x1024xf32>) -> tensor<1x12x1x1024xbf16>
      %116 = stablehlo.reshape %115 : (tensor<1x12x1x1024xbf16>) -> tensor<12x1x1024xbf16>
      %117 = stablehlo.reshape %arg26 : (tensor<512x3072xbf16>) -> tensor<1x512x3072xbf16>
      %118 = stablehlo.reshape %117 : (tensor<1x512x3072xbf16>) -> tensor<512x3072xbf16>
      %119 = stablehlo.transpose %118, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<512x3072xbf16>) -> tensor<3072x512xbf16>
      %120 = stablehlo.dot_general %28, %119, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x512xbf16>) -> tensor<1x512xbf16>
      %121 = stablehlo.reshape %120 : (tensor<1x512xbf16>) -> tensor<1x4x1x128xbf16>
      %122 = "stablehlo.scatter"(%arg32, %66, %121) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg42: tensor<bf16>, %arg43: tensor<bf16>):
        stablehlo.return %arg43 : tensor<bf16>
      }) : (tensor<1x4x1024x128xbf16>, tensor<1x1xi64>, tensor<1x4x1x128xbf16>) -> tensor<1x4x1024x128xbf16>
      %123 = stablehlo.broadcast_in_dim %122, dims = [0, 1, 3, 4] : (tensor<1x4x1024x128xbf16>) -> tensor<1x4x3x1024x128xbf16>
      %124 = stablehlo.reshape %123 : (tensor<1x4x3x1024x128xbf16>) -> tensor<12x1024x128xbf16>
      %125 = stablehlo.dot_general %116, %124, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<12x1x1024xbf16>, tensor<12x1024x128xbf16>) -> tensor<12x1x128xbf16>
      %126 = stablehlo.reshape %125 : (tensor<12x1x128xbf16>) -> tensor<1x1536xbf16>
      %127 = stablehlo.reshape %arg25 : (tensor<3072x1536xbf16>) -> tensor<1x3072x1536xbf16>
      %128 = stablehlo.reshape %127 : (tensor<1x3072x1536xbf16>) -> tensor<3072x1536xbf16>
      %129 = stablehlo.transpose %128, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x1536xbf16>) -> tensor<1536x3072xbf16>
      %130 = stablehlo.dot_general %126, %129, contracting_dims = [1] x [0] : (tensor<1x1536xbf16>, tensor<1536x3072xbf16>) -> tensor<1x3072xbf16>
      %131 = "stablehlo.all_reduce"(%130) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> ({
      ^bb0(%arg42: tensor<bf16>, %arg43: tensor<bf16>):
        %198 = stablehlo.add %arg42, %arg43 : tensor<bf16>
        stablehlo.return %198 : tensor<bf16>
      }) : (tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
      %132 = stablehlo.reshape %131 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
      %133 = stablehlo.add %10, %132 : tensor<1x1x3072xbf16>
      %134 = stablehlo.reshape %arg39 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
      %135 = stablehlo.convert %134 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %136 = stablehlo.convert %133 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %137 = stablehlo.power %136, %1 : tensor<1x1x3072xf32>
      %138 = stablehlo.reduce(%137 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
      %139 = stablehlo.multiply %138, %cst_2 : tensor<1x1xf32>
      %140 = stablehlo.reshape %139 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
      %141 = stablehlo.add %140, %18 : tensor<1x1x1xf32>
      %142 = stablehlo.rsqrt %141 : tensor<1x1x1xf32>
      %143 = stablehlo.reshape %142 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
      %144 = stablehlo.broadcast_in_dim %143, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
      %145 = stablehlo.multiply %136, %144 : tensor<1x1x3072xf32>
      %146 = stablehlo.convert %145 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
      %147 = stablehlo.convert %146 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %148 = stablehlo.multiply %135, %147 : tensor<1x1x3072xf32>
      %149 = stablehlo.convert %148 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
      %150 = stablehlo.reshape %149 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
      %151 = stablehlo.reshape %arg40 : (tensor<4096x3072xbf16>) -> tensor<1x4096x3072xbf16>
      %152 = stablehlo.reshape %151 : (tensor<1x4096x3072xbf16>) -> tensor<4096x3072xbf16>
      %153 = stablehlo.transpose %152, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<4096x3072xbf16>) -> tensor<3072x4096xbf16>
      %154 = stablehlo.dot_general %150, %153, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<1x4096xbf16>
      %155 = stablehlo.reshape %154 : (tensor<1x4096xbf16>) -> tensor<1x1x4096xbf16>
      %156 = stablehlo.convert %155 : (tensor<1x1x4096xbf16>) -> tensor<1x1x4096xf32>
      %157 = stablehlo.logistic %155 : tensor<1x1x4096xbf16>
      %158 = stablehlo.convert %157 : (tensor<1x1x4096xbf16>) -> tensor<1x1x4096xf32>
      %159 = stablehlo.multiply %156, %158 : tensor<1x1x4096xf32>
      %160 = stablehlo.convert %159 : (tensor<1x1x4096xf32>) -> tensor<1x1x4096xbf16>
      %161 = stablehlo.convert %160 : (tensor<1x1x4096xbf16>) -> tensor<1x1x4096xf32>
      %162 = stablehlo.reshape %arg24 : (tensor<4096x3072xbf16>) -> tensor<1x4096x3072xbf16>
      %163 = stablehlo.reshape %162 : (tensor<1x4096x3072xbf16>) -> tensor<4096x3072xbf16>
      %164 = stablehlo.transpose %163, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<4096x3072xbf16>) -> tensor<3072x4096xbf16>
      %165 = stablehlo.dot_general %150, %164, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<1x4096xbf16>
      %166 = stablehlo.convert %165 : (tensor<1x4096xbf16>) -> tensor<1x4096xf32>
      %167 = stablehlo.reshape %166 : (tensor<1x4096xf32>) -> tensor<1x1x4096xf32>
      %168 = stablehlo.multiply %161, %167 : tensor<1x1x4096xf32>
      %169 = stablehlo.convert %168 : (tensor<1x1x4096xf32>) -> tensor<1x1x4096xbf16>
      %170 = stablehlo.reshape %169 : (tensor<1x1x4096xbf16>) -> tensor<1x4096xbf16>
      %171 = stablehlo.reshape %arg23 : (tensor<3072x4096xbf16>) -> tensor<1x3072x4096xbf16>
      %172 = stablehlo.reshape %171 : (tensor<1x3072x4096xbf16>) -> tensor<3072x4096xbf16>
      %173 = stablehlo.transpose %172, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x4096xbf16>) -> tensor<4096x3072xbf16>
      %174 = stablehlo.dot_general %170, %173, contracting_dims = [1] x [0] : (tensor<1x4096xbf16>, tensor<4096x3072xbf16>) -> tensor<1x3072xbf16>
      %175 = "stablehlo.all_reduce"(%174) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> ({
      ^bb0(%arg42: tensor<bf16>, %arg43: tensor<bf16>):
        %198 = stablehlo.add %arg42, %arg43 : tensor<bf16>
        stablehlo.return %198 : tensor<bf16>
      }) : (tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
      %176 = stablehlo.reshape %175 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
      %177 = stablehlo.add %133, %176 : tensor<1x1x3072xbf16>
      %178 = stablehlo.convert %177 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %179 = stablehlo.power %178, %1 : tensor<1x1x3072xf32>
      %180 = stablehlo.reduce(%179 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
      %181 = stablehlo.multiply %180, %cst_2 : tensor<1x1xf32>
      %182 = stablehlo.reshape %181 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
      %183 = stablehlo.add %182, %18 : tensor<1x1x1xf32>
      %184 = stablehlo.rsqrt %183 : tensor<1x1x1xf32>
      %185 = stablehlo.reshape %184 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
      %186 = stablehlo.broadcast_in_dim %185, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
      %187 = stablehlo.multiply %178, %186 : tensor<1x1x3072xf32>
      %188 = stablehlo.convert %187 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
      %189 = stablehlo.convert %188 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %190 = stablehlo.multiply %3, %189 : tensor<1x1x3072xf32>
      %191 = stablehlo.convert %190 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
      %192 = stablehlo.reshape %191 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
      %193 = stablehlo.reshape %arg21 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
      %194 = stablehlo.reshape %193 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16>
      %195 = stablehlo.transpose %194, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16>
      %196 = stablehlo.dot_general %192, %195, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<1x128256xbf16>
      %197 = stablehlo.reshape %196 : (tensor<1x128256xbf16>) -> tensor<1x1x128256xbf16>
      sdy.return %196, %197 : tensor<1x128256xbf16>, tensor<1x1x128256xbf16>
    } : (tensor<128256x3072xbf16>, tensor<f32>, tensor<3072x8192xbf16>, tensor<8192x3072xbf16>, tensor<3072x3072xbf16>, tensor<1024x3072xbf16>, tensor<1x1xi64>, tensor<128256x3072xbf16>, tensor<3072xbf16>, tensor<1xi64>, tensor<i64>, tensor<1x8x1024x128xbf16>, tensor<bf16>, tensor<f32>, tensor<64xf32>, tensor<1024x3072xbf16>, tensor<1x8x1024x128xbf16>, tensor<3072x3072xbf16>, tensor<3072xbf16>, tensor<8192x3072xbf16>, tensor<3072xbf16>) -> (tensor<1x128256xbf16>, tensor<1x1x128256xbf16>)
    return %0#0, %0#1 : tensor<1x128256xbf16>, tensor<1x1x128256xbf16>
  }
}
2025-09-15 15:42:03.839 (  72.166s) [        F073E000]      module_builder.cc:510      1| TTIR Module:
module @SyncTensorsGraph.420 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  func.func @main(%arg0: tensor<128256x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg1: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg2: tensor<3072x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg3: tensor<8192x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg4: tensor<3072x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg5: tensor<1024x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg6: tensor<1x1xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg7: tensor<128256x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg8: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg9: tensor<1xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg10: tensor<i64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg11: tensor<1x8x1024x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg12: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg13: tensor<f32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg14: tensor<64xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg15: tensor<1024x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg16: tensor<1x8x1024x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg17: tensor<3072x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x1x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = ttir.empty() : tensor<128256x3072xbf16>
    %1 = "ttir.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16>, tensor<128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %2 = ttir.empty() : tensor<f32>
    %3 = "ttir.mesh_shard"(%arg1, %2) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %4 = ttir.empty() : tensor<3072x4096xbf16>
    %5 = "ttir.mesh_shard"(%arg2, %4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x8192xbf16>, tensor<3072x4096xbf16>) -> tensor<3072x4096xbf16>
    %6 = ttir.empty() : tensor<4096x3072xbf16>
    %7 = "ttir.mesh_shard"(%arg3, %6) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16>, tensor<4096x3072xbf16>) -> tensor<4096x3072xbf16>
    %8 = ttir.empty() : tensor<3072x1536xbf16>
    %9 = "ttir.mesh_shard"(%arg4, %8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16>, tensor<3072x1536xbf16>) -> tensor<3072x1536xbf16>
    %10 = ttir.empty() : tensor<512x3072xbf16>
    %11 = "ttir.mesh_shard"(%arg5, %10) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16>, tensor<512x3072xbf16>) -> tensor<512x3072xbf16>
    %12 = ttir.empty() : tensor<1x1xi64>
    %13 = "ttir.mesh_shard"(%arg6, %12) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x1xi64>, tensor<1x1xi64>) -> tensor<1x1xi64>
    %14 = ttir.empty() : tensor<128256x3072xbf16>
    %15 = "ttir.mesh_shard"(%arg7, %14) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16>, tensor<128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %16 = ttir.empty() : tensor<3072xbf16>
    %17 = "ttir.mesh_shard"(%arg8, %16) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16>, tensor<3072xbf16>) -> tensor<3072xbf16>
    %18 = ttir.empty() : tensor<1xi64>
    %19 = "ttir.mesh_shard"(%arg9, %18) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %20 = ttir.empty() : tensor<i64>
    %21 = "ttir.mesh_shard"(%arg10, %20) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %22 = ttir.empty() : tensor<1x4x1024x128xbf16>
    %23 = "ttir.mesh_shard"(%arg11, %22) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16>, tensor<1x4x1024x128xbf16>) -> tensor<1x4x1024x128xbf16>
    %24 = ttir.empty() : tensor<bf16>
    %25 = "ttir.mesh_shard"(%arg12, %24) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
    %26 = ttir.empty() : tensor<f32>
    %27 = "ttir.mesh_shard"(%arg13, %26) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %28 = ttir.empty() : tensor<64xf32>
    %29 = "ttir.mesh_shard"(%arg14, %28) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32>, tensor<64xf32>) -> tensor<64xf32>
    %30 = ttir.empty() : tensor<512x3072xbf16>
    %31 = "ttir.mesh_shard"(%arg15, %30) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16>, tensor<512x3072xbf16>) -> tensor<512x3072xbf16>
    %32 = ttir.empty() : tensor<1x4x1024x128xbf16>
    %33 = "ttir.mesh_shard"(%arg16, %32) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16>, tensor<1x4x1024x128xbf16>) -> tensor<1x4x1024x128xbf16>
    %34 = ttir.empty() : tensor<1536x3072xbf16>
    %35 = "ttir.mesh_shard"(%arg17, %34) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16>, tensor<1536x3072xbf16>) -> tensor<1536x3072xbf16>
    %36 = ttir.empty() : tensor<3072xbf16>
    %37 = "ttir.mesh_shard"(%arg18, %36) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16>, tensor<3072xbf16>) -> tensor<3072xbf16>
    %38 = ttir.empty() : tensor<4096x3072xbf16>
    %39 = "ttir.mesh_shard"(%arg19, %38) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16>, tensor<4096x3072xbf16>) -> tensor<4096x3072xbf16>
    %40 = ttir.empty() : tensor<3072xbf16>
    %41 = "ttir.mesh_shard"(%arg20, %40) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16>, tensor<3072xbf16>) -> tensor<3072xbf16>
    %42 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<f32>}> : () -> tensor<f32>
    %43 = "ttir.constant"() <{value = dense<0xFF800000> : tensor<f32>}> : () -> tensor<f32>
    %44 = "ttir.constant"() <{value = dense<"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tensor<1x1024xi64>}> : () -> tensor<1x1024xi64>
    %45 = "ttir.constant"() <{value = dense<0> : tensor<1xi64>}> : () -> tensor<1xi64>
    %46 = "ttir.constant"() <{value = dense<3.25520843E-4> : tensor<1x1xf32>}> : () -> tensor<1x1xf32>
    %47 = "ttir.constant"() <{value = dense<2.000000e+00> : tensor<f32>}> : () -> tensor<f32>
    %48 = ttir.empty() : tensor<1x1x1xf32>
    %49 = "ttir.reshape"(%47, %48) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %50 = ttir.empty() : tensor<1x1x3072xf32>
    %51 = "ttir.broadcast"(%49, %50) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x1x1xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %52 = ttir.empty() : tensor<1x1x3072xbf16>
    %53 = "ttir.reshape"(%41, %52) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %54 = ttir.empty() : tensor<1x1x3072xf32>
    %55 = "ttir.typecast"(%53, %54) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %56 = ttir.empty() : tensor<1x128256x3072xbf16>
    %57 = "ttir.reshape"(%15, %56) <{shape = [1 : i32, 128256 : i32, 3072 : i32]}> : (tensor<128256x3072xbf16>, tensor<1x128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %58 = ttir.empty() : tensor<128256x3072xbf16>
    %59 = "ttir.reshape"(%57, %58) <{shape = [128256 : i32, 3072 : i32]}> : (tensor<1x128256x3072xbf16>, tensor<128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %60 = ttir.empty() : tensor<1x1x1xi64>
    %61 = "ttir.reshape"(%13, %60) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xi64>, tensor<1x1x1xi64>) -> tensor<1x1x1xi64>
    %62 = ttir.empty() : tensor<1x1x1xui32>
    %63 = "ttir.typecast"(%61, %62) <{conservative_folding = false}> : (tensor<1x1x1xi64>, tensor<1x1x1xui32>) -> tensor<1x1x1xui32>
    %64 = ttir.empty() : tensor<1xui32>
    %65 = "ttir.reshape"(%63, %64) <{shape = [1 : i32]}> : (tensor<1x1x1xui32>, tensor<1xui32>) -> tensor<1xui32>
    %66 = ttir.empty() : tensor<1x3072xbf16>
    %67 = "ttir.gather"(%59, %65, %66) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 3072>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<128256x3072xbf16>, tensor<1xui32>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %68 = ttir.empty() : tensor<1x1x3072xbf16>
    %69 = "ttir.reshape"(%67, %68) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %70 = ttir.empty() : tensor<1x1x3072xbf16>
    %71 = "ttir.reshape"(%17, %70) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %72 = ttir.empty() : tensor<1x1x3072xf32>
    %73 = "ttir.typecast"(%71, %72) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %74 = ttir.empty() : tensor<1x1x3072xf32>
    %75 = "ttir.typecast"(%69, %74) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %76 = ttir.empty() : tensor<1x1x3072xf32>
    %77 = "ttir.pow"(%75, %51, %76) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %78 = ttir.empty() : tensor<1x1xf32>
    %79 = "ttir.sum"(%77, %78) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %80 = ttir.empty() : tensor<1x1xf32>
    %81 = "ttir.multiply"(%79, %46, %80) : (tensor<1x1xf32>, tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %82 = ttir.empty() : tensor<1x1x1xf32>
    %83 = "ttir.reshape"(%81, %82) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %84 = ttir.empty() : tensor<1x1x1xf32>
    %85 = "ttir.reshape"(%3, %84) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %86 = ttir.empty() : tensor<1x1x1xf32>
    %87 = "ttir.add"(%83, %85, %86) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %88 = ttir.empty() : tensor<1x1x1xf32>
    %89 = "ttir.rsqrt"(%87, %88) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %90 = ttir.empty() : tensor<1x1xf32>
    %91 = "ttir.reshape"(%89, %90) <{shape = [1 : i32, 1 : i32]}> : (tensor<1x1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %92 = ttir.empty() : tensor<1x1x1xf32>
    %93 = "ttir.reshape"(%91, %92) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %94 = ttir.empty() : tensor<1x1x3072xf32>
    %95 = "ttir.broadcast"(%93, %94) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x1x1xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %96 = ttir.empty() : tensor<1x1x3072xf32>
    %97 = "ttir.multiply"(%75, %95, %96) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %98 = ttir.empty() : tensor<1x1x3072xbf16>
    %99 = "ttir.typecast"(%97, %98) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %100 = ttir.empty() : tensor<1x1x3072xf32>
    %101 = "ttir.typecast"(%99, %100) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %102 = ttir.empty() : tensor<1x1x3072xf32>
    %103 = "ttir.multiply"(%73, %101, %102) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %104 = ttir.empty() : tensor<1x1x3072xbf16>
    %105 = "ttir.typecast"(%103, %104) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %106 = ttir.empty() : tensor<1x3072xbf16>
    %107 = "ttir.reshape"(%105, %106) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x3072xbf16>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %108 = ttir.empty() : tensor<1x1536x3072xbf16>
    %109 = "ttir.reshape"(%35, %108) <{shape = [1 : i32, 1536 : i32, 3072 : i32]}> : (tensor<1536x3072xbf16>, tensor<1x1536x3072xbf16>) -> tensor<1x1536x3072xbf16>
    %110 = ttir.empty() : tensor<1536x3072xbf16>
    %111 = "ttir.reshape"(%109, %110) <{shape = [1536 : i32, 3072 : i32]}> : (tensor<1x1536x3072xbf16>, tensor<1536x3072xbf16>) -> tensor<1536x3072xbf16>
    %112 = ttir.empty() : tensor<3072x1536xbf16>
    %113 = "ttir.permute"(%111, %112) <{permutation = array<i64: 1, 0>}> : (tensor<1536x3072xbf16>, tensor<3072x1536xbf16>) -> tensor<3072x1536xbf16>
    %114 = "ttir.dot_general"(%107, %113) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x1536xbf16>) -> tensor<1x1536xbf16>
    %115 = ttir.empty() : tensor<1x12x1x128xbf16>
    %116 = "ttir.reshape"(%114, %115) <{shape = [1 : i32, 12 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1536xbf16>, tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xbf16>
    %117 = ttir.empty() : tensor<1x12x1x128xf32>
    %118 = "ttir.typecast"(%116, %117) <{conservative_folding = false}> : (tensor<1x12x1x128xbf16>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %119 = ttir.empty() : tensor<1x1x64xf32>
    %120 = "ttir.reshape"(%29, %119) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xf32>, tensor<1x1x64xf32>) -> tensor<1x1x64xf32>
    %121 = ttir.empty() : tensor<1x64x1xf32>
    %122 = "ttir.reshape"(%120, %121) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<1x1x64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
    %123 = ttir.empty() : tensor<1x1x1xi64>
    %124 = "ttir.reshape"(%19, %123) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xi64>, tensor<1x1x1xi64>) -> tensor<1x1x1xi64>
    %125 = ttir.empty() : tensor<1xi64>
    %126 = "ttir.reshape"(%124, %125) <{shape = [1 : i32]}> : (tensor<1x1x1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %127 = ttir.empty() : tensor<1x1x1xf32>
    %128 = "ttir.typecast"(%124, %127) <{conservative_folding = false}> : (tensor<1x1x1xi64>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %129 = "ttir.dot_general"(%122, %128) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x64x1xf32>, tensor<1x1x1xf32>) -> tensor<1x64x1xf32>
    %130 = ttir.empty() : tensor<1x1x64xf32>
    %131 = "ttir.reshape"(%129, %130) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<1x64x1xf32>, tensor<1x1x64xf32>) -> tensor<1x1x64xf32>
    %132 = ttir.empty() : tensor<1x1x128xf32>
    %133 = "ttir.concat"(%131, %131, %132) <{dim = 2 : si32}> : (tensor<1x1x64xf32>, tensor<1x1x64xf32>, tensor<1x1x128xf32>) -> tensor<1x1x128xf32>
    %134 = ttir.empty() : tensor<1x1x128xf32>
    %135 = "ttir.cos"(%133, %134) : (tensor<1x1x128xf32>, tensor<1x1x128xf32>) -> tensor<1x1x128xf32>
    %136 = ttir.empty() : tensor<1x1x128xbf16>
    %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<1x1x128xf32>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16>
    %138 = ttir.empty() : tensor<1x1x128xf32>
    %139 = "ttir.typecast"(%137, %138) <{conservative_folding = false}> : (tensor<1x1x128xbf16>, tensor<1x1x128xf32>) -> tensor<1x1x128xf32>
    %140 = ttir.empty() : tensor<1x1x1x128xf32>
    %141 = "ttir.reshape"(%139, %140) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32>, tensor<1x1x1x128xf32>) -> tensor<1x1x1x128xf32>
    %142 = ttir.empty() : tensor<1x12x1x128xf32>
    %143 = "ttir.broadcast"(%141, %142) <{broadcast_dimensions = array<i64: 1, 12, 1, 1>}> : (tensor<1x1x1x128xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %144 = ttir.empty() : tensor<1x12x1x128xf32>
    %145 = "ttir.multiply"(%118, %143, %144) : (tensor<1x12x1x128xf32>, tensor<1x12x1x128xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %146 = ttir.empty() : tensor<1x12x1x128xbf16>
    %147 = "ttir.typecast"(%145, %146) <{conservative_folding = false}> : (tensor<1x12x1x128xf32>, tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xbf16>
    %148 = ttir.empty() : tensor<1x12x1x64xbf16>
    %149 = "ttir.slice_static"(%116, %148) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 12 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x1x128xbf16>, tensor<1x12x1x64xbf16>) -> tensor<1x12x1x64xbf16>
    %150 = ttir.empty() : tensor<1x12x1x64xbf16>
    %151 = "ttir.neg"(%149, %150) : (tensor<1x12x1x64xbf16>, tensor<1x12x1x64xbf16>) -> tensor<1x12x1x64xbf16>
    %152 = ttir.empty() : tensor<1x12x1x64xbf16>
    %153 = "ttir.slice_static"(%116, %152) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 12 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x1x128xbf16>, tensor<1x12x1x64xbf16>) -> tensor<1x12x1x64xbf16>
    %154 = ttir.empty() : tensor<1x12x1x128xbf16>
    %155 = "ttir.concat"(%151, %153, %154) <{dim = 3 : si32}> : (tensor<1x12x1x64xbf16>, tensor<1x12x1x64xbf16>, tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xbf16>
    %156 = ttir.empty() : tensor<1x12x1x128xf32>
    %157 = "ttir.typecast"(%155, %156) <{conservative_folding = false}> : (tensor<1x12x1x128xbf16>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %158 = ttir.empty() : tensor<1x1x128xf32>
    %159 = "ttir.sin"(%133, %158) : (tensor<1x1x128xf32>, tensor<1x1x128xf32>) -> tensor<1x1x128xf32>
    %160 = ttir.empty() : tensor<1x1x128xbf16>
    %161 = "ttir.typecast"(%159, %160) <{conservative_folding = false}> : (tensor<1x1x128xf32>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16>
    %162 = ttir.empty() : tensor<1x1x128xf32>
    %163 = "ttir.typecast"(%161, %162) <{conservative_folding = false}> : (tensor<1x1x128xbf16>, tensor<1x1x128xf32>) -> tensor<1x1x128xf32>
    %164 = ttir.empty() : tensor<1x1x1x128xf32>
    %165 = "ttir.reshape"(%163, %164) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32>, tensor<1x1x1x128xf32>) -> tensor<1x1x1x128xf32>
    %166 = ttir.empty() : tensor<1x12x1x128xf32>
    %167 = "ttir.broadcast"(%165, %166) <{broadcast_dimensions = array<i64: 1, 12, 1, 1>}> : (tensor<1x1x1x128xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %168 = ttir.empty() : tensor<1x12x1x128xf32>
    %169 = "ttir.multiply"(%157, %167, %168) : (tensor<1x12x1x128xf32>, tensor<1x12x1x128xf32>, tensor<1x12x1x128xf32>) -> tensor<1x12x1x128xf32>
    %170 = ttir.empty() : tensor<1x12x1x128xbf16>
    %171 = "ttir.typecast"(%169, %170) <{conservative_folding = false}> : (tensor<1x12x1x128xf32>, tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xbf16>
    %172 = ttir.empty() : tensor<1x12x1x128xbf16>
    %173 = "ttir.add"(%147, %171, %172) : (tensor<1x12x1x128xbf16>, tensor<1x12x1x128xbf16>, tensor<1x12x1x128xbf16>) -> tensor<1x12x1x128xbf16>
    %174 = ttir.empty() : tensor<12x1x128xbf16>
    %175 = "ttir.reshape"(%173, %174) <{shape = [12 : i32, 1 : i32, 128 : i32]}> : (tensor<1x12x1x128xbf16>, tensor<12x1x128xbf16>) -> tensor<12x1x128xbf16>
    %176 = ttir.empty() : tensor<1xi1>
    %177 = "ttir.lt"(%126, %45, %176) : (tensor<1xi64>, tensor<1xi64>, tensor<1xi1>) -> tensor<1xi1>
    %178 = ttir.empty() : tensor<1xi64>
    %179 = "ttir.reshape"(%21, %178) <{shape = [1 : i32]}> : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %180 = ttir.empty() : tensor<1xi64>
    %181 = "ttir.add"(%126, %179, %180) : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %182 = ttir.empty() : tensor<1xi64>
    %183 = "ttir.where"(%177, %181, %126, %182) : (tensor<1xi1>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %184 = ttir.empty() : tensor<1x1xi64>
    %185 = "ttir.reshape"(%183, %184) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xi64>, tensor<1x1xi64>) -> tensor<1x1xi64>
    %186 = ttir.empty() : tensor<1x512x3072xbf16>
    %187 = "ttir.reshape"(%31, %186) <{shape = [1 : i32, 512 : i32, 3072 : i32]}> : (tensor<512x3072xbf16>, tensor<1x512x3072xbf16>) -> tensor<1x512x3072xbf16>
    %188 = ttir.empty() : tensor<512x3072xbf16>
    %189 = "ttir.reshape"(%187, %188) <{shape = [512 : i32, 3072 : i32]}> : (tensor<1x512x3072xbf16>, tensor<512x3072xbf16>) -> tensor<512x3072xbf16>
    %190 = ttir.empty() : tensor<3072x512xbf16>
    %191 = "ttir.permute"(%189, %190) <{permutation = array<i64: 1, 0>}> : (tensor<512x3072xbf16>, tensor<3072x512xbf16>) -> tensor<3072x512xbf16>
    %192 = "ttir.dot_general"(%107, %191) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x512xbf16>) -> tensor<1x512xbf16>
    %193 = ttir.empty() : tensor<1x4x1x128xbf16>
    %194 = "ttir.reshape"(%192, %193) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32]}> : (tensor<1x512xbf16>, tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xbf16>
    %195 = ttir.empty() : tensor<1x4x1x128xf32>
    %196 = "ttir.typecast"(%194, %195) <{conservative_folding = false}> : (tensor<1x4x1x128xbf16>, tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xf32>
    %197 = ttir.empty() : tensor<1x1x1x128xf32>
    %198 = "ttir.reshape"(%139, %197) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32>, tensor<1x1x1x128xf32>) -> tensor<1x1x1x128xf32>
    %199 = ttir.empty() : tensor<1x4x1x128xf32>
    %200 = "ttir.broadcast"(%198, %199) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x1x128xf32>, tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xf32>
    %201 = ttir.empty() : tensor<1x4x1x128xf32>
    %202 = "ttir.multiply"(%196, %200, %201) : (tensor<1x4x1x128xf32>, tensor<1x4x1x128xf32>, tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xf32>
    %203 = ttir.empty() : tensor<1x4x1x128xbf16>
    %204 = "ttir.typecast"(%202, %203) <{conservative_folding = false}> : (tensor<1x4x1x128xf32>, tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xbf16>
    %205 = ttir.empty() : tensor<1x4x1x64xbf16>
    %206 = "ttir.slice_static"(%194, %205) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x1x128xbf16>, tensor<1x4x1x64xbf16>) -> tensor<1x4x1x64xbf16>
    %207 = ttir.empty() : tensor<1x4x1x64xbf16>
    %208 = "ttir.neg"(%206, %207) : (tensor<1x4x1x64xbf16>, tensor<1x4x1x64xbf16>) -> tensor<1x4x1x64xbf16>
    %209 = ttir.empty() : tensor<1x4x1x64xbf16>
    %210 = "ttir.slice_static"(%194, %209) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x1x128xbf16>, tensor<1x4x1x64xbf16>) -> tensor<1x4x1x64xbf16>
    %211 = ttir.empty() : tensor<1x4x1x128xbf16>
    %212 = "ttir.concat"(%208, %210, %211) <{dim = 3 : si32}> : (tensor<1x4x1x64xbf16>, tensor<1x4x1x64xbf16>, tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xbf16>
    %213 = ttir.empty() : tensor<1x4x1x128xf32>
    %214 = "ttir.typecast"(%212, %213) <{conservative_folding = false}> : (tensor<1x4x1x128xbf16>, tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xf32>
    %215 = ttir.empty() : tensor<1x1x1x128xf32>
    %216 = "ttir.reshape"(%163, %215) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32>, tensor<1x1x1x128xf32>) -> tensor<1x1x1x128xf32>
    %217 = ttir.empty() : tensor<1x4x1x128xf32>
    %218 = "ttir.broadcast"(%216, %217) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x1x128xf32>, tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xf32>
    %219 = ttir.empty() : tensor<1x4x1x128xf32>
    %220 = "ttir.multiply"(%214, %218, %219) : (tensor<1x4x1x128xf32>, tensor<1x4x1x128xf32>, tensor<1x4x1x128xf32>) -> tensor<1x4x1x128xf32>
    %221 = ttir.empty() : tensor<1x4x1x128xbf16>
    %222 = "ttir.typecast"(%220, %221) <{conservative_folding = false}> : (tensor<1x4x1x128xf32>, tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xbf16>
    %223 = ttir.empty() : tensor<1x4x1x128xbf16>
    %224 = "ttir.add"(%204, %222, %223) : (tensor<1x4x1x128xbf16>, tensor<1x4x1x128xbf16>, tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xbf16>
    %225 = ttir.empty() : tensor<1x4x1024x128xbf16>
    %226 = "ttir.scatter"(%33, %185, %224, %225) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x4x1024x128xbf16>, tensor<1x1xi64>, tensor<1x4x1x128xbf16>, tensor<1x4x1024x128xbf16>) -> tensor<1x4x1024x128xbf16>
    %227 = ttir.empty() : tensor<1x4x1x1024x128xbf16>
    %228 = "ttir.reshape"(%226, %227) <{shape = [1 : i32, 4 : i32, 1 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x1024x128xbf16>, tensor<1x4x1x1024x128xbf16>) -> tensor<1x4x1x1024x128xbf16>
    %229 = ttir.empty() : tensor<1x4x3x1024x128xbf16>
    %230 = "ttir.broadcast"(%228, %229) <{broadcast_dimensions = array<i64: 1, 1, 3, 1, 1>}> : (tensor<1x4x1x1024x128xbf16>, tensor<1x4x3x1024x128xbf16>) -> tensor<1x4x3x1024x128xbf16>
    %231 = ttir.empty() : tensor<1x12x1024x128xbf16>
    %232 = "ttir.reshape"(%230, %231) <{shape = [1 : i32, 12 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x3x1024x128xbf16>, tensor<1x12x1024x128xbf16>) -> tensor<1x12x1024x128xbf16>
    %233 = ttir.empty() : tensor<1x12x128x1024xbf16>
    %234 = "ttir.permute"(%232, %233) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x12x1024x128xbf16>, tensor<1x12x128x1024xbf16>) -> tensor<1x12x128x1024xbf16>
    %235 = ttir.empty() : tensor<12x128x1024xbf16>
    %236 = "ttir.reshape"(%234, %235) <{shape = [12 : i32, 128 : i32, 1024 : i32]}> : (tensor<1x12x128x1024xbf16>, tensor<12x128x1024xbf16>) -> tensor<12x128x1024xbf16>
    %237 = "ttir.dot_general"(%175, %236) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<12x1x128xbf16>, tensor<12x128x1024xbf16>) -> tensor<12x1x1024xbf16>
    %238 = ttir.empty() : tensor<12x1x1024xf32>
    %239 = "ttir.typecast"(%237, %238) <{conservative_folding = false}> : (tensor<12x1x1024xbf16>, tensor<12x1x1024xf32>) -> tensor<12x1x1024xf32>
    %240 = ttir.empty() : tensor<1x12x1x1024xf32>
    %241 = "ttir.reshape"(%239, %240) <{shape = [1 : i32, 12 : i32, 1 : i32, 1024 : i32]}> : (tensor<12x1x1024xf32>, tensor<1x12x1x1024xf32>) -> tensor<1x12x1x1024xf32>
    %242 = ttir.empty() : tensor<1x1x1x1xf32>
    %243 = "ttir.reshape"(%27, %242) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %244 = ttir.empty() : tensor<1x12x1x1024xf32>
    %245 = "ttir.broadcast"(%243, %244) <{broadcast_dimensions = array<i64: 1, 12, 1, 1024>}> : (tensor<1x1x1x1xf32>, tensor<1x12x1x1024xf32>) -> tensor<1x12x1x1024xf32>
    %246 = ttir.empty() : tensor<1x12x1x1024xf32>
    %247 = "ttir.multiply"(%241, %245, %246) : (tensor<1x12x1x1024xf32>, tensor<1x12x1x1024xf32>, tensor<1x12x1x1024xf32>) -> tensor<1x12x1x1024xf32>
    %248 = ttir.empty() : tensor<1x12x1x1024xbf16>
    %249 = "ttir.typecast"(%247, %248) <{conservative_folding = false}> : (tensor<1x12x1x1024xf32>, tensor<1x12x1x1024xbf16>) -> tensor<1x12x1x1024xbf16>
    %250 = ttir.empty() : tensor<1xbf16>
    %251 = "ttir.reshape"(%25, %250) <{shape = [1 : i32]}> : (tensor<bf16>, tensor<1xbf16>) -> tensor<1xbf16>
    %252 = ttir.empty() : tensor<1x1xbf16>
    %253 = "ttir.reshape"(%251, %252) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xbf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %254 = ttir.empty() : tensor<1x1024xbf16>
    %255 = "ttir.broadcast"(%253, %254) <{broadcast_dimensions = array<i64: 1, 1024>}> : (tensor<1x1xbf16>, tensor<1x1024xbf16>) -> tensor<1x1024xbf16>
    %256 = ttir.empty() : tensor<1x1024xf32>
    %257 = "ttir.typecast"(%255, %256) <{conservative_folding = false}> : (tensor<1x1024xbf16>, tensor<1x1024xf32>) -> tensor<1x1024xf32>
    %258 = ttir.empty() : tensor<1x1xi64>
    %259 = "ttir.reshape"(%126, %258) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xi64>, tensor<1x1xi64>) -> tensor<1x1xi64>
    %260 = ttir.empty() : tensor<1x1024xi64>
    %261 = "ttir.broadcast"(%259, %260) <{broadcast_dimensions = array<i64: 1, 1024>}> : (tensor<1x1xi64>, tensor<1x1024xi64>) -> tensor<1x1024xi64>
    %262 = ttir.empty() : tensor<1x1024xi1>
    %263 = "ttir.gt"(%44, %261, %262) : (tensor<1x1024xi64>, tensor<1x1024xi64>, tensor<1x1024xi1>) -> tensor<1x1024xi1>
    %264 = ttir.empty() : tensor<1x1024xf32>
    %265 = "ttir.typecast"(%263, %264) <{conservative_folding = false}> : (tensor<1x1024xi1>, tensor<1x1024xf32>) -> tensor<1x1024xf32>
    %266 = ttir.empty() : tensor<1x1024xf32>
    %267 = "ttir.multiply"(%257, %265, %266) : (tensor<1x1024xf32>, tensor<1x1024xf32>, tensor<1x1024xf32>) -> tensor<1x1024xf32>
    %268 = ttir.empty() : tensor<1x1024xbf16>
    %269 = "ttir.typecast"(%267, %268) <{conservative_folding = false}> : (tensor<1x1024xf32>, tensor<1x1024xbf16>) -> tensor<1x1024xbf16>
    %270 = ttir.empty() : tensor<1x1x1024xbf16>
    %271 = "ttir.reshape"(%269, %270) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1x1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16>
    %272 = ttir.empty() : tensor<1x1x1x1024xbf16>
    %273 = "ttir.reshape"(%271, %272) <{shape = [1 : i32, 1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1x1x1x1024xbf16>) -> tensor<1x1x1x1024xbf16>
    %274 = ttir.empty() : tensor<1x12x1x1024xbf16>
    %275 = "ttir.broadcast"(%273, %274) <{broadcast_dimensions = array<i64: 1, 12, 1, 1>}> : (tensor<1x1x1x1024xbf16>, tensor<1x12x1x1024xbf16>) -> tensor<1x12x1x1024xbf16>
    %276 = ttir.empty() : tensor<1x12x1x1024xbf16>
    %277 = "ttir.add"(%249, %275, %276) : (tensor<1x12x1x1024xbf16>, tensor<1x12x1x1024xbf16>, tensor<1x12x1x1024xbf16>) -> tensor<1x12x1x1024xbf16>
    %278 = ttir.empty() : tensor<1x12x1x1024xf32>
    %279 = "ttir.typecast"(%277, %278) <{conservative_folding = false}> : (tensor<1x12x1x1024xbf16>, tensor<1x12x1x1024xf32>) -> tensor<1x12x1x1024xf32>
    %280 = ttir.empty() : tensor<1x12x1xf32>
    %281 = "ttir.max"(%279, %280) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x12x1x1024xf32>, tensor<1x12x1xf32>) -> tensor<1x12x1xf32>
    %282 = ttir.empty() : tensor<1x12x1x1xf32>
    %283 = "ttir.reshape"(%281, %282) <{shape = [1 : i32, 12 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x1xf32>, tensor<1x12x1x1xf32>) -> tensor<1x12x1x1xf32>
    %284 = ttir.empty() : tensor<1x12x1x1024xf32>
    %285 = "ttir.broadcast"(%283, %284) <{broadcast_dimensions = array<i64: 1, 1, 1, 1024>}> : (tensor<1x12x1x1xf32>, tensor<1x12x1x1024xf32>) -> tensor<1x12x1x1024xf32>
    %286 = ttir.empty() : tensor<1x12x1x1024xf32>
    %287 = "ttir.subtract"(%279, %285, %286) : (tensor<1x12x1x1024xf32>, tensor<1x12x1x1024xf32>, tensor<1x12x1x1024xf32>) -> tensor<1x12x1x1024xf32>
    %288 = ttir.empty() : tensor<1x12x1x1024xf32>
    %289 = "ttir.exp"(%287, %288) : (tensor<1x12x1x1024xf32>, tensor<1x12x1x1024xf32>) -> tensor<1x12x1x1024xf32>
    %290 = ttir.empty() : tensor<1x12x1xf32>
    %291 = "ttir.sum"(%289, %290) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x12x1x1024xf32>, tensor<1x12x1xf32>) -> tensor<1x12x1xf32>
    %292 = ttir.empty() : tensor<1x12x1x1xf32>
    %293 = "ttir.reshape"(%291, %292) <{shape = [1 : i32, 12 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x1xf32>, tensor<1x12x1x1xf32>) -> tensor<1x12x1x1xf32>
    %294 = ttir.empty() : tensor<1x12x1x1024xf32>
    %295 = "ttir.broadcast"(%293, %294) <{broadcast_dimensions = array<i64: 1, 1, 1, 1024>}> : (tensor<1x12x1x1xf32>, tensor<1x12x1x1024xf32>) -> tensor<1x12x1x1024xf32>
    %296 = ttir.empty() : tensor<1x12x1x1024xf32>
    %297 = "ttir.div"(%289, %295, %296) : (tensor<1x12x1x1024xf32>, tensor<1x12x1x1024xf32>, tensor<1x12x1x1024xf32>) -> tensor<1x12x1x1024xf32>
    %298 = ttir.empty() : tensor<1x12x1x1024xbf16>
    %299 = "ttir.typecast"(%297, %298) <{conservative_folding = false}> : (tensor<1x12x1x1024xf32>, tensor<1x12x1x1024xbf16>) -> tensor<1x12x1x1024xbf16>
    %300 = ttir.empty() : tensor<12x1x1024xbf16>
    %301 = "ttir.reshape"(%299, %300) <{shape = [12 : i32, 1 : i32, 1024 : i32]}> : (tensor<1x12x1x1024xbf16>, tensor<12x1x1024xbf16>) -> tensor<12x1x1024xbf16>
    %302 = ttir.empty() : tensor<1x512x3072xbf16>
    %303 = "ttir.reshape"(%11, %302) <{shape = [1 : i32, 512 : i32, 3072 : i32]}> : (tensor<512x3072xbf16>, tensor<1x512x3072xbf16>) -> tensor<1x512x3072xbf16>
    %304 = ttir.empty() : tensor<512x3072xbf16>
    %305 = "ttir.reshape"(%303, %304) <{shape = [512 : i32, 3072 : i32]}> : (tensor<1x512x3072xbf16>, tensor<512x3072xbf16>) -> tensor<512x3072xbf16>
    %306 = ttir.empty() : tensor<3072x512xbf16>
    %307 = "ttir.permute"(%305, %306) <{permutation = array<i64: 1, 0>}> : (tensor<512x3072xbf16>, tensor<3072x512xbf16>) -> tensor<3072x512xbf16>
    %308 = "ttir.dot_general"(%107, %307) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x512xbf16>) -> tensor<1x512xbf16>
    %309 = ttir.empty() : tensor<1x4x1x128xbf16>
    %310 = "ttir.reshape"(%308, %309) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32]}> : (tensor<1x512xbf16>, tensor<1x4x1x128xbf16>) -> tensor<1x4x1x128xbf16>
    %311 = ttir.empty() : tensor<1x4x1024x128xbf16>
    %312 = "ttir.scatter"(%23, %185, %310, %311) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x4x1024x128xbf16>, tensor<1x1xi64>, tensor<1x4x1x128xbf16>, tensor<1x4x1024x128xbf16>) -> tensor<1x4x1024x128xbf16>
    %313 = ttir.empty() : tensor<1x4x1x1024x128xbf16>
    %314 = "ttir.reshape"(%312, %313) <{shape = [1 : i32, 4 : i32, 1 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x1024x128xbf16>, tensor<1x4x1x1024x128xbf16>) -> tensor<1x4x1x1024x128xbf16>
    %315 = ttir.empty() : tensor<1x4x3x1024x128xbf16>
    %316 = "ttir.broadcast"(%314, %315) <{broadcast_dimensions = array<i64: 1, 1, 3, 1, 1>}> : (tensor<1x4x1x1024x128xbf16>, tensor<1x4x3x1024x128xbf16>) -> tensor<1x4x3x1024x128xbf16>
    %317 = ttir.empty() : tensor<12x1024x128xbf16>
    %318 = "ttir.reshape"(%316, %317) <{shape = [12 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x3x1024x128xbf16>, tensor<12x1024x128xbf16>) -> tensor<12x1024x128xbf16>
    %319 = "ttir.dot_general"(%301, %318) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<12x1x1024xbf16>, tensor<12x1024x128xbf16>) -> tensor<12x1x128xbf16>
    %320 = ttir.empty() : tensor<1x1536xbf16>
    %321 = "ttir.reshape"(%319, %320) <{shape = [1 : i32, 1536 : i32]}> : (tensor<12x1x128xbf16>, tensor<1x1536xbf16>) -> tensor<1x1536xbf16>
    %322 = ttir.empty() : tensor<1x3072x1536xbf16>
    %323 = "ttir.reshape"(%9, %322) <{shape = [1 : i32, 3072 : i32, 1536 : i32]}> : (tensor<3072x1536xbf16>, tensor<1x3072x1536xbf16>) -> tensor<1x3072x1536xbf16>
    %324 = ttir.empty() : tensor<3072x1536xbf16>
    %325 = "ttir.reshape"(%323, %324) <{shape = [3072 : i32, 1536 : i32]}> : (tensor<1x3072x1536xbf16>, tensor<3072x1536xbf16>) -> tensor<3072x1536xbf16>
    %326 = ttir.empty() : tensor<1536x3072xbf16>
    %327 = "ttir.permute"(%325, %326) <{permutation = array<i64: 1, 0>}> : (tensor<3072x1536xbf16>, tensor<1536x3072xbf16>) -> tensor<1536x3072xbf16>
    %328 = "ttir.dot_general"(%321, %327) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x1536xbf16>, tensor<1536x3072xbf16>) -> tensor<1x3072xbf16>
    %329 = ttir.empty() : tensor<1x3072xbf16>
    %330 = "ttir.all_reduce"(%328, %329) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<1x3072xbf16>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %331 = ttir.empty() : tensor<1x1x3072xbf16>
    %332 = "ttir.reshape"(%330, %331) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %333 = ttir.empty() : tensor<1x1x3072xbf16>
    %334 = "ttir.add"(%69, %332, %333) : (tensor<1x1x3072xbf16>, tensor<1x1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %335 = ttir.empty() : tensor<1x1x3072xbf16>
    %336 = "ttir.reshape"(%37, %335) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %337 = ttir.empty() : tensor<1x1x3072xf32>
    %338 = "ttir.typecast"(%336, %337) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %339 = ttir.empty() : tensor<1x1x3072xf32>
    %340 = "ttir.typecast"(%334, %339) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %341 = ttir.empty() : tensor<1x1x3072xf32>
    %342 = "ttir.pow"(%340, %51, %341) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %343 = ttir.empty() : tensor<1x1xf32>
    %344 = "ttir.sum"(%342, %343) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %345 = ttir.empty() : tensor<1x1xf32>
    %346 = "ttir.multiply"(%344, %46, %345) : (tensor<1x1xf32>, tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %347 = ttir.empty() : tensor<1x1x1xf32>
    %348 = "ttir.reshape"(%346, %347) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %349 = ttir.empty() : tensor<1x1x1xf32>
    %350 = "ttir.add"(%348, %85, %349) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %351 = ttir.empty() : tensor<1x1x1xf32>
    %352 = "ttir.rsqrt"(%350, %351) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %353 = ttir.empty() : tensor<1x1xf32>
    %354 = "ttir.reshape"(%352, %353) <{shape = [1 : i32, 1 : i32]}> : (tensor<1x1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %355 = ttir.empty() : tensor<1x1x1xf32>
    %356 = "ttir.reshape"(%354, %355) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %357 = ttir.empty() : tensor<1x1x3072xf32>
    %358 = "ttir.broadcast"(%356, %357) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x1x1xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %359 = ttir.empty() : tensor<1x1x3072xf32>
    %360 = "ttir.multiply"(%340, %358, %359) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %361 = ttir.empty() : tensor<1x1x3072xbf16>
    %362 = "ttir.typecast"(%360, %361) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %363 = ttir.empty() : tensor<1x1x3072xf32>
    %364 = "ttir.typecast"(%362, %363) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %365 = ttir.empty() : tensor<1x1x3072xf32>
    %366 = "ttir.multiply"(%338, %364, %365) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %367 = ttir.empty() : tensor<1x1x3072xbf16>
    %368 = "ttir.typecast"(%366, %367) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %369 = ttir.empty() : tensor<1x3072xbf16>
    %370 = "ttir.reshape"(%368, %369) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x3072xbf16>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %371 = ttir.empty() : tensor<1x4096x3072xbf16>
    %372 = "ttir.reshape"(%39, %371) <{shape = [1 : i32, 4096 : i32, 3072 : i32]}> : (tensor<4096x3072xbf16>, tensor<1x4096x3072xbf16>) -> tensor<1x4096x3072xbf16>
    %373 = ttir.empty() : tensor<4096x3072xbf16>
    %374 = "ttir.reshape"(%372, %373) <{shape = [4096 : i32, 3072 : i32]}> : (tensor<1x4096x3072xbf16>, tensor<4096x3072xbf16>) -> tensor<4096x3072xbf16>
    %375 = ttir.empty() : tensor<3072x4096xbf16>
    %376 = "ttir.permute"(%374, %375) <{permutation = array<i64: 1, 0>}> : (tensor<4096x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<3072x4096xbf16>
    %377 = "ttir.dot_general"(%370, %376) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<1x4096xbf16>
    %378 = ttir.empty() : tensor<1x1x4096xbf16>
    %379 = "ttir.reshape"(%377, %378) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<1x4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
    %380 = ttir.empty() : tensor<1x1x4096xf32>
    %381 = "ttir.typecast"(%379, %380) <{conservative_folding = false}> : (tensor<1x1x4096xbf16>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %382 = ttir.empty() : tensor<1x1x4096xbf16>
    %383 = "ttir.sigmoid"(%379, %382) : (tensor<1x1x4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
    %384 = ttir.empty() : tensor<1x1x4096xf32>
    %385 = "ttir.typecast"(%383, %384) <{conservative_folding = false}> : (tensor<1x1x4096xbf16>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %386 = ttir.empty() : tensor<1x1x4096xf32>
    %387 = "ttir.multiply"(%381, %385, %386) : (tensor<1x1x4096xf32>, tensor<1x1x4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %388 = ttir.empty() : tensor<1x1x4096xbf16>
    %389 = "ttir.typecast"(%387, %388) <{conservative_folding = false}> : (tensor<1x1x4096xf32>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
    %390 = ttir.empty() : tensor<1x1x4096xf32>
    %391 = "ttir.typecast"(%389, %390) <{conservative_folding = false}> : (tensor<1x1x4096xbf16>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %392 = ttir.empty() : tensor<1x4096x3072xbf16>
    %393 = "ttir.reshape"(%7, %392) <{shape = [1 : i32, 4096 : i32, 3072 : i32]}> : (tensor<4096x3072xbf16>, tensor<1x4096x3072xbf16>) -> tensor<1x4096x3072xbf16>
    %394 = ttir.empty() : tensor<4096x3072xbf16>
    %395 = "ttir.reshape"(%393, %394) <{shape = [4096 : i32, 3072 : i32]}> : (tensor<1x4096x3072xbf16>, tensor<4096x3072xbf16>) -> tensor<4096x3072xbf16>
    %396 = ttir.empty() : tensor<3072x4096xbf16>
    %397 = "ttir.permute"(%395, %396) <{permutation = array<i64: 1, 0>}> : (tensor<4096x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<3072x4096xbf16>
    %398 = "ttir.dot_general"(%370, %397) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<1x4096xbf16>
    %399 = ttir.empty() : tensor<1x4096xf32>
    %400 = "ttir.typecast"(%398, %399) <{conservative_folding = false}> : (tensor<1x4096xbf16>, tensor<1x4096xf32>) -> tensor<1x4096xf32>
    %401 = ttir.empty() : tensor<1x1x4096xf32>
    %402 = "ttir.reshape"(%400, %401) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<1x4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %403 = ttir.empty() : tensor<1x1x4096xf32>
    %404 = "ttir.multiply"(%391, %402, %403) : (tensor<1x1x4096xf32>, tensor<1x1x4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %405 = ttir.empty() : tensor<1x1x4096xbf16>
    %406 = "ttir.typecast"(%404, %405) <{conservative_folding = false}> : (tensor<1x1x4096xf32>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
    %407 = ttir.empty() : tensor<1x4096xbf16>
    %408 = "ttir.reshape"(%406, %407) <{shape = [1 : i32, 4096 : i32]}> : (tensor<1x1x4096xbf16>, tensor<1x4096xbf16>) -> tensor<1x4096xbf16>
    %409 = ttir.empty() : tensor<1x3072x4096xbf16>
    %410 = "ttir.reshape"(%5, %409) <{shape = [1 : i32, 3072 : i32, 4096 : i32]}> : (tensor<3072x4096xbf16>, tensor<1x3072x4096xbf16>) -> tensor<1x3072x4096xbf16>
    %411 = ttir.empty() : tensor<3072x4096xbf16>
    %412 = "ttir.reshape"(%410, %411) <{shape = [3072 : i32, 4096 : i32]}> : (tensor<1x3072x4096xbf16>, tensor<3072x4096xbf16>) -> tensor<3072x4096xbf16>
    %413 = ttir.empty() : tensor<4096x3072xbf16>
    %414 = "ttir.permute"(%412, %413) <{permutation = array<i64: 1, 0>}> : (tensor<3072x4096xbf16>, tensor<4096x3072xbf16>) -> tensor<4096x3072xbf16>
    %415 = "ttir.dot_general"(%408, %414) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x4096xbf16>, tensor<4096x3072xbf16>) -> tensor<1x3072xbf16>
    %416 = ttir.empty() : tensor<1x3072xbf16>
    %417 = "ttir.all_reduce"(%415, %416) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<1x3072xbf16>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %418 = ttir.empty() : tensor<1x1x3072xbf16>
    %419 = "ttir.reshape"(%417, %418) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %420 = ttir.empty() : tensor<1x1x3072xbf16>
    %421 = "ttir.add"(%334, %419, %420) : (tensor<1x1x3072xbf16>, tensor<1x1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %422 = ttir.empty() : tensor<1x1x3072xf32>
    %423 = "ttir.typecast"(%421, %422) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %424 = ttir.empty() : tensor<1x1x3072xf32>
    %425 = "ttir.pow"(%423, %51, %424) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %426 = ttir.empty() : tensor<1x1xf32>
    %427 = "ttir.sum"(%425, %426) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %428 = ttir.empty() : tensor<1x1xf32>
    %429 = "ttir.multiply"(%427, %46, %428) : (tensor<1x1xf32>, tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %430 = ttir.empty() : tensor<1x1x1xf32>
    %431 = "ttir.reshape"(%429, %430) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %432 = ttir.empty() : tensor<1x1x1xf32>
    %433 = "ttir.add"(%431, %85, %432) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %434 = ttir.empty() : tensor<1x1x1xf32>
    %435 = "ttir.rsqrt"(%433, %434) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %436 = ttir.empty() : tensor<1x1xf32>
    %437 = "ttir.reshape"(%435, %436) <{shape = [1 : i32, 1 : i32]}> : (tensor<1x1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %438 = ttir.empty() : tensor<1x1x1xf32>
    %439 = "ttir.reshape"(%437, %438) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %440 = ttir.empty() : tensor<1x1x3072xf32>
    %441 = "ttir.broadcast"(%439, %440) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x1x1xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %442 = ttir.empty() : tensor<1x1x3072xf32>
    %443 = "ttir.multiply"(%423, %441, %442) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %444 = ttir.empty() : tensor<1x1x3072xbf16>
    %445 = "ttir.typecast"(%443, %444) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %446 = ttir.empty() : tensor<1x1x3072xf32>
    %447 = "ttir.typecast"(%445, %446) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %448 = ttir.empty() : tensor<1x1x3072xf32>
    %449 = "ttir.multiply"(%55, %447, %448) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %450 = ttir.empty() : tensor<1x1x3072xbf16>
    %451 = "ttir.typecast"(%449, %450) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %452 = ttir.empty() : tensor<1x3072xbf16>
    %453 = "ttir.reshape"(%451, %452) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x3072xbf16>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %454 = ttir.empty() : tensor<1x128256x3072xbf16>
    %455 = "ttir.reshape"(%1, %454) <{shape = [1 : i32, 128256 : i32, 3072 : i32]}> : (tensor<128256x3072xbf16>, tensor<1x128256x3072xbf16>) -> tensor<1x128256x3072xbf16>
    %456 = ttir.empty() : tensor<128256x3072xbf16>
    %457 = "ttir.reshape"(%455, %456) <{shape = [128256 : i32, 3072 : i32]}> : (tensor<1x128256x3072xbf16>, tensor<128256x3072xbf16>) -> tensor<128256x3072xbf16>
    %458 = ttir.empty() : tensor<3072x128256xbf16>
    %459 = "ttir.permute"(%457, %458) <{permutation = array<i64: 1, 0>}> : (tensor<128256x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<3072x128256xbf16>
    %460 = "ttir.dot_general"(%453, %459) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<1x128256xbf16>
    %461 = ttir.empty() : tensor<1x1x128256xbf16>
    %462 = "ttir.reshape"(%460, %461) <{shape = [1 : i32, 1 : i32, 128256 : i32]}> : (tensor<1x128256xbf16>, tensor<1x1x128256xbf16>) -> tensor<1x1x128256xbf16>
    %463 = ttir.empty() : tensor<1x128256xbf16>
    %464 = "ttir.mesh_shard"(%460, %463) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x128256xbf16>, tensor<1x128256xbf16>) -> tensor<1x128256xbf16>
    %465 = ttir.empty() : tensor<1x1x128256xbf16>
    %466 = "ttir.mesh_shard"(%462, %465) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x1x128256xbf16>, tensor<1x1x128256xbf16>) -> tensor<1x1x128256xbf16>
    return %464, %466 : tensor<1x128256xbf16>, tensor<1x1x128256xbf16>
  }
}
2025-09-15 15:42:03.851 (  72.178s) [        F073E000]      module_builder.cc:561   WARN| `mhlo.num_partitions` attribute not found, assuming default number of partitions: 1
2025-09-15 15:42:03.851 (  72.178s) [        F073E000]      module_builder.cc:575   WARN| `mhlo.num_replicas` attribute not found, assuming default number of replicas: 1
2025-09-15 15:42:03.851 (  72.178s) [        F073E000]      module_builder.cc:585   WARN| Num replicas and num partitions are not set, inferring the number of devices from mesh shape
2025-09-15 15:42:04.119 (  72.446s) [        F073E000]      module_builder.cc:630      1| TTNN Module:
module @SyncTensorsGraph.420 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.420 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073185088, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073193408, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x16, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 2)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 2, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main_const_eval_0(%arg0: tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_1(%arg0: tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_2(%arg0: tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %3 : tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_3(%arg0: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_4(%arg0: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.repeat"(%2) <{repeat_dims = #ttnn.shape<1x12x1x1024>}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %4 = "ttnn.reshape"(%3) <{shape = [12 : i32, 1 : i32, 1024 : i32]}> : (tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %4 : tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_5(%arg0: tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_6(%arg0: tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_7(%arg0: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %3 : tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_8(%arg0: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %3 : tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_9(%arg0: tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_10(%arg0: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_11(%arg0: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %3 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %4 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %2, %3, %4 : tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_12(%arg0: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %3 : tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_13() -> tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.constant"(%0) <{dtype = #ttcore.supportedDataTypes<si32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <interleaved>>, value = dense<"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tensor<1x1024xsi32>}> : (!ttnn.device) -> tensor<1x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1024xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x1024xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1x1x1024xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %3 : tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_14(%arg0: tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_15() -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %2 : tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main_const_eval_16(%arg0: tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        return %1 : tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main(%arg0: tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg1: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_0"}, %arg2: tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg3: tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg4: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg5: tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg6: tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg7: tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg8: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg9: tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg10: tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_1"}, %arg11: tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"}, %arg12: tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_2"}, %arg13: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "auto_annotated_const_3"}, %arg14: tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg15: tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg16: tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"}, %arg17: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg18: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg19: tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg20: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, [%arg0]) : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg10) <{force = false}> : (tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %1 = ttcore.load_cached(@main_const_eval_1, [%arg7]) : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg7) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %2 = ttcore.load_cached(@main_const_eval_2, [%arg12]) : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg12) <{force = false}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = ttcore.load_cached(@main_const_eval_3, [%arg17]) : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg17) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %4 = ttcore.load_cached(@main_const_eval_4, [%arg13]) : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg13) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %5 = ttcore.load_cached(@main_const_eval_5, [%arg19]) : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg19) <{force = false}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %6 = ttcore.load_cached(@main_const_eval_6, [%arg5]) : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg5) <{force = false}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %7 = ttcore.load_cached(@main_const_eval_7, [%arg18]) : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg18) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %8 = ttcore.load_cached(@main_const_eval_8, [%arg8]) : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg8) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %9 = ttcore.load_cached(@main_const_eval_9, [%arg2]) : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %10 = ttcore.load_cached(@main_const_eval_10, [%arg4]) : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %11:3 = ttcore.load_cached(@main_const_eval_11, [%arg1]) : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>)
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %12 = ttcore.load_cached(@main_const_eval_12, [%arg20]) : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg20) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %13 = ttcore.load_cached(@main_const_eval_13, []) : () -> tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %14 = ttcore.load_cached(@main_const_eval_14, [%arg15]) : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg15) <{force = false}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %15 = ttcore.load_cached(@main_const_eval_15, []) : () -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %16 = ttcore.load_cached(@main_const_eval_16, [%arg3]) : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %17 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %18 = "ttnn.full"(%17) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 3.25520843E-4 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %19 = "ttnn.mesh_shard"(%arg6, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg6) <{force = false}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %20 = "ttnn.mesh_shard"(%arg9, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %21 = "ttnn.mesh_shard"(%arg11, %17) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg11) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %22 = "ttnn.mesh_shard"(%arg14, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg14) <{force = false}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %23 = "ttnn.mesh_shard"(%arg16, %17) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg16) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %24 = "ttnn.typecast"(%19) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %25 = "ttnn.reshape"(%24) <{shape = [1 : i32]}> : (tensor<1x1xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%24) <{force = false}> : (tensor<1x1xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %26 = "ttnn.from_device"(%25) : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%25) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %27 = "ttnn.to_layout"(%26) <{layout = #ttnn.layout<row_major>}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%26) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> ()
        %28 = "ttnn.to_device"(%27, %17) <{memory_config = #ttnn.memory_config<<dram>, <interleaved>>}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%27) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<system_memory>>>>) -> ()
        %29 = "ttnn.embedding"(%28, %1) : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%28) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %30 = "ttnn.typecast"(%29) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %31 = "ttnn.reshape"(%30) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %32 = "ttnn.pow"(%31, %15) : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%31) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %33 = "ttnn.sum"(%32) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%32) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %34 = "ttnn.multiply"(%33, %18) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%33) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %35 = "ttnn.add"(%34, %11#0) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%34) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%11#0) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %36 = "ttnn.rsqrt"(%35) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%35) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %37 = "ttnn.multiply"(%30, %36) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%36) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%30) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %38 = "ttnn.multiply"(%8, %37) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%37) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %39 = "ttnn.typecast"(%38) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%38) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %40 = "ttnn.matmul"(%39, %3) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %41 = "ttnn.reshape"(%40) <{shape = [1 : i32, 12 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %42 = "ttnn.typecast"(%40) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1536xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%40) <{force = false}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %43 = "ttnn.reshape"(%42) <{shape = [12 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1536xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%42) <{force = false}> : (tensor<1x1536xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %44 = "ttnn.reshape"(%22) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%22) <{force = false}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %45 = "ttnn.typecast"(%20) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %46 = "ttnn.reshape"(%45) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%45) <{force = false}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %47 = "ttnn.matmul"(%44, %46) <{transpose_a = false, transpose_b = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%46) <{force = false}> : (tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%44) <{force = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %48 = "ttnn.reshape"(%47) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%47) <{force = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %49 = "ttnn.concat"(%48, %48) <{dim = 2 : si32}> : (tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%48) <{force = false}> : (tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %50 = "ttnn.cos"(%49) : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %51 = "ttnn.multiply"(%43, %50) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%43) <{force = false}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %52 = "ttnn.typecast"(%51) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%51) <{force = false}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %53 = "ttnn.slice_static"(%41) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 12 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %54 = "ttnn.neg"(%53) : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%53) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %55 = "ttnn.reshape"(%54) <{shape = [12 : i32, 1 : i32, 64 : i32]}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%54) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %56 = "ttnn.slice_static"(%41) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 12 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%41) <{force = false}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %57 = "ttnn.reshape"(%56) <{shape = [12 : i32, 1 : i32, 64 : i32]}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%56) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %58 = "ttnn.concat"(%55, %57) <{dim = 2 : si32}> : (tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%57) <{force = false}> : (tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%55) <{force = false}> : (tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %59 = "ttnn.typecast"(%58) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%58) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %60 = "ttnn.sin"(%49) : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%49) <{force = false}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %61 = "ttnn.multiply"(%59, %60) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%59) <{force = false}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %62 = "ttnn.typecast"(%61) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%61) <{force = false}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %63 = "ttnn.add"(%52, %62) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%62) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%52) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %64 = "ttnn.matmul"(%39, %14) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %65 = "ttnn.reshape"(%64) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32]}> : (tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%64) <{force = false}> : (tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %66 = "ttnn.typecast"(%65) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %67 = "ttnn.reshape"(%50) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%50) <{force = false}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %68 = "ttnn.multiply"(%66, %67) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%67) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%66) <{force = false}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %69 = "ttnn.typecast"(%68) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%68) <{force = false}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %70 = "ttnn.slice_static"(%65) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %71 = "ttnn.neg"(%70) : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%70) <{force = false}> : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %72 = "ttnn.slice_static"(%65) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%65) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %73 = "ttnn.concat"(%71, %72) <{dim = 3 : si32}> : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%72) <{force = false}> : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%71) <{force = false}> : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %74 = "ttnn.typecast"(%73) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%73) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %75 = "ttnn.reshape"(%60) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%60) <{force = false}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %76 = "ttnn.multiply"(%74, %75) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%75) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%74) <{force = false}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %77 = "ttnn.typecast"(%76) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%76) <{force = false}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %78 = "ttnn.add"(%69, %77) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%77) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%69) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %79 = "ttnn.typecast"(%arg9) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.update_cache"(%23, %78, %79) <{batch_offset = 0 : i32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%79) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%78) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %80 = "ttnn.reshape"(%23) <{shape = [1 : i32, 4 : i32, 1 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%23) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %81 = "ttnn.repeat"(%80) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%80) <{force = false}> : (tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %82 = "ttnn.reshape"(%81) <{shape = [1 : i32, 12 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12288 + d1 * 1024 + d2, d3), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%81) <{force = false}> : (tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %83 = "ttnn.permute"(%82) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12288 + d1 * 1024 + d2, d3), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%82) <{force = false}> : (tensor<1x12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12288 + d1 * 1024 + d2, d3), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %84 = "ttnn.reshape"(%83) <{shape = [12 : i32, 128 : i32, 1024 : i32]}> : (tensor<1x12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%83) <{force = false}> : (tensor<1x12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %85 = "ttnn.matmul"(%63, %84) <{transpose_a = false, transpose_b = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%84) <{force = false}> : (tensor<12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%63) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %86 = "ttnn.typecast"(%85) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%85) <{force = false}> : (tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %87 = "ttnn.multiply"(%86, %4) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%86) <{force = false}> : (tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %88 = "ttnn.typecast"(%87) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%87) <{force = false}> : (tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %89 = "ttnn.reshape"(%88) <{shape = [1 : i32, 12 : i32, 1 : i32, 1024 : i32]}> : (tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%88) <{force = false}> : (tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %90 = "ttnn.reshape"(%20) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %91 = "ttnn.typecast"(%90) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%90) <{force = false}> : (tensor<1x1x1x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %92 = "ttnn.gt"(%13, %91) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%91) <{force = false}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %93 = "ttnn.typecast"(%92) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%92) <{force = false}> : (tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %94 = "ttnn.typecast"(%93) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%93) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %95 = "ttnn.multiply"(%2, %94) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%94) <{force = false}> : (tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %96 = "ttnn.typecast"(%95) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%95) <{force = false}> : (tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %97 = "ttnn.add"(%89, %96) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%96) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%89) <{force = false}> : (tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %98 = "ttnn.typecast"(%97) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%97) <{force = false}> : (tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %99 = "ttnn.softmax"(%98) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%98) <{force = false}> : (tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %100 = "ttnn.typecast"(%99) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%99) <{force = false}> : (tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %101 = "ttnn.reshape"(%100) <{shape = [12 : i32, 1 : i32, 1024 : i32]}> : (tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%100) <{force = false}> : (tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %102 = "ttnn.matmul"(%39, %6) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%39) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %103 = "ttnn.reshape"(%102) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32]}> : (tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%102) <{force = false}> : (tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %104 = "ttnn.typecast"(%arg9) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg9) <{force = false}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.update_cache"(%21, %103, %104) <{batch_offset = 0 : i32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%104) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%103) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %105 = "ttnn.reshape"(%21) <{shape = [1 : i32, 4 : i32, 1 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%21) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %106 = "ttnn.repeat"(%105) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%105) <{force = false}> : (tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %107 = "ttnn.reshape"(%106) <{shape = [12 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 1024 + d1, d2), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%106) <{force = false}> : (tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %108 = "ttnn.matmul"(%101, %107) <{transpose_a = false, transpose_b = false}> : (tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 1024 + d1, d2), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%107) <{force = false}> : (tensor<12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 1024 + d1, d2), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%101) <{force = false}> : (tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %109 = "ttnn.reshape"(%108) <{shape = [1 : i32, 1536 : i32]}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%108) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %110 = "ttnn.matmul"(%109, %10) <{transpose_a = false, transpose_b = true}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%109) <{force = false}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %111 = "ttnn.reshape"(%110) <{shape = [1 : i32, 1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%110) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %112 = "ttnn.reduce_scatter"(%111, %17) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 3 : si32}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%111) <{force = false}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %113 = "ttnn.all_gather"(%112, %17) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%112) <{force = false}> : (tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %114 = "ttnn.reshape"(%113) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%113) <{force = false}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %115 = "ttnn.add"(%29, %114) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%114) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%29) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %116 = "ttnn.typecast"(%115) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %117 = "ttnn.reshape"(%116) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %118 = "ttnn.pow"(%117, %15) : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%117) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %119 = "ttnn.sum"(%118) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%118) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %120 = "ttnn.multiply"(%119, %18) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%119) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %121 = "ttnn.add"(%120, %11#1) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%120) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%11#1) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %122 = "ttnn.rsqrt"(%121) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%121) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %123 = "ttnn.multiply"(%116, %122) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%122) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%116) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %124 = "ttnn.multiply"(%7, %123) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%123) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %125 = "ttnn.typecast"(%124) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%124) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %126 = "ttnn.matmul"(%125, %5) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %127 = "ttnn.typecast"(%126) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %128 = "ttnn.sigmoid"(%126) : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%126) <{force = false}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %129 = "ttnn.typecast"(%128) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%128) <{force = false}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %130 = "ttnn.multiply"(%127, %129) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%129) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%127) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %131 = "ttnn.matmul"(%125, %16) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%125) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %132 = "ttnn.typecast"(%131) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%131) <{force = false}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %133 = "ttnn.multiply"(%130, %132) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%132) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%130) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %134 = "ttnn.typecast"(%133) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%133) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %135 = "ttnn.matmul"(%134, %9) <{transpose_a = false, transpose_b = true}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%134) <{force = false}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %136 = "ttnn.reshape"(%135) <{shape = [1 : i32, 1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%135) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %137 = "ttnn.reduce_scatter"(%136, %17) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 3 : si32}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%136) <{force = false}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %138 = "ttnn.all_gather"(%137, %17) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%137) <{force = false}> : (tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %139 = "ttnn.reshape"(%138) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%138) <{force = false}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %140 = "ttnn.add"(%115, %139) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%139) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%115) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %141 = "ttnn.typecast"(%140) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%140) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %142 = "ttnn.reshape"(%141) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %143 = "ttnn.pow"(%142, %15) : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%142) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %144 = "ttnn.sum"(%143) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%143) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %145 = "ttnn.multiply"(%144, %18) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%144) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %146 = "ttnn.add"(%145, %11#2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%145) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%11#2) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %147 = "ttnn.rsqrt"(%146) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%146) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %148 = "ttnn.multiply"(%141, %147) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%147) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%141) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %149 = "ttnn.multiply"(%12, %148) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%148) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %150 = "ttnn.typecast"(%149) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%149) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %151 = "ttnn.matmul"(%150, %0) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%150) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %152 = "ttnn.reshape"(%151) <{shape = [1 : i32, 1 : i32, 128256 : i32]}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %153 = "ttnn.to_layout"(%151) <{layout = #ttnn.layout<row_major>}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%151) <{force = false}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %154 = "ttnn.from_device"(%153) : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%153) <{force = false}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %155 = "ttnn.mesh_shard"(%154, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%154) <{force = false}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>) -> ()
        %156 = "ttnn.to_layout"(%152) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%152) <{force = false}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %157 = "ttnn.from_device"(%156) : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%156) <{force = false}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %158 = "ttnn.mesh_shard"(%157, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%157) <{force = false}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>) -> ()
        return %155, %158 : tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>, tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>
      }
    }
  }
}
2025-09-15 15:42:04.197 (  72.524s) [        F073E000]loaded_executable_insta:441      1| LoadedExecutableInstance::PJRT_LoadedExecutable_GetExecutable
2025-09-15 15:42:04.197 (  72.524s) [        F073E000]loaded_executable_insta:460      1| LoadedExecutableInstance::PJRT_LoadedExecutable_AddressableDevices
2025-09-15 15:42:04.198 (  72.524s) [        F073E000]              stubs.inc:76    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2025-09-15 15:42:04.198 (  72.524s) [        F073E000]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-09-15 15:42:04.198 (  72.524s) [        F073E000]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-09-15 15:42:04.198 (  72.524s) [        F073E000]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-09-15 15:42:04.198 (  72.524s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:04.198 (  72.524s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:04.210 (  72.536s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:04.210 (  72.537s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:04.220 (  72.547s) [        F2FFD640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:04.220 (  72.547s) [        F2FFD640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:04.221 (  72.547s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        6FFFF640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        6FFFF640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        6FFFF640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        6FFFF640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.547s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.547s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.548s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.548s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.548s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.548s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.548s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:04.221 (  72.548s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:04.221 (  72.548s) [        F2FFD640] executable_instance.cc:138      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-09-15 15:42:04.221 (  72.548s) [        F2FFD640]loaded_executable_insta:497      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-09-15 15:42:04.221 (  72.548s) [        F2FFD640]loaded_executable_insta:81       1| LoadedExecutableInstance::Execute
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Device grid size = { 8, 8 }
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = ttcore.load_cached(@main_const_eval_0, [%arg0]) : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_0
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_0
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_0
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_0
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg10) <{force = false}> : (tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = ttcore.load_cached(@main_const_eval_1, [%arg7]) : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_1
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_1
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_1
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_1
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg7) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = ttcore.load_cached(@main_const_eval_2, [%arg12]) : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_2
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_2
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.278_tm0_tm0_tm0"("broadcast.278_tm0_tm0"("broadcast.278_tm0"("broadcast.278"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.278_tm0_tm0_tm0"("broadcast.278_tm0_tm0"("broadcast.278_tm0"("broadcast.278"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.136")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.136")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_2
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_2
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg12) <{force = false}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = ttcore.load_cached(@main_const_eval_3, [%arg17]) : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_3
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_3
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_3
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_3
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg17) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %4 = ttcore.load_cached(@main_const_eval_4, [%arg13]) : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_4
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_4
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.272")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.272")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.repeat"(%2) <{repeat_dims = #ttnn.shape<1x12x1x1024>}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.272")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.272")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %4 = "ttnn.reshape"(%3) <{shape = [12 : i32, 1 : i32, 1024 : i32]}> : (tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.271")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.271")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_4
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_4
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg13) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %5 = ttcore.load_cached(@main_const_eval_5, [%arg19]) : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_5
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_5
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_5
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_5
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg19) <{force = false}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %6 = ttcore.load_cached(@main_const_eval_6, [%arg5]) : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_6
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_6
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_6
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_6
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg5) <{force = false}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %7 = ttcore.load_cached(@main_const_eval_7, [%arg18]) : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_7
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_7
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.358_tm0_tm0_tm0"("reshape.358_tm0_tm0"("reshape.358_tm0"("reshape.358"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.358_tm0_tm0_tm0"("reshape.358_tm0_tm0"("reshape.358_tm0"("reshape.358"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.345")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.345")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_7
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_7
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg18) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %8 = ttcore.load_cached(@main_const_eval_8, [%arg8]) : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_8
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_8
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.244_tm0_tm0_tm0"("reshape.244_tm0_tm0"("reshape.244_tm0"("reshape.244"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.244_tm0_tm0_tm0"("reshape.244_tm0_tm0"("reshape.244_tm0"("reshape.244"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.81")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.81")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_8
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_8
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg8) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %9 = ttcore.load_cached(@main_const_eval_9, [%arg2]) : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_9
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_9
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_9
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_9
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %10 = ttcore.load_cached(@main_const_eval_10, [%arg4]) : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_10
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_10
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_10
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_10
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %11:3 = ttcore.load_cached(@main_const_eval_11, [%arg1]) : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_11
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_11
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.244_tm0_tm1_tm1_tm0_tm1"("reshape.244_tm0_tm1_tm1_tm0"("reshape.244_tm0_tm1_tm1"("reshape.244_tm0_tm1"("reshape.244_tm0"("reshape.244"))))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.358_tm0_tm1_tm1_tm0_tm1"("reshape.358_tm0_tm1_tm1_tm0"("reshape.358_tm0_tm1_tm1"("reshape.358_tm0_tm1"("reshape.358_tm0"("reshape.358"))))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %4 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.416_tm0_tm1_tm1_tm0_tm1"("reshape.416_tm0_tm1_tm1_tm0"("reshape.416_tm0_tm1_tm1"("reshape.416_tm0_tm1"("reshape.416_tm0"("reshape.416"))))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.416_tm0_tm1_tm1_tm0_tm1"("reshape.416_tm0_tm1_tm1_tm0"("reshape.416_tm0_tm1_tm1"("reshape.416_tm0_tm1"("reshape.416_tm0"("reshape.416"))))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_11
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_11
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %12 = ttcore.load_cached(@main_const_eval_12, [%arg20]) : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_12
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_12
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.416_tm0_tm0_tm0"("reshape.416_tm0_tm0"("reshape.416_tm0"("reshape.416"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.416_tm0_tm0_tm0"("reshape.416_tm0_tm0"("reshape.416_tm0"("reshape.416"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.409")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.409")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_12
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_12
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg20) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %13 = ttcore.load_cached(@main_const_eval_13, []) : () -> tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_13
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_13
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.constant"(%0) <{dtype = #ttcore.supportedDataTypes<si32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <interleaved>>, value = dense_resource<__elided__> : tensor<1x1024xsi32>}> : (!ttnn.device) -> tensor<1x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1024xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.278_tm0_tm1_tm0_tm0"("broadcast.278_tm0_tm1_tm0"("broadcast.278_tm0_tm1"("broadcast.278_tm0"("broadcast.278")))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.278_tm0_tm1_tm0_tm0"("broadcast.278_tm0_tm1_tm0"("broadcast.278_tm0_tm1"("broadcast.278_tm0"("broadcast.278")))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x1024xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("compare.129_workaround"("compare.129"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1x1x1024xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("compare.129_workaround"("compare.129"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_13
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_13
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %14 = ttcore.load_cached(@main_const_eval_14, [%arg15]) : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_14
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_14
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_14
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_14
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg15) <{force = false}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %15 = ttcore.load_cached(@main_const_eval_15, []) : () -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_15
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_15
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_15
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_15
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %16 = ttcore.load_cached(@main_const_eval_16, [%arg3]) : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_16
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_16
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_16
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_16
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %17 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %18 = "ttnn.full"(%17) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 3.25520843E-4 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %19 = "ttnn.mesh_shard"(%arg6, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg6) <{force = false}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %20 = "ttnn.mesh_shard"(%arg9, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %21 = "ttnn.mesh_shard"(%arg11, %17) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg11) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %22 = "ttnn.mesh_shard"(%arg14, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg14) <{force = false}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %23 = "ttnn.mesh_shard"(%arg16, %17) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg16) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %24 = "ttnn.typecast"(%19) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.44")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.44")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %25 = "ttnn.reshape"(%24) <{shape = [1 : i32]}> : (tensor<1x1xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.44")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%24) <{force = false}> : (tensor<1x1xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.44")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %26 = "ttnn.from_device"(%25) : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>> loc("gather.45_workaround"("gather.45"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%25) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("gather.45_workaround"("gather.45"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %27 = "ttnn.to_layout"(%26) <{layout = #ttnn.layout<row_major>}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<system_memory>>>> loc("gather.45_workaround"("gather.45"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%26) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> () loc("gather.45_workaround"("gather.45"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %28 = "ttnn.to_device"(%27, %17) <{memory_config = #ttnn.memory_config<<dram>, <interleaved>>}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<dram>>, <interleaved>>> loc("gather.45_workaround"("gather.45"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%27) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<system_memory>>>>) -> () loc("gather.45_workaround"("gather.45"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %29 = "ttnn.embedding"(%28, %1) : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("gather.45")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%28) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("gather.45")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("gather.45")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %30 = "ttnn.typecast"(%29) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.47")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %31 = "ttnn.reshape"(%30) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.47")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %32 = "ttnn.pow"(%31, %15) : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("power.49")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%31) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("power.49")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %33 = "ttnn.sum"(%32) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reduce.56")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%32) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reduce.56")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %34 = "ttnn.multiply"(%33, %18) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.65")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%33) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.65")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %35 = "ttnn.add"(%34, %11#0) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.70")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%34) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.70")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%11#0) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.70")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %36 = "ttnn.rsqrt"(%35) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("rsqrt.71")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%35) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("rsqrt.71")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %37 = "ttnn.multiply"(%30, %36) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.74")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%36) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.74")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%30) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.74")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %38 = "ttnn.multiply"(%8, %37) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.83")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%37) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.83")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.83")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %39 = "ttnn.typecast"(%38) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.84")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%38) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.84")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %40 = "ttnn.matmul"(%39, %3) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.245")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%3) <{force = false}> : (tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.245")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %41 = "ttnn.reshape"(%40) <{shape = [1 : i32, 12 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("transpose.248")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %42 = "ttnn.typecast"(%40) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1536xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.259")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%40) <{force = false}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.259")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %43 = "ttnn.reshape"(%42) <{shape = [12 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1536xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.259")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%42) <{force = false}> : (tensor<1x1536xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.259")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %44 = "ttnn.reshape"(%22) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.162")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%22) <{force = false}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.162")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %45 = "ttnn.typecast"(%20) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.154")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %46 = "ttnn.reshape"(%45) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.154")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%45) <{force = false}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.154")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %47 = "ttnn.matmul"(%44, %46) <{transpose_a = false, transpose_b = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.165")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%46) <{force = false}> : (tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.165")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%44) <{force = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.165")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %48 = "ttnn.reshape"(%47) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("transpose.166")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%47) <{force = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("transpose.166")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %49 = "ttnn.concat"(%48, %48) <{dim = 2 : si32}> : (tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("concatenate.167")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%48) <{force = false}> : (tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("concatenate.167")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %50 = "ttnn.cos"(%49) : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("cosine.194")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %51 = "ttnn.multiply"(%43, %50) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.262")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%43) <{force = false}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.262")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %52 = "ttnn.typecast"(%51) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.263")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%51) <{force = false}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.263")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %53 = "ttnn.slice_static"(%41) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 12 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("slice.250")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %54 = "ttnn.neg"(%53) : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("negate.251")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%53) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("negate.251")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %55 = "ttnn.reshape"(%54) <{shape = [12 : i32, 1 : i32, 64 : i32]}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("negate.251")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%54) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("negate.251")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %56 = "ttnn.slice_static"(%41) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 12 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("slice.249")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%41) <{force = false}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("slice.249")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %57 = "ttnn.reshape"(%56) <{shape = [12 : i32, 1 : i32, 64 : i32]}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("concatenate.252")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%56) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("concatenate.252")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %58 = "ttnn.concat"(%55, %57) <{dim = 2 : si32}> : (tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("concatenate.252")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%57) <{force = false}> : (tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("concatenate.252")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%55) <{force = false}> : (tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("concatenate.252")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %59 = "ttnn.typecast"(%58) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.253")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%58) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.253")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %60 = "ttnn.sin"(%49) : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("sine.168")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%49) <{force = false}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("sine.168")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %61 = "ttnn.multiply"(%59, %60) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.256")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%59) <{force = false}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.256")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %62 = "ttnn.typecast"(%61) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.257")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%61) <{force = false}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.257")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %63 = "ttnn.add"(%52, %62) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.266")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%62) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.266")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%52) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.266")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %64 = "ttnn.matmul"(%39, %14) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.180")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%14) <{force = false}> : (tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.180")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %65 = "ttnn.reshape"(%64) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32]}> : (tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("transpose.183")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%64) <{force = false}> : (tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("transpose.183")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %66 = "ttnn.typecast"(%65) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.200")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %67 = "ttnn.reshape"(%50) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.202")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%50) <{force = false}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.202")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %68 = "ttnn.multiply"(%66, %67) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.203")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%67) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.203")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%66) <{force = false}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.203")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %69 = "ttnn.typecast"(%68) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.204")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%68) <{force = false}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.204")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %70 = "ttnn.slice_static"(%65) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("slice.185")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %71 = "ttnn.neg"(%70) : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("negate.186")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%70) <{force = false}> : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("negate.186")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %72 = "ttnn.slice_static"(%65) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("slice.184")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%65) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("slice.184")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %73 = "ttnn.concat"(%71, %72) <{dim = 3 : si32}> : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("concatenate.187")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%72) <{force = false}> : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("concatenate.187")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%71) <{force = false}> : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("concatenate.187")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %74 = "ttnn.typecast"(%73) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.188")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%73) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.188")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %75 = "ttnn.reshape"(%60) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.190")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%60) <{force = false}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.190")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %76 = "ttnn.multiply"(%74, %75) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.191")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%75) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.191")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%74) <{force = false}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.191")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %77 = "ttnn.typecast"(%76) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.192")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%76) <{force = false}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.192")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %78 = "ttnn.add"(%69, %77) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.207")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%77) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.207")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%69) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.207")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %79 = "ttnn.typecast"(%arg9) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("scatter.223_workaround"("scatter.223"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.update_cache"(%23, %78, %79) <{batch_offset = 0 : i32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("scatter.223")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%79) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("scatter.223")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%78) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("scatter.223")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %80 = "ttnn.reshape"(%23) <{shape = [1 : i32, 4 : i32, 1 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.232")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%23) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.232")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %81 = "ttnn.repeat"(%80) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.232")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%80) <{force = false}> : (tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.232")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %82 = "ttnn.reshape"(%81) <{shape = [1 : i32, 12 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12288 + d1 * 1024 + d2, d3), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.233")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%81) <{force = false}> : (tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.233")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %83 = "ttnn.permute"(%82) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12288 + d1 * 1024 + d2, d3), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("transpose.234")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%82) <{force = false}> : (tensor<1x12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12288 + d1 * 1024 + d2, d3), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("transpose.234")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %84 = "ttnn.reshape"(%83) <{shape = [12 : i32, 128 : i32, 1024 : i32]}> : (tensor<1x12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.236")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%83) <{force = false}> : (tensor<1x12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.236")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %85 = "ttnn.matmul"(%63, %84) <{transpose_a = false, transpose_b = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.269")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%84) <{force = false}> : (tensor<12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.269")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%63) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.269")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %86 = "ttnn.typecast"(%85) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.271")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%85) <{force = false}> : (tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.271")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %87 = "ttnn.multiply"(%86, %4) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.273")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%86) <{force = false}> : (tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.273")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%4) <{force = false}> : (tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.273")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %88 = "ttnn.typecast"(%87) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.274")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%87) <{force = false}> : (tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.274")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %89 = "ttnn.reshape"(%88) <{shape = [1 : i32, 12 : i32, 1 : i32, 1024 : i32]}> : (tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.274")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%88) <{force = false}> : (tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.274")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %90 = "ttnn.reshape"(%20) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.278_tm0_tm1_tm0_tm1"("broadcast.278_tm0_tm1_tm0"("broadcast.278_tm0_tm1"("broadcast.278_tm0"("broadcast.278")))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%20) <{force = false}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.278_tm0_tm1_tm0_tm1"("broadcast.278_tm0_tm1_tm0"("broadcast.278_tm0_tm1"("broadcast.278_tm0"("broadcast.278")))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %91 = "ttnn.typecast"(%90) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("compare.129_workaround"("compare.129"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%90) <{force = false}> : (tensor<1x1x1x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("compare.129_workaround"("compare.129"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %92 = "ttnn.gt"(%13, %91) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("compare.129")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%91) <{force = false}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("compare.129")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("compare.129")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %93 = "ttnn.typecast"(%92) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("compare.129_workaround"("compare.129"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%92) <{force = false}> : (tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("compare.129_workaround"("compare.129"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %94 = "ttnn.typecast"(%93) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.130")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%93) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.130")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %95 = "ttnn.multiply"(%2, %94) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.137")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%94) <{force = false}> : (tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.137")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.137")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %96 = "ttnn.typecast"(%95) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.138")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%95) <{force = false}> : (tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.138")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %97 = "ttnn.add"(%89, %96) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.279")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%96) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.279")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%89) <{force = false}> : (tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.279")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %98 = "ttnn.typecast"(%97) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.280")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%97) <{force = false}> : (tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.280")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %99 = "ttnn.softmax"(%98) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("divide.297")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%98) <{force = false}> : (tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("divide.297")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %100 = "ttnn.typecast"(%99) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.298")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%99) <{force = false}> : (tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.298")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %101 = "ttnn.reshape"(%100) <{shape = [12 : i32, 1 : i32, 1024 : i32]}> : (tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.298")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%100) <{force = false}> : (tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.298")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %102 = "ttnn.matmul"(%39, %6) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.86")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%39) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.86")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%6) <{force = false}> : (tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.86")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %103 = "ttnn.reshape"(%102) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32]}> : (tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("transpose.89")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%102) <{force = false}> : (tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("transpose.89")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %104 = "ttnn.typecast"(%arg9) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("scatter.110_workaround"("scatter.110"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg9) <{force = false}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("scatter.110_workaround"("scatter.110"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.update_cache"(%21, %103, %104) <{batch_offset = 0 : i32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("scatter.110")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%104) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("scatter.110")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%103) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("scatter.110")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %105 = "ttnn.reshape"(%21) <{shape = [1 : i32, 4 : i32, 1 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.119")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%21) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.119")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %106 = "ttnn.repeat"(%105) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.119")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%105) <{force = false}> : (tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.119")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %107 = "ttnn.reshape"(%106) <{shape = [12 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 1024 + d1, d2), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.122")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%106) <{force = false}> : (tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.122")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %108 = "ttnn.matmul"(%101, %107) <{transpose_a = false, transpose_b = false}> : (tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 1024 + d1, d2), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.301")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%107) <{force = false}> : (tensor<12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 1024 + d1, d2), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.301")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%101) <{force = false}> : (tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.301")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %109 = "ttnn.reshape"(%108) <{shape = [1 : i32, 1536 : i32]}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.305")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%108) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.305")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %110 = "ttnn.matmul"(%109, %10) <{transpose_a = false, transpose_b = true}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.306")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%109) <{force = false}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.306")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%10) <{force = false}> : (tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.306")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %111 = "ttnn.reshape"(%110) <{shape = [1 : i32, 1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.306_reduceScatter_reshape_to_4d"("dot.306_reduceScatter"("dot.306")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%110) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.306_reduceScatter_reshape_to_4d"("dot.306_reduceScatter"("dot.306")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %112 = "ttnn.reduce_scatter"(%111, %17) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 3 : si32}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.306_reduceScatter_reduce_scatter_4d"("dot.306_reduceScatter"("dot.306")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%111) <{force = false}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.306_reduceScatter_reduce_scatter_4d"("dot.306_reduceScatter"("dot.306")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %113 = "ttnn.all_gather"(%112, %17) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.306_all_gather_4d"("dot.306"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%112) <{force = false}> : (tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.306_all_gather_4d"("dot.306"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %114 = "ttnn.reshape"(%113) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.306")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%113) <{force = false}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.306")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %115 = "ttnn.add"(%29, %114) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.310")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%114) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.310")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%29) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.310")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %116 = "ttnn.typecast"(%115) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.311")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %117 = "ttnn.reshape"(%116) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.311")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %118 = "ttnn.pow"(%117, %15) : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("power.313")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%117) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("power.313")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %119 = "ttnn.sum"(%118) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reduce.320")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%118) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reduce.320")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %120 = "ttnn.multiply"(%119, %18) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.329")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%119) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.329")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %121 = "ttnn.add"(%120, %11#1) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.334")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%120) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.334")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%11#1) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.334")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %122 = "ttnn.rsqrt"(%121) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("rsqrt.335")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%121) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("rsqrt.335")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %123 = "ttnn.multiply"(%116, %122) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.338")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%122) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.338")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%116) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.338")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %124 = "ttnn.multiply"(%7, %123) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.347")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%123) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.347")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.347")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %125 = "ttnn.typecast"(%124) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.348")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%124) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.348")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %126 = "ttnn.matmul"(%125, %5) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.359")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%5) <{force = false}> : (tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.359")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %127 = "ttnn.typecast"(%126) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.363")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %128 = "ttnn.sigmoid"(%126) : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("logistic.361")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%126) <{force = false}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("logistic.361")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %129 = "ttnn.typecast"(%128) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.362")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%128) <{force = false}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.362")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %130 = "ttnn.multiply"(%127, %129) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.364")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%129) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.364")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%127) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.364")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %131 = "ttnn.matmul"(%125, %16) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.350")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%125) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.350")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%16) <{force = false}> : (tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.350")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %132 = "ttnn.typecast"(%131) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.352")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%131) <{force = false}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.352")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %133 = "ttnn.multiply"(%130, %132) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.367")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%132) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.367")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%130) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.367")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %134 = "ttnn.typecast"(%133) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.368")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%133) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.368")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %135 = "ttnn.matmul"(%134, %9) <{transpose_a = false, transpose_b = true}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.370")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%134) <{force = false}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.370")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%9) <{force = false}> : (tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.370")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %136 = "ttnn.reshape"(%135) <{shape = [1 : i32, 1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.370_reduceScatter_reshape_to_4d"("dot.370_reduceScatter"("dot.370")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%135) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.370_reduceScatter_reshape_to_4d"("dot.370_reduceScatter"("dot.370")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %137 = "ttnn.reduce_scatter"(%136, %17) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 3 : si32}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.370_reduceScatter_reduce_scatter_4d"("dot.370_reduceScatter"("dot.370")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%136) <{force = false}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.370_reduceScatter_reduce_scatter_4d"("dot.370_reduceScatter"("dot.370")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %138 = "ttnn.all_gather"(%137, %17) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.370_all_gather_4d"("dot.370"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%137) <{force = false}> : (tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.370_all_gather_4d"("dot.370"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %139 = "ttnn.reshape"(%138) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.370")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%138) <{force = false}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.370")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %140 = "ttnn.add"(%115, %139) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.374")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%139) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.374")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%115) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.374")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %141 = "ttnn.typecast"(%140) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.375")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%140) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.375")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %142 = "ttnn.reshape"(%141) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.375")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %143 = "ttnn.pow"(%142, %15) : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("power.377")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%142) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("power.377")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("power.377")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %144 = "ttnn.sum"(%143) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reduce.384")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%143) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reduce.384")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %145 = "ttnn.multiply"(%144, %18) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.393")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%144) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.393")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%18) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.393")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %146 = "ttnn.add"(%145, %11#2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.398")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%145) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.398")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%11#2) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.398")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %147 = "ttnn.rsqrt"(%146) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("rsqrt.399")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%146) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("rsqrt.399")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %148 = "ttnn.multiply"(%141, %147) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.402")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%147) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.402")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%141) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.402")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %149 = "ttnn.multiply"(%12, %148) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.411")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%148) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.411")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.411")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %150 = "ttnn.typecast"(%149) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.412")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%149) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.412")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %151 = "ttnn.matmul"(%150, %0) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.417")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%150) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.417")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%0) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.417")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %152 = "ttnn.reshape"(%151) <{shape = [1 : i32, 1 : i32, 128256 : i32]}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.418")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %153 = "ttnn.to_layout"(%151) <{layout = #ttnn.layout<row_major>}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%151) <{force = false}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %154 = "ttnn.from_device"(%153) : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%153) <{force = false}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %155 = "ttnn.mesh_shard"(%154, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%154) <{force = false}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %156 = "ttnn.to_layout"(%152) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%152) <{force = false}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %157 = "ttnn.from_device"(%156) : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%156) <{force = false}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %158 = "ttnn.mesh_shard"(%157, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%157) <{force = false}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main
2025-09-15 15:42:09.846 (  78.172s) [        F2FFD640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:09.846 (  78.172s) [        F2FFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:09.846 (  78.172s) [        F2FFD640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:09.846 (  78.172s) [        F2FFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:09.846 (  78.172s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:09.846 (  78.172s) [        F2FFD640]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:09.846 (  78.172s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:09.846 (  78.172s) [        F2FFD640]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:09.846 (  78.172s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:09.846 (  78.172s) [        F2FFD640]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:09.846 (  78.172s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:09.846 (  78.172s) [        F2FFD640]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:09.846 (  78.172s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:09.846 (  78.172s) [        F2FFD640]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:09.846 (  78.172s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:09.846 (  78.172s) [        F2FFD640]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:09.846 (  78.172s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:09.846 (  78.172s) [        F2FFD640]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:09.846 (  78.172s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:09.846 (  78.172s) [        F2FFD640]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:09.846 (  78.173s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:09.846 (  78.173s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:09.846 (  78.173s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:42:09.846 (  78.173s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:09.846 (  78.173s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:09.846 (  78.173s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:42:09.847 (  78.173s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User is requesting to copy the data from a runtime tensor with data type: Int32 into buffer with expected data type: Int64, the values will be casted, this may impact the throughput and the integrity of the data.
2025-09-15 15:42:09.847 (  78.173s) [        11FFB640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:09.848 (  78.174s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:09.848 (  78.174s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:09.848 (  78.174s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:42:09.848 (  78.174s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:09.848 (  78.174s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:09.848 (  78.174s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:42:09.848 (  78.174s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User is requesting to copy the data from a runtime tensor with data type: Int32 into buffer with expected data type: Int64, the values will be casted, this may impact the throughput and the integrity of the data.
2025-09-15 15:42:09.848 (  78.175s) [        11FFB640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:09.848 (  78.175s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:09.848 (  78.175s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:09.849 (  78.176s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:09.855 (  78.182s) [        F073E000]     client_instance.cc:428      1| ClientInstance::PJRT_Client_Compile
2025-09-15 15:42:09.855 (  78.182s) [        F073E000]      module_builder.cc:103      1| ModuleBuilder::buildModule
2025-09-15 15:42:09.856 (  78.182s) [        F073E000]      module_builder.cc:165      1| VHLO Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) {
    %0 = "vhlo.custom_call_v1"(%arg0) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"xla.sdy.FuncResultSharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<true>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {}, {}, {}]>]>">}>} : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>
    "vhlo.return_v1"(%0) : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>]>, res_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, sym_visibility = #vhlo.string_v1<"">}
}
2025-09-15 15:42:09.857 (  78.184s) [        F073E000]      module_builder.cc:185      1| Is using shardy? true
2025-09-15 15:42:09.859 (  78.186s) [        F073E000]      module_builder.cc:203      1| SHLO Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}) -> (tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>}) {
    return %arg0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:09.860 (  78.186s) [        F073E000]      module_builder.cc:212      1| SHLO Module after frontend StableHLO pipeline:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}) -> (tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>}) {
    return %arg0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:09.863 (  78.190s) [        F073E000]      module_builder.cc:486      1| SHLO Module after compiler StableHLO pipeline:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = sdy.manual_computation(%arg0) in_shardings=[<@mesh, [{}, {"_axis_0"}, {}, {}]>] out_shardings=[<@mesh, [{}, {}, {}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg1: tensor<1x4x1024x128xbf16>) {
      %1 = "stablehlo.all_gather"(%arg1) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> : (tensor<1x4x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
      sdy.return %1 : tensor<1x8x1024x128xbf16>
    } : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    return %0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:09.866 (  78.192s) [        F073E000]      module_builder.cc:510      1| TTIR Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = ttir.empty() : tensor<1x4x1024x128xbf16>
    %1 = "ttir.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16>, tensor<1x4x1024x128xbf16>) -> tensor<1x4x1024x128xbf16>
    %2 = ttir.empty() : tensor<1x8x1024x128xbf16>
    %3 = "ttir.all_gather"(%1, %2) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<1x4x1024x128xbf16>, tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %4 = ttir.empty() : tensor<1x8x1024x128xbf16>
    %5 = "ttir.mesh_shard"(%3, %4) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16>, tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    return %5 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:09.866 (  78.193s) [        F073E000]      module_builder.cc:561   WARN| `mhlo.num_partitions` attribute not found, assuming default number of partitions: 1
2025-09-15 15:42:09.866 (  78.193s) [        F073E000]      module_builder.cc:575   WARN| `mhlo.num_replicas` attribute not found, assuming default number of replicas: 1
2025-09-15 15:42:09.866 (  78.193s) [        F073E000]      module_builder.cc:585   WARN| Num replicas and num partitions are not set, inferring the number of devices from mesh shape
2025-09-15 15:42:09.897 (  78.224s) [        F073E000]      module_builder.cc:630      1| TTNN Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073185088, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073193408, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x16, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 2)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 2, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %2 = "ttnn.all_gather"(%1, %0) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.mesh_shard"(%2, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %3 : tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
    }
  }
}
2025-09-15 15:42:09.910 (  78.236s) [        F073E000]loaded_executable_insta:441      1| LoadedExecutableInstance::PJRT_LoadedExecutable_GetExecutable
2025-09-15 15:42:09.910 (  78.236s) [        F073E000]loaded_executable_insta:460      1| LoadedExecutableInstance::PJRT_LoadedExecutable_AddressableDevices
2025-09-15 15:42:09.911 (  78.237s) [        F073E000]              stubs.inc:76    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2025-09-15 15:42:09.911 (  78.237s) [        F073E000]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-09-15 15:42:09.911 (  78.237s) [        F073E000]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-09-15 15:42:09.911 (  78.237s) [        F073E000]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-09-15 15:42:09.911 (  78.237s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:09.911 (  78.237s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:09.914 (  78.240s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:09.914 (  78.240s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:09.916 (  78.242s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:09.916 (  78.242s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:09.916 (  78.242s) [        F073E000]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:09.916 (  78.242s) [        F073E000]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:09.916 (  78.243s) [        F073E000]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:09.916 (  78.243s) [        F073E000]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:09.916 (  78.243s) [        F073E000] executable_instance.cc:138      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-09-15 15:42:09.916 (  78.243s) [        F073E000]loaded_executable_insta:497      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-09-15 15:42:09.916 (  78.243s) [        F073E000]loaded_executable_insta:81       1| LoadedExecutableInstance::Execute
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Device grid size = { 8, 8 }
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.all_gather"(%1, %0) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("p0.1")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("p0.1")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.mesh_shard"(%2, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main
2025-09-15 15:42:10.287 (  78.613s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:10.287 (  78.613s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:10.287 (  78.613s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:10.287 (  78.613s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:10.287 (  78.613s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:10.287 (  78.613s) [        F073E000]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:10.287 (  78.613s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:10.287 (  78.613s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:10.287 (  78.613s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:10.287 (  78.613s) [        F073E000]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:10.287 (  78.613s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:10.287 (  78.613s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:10.287 (  78.613s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:10.287 (  78.613s) [        F073E000] executable_instance.cc:58       1| ExecutableInstance::PJRT_Executable_Destroy
2025-09-15 15:42:10.287 (  78.613s) [        F073E000]loaded_executable_insta:432      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Destroy
2025-09-15 15:42:10.287 (  78.614s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:42:10.287 (  78.614s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:10.287 (  78.614s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:10.287 (  78.614s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:42:10.287 (  78.614s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:10.288 (  78.614s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:10.289 (  78.615s) [         AFFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:10.292 (  78.618s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.292 (  78.618s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.292 (  78.618s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:10.298 (  78.624s) [        F073E000]     client_instance.cc:428      1| ClientInstance::PJRT_Client_Compile
2025-09-15 15:42:10.298 (  78.624s) [        F073E000]      module_builder.cc:103      1| ModuleBuilder::buildModule
2025-09-15 15:42:10.298 (  78.625s) [        F073E000]      module_builder.cc:165      1| VHLO Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) {
    %0 = "vhlo.custom_call_v1"(%arg0) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"xla.sdy.FuncResultSharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<true>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {}, {}, {}]>]>">}>} : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>
    "vhlo.return_v1"(%0) : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>]>, res_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, sym_visibility = #vhlo.string_v1<"">}
}
2025-09-15 15:42:10.300 (  78.626s) [        F073E000]      module_builder.cc:185      1| Is using shardy? true
2025-09-15 15:42:10.302 (  78.628s) [        F073E000]      module_builder.cc:203      1| SHLO Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}) -> (tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>}) {
    return %arg0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:10.302 (  78.629s) [        F073E000]      module_builder.cc:212      1| SHLO Module after frontend StableHLO pipeline:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}) -> (tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>}) {
    return %arg0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:10.306 (  78.633s) [        F073E000]      module_builder.cc:486      1| SHLO Module after compiler StableHLO pipeline:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = sdy.manual_computation(%arg0) in_shardings=[<@mesh, [{}, {"_axis_0"}, {}, {}]>] out_shardings=[<@mesh, [{}, {}, {}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg1: tensor<1x4x1024x128xbf16>) {
      %1 = "stablehlo.all_gather"(%arg1) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> : (tensor<1x4x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
      sdy.return %1 : tensor<1x8x1024x128xbf16>
    } : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    return %0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:10.309 (  78.635s) [        F073E000]      module_builder.cc:510      1| TTIR Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = ttir.empty() : tensor<1x4x1024x128xbf16>
    %1 = "ttir.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16>, tensor<1x4x1024x128xbf16>) -> tensor<1x4x1024x128xbf16>
    %2 = ttir.empty() : tensor<1x8x1024x128xbf16>
    %3 = "ttir.all_gather"(%1, %2) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<1x4x1024x128xbf16>, tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %4 = ttir.empty() : tensor<1x8x1024x128xbf16>
    %5 = "ttir.mesh_shard"(%3, %4) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16>, tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    return %5 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:10.309 (  78.635s) [        F073E000]      module_builder.cc:561   WARN| `mhlo.num_partitions` attribute not found, assuming default number of partitions: 1
2025-09-15 15:42:10.309 (  78.635s) [        F073E000]      module_builder.cc:575   WARN| `mhlo.num_replicas` attribute not found, assuming default number of replicas: 1
2025-09-15 15:42:10.309 (  78.635s) [        F073E000]      module_builder.cc:585   WARN| Num replicas and num partitions are not set, inferring the number of devices from mesh shape
2025-09-15 15:42:10.340 (  78.666s) [        F073E000]      module_builder.cc:630      1| TTNN Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073185088, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073193408, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x16, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 2)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 2, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %2 = "ttnn.all_gather"(%1, %0) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.mesh_shard"(%2, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %3 : tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
    }
  }
}
2025-09-15 15:42:10.351 (  78.677s) [        F073E000]loaded_executable_insta:441      1| LoadedExecutableInstance::PJRT_LoadedExecutable_GetExecutable
2025-09-15 15:42:10.351 (  78.677s) [        F073E000]loaded_executable_insta:460      1| LoadedExecutableInstance::PJRT_LoadedExecutable_AddressableDevices
2025-09-15 15:42:10.352 (  78.678s) [        F073E000]              stubs.inc:76    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2025-09-15 15:42:10.352 (  78.678s) [        F073E000]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-09-15 15:42:10.352 (  78.678s) [        F073E000]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-09-15 15:42:10.352 (  78.678s) [        F073E000]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-09-15 15:42:10.352 (  78.678s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:10.352 (  78.678s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:10.355 (  78.681s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:10.355 (  78.681s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:10.357 (  78.683s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:10.357 (  78.683s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:10.357 (  78.683s) [        F073E000]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.357 (  78.683s) [        F073E000]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.357 (  78.683s) [        F073E000]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.357 (  78.683s) [        F073E000]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.357 (  78.683s) [        F073E000] executable_instance.cc:138      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-09-15 15:42:10.357 (  78.683s) [        F073E000]loaded_executable_insta:497      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-09-15 15:42:10.357 (  78.683s) [        F073E000]loaded_executable_insta:81       1| LoadedExecutableInstance::Execute
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Device grid size = { 8, 8 }
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.all_gather"(%1, %0) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("p0.1")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("p0.1")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.mesh_shard"(%2, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main
2025-09-15 15:42:10.640 (  78.967s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:10.640 (  78.967s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:10.641 (  78.967s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:10.641 (  78.967s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:10.641 (  78.967s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:10.641 (  78.967s) [        F073E000]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:10.641 (  78.967s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:10.641 (  78.967s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:10.641 (  78.967s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:10.641 (  78.967s) [        F073E000]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:10.641 (  78.967s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:10.641 (  78.967s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:10.641 (  78.967s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:10.641 (  78.967s) [        F073E000] executable_instance.cc:58       1| ExecutableInstance::PJRT_Executable_Destroy
2025-09-15 15:42:10.641 (  78.967s) [        F073E000]loaded_executable_insta:432      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Destroy
2025-09-15 15:42:10.641 (  78.967s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:42:10.641 (  78.967s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:10.641 (  78.967s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:10.641 (  78.967s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:42:10.641 (  78.968s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:10.641 (  78.968s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:10.642 (  78.968s) [        11FFB640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:10.645 (  78.972s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:10.645 (  78.972s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:10.645 (  78.972s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:10.645 (  78.972s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:10.646 (  78.972s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.646 (  78.972s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.646 (  78.972s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:42:10.646 (  78.972s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:10.646 (  78.972s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:10.646 (  78.972s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:42:10.646 (  78.972s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:10.646 (  78.972s) [        10FF9640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:10.647 (  78.973s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.647 (  78.973s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.647 (  78.973s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:42:10.647 (  78.973s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:10.647 (  78.973s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:42:10.647 (  78.973s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:10.647 (  78.973s) [        11FFB640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:10.649 (  78.975s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:10.649 (  78.975s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:10.649 (  78.975s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.649 (  78.975s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.649 (  78.975s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:42:10.649 (  78.975s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:10.649 (  78.975s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:42:10.649 (  78.975s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User is requesting to copy the data from a runtime tensor with data type: Int32 into buffer with expected data type: Int64, the values will be casted, this may impact the throughput and the integrity of the data.
2025-09-15 15:42:10.649 (  78.975s) [        FA6C4640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:10.649 (  78.976s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:10.649 (  78.976s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first

Args after sync (tensor([[279]], device='xla:0'), tensor([7], device='xla:0'), tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='xla:0',
       dtype=torch.bfloat16), tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='xla:0',
       dtype=torch.bfloat16))
CausalLMOutputWithPast(loss=None, logits=tensor([[[ 2.5312,  1.1094, -0.8086,  ..., -3.4844, -3.4844, -3.4844]]],
       device='xla:0', dtype=torch.bfloat16), past_key_values=<transformers.cache_utils.StaticCache object at 0x7f24ac6c3e50>, hidden_states=None, attentions=None)
Generated token:  the
readable graph module
class GraphModule(torch.nn.Module):
    def forward(self, args_0, args_1, args_2, args_3):
        args_0: "i64[1, 1]"; args_1: "i64[1]"; args_2: "bf16[1, 8, 1024, 128]"; args_3: "bf16[1, 8, 1024, 128]"; 
    
        args_0, args_1, args_2, args_3, = fx_pytree.tree_flatten_spec(([args_0, args_1, args_2, args_3], {}), self._in_spec)
        # No stacktrace found for following nodes
        l__self___model_layers__modules__0___input_layernorm_weight: "bf16[3072]" = self.L__self___model_layers__modules__0___input_layernorm_weight
        mark_argument_attributes = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___input_layernorm_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___input_layernorm_weight');  l__self___model_layers__modules__0___input_layernorm_weight = None
        l__self___model_layers__modules__0___post_attention_layernorm_weight: "bf16[3072]" = self.L__self___model_layers__modules__0___post_attention_layernorm_weight
        mark_argument_attributes_1 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___post_attention_layernorm_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___post_attention_layernorm_weight');  l__self___model_layers__modules__0___post_attention_layernorm_weight = None
        l__self___model_norm_weight: "bf16[3072]" = self.L__self___model_norm_weight
        mark_argument_attributes_2 = torch.ops.tt.mark_argument_attributes(l__self___model_norm_weight, argument_type = 'parameter', name = 'l__self___model_norm_weight');  l__self___model_norm_weight = None
        l__self___model_embed_tokens_weight: "bf16[128256, 3072]" = self.L__self___model_embed_tokens.weight
        mark_argument_attributes_3 = torch.ops.tt.mark_argument_attributes(l__self___model_embed_tokens_weight, argument_type = 'parameter', name = 'l__self___model_embed_tokens_weight');  l__self___model_embed_tokens_weight = None
        l__self___model_layers__modules__0___self_attn_q_proj_weight: "bf16[3072, 3072]" = self.L__self___model_layers__modules__0___self_attn_q_proj.weight
        mark_argument_attributes_4 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___self_attn_q_proj_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___self_attn_q_proj_weight');  l__self___model_layers__modules__0___self_attn_q_proj_weight = None
        l__self___model_layers__modules__0___self_attn_k_proj_weight: "bf16[1024, 3072]" = self.L__self___model_layers__modules__0___self_attn_k_proj.weight
        mark_argument_attributes_5 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___self_attn_k_proj_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___self_attn_k_proj_weight');  l__self___model_layers__modules__0___self_attn_k_proj_weight = None
        l__self___model_layers__modules__0___self_attn_v_proj_weight: "bf16[1024, 3072]" = self.L__self___model_layers__modules__0___self_attn_v_proj.weight
        mark_argument_attributes_6 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___self_attn_v_proj_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___self_attn_v_proj_weight');  l__self___model_layers__modules__0___self_attn_v_proj_weight = None
        l__self___model_layers__modules__0___self_attn_o_proj_weight: "bf16[3072, 3072]" = self.L__self___model_layers__modules__0___self_attn_o_proj.weight
        mark_argument_attributes_7 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___self_attn_o_proj_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___self_attn_o_proj_weight');  l__self___model_layers__modules__0___self_attn_o_proj_weight = None
        l__self___model_layers__modules__0___mlp_gate_proj_weight: "bf16[8192, 3072]" = self.L__self___model_layers__modules__0___mlp_gate_proj.weight
        mark_argument_attributes_8 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___mlp_gate_proj_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___mlp_gate_proj_weight');  l__self___model_layers__modules__0___mlp_gate_proj_weight = None
        l__self___model_layers__modules__0___mlp_up_proj_weight: "bf16[8192, 3072]" = self.L__self___model_layers__modules__0___mlp_up_proj.weight
        mark_argument_attributes_9 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___mlp_up_proj_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___mlp_up_proj_weight');  l__self___model_layers__modules__0___mlp_up_proj_weight = None
        l__self___model_layers__modules__0___mlp_down_proj_weight: "bf16[3072, 8192]" = self.L__self___model_layers__modules__0___mlp_down_proj.weight
        mark_argument_attributes_10 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___mlp_down_proj_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___mlp_down_proj_weight');  l__self___model_layers__modules__0___mlp_down_proj_weight = None
        l__self___lm_head_weight: "bf16[128256, 3072]" = self.L__self___lm_head.weight
        mark_argument_attributes_11 = torch.ops.tt.mark_argument_attributes(l__self___lm_head_weight, argument_type = 'parameter', name = 'l__self___lm_head_weight');  l__self___lm_head_weight = None
        l__self___model_rotary_emb_inv_freq: "f32[64]" = self.L__self___model_rotary_emb_inv_freq
        mark_argument_attributes_12 = torch.ops.tt.mark_argument_attributes(l__self___model_rotary_emb_inv_freq, argument_type = 'input', name = 'l__self___model_rotary_emb_inv_freq');  l__self___model_rotary_emb_inv_freq = None
        mark_argument_attributes_13 = torch.ops.tt.mark_argument_attributes(args_0, argument_type = 'input', name = 'args_0');  args_0 = None
        mark_argument_attributes_14 = torch.ops.tt.mark_argument_attributes(args_1, argument_type = 'input', name = 'args_1');  args_1 = None
        mark_argument_attributes_15 = torch.ops.tt.mark_argument_attributes(args_2, argument_type = 'input', name = 'args_2');  args_2 = None
        mark_argument_attributes_16 = torch.ops.tt.mark_argument_attributes(args_3, argument_type = 'input', name = 'args_3');  args_3 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:422 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        embedding: "bf16[1, 1, 3072]" = torch.ops.aten.embedding.default(mark_argument_attributes_3, mark_argument_attributes_13);  mark_argument_attributes_3 = mark_argument_attributes_13 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:434 in forward, code: position_ids = cache_position.unsqueeze(0)
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(mark_argument_attributes_14, 0)
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:586 in _prepare_4d_causal_attention_mask_with_cache_position, code: causal_mask = torch.full(
        empty_strided: "bf16[1, 1024]" = torch.ops.aten.empty_strided.default([1, 1024], [1024, 1], dtype = torch.bfloat16, layout = torch.strided, device = device(type='xla', index=0), pin_memory = False)
        full_like: "bf16[1, 1024]" = torch.ops.aten.full_like.default(empty_strided, -3.3895313892515355e+38, pin_memory = False);  empty_strided = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:591 in _prepare_4d_causal_attention_mask_with_cache_position, code: causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)
        arange: "i64[1024]" = torch.ops.aten.arange.start_step(0, 1024, layout = torch.strided, device = device(type='xla', index=0), pin_memory = False)
        view: "i64[1, 1]" = torch.ops.aten.view.default(mark_argument_attributes_14, [-1, 1])
        gt: "b8[1, 1024]" = torch.ops.aten.gt.Tensor(arange, view);  arange = view = None
        mul: "bf16[1, 1024]" = torch.ops.aten.mul.Tensor(full_like, gt);  full_like = gt = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:103 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        unsqueeze_3: "f32[1, 64]" = torch.ops.aten.unsqueeze.default(mark_argument_attributes_12, 0);  mark_argument_attributes_12 = None
        slice_3: "f32[1, 64]" = torch.ops.aten.slice.Tensor(unsqueeze_3, 1, 0, 9223372036854775807);  unsqueeze_3 = None
        unsqueeze_4: "f32[1, 64, 1]" = torch.ops.aten.unsqueeze.default(slice_3, 2);  slice_3 = None
        convert_element_type: "f32[1, 64, 1]" = torch.ops.prims.convert_element_type.default(unsqueeze_4, torch.float32);  convert_element_type = None
        expand_1: "f32[1, 64, 1]" = torch.ops.aten.expand.default(unsqueeze_4, [1, -1, 1]);  unsqueeze_4 = None
        convert_element_type_1: "f32[1, 64, 1]" = torch.ops.prims.convert_element_type.default(expand_1, torch.float32);  convert_element_type_1 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:104 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        slice_4: "i64[1, 1]" = torch.ops.aten.slice.Tensor(unsqueeze, 0, 0, 9223372036854775807);  unsqueeze = None
        unsqueeze_5: "i64[1, 1, 1]" = torch.ops.aten.unsqueeze.default(slice_4, 1);  slice_4 = None
        slice_5: "i64[1, 1, 1]" = torch.ops.aten.slice.Tensor(unsqueeze_5, 2, 0, 9223372036854775807);  unsqueeze_5 = None
        convert_element_type_2: "f32[1, 1, 1]" = torch.ops.prims.convert_element_type.default(slice_5, torch.float32);  slice_5 = None
        
         # File: <eval_with_key>.32:5 in forward, code: to_3 = torch.ops.aten.to.dtype(to_1, torch.float32);  to_1 = None
        convert_element_type_3: "f32[1, 64, 1]" = torch.ops.prims.convert_element_type.default(expand_1, torch.float32);  convert_element_type_3 = None
        
         # File: <eval_with_key>.32:6 in forward, code: to_4 = torch.ops.aten.to.dtype(to_2, torch.float32);  to_2 = None
        convert_element_type_4: "f32[1, 1, 1]" = torch.ops.prims.convert_element_type.default(convert_element_type_2, torch.float32);  convert_element_type_4 = None
        
         # File: <eval_with_key>.32:7 in forward, code: matmul = torch.ops.aten.matmul.default(to_3, to_4);  to_3 = to_4 = None
        expand_2: "f32[1, 64, 1]" = torch.ops.aten.expand.default(expand_1, [1, 64, 1]);  expand_1 = None
        view_1: "f32[1, 64, 1]" = torch.ops.aten.view.default(expand_2, [1, 64, 1]);  expand_2 = None
        expand_3: "f32[1, 1, 1]" = torch.ops.aten.expand.default(convert_element_type_2, [1, 1, 1]);  convert_element_type_2 = None
        view_2: "f32[1, 1, 1]" = torch.ops.aten.view.default(expand_3, [1, 1, 1]);  expand_3 = None
        bmm: "f32[1, 64, 1]" = torch.ops.aten.bmm.default(view_1, view_2);  view_1 = view_2 = None
        view_3: "f32[1, 64, 1]" = torch.ops.aten.view.default(bmm, [1, 64, 1]);  bmm = None
        
         # File: <eval_with_key>.32:8 in forward, code: transpose = torch.ops.aten.transpose.int(matmul, 1, 2);  matmul = None
        permute: "f32[1, 1, 64]" = torch.ops.aten.permute.default(view_3, [0, 2, 1]);  view_3 = None
        
         # File: <eval_with_key>.32:9 in forward, code: cat = torch.ops.aten.cat.default([transpose, transpose], -1);  transpose = None
        cat: "f32[1, 1, 128]" = torch.ops.aten.cat.default([permute, permute], -1);  permute = None
        
         # File: <eval_with_key>.32:10 in forward, code: cos = torch.ops.aten.cos.default(cat)
        cos: "f32[1, 1, 128]" = torch.ops.aten.cos.default(cat)
        
         # File: <eval_with_key>.32:11 in forward, code: mul = torch.ops.aten.mul.Tensor(cos, 1.0);  cos = None
        mul_1: "f32[1, 1, 128]" = torch.ops.aten.mul.Tensor(cos, 1.0);  cos = None
        
         # File: <eval_with_key>.32:12 in forward, code: sin = torch.ops.aten.sin.default(cat);  cat = None
        sin: "f32[1, 1, 128]" = torch.ops.aten.sin.default(cat);  cat = None
        
         # File: <eval_with_key>.32:13 in forward, code: mul_1 = torch.ops.aten.mul.Tensor(sin, 1.0);  sin = None
        mul_2: "f32[1, 1, 128]" = torch.ops.aten.mul.Tensor(sin, 1.0);  sin = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        convert_element_type_5: "bf16[1, 1, 128]" = torch.ops.prims.convert_element_type.default(mul_1, torch.bfloat16);  mul_1 = None
        convert_element_type_6: "bf16[1, 1, 128]" = torch.ops.prims.convert_element_type.default(mul_2, torch.bfloat16);  mul_2 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:70 in forward, code: hidden_states = hidden_states.to(torch.float32)
        convert_element_type_7: "f32[1, 1, 3072]" = torch.ops.prims.convert_element_type.default(embedding, torch.float32)
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:71 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 1, 3072]" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_7, 2)
        mean: "f32[1, 1, 1]" = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:72 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 1, 1]" = torch.ops.aten.add.Tensor(mean, 1e-05);  mean = None
        rsqrt: "f32[1, 1, 1]" = torch.ops.aten.rsqrt.default(add);  add = None
        mul_3: "f32[1, 1, 3072]" = torch.ops.aten.mul.Tensor(convert_element_type_7, rsqrt);  convert_element_type_7 = rsqrt = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:73 in forward, code: return self.weight * hidden_states.to(input_dtype)
        convert_element_type_8: "bf16[1, 1, 3072]" = torch.ops.prims.convert_element_type.default(mul_3, torch.bfloat16);  mul_3 = None
        mul_4: "bf16[1, 1, 3072]" = torch.ops.aten.mul.Tensor(mark_argument_attributes, convert_element_type_8);  mark_argument_attributes = convert_element_type_8 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:242 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_1: "bf16[3072, 3072]" = torch.ops.aten.permute.default(mark_argument_attributes_4, [1, 0]);  mark_argument_attributes_4 = None
        view_4: "bf16[1, 3072]" = torch.ops.aten.view.default(mul_4, [1, 3072])
        mm: "bf16[1, 3072]" = torch.ops.aten.mm.default(view_4, permute_1);  view_4 = permute_1 = None
        view_5: "bf16[1, 1, 3072]" = torch.ops.aten.view.default(mm, [1, 1, 3072]);  mm = None
        view_6: "bf16[1, 1, 24, 128]" = torch.ops.aten.view.default(view_5, [1, 1, -1, 128]);  view_5 = None
        permute_2: "bf16[1, 24, 1, 128]" = torch.ops.aten.permute.default(view_6, [0, 2, 1, 3]);  view_6 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:243 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_3: "bf16[3072, 1024]" = torch.ops.aten.permute.default(mark_argument_attributes_5, [1, 0]);  mark_argument_attributes_5 = None
        view_7: "bf16[1, 3072]" = torch.ops.aten.view.default(mul_4, [1, 3072])
        mm_1: "bf16[1, 1024]" = torch.ops.aten.mm.default(view_7, permute_3);  view_7 = permute_3 = None
        view_8: "bf16[1, 1, 1024]" = torch.ops.aten.view.default(mm_1, [1, 1, 1024]);  mm_1 = None
        view_9: "bf16[1, 1, 8, 128]" = torch.ops.aten.view.default(view_8, [1, 1, -1, 128]);  view_8 = None
        permute_4: "bf16[1, 8, 1, 128]" = torch.ops.aten.permute.default(view_9, [0, 2, 1, 3]);  view_9 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:244 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_5: "bf16[3072, 1024]" = torch.ops.aten.permute.default(mark_argument_attributes_6, [1, 0]);  mark_argument_attributes_6 = None
        view_10: "bf16[1, 3072]" = torch.ops.aten.view.default(mul_4, [1, 3072]);  mul_4 = None
        mm_2: "bf16[1, 1024]" = torch.ops.aten.mm.default(view_10, permute_5);  view_10 = permute_5 = None
        view_11: "bf16[1, 1, 1024]" = torch.ops.aten.view.default(mm_2, [1, 1, 1024]);  mm_2 = None
        view_12: "bf16[1, 1, 8, 128]" = torch.ops.aten.view.default(view_11, [1, 1, -1, 128]);  view_11 = None
        permute_6: "bf16[1, 8, 1, 128]" = torch.ops.aten.permute.default(view_12, [0, 2, 1, 3]);  view_12 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:143 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_6: "bf16[1, 1, 1, 128]" = torch.ops.aten.unsqueeze.default(convert_element_type_5, 1);  convert_element_type_5 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:144 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_7: "bf16[1, 1, 1, 128]" = torch.ops.aten.unsqueeze.default(convert_element_type_6, 1);  convert_element_type_6 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:145 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 24, 1, 128]" = torch.ops.aten.mul.Tensor(permute_2, unsqueeze_6)
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_6: "bf16[1, 24, 1, 64]" = torch.ops.aten.slice.Tensor(permute_2, 3, 0, 64)
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_7: "bf16[1, 24, 1, 64]" = torch.ops.aten.slice.Tensor(permute_2, 3, 64, 9223372036854775807);  permute_2 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 24, 1, 64]" = torch.ops.aten.neg.default(slice_7);  slice_7 = None
        cat_1: "bf16[1, 24, 1, 128]" = torch.ops.aten.cat.default([neg, slice_6], -1);  neg = slice_6 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:145 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_6: "bf16[1, 24, 1, 128]" = torch.ops.aten.mul.Tensor(cat_1, unsqueeze_7);  cat_1 = None
        add_1: "bf16[1, 24, 1, 128]" = torch.ops.aten.add.Tensor(mul_5, mul_6);  mul_5 = mul_6 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:146 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 1, 128]" = torch.ops.aten.mul.Tensor(permute_4, unsqueeze_6);  unsqueeze_6 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_8: "bf16[1, 8, 1, 64]" = torch.ops.aten.slice.Tensor(permute_4, 3, 0, 64)
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_9: "bf16[1, 8, 1, 64]" = torch.ops.aten.slice.Tensor(permute_4, 3, 64, 9223372036854775807);  permute_4 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 1, 64]" = torch.ops.aten.neg.default(slice_9);  slice_9 = None
        cat_2: "bf16[1, 8, 1, 128]" = torch.ops.aten.cat.default([neg_1, slice_8], -1);  neg_1 = slice_8 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:146 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_8: "bf16[1, 8, 1, 128]" = torch.ops.aten.mul.Tensor(cat_2, unsqueeze_7);  cat_2 = unsqueeze_7 = None
        add_2: "bf16[1, 8, 1, 128]" = torch.ops.aten.add.Tensor(mul_7, mul_8);  mul_7 = mul_8 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/cache_utils.py:1368 in update, code: key_states = key_states.to(self.key_cache[layer_idx].dtype)
        convert_element_type_9: "bf16[1, 8, 1, 128]" = torch.ops.prims.convert_element_type.default(add_2, torch.bfloat16);  convert_element_type_9 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/cache_utils.py:1369 in update, code: value_states = value_states.to(self.value_cache[layer_idx].dtype)
        convert_element_type_10: "bf16[1, 8, 1, 128]" = torch.ops.prims.convert_element_type.default(permute_6, torch.bfloat16);  convert_element_type_10 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/cache_utils.py:54 in _static_cache_update, code: k_cache.index_copy_(2, cache_position, key_states)
        index_put: "bf16[1, 8, 1024, 128]" = torch.ops.aten.index_put.default(mark_argument_attributes_15, [None, None, mark_argument_attributes_14], add_2);  add_2 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/cache_utils.py:55 in _static_cache_update, code: v_cache.index_copy_(2, cache_position, value_states)
        index_put_1: "bf16[1, 8, 1024, 128]" = torch.ops.aten.index_put.default(mark_argument_attributes_16, [None, None, mark_argument_attributes_14], permute_6);  mark_argument_attributes_14 = permute_6 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        slice_14: "bf16[1, 8, 1024, 128]" = torch.ops.aten.slice.Tensor(index_put, 0, 0, 9223372036854775807)
        slice_15: "bf16[1, 8, 1024, 128]" = torch.ops.aten.slice.Tensor(slice_14, 1, 0, 9223372036854775807);  slice_14 = None
        unsqueeze_9: "bf16[1, 8, 1, 1024, 128]" = torch.ops.aten.unsqueeze.default(slice_15, 2);  slice_15 = None
        slice_16: "bf16[1, 8, 1, 1024, 128]" = torch.ops.aten.slice.Tensor(unsqueeze_9, 3, 0, 9223372036854775807);  unsqueeze_9 = None
        slice_17: "bf16[1, 8, 1, 1024, 128]" = torch.ops.aten.slice.Tensor(slice_16, 4, 0, 9223372036854775807);  slice_16 = None
        expand_5: "bf16[1, 8, 3, 1024, 128]" = torch.ops.aten.expand.default(slice_17, [1, 8, 3, 1024, 128]);  slice_17 = None
        clone: "bf16[1, 8, 3, 1024, 128]" = torch.ops.aten.clone.default(expand_5, memory_format = torch.contiguous_format);  expand_5 = None
        view_13: "bf16[1, 24, 1024, 128]" = torch.ops.aten.view.default(clone, [1, 24, 1024, 128]);  clone = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        slice_22: "bf16[1, 8, 1024, 128]" = torch.ops.aten.slice.Tensor(index_put_1, 0, 0, 9223372036854775807)
        slice_23: "bf16[1, 8, 1024, 128]" = torch.ops.aten.slice.Tensor(slice_22, 1, 0, 9223372036854775807);  slice_22 = None
        unsqueeze_11: "bf16[1, 8, 1, 1024, 128]" = torch.ops.aten.unsqueeze.default(slice_23, 2);  slice_23 = None
        slice_24: "bf16[1, 8, 1, 1024, 128]" = torch.ops.aten.slice.Tensor(unsqueeze_11, 3, 0, 9223372036854775807);  unsqueeze_11 = None
        slice_25: "bf16[1, 8, 1, 1024, 128]" = torch.ops.aten.slice.Tensor(slice_24, 4, 0, 9223372036854775807);  slice_24 = None
        expand_7: "bf16[1, 8, 3, 1024, 128]" = torch.ops.aten.expand.default(slice_25, [1, 8, 3, 1024, 128]);  slice_25 = None
        clone_1: "bf16[1, 8, 3, 1024, 128]" = torch.ops.aten.clone.default(expand_7, memory_format = torch.contiguous_format);  expand_7 = None
        view_14: "bf16[1, 24, 1024, 128]" = torch.ops.aten.view.default(clone_1, [1, 24, 1024, 128]);  clone_1 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:191 in eager_attention_forward, code: attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling
        permute_7: "bf16[1, 24, 128, 1024]" = torch.ops.aten.permute.default(view_13, [0, 1, 3, 2]);  view_13 = None
        expand_8: "bf16[1, 24, 1, 128]" = torch.ops.aten.expand.default(add_1, [1, 24, 1, 128]);  add_1 = None
        view_15: "bf16[24, 1, 128]" = torch.ops.aten.view.default(expand_8, [24, 1, 128]);  expand_8 = None
        expand_9: "bf16[1, 24, 128, 1024]" = torch.ops.aten.expand.default(permute_7, [1, 24, 128, 1024]);  permute_7 = None
        view_16: "bf16[24, 128, 1024]" = torch.ops.aten.view.default(expand_9, [24, 128, 1024]);  expand_9 = None
        bmm_1: "bf16[24, 1, 1024]" = torch.ops.aten.bmm.default(view_15, view_16);  view_15 = view_16 = None
        view_17: "bf16[1, 24, 1, 1024]" = torch.ops.aten.view.default(bmm_1, [1, 24, 1, 1024]);  bmm_1 = None
        mul_9: "bf16[1, 24, 1, 1024]" = torch.ops.aten.mul.Tensor(view_17, 0.08838834764831845);  view_17 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:194 in eager_attention_forward, code: attn_weights = attn_weights + causal_mask
        unsqueeze_12: "bf16[1, 1, 1024]" = torch.ops.aten.unsqueeze.default(mul, 0);  mul = None
        unsqueeze_13: "bf16[1, 1, 1, 1024]" = torch.ops.aten.unsqueeze.default(unsqueeze_12, 1);  unsqueeze_12 = None
        slice_29: "bf16[1, 1, 1, 1024]" = torch.ops.aten.slice.Tensor(unsqueeze_13, 2, 0, 9223372036854775807);  unsqueeze_13 = None
        slice_30: "bf16[1, 1, 1, 1024]" = torch.ops.aten.slice.Tensor(slice_29, 3, 0, 9223372036854775807);  slice_29 = None
        expand_10: "bf16[1, 1, 1, 1024]" = torch.ops.aten.expand.default(slice_30, [1, 1, -1, -1]);  slice_30 = None
        slice_31: "bf16[1, 1, 1, 1024]" = torch.ops.aten.slice.Tensor(expand_10, 0, 0, 9223372036854775807);  expand_10 = None
        slice_32: "bf16[1, 1, 1, 1024]" = torch.ops.aten.slice.Tensor(slice_31, 1, 0, 9223372036854775807);  slice_31 = None
        slice_33: "bf16[1, 1, 1, 1024]" = torch.ops.aten.slice.Tensor(slice_32, 2, 0, 9223372036854775807);  slice_32 = None
        add_3: "bf16[1, 24, 1, 1024]" = torch.ops.aten.add.Tensor(mul_9, slice_33);  mul_9 = slice_33 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:196 in eager_attention_forward, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
        convert_element_type_11: "f32[1, 24, 1, 1024]" = torch.ops.prims.convert_element_type.default(add_3, torch.float32);  add_3 = None
        _softmax: "f32[1, 24, 1, 1024]" = torch.ops.aten._softmax.default(convert_element_type_11, -1, False);  convert_element_type_11 = None
        convert_element_type_12: "bf16[1, 24, 1, 1024]" = torch.ops.prims.convert_element_type.default(_softmax, torch.bfloat16);  _softmax = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:197 in eager_attention_forward, code: attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
        clone_2: "bf16[1, 24, 1, 1024]" = torch.ops.aten.clone.default(convert_element_type_12);  convert_element_type_12 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:198 in eager_attention_forward, code: attn_output = torch.matmul(attn_weights, value_states)
        expand_11: "bf16[1, 24, 1, 1024]" = torch.ops.aten.expand.default(clone_2, [1, 24, 1, 1024]);  clone_2 = None
        view_18: "bf16[24, 1, 1024]" = torch.ops.aten.view.default(expand_11, [24, 1, 1024]);  expand_11 = None
        expand_12: "bf16[1, 24, 1024, 128]" = torch.ops.aten.expand.default(view_14, [1, 24, 1024, 128]);  view_14 = None
        view_19: "bf16[24, 1024, 128]" = torch.ops.aten.view.default(expand_12, [24, 1024, 128]);  expand_12 = None
        bmm_2: "bf16[24, 1, 128]" = torch.ops.aten.bmm.default(view_18, view_19);  view_18 = view_19 = None
        view_20: "bf16[1, 24, 1, 128]" = torch.ops.aten.view.default(bmm_2, [1, 24, 1, 128]);  bmm_2 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:199 in eager_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_8: "bf16[1, 1, 24, 128]" = torch.ops.aten.permute.default(view_20, [0, 2, 1, 3]);  view_20 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:276 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_21: "bf16[1, 1, 3072]" = torch.ops.aten.view.default(permute_8, [1, 1, -1]);  permute_8 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:277 in forward, code: attn_output = self.o_proj(attn_output)
        permute_9: "bf16[3072, 3072]" = torch.ops.aten.permute.default(mark_argument_attributes_7, [1, 0]);  mark_argument_attributes_7 = None
        view_22: "bf16[1, 3072]" = torch.ops.aten.view.default(view_21, [1, 3072]);  view_21 = None
        mm_3: "bf16[1, 3072]" = torch.ops.aten.mm.default(view_22, permute_9);  view_22 = permute_9 = None
        view_23: "bf16[1, 1, 3072]" = torch.ops.aten.view.default(mm_3, [1, 1, 3072]);  mm_3 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:319 in forward, code: hidden_states = residual + hidden_states
        add_4: "bf16[1, 1, 3072]" = torch.ops.aten.add.Tensor(embedding, view_23);  embedding = view_23 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:70 in forward, code: hidden_states = hidden_states.to(torch.float32)
        convert_element_type_13: "f32[1, 1, 3072]" = torch.ops.prims.convert_element_type.default(add_4, torch.float32)
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:71 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_2: "f32[1, 1, 3072]" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_13, 2)
        mean_1: "f32[1, 1, 1]" = torch.ops.aten.mean.dim(pow_2, [-1], True);  pow_2 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:72 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add_5: "f32[1, 1, 1]" = torch.ops.aten.add.Tensor(mean_1, 1e-05);  mean_1 = None
        rsqrt_1: "f32[1, 1, 1]" = torch.ops.aten.rsqrt.default(add_5);  add_5 = None
        mul_10: "f32[1, 1, 3072]" = torch.ops.aten.mul.Tensor(convert_element_type_13, rsqrt_1);  convert_element_type_13 = rsqrt_1 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:73 in forward, code: return self.weight * hidden_states.to(input_dtype)
        convert_element_type_14: "bf16[1, 1, 3072]" = torch.ops.prims.convert_element_type.default(mul_10, torch.bfloat16);  mul_10 = None
        mul_11: "bf16[1, 1, 3072]" = torch.ops.aten.mul.Tensor(mark_argument_attributes_1, convert_element_type_14);  mark_argument_attributes_1 = convert_element_type_14 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:162 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_10: "bf16[3072, 8192]" = torch.ops.aten.permute.default(mark_argument_attributes_8, [1, 0]);  mark_argument_attributes_8 = None
        view_24: "bf16[1, 3072]" = torch.ops.aten.view.default(mul_11, [1, 3072])
        mm_4: "bf16[1, 8192]" = torch.ops.aten.mm.default(view_24, permute_10);  view_24 = permute_10 = None
        view_25: "bf16[1, 1, 8192]" = torch.ops.aten.view.default(mm_4, [1, 1, 8192]);  mm_4 = None
        convert_element_type_15: "f32[1, 1, 8192]" = torch.ops.prims.convert_element_type.default(view_25, torch.float32);  convert_element_type_15 = None
        sigmoid: "f32[1, 1, 8192]" = torch.ops.aten.sigmoid.default(view_25)
        mul_12: "f32[1, 1, 8192]" = torch.ops.aten.mul.Tensor(view_25, sigmoid);  view_25 = sigmoid = None
        convert_element_type_16: "bf16[1, 1, 8192]" = torch.ops.prims.convert_element_type.default(mul_12, torch.bfloat16);  mul_12 = None
        permute_11: "bf16[3072, 8192]" = torch.ops.aten.permute.default(mark_argument_attributes_9, [1, 0]);  mark_argument_attributes_9 = None
        view_26: "bf16[1, 3072]" = torch.ops.aten.view.default(mul_11, [1, 3072]);  mul_11 = None
        mm_5: "bf16[1, 8192]" = torch.ops.aten.mm.default(view_26, permute_11);  view_26 = permute_11 = None
        view_27: "bf16[1, 1, 8192]" = torch.ops.aten.view.default(mm_5, [1, 1, 8192]);  mm_5 = None
        mul_13: "bf16[1, 1, 8192]" = torch.ops.aten.mul.Tensor(convert_element_type_16, view_27);  convert_element_type_16 = view_27 = None
        permute_12: "bf16[8192, 3072]" = torch.ops.aten.permute.default(mark_argument_attributes_10, [1, 0]);  mark_argument_attributes_10 = None
        view_28: "bf16[1, 8192]" = torch.ops.aten.view.default(mul_13, [1, 8192]);  mul_13 = None
        mm_6: "bf16[1, 3072]" = torch.ops.aten.mm.default(view_28, permute_12);  view_28 = permute_12 = None
        view_29: "bf16[1, 1, 3072]" = torch.ops.aten.view.default(mm_6, [1, 1, 3072]);  mm_6 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325 in forward, code: hidden_states = residual + hidden_states
        add_6: "bf16[1, 1, 3072]" = torch.ops.aten.add.Tensor(add_4, view_29);  add_4 = view_29 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:70 in forward, code: hidden_states = hidden_states.to(torch.float32)
        convert_element_type_17: "f32[1, 1, 3072]" = torch.ops.prims.convert_element_type.default(add_6, torch.float32);  add_6 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:71 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_3: "f32[1, 1, 3072]" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_17, 2)
        mean_2: "f32[1, 1, 1]" = torch.ops.aten.mean.dim(pow_3, [-1], True);  pow_3 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:72 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add_7: "f32[1, 1, 1]" = torch.ops.aten.add.Tensor(mean_2, 1e-05);  mean_2 = None
        rsqrt_2: "f32[1, 1, 1]" = torch.ops.aten.rsqrt.default(add_7);  add_7 = None
        mul_14: "f32[1, 1, 3072]" = torch.ops.aten.mul.Tensor(convert_element_type_17, rsqrt_2);  convert_element_type_17 = rsqrt_2 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:73 in forward, code: return self.weight * hidden_states.to(input_dtype)
        convert_element_type_18: "bf16[1, 1, 3072]" = torch.ops.prims.convert_element_type.default(mul_14, torch.bfloat16);  mul_14 = None
        mul_15: "bf16[1, 1, 3072]" = torch.ops.aten.mul.Tensor(mark_argument_attributes_2, convert_element_type_18);  mark_argument_attributes_2 = convert_element_type_18 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:704 in forward, code: logits = self.lm_head(hidden_states[:, slice_indices, :])
        slice_34: "bf16[1, 1, 3072]" = torch.ops.aten.slice.Tensor(mul_15, 0, 0, 9223372036854775807);  mul_15 = None
        slice_35: "bf16[1, 1, 3072]" = torch.ops.aten.slice.Tensor(slice_34, 1, 0, 9223372036854775807);  slice_34 = None
        slice_36: "bf16[1, 1, 3072]" = torch.ops.aten.slice.Tensor(slice_35, 2, 0, 9223372036854775807);  slice_35 = None
        permute_13: "bf16[3072, 128256]" = torch.ops.aten.permute.default(mark_argument_attributes_11, [1, 0]);  mark_argument_attributes_11 = None
        view_30: "bf16[1, 3072]" = torch.ops.aten.view.default(slice_36, [1, 3072]);  slice_36 = None
        mm_7: "bf16[1, 128256]" = torch.ops.aten.mm.default(view_30, permute_13);  view_30 = permute_13 = None
        view_31: "bf16[1, 1, 128256]" = torch.ops.aten.view.default(mm_7, [1, 1, 128256]);  mm_7 = None
        
        # No stacktrace found for following nodes
        copy__default = torch.ops.aten.copy_.default(mark_argument_attributes_15, index_put);  mark_argument_attributes_15 = index_put = copy__default = None
        copy__default_1 = torch.ops.aten.copy_.default(mark_argument_attributes_16, index_put_1);  mark_argument_attributes_16 = index_put_1 = copy__default_1 = None
        return pytree.tree_unflatten((view_31,), self._out_spec)
        2025-09-15 15:42:10.658 (  78.984s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:10.658 (  78.984s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:10.658 (  78.984s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:42:10.658 (  78.984s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:42:10.658 (  78.984s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-09-15 15:42:10.658 (  78.984s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:10.658 (  78.984s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:10.658 (  78.984s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:42:10.658 (  78.984s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:42:10.658 (  78.984s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-09-15 15:42:10.658 (  78.984s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:10.658 (  78.984s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:10.658 (  78.985s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:10.658 (  78.985s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:10.658 (  78.985s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:42:10.658 (  78.985s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:42:10.658 (  78.985s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-09-15 15:42:10.658 (  78.985s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:10.658 (  78.985s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:10.658 (  78.985s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:42:10.658 (  78.985s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:42:10.658 (  78.985s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-09-15 15:42:10.659 (  78.985s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:10.659 (  78.985s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.665 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.666 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.666 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.666 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.666 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.666 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.666 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.666 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.666 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.666 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.666 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.666 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.666 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.666 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.666 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.666 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.666 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.666 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.666 (  78.992s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:10.666 (  78.992s) [        F2FFD640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:10.666 (  78.992s) [        F2FFD640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:10.666 (  78.992s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        6FFFF640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        6FFFF640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        6FFFF640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        6FFFF640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.993s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.992s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.993s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.992s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.993s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.993s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.993s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.993s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:10.666 (  78.993s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:10.666 (  78.993s) [        F2FFD640] executable_instance.cc:138      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-09-15 15:42:10.666 (  78.993s) [        F2FFD640]loaded_executable_insta:497      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-09-15 15:42:10.666 (  78.993s) [        F2FFD640]loaded_executable_insta:81       1| LoadedExecutableInstance::Execute
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Device grid size = { 8, 8 }
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = ttcore.load_cached(@main_const_eval_0, [%arg0]) : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_0
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_0
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_0
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_0
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg10) <{force = false}> : (tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = ttcore.load_cached(@main_const_eval_1, [%arg7]) : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_1
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_1
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_1
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_1
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg7) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = ttcore.load_cached(@main_const_eval_2, [%arg12]) : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_2
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_2
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.278_tm0_tm0_tm0"("broadcast.278_tm0_tm0"("broadcast.278_tm0"("broadcast.278"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.278_tm0_tm0_tm0"("broadcast.278_tm0_tm0"("broadcast.278_tm0"("broadcast.278"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.136")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.136")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_2
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_2
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg12) <{force = false}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = ttcore.load_cached(@main_const_eval_3, [%arg17]) : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_3
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_3
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_3
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_3
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg17) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %4 = ttcore.load_cached(@main_const_eval_4, [%arg13]) : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_4
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_4
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.272")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.272")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.repeat"(%2) <{repeat_dims = #ttnn.shape<1x12x1x1024>}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.272")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.272")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %4 = "ttnn.reshape"(%3) <{shape = [12 : i32, 1 : i32, 1024 : i32]}> : (tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.271")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.271")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_4
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_4
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg13) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %5 = ttcore.load_cached(@main_const_eval_5, [%arg19]) : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_5
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_5
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_5
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_5
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg19) <{force = false}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %6 = ttcore.load_cached(@main_const_eval_6, [%arg5]) : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_6
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_6
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_6
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_6
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg5) <{force = false}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %7 = ttcore.load_cached(@main_const_eval_7, [%arg18]) : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_7
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_7
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.358_tm0_tm0_tm0"("reshape.358_tm0_tm0"("reshape.358_tm0"("reshape.358"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.358_tm0_tm0_tm0"("reshape.358_tm0_tm0"("reshape.358_tm0"("reshape.358"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.345")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.345")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_7
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_7
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg18) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %8 = ttcore.load_cached(@main_const_eval_8, [%arg8]) : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_8
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_8
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.244_tm0_tm0_tm0"("reshape.244_tm0_tm0"("reshape.244_tm0"("reshape.244"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.244_tm0_tm0_tm0"("reshape.244_tm0_tm0"("reshape.244_tm0"("reshape.244"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.81")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.81")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_8
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_8
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg8) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %9 = ttcore.load_cached(@main_const_eval_9, [%arg2]) : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_9
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_9
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_9
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_9
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %10 = ttcore.load_cached(@main_const_eval_10, [%arg4]) : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_10
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_10
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_10
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_10
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %11:3 = ttcore.load_cached(@main_const_eval_11, [%arg1]) : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_11
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_11
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.244_tm0_tm1_tm1_tm0_tm1"("reshape.244_tm0_tm1_tm1_tm0"("reshape.244_tm0_tm1_tm1"("reshape.244_tm0_tm1"("reshape.244_tm0"("reshape.244"))))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.358_tm0_tm1_tm1_tm0_tm1"("reshape.358_tm0_tm1_tm1_tm0"("reshape.358_tm0_tm1_tm1"("reshape.358_tm0_tm1"("reshape.358_tm0"("reshape.358"))))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %4 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.416_tm0_tm1_tm1_tm0_tm1"("reshape.416_tm0_tm1_tm1_tm0"("reshape.416_tm0_tm1_tm1"("reshape.416_tm0_tm1"("reshape.416_tm0"("reshape.416"))))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.416_tm0_tm1_tm1_tm0_tm1"("reshape.416_tm0_tm1_tm1_tm0"("reshape.416_tm0_tm1_tm1"("reshape.416_tm0_tm1"("reshape.416_tm0"("reshape.416"))))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_11
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_11
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %12 = ttcore.load_cached(@main_const_eval_12, [%arg20]) : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_12
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_12
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.416_tm0_tm0_tm0"("reshape.416_tm0_tm0"("reshape.416_tm0"("reshape.416"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.416_tm0_tm0_tm0"("reshape.416_tm0_tm0"("reshape.416_tm0"("reshape.416"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.409")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.409")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_12
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_12
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg20) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %13 = ttcore.load_cached(@main_const_eval_13, []) : () -> tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_13
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_13
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.constant"(%0) <{dtype = #ttcore.supportedDataTypes<si32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <interleaved>>, value = dense_resource<__elided__> : tensor<1x1024xsi32>}> : (!ttnn.device) -> tensor<1x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1024xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.278_tm0_tm1_tm0_tm0"("broadcast.278_tm0_tm1_tm0"("broadcast.278_tm0_tm1"("broadcast.278_tm0"("broadcast.278")))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.278_tm0_tm1_tm0_tm0"("broadcast.278_tm0_tm1_tm0"("broadcast.278_tm0_tm1"("broadcast.278_tm0"("broadcast.278")))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x1024xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("compare.129_workaround"("compare.129"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1x1x1024xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("compare.129_workaround"("compare.129"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_13
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_13
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %14 = ttcore.load_cached(@main_const_eval_14, [%arg15]) : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_14
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_14
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_14
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_14
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg15) <{force = false}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %15 = ttcore.load_cached(@main_const_eval_15, []) : () -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_15
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_15
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_15
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_15
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %16 = ttcore.load_cached(@main_const_eval_16, [%arg3]) : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_16
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_16
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_16
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_16
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %17 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %18 = "ttnn.full"(%17) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 3.25520843E-4 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %19 = "ttnn.mesh_shard"(%arg6, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg6) <{force = false}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %20 = "ttnn.mesh_shard"(%arg9, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %21 = "ttnn.mesh_shard"(%arg11, %17) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg11) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %22 = "ttnn.mesh_shard"(%arg14, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg14) <{force = false}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %23 = "ttnn.mesh_shard"(%arg16, %17) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg16) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %24 = "ttnn.typecast"(%19) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.44")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.44")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %25 = "ttnn.reshape"(%24) <{shape = [1 : i32]}> : (tensor<1x1xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.44")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%24) <{force = false}> : (tensor<1x1xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.44")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %26 = "ttnn.from_device"(%25) : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>> loc("gather.45_workaround"("gather.45"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%25) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("gather.45_workaround"("gather.45"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %27 = "ttnn.to_layout"(%26) <{layout = #ttnn.layout<row_major>}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<system_memory>>>> loc("gather.45_workaround"("gather.45"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%26) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> () loc("gather.45_workaround"("gather.45"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %28 = "ttnn.to_device"(%27, %17) <{memory_config = #ttnn.memory_config<<dram>, <interleaved>>}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<dram>>, <interleaved>>> loc("gather.45_workaround"("gather.45"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%27) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<system_memory>>>>) -> () loc("gather.45_workaround"("gather.45"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %29 = "ttnn.embedding"(%28, %1) : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("gather.45")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%28) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("gather.45")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("gather.45")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %30 = "ttnn.typecast"(%29) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.47")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %31 = "ttnn.reshape"(%30) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.47")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %32 = "ttnn.pow"(%31, %15) : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("power.49")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%31) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("power.49")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %33 = "ttnn.sum"(%32) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reduce.56")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%32) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reduce.56")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %34 = "ttnn.multiply"(%33, %18) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.65")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%33) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.65")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %35 = "ttnn.add"(%34, %11#0) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.70")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%34) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.70")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%11#0) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.70")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %36 = "ttnn.rsqrt"(%35) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("rsqrt.71")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%35) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("rsqrt.71")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %37 = "ttnn.multiply"(%30, %36) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.74")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%36) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.74")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%30) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.74")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %38 = "ttnn.multiply"(%8, %37) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.83")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%37) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.83")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.83")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %39 = "ttnn.typecast"(%38) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.84")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%38) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.84")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %40 = "ttnn.matmul"(%39, %3) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.245")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%3) <{force = false}> : (tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.245")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %41 = "ttnn.reshape"(%40) <{shape = [1 : i32, 12 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("transpose.248")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %42 = "ttnn.typecast"(%40) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1536xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.259")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%40) <{force = false}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.259")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %43 = "ttnn.reshape"(%42) <{shape = [12 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1536xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.259")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%42) <{force = false}> : (tensor<1x1536xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.259")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %44 = "ttnn.reshape"(%22) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.162")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%22) <{force = false}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.162")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %45 = "ttnn.typecast"(%20) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.154")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %46 = "ttnn.reshape"(%45) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.154")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%45) <{force = false}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.154")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %47 = "ttnn.matmul"(%44, %46) <{transpose_a = false, transpose_b = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.165")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%46) <{force = false}> : (tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.165")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%44) <{force = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.165")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %48 = "ttnn.reshape"(%47) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("transpose.166")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%47) <{force = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("transpose.166")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %49 = "ttnn.concat"(%48, %48) <{dim = 2 : si32}> : (tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("concatenate.167")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%48) <{force = false}> : (tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("concatenate.167")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %50 = "ttnn.cos"(%49) : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("cosine.194")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %51 = "ttnn.multiply"(%43, %50) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.262")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%43) <{force = false}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.262")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %52 = "ttnn.typecast"(%51) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.263")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%51) <{force = false}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.263")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %53 = "ttnn.slice_static"(%41) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 12 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("slice.250")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %54 = "ttnn.neg"(%53) : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("negate.251")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%53) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("negate.251")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %55 = "ttnn.reshape"(%54) <{shape = [12 : i32, 1 : i32, 64 : i32]}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("negate.251")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%54) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("negate.251")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %56 = "ttnn.slice_static"(%41) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 12 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("slice.249")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%41) <{force = false}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("slice.249")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %57 = "ttnn.reshape"(%56) <{shape = [12 : i32, 1 : i32, 64 : i32]}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("concatenate.252")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%56) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("concatenate.252")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %58 = "ttnn.concat"(%55, %57) <{dim = 2 : si32}> : (tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("concatenate.252")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%57) <{force = false}> : (tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("concatenate.252")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%55) <{force = false}> : (tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("concatenate.252")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %59 = "ttnn.typecast"(%58) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.253")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%58) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.253")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %60 = "ttnn.sin"(%49) : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("sine.168")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%49) <{force = false}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("sine.168")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %61 = "ttnn.multiply"(%59, %60) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.256")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%59) <{force = false}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.256")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %62 = "ttnn.typecast"(%61) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.257")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%61) <{force = false}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.257")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %63 = "ttnn.add"(%52, %62) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.266")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%62) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.266")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%52) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.266")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %64 = "ttnn.matmul"(%39, %14) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.180")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%14) <{force = false}> : (tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.180")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %65 = "ttnn.reshape"(%64) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32]}> : (tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("transpose.183")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%64) <{force = false}> : (tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("transpose.183")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %66 = "ttnn.typecast"(%65) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.200")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %67 = "ttnn.reshape"(%50) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.202")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%50) <{force = false}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.202")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %68 = "ttnn.multiply"(%66, %67) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.203")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%67) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.203")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%66) <{force = false}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.203")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %69 = "ttnn.typecast"(%68) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.204")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%68) <{force = false}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.204")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %70 = "ttnn.slice_static"(%65) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("slice.185")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %71 = "ttnn.neg"(%70) : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("negate.186")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%70) <{force = false}> : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("negate.186")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %72 = "ttnn.slice_static"(%65) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("slice.184")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%65) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("slice.184")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %73 = "ttnn.concat"(%71, %72) <{dim = 3 : si32}> : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("concatenate.187")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%72) <{force = false}> : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("concatenate.187")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%71) <{force = false}> : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("concatenate.187")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %74 = "ttnn.typecast"(%73) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.188")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%73) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.188")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %75 = "ttnn.reshape"(%60) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.190")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%60) <{force = false}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.190")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %76 = "ttnn.multiply"(%74, %75) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.191")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%75) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.191")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%74) <{force = false}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.191")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %77 = "ttnn.typecast"(%76) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.192")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%76) <{force = false}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.192")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %78 = "ttnn.add"(%69, %77) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.207")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%77) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.207")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%69) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.207")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %79 = "ttnn.typecast"(%arg9) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("scatter.223_workaround"("scatter.223"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.update_cache"(%23, %78, %79) <{batch_offset = 0 : i32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("scatter.223")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%79) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("scatter.223")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%78) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("scatter.223")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %80 = "ttnn.reshape"(%23) <{shape = [1 : i32, 4 : i32, 1 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.232")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%23) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.232")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %81 = "ttnn.repeat"(%80) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.232")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%80) <{force = false}> : (tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.232")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %82 = "ttnn.reshape"(%81) <{shape = [1 : i32, 12 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12288 + d1 * 1024 + d2, d3), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.233")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%81) <{force = false}> : (tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.233")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %83 = "ttnn.permute"(%82) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12288 + d1 * 1024 + d2, d3), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("transpose.234")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%82) <{force = false}> : (tensor<1x12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12288 + d1 * 1024 + d2, d3), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("transpose.234")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %84 = "ttnn.reshape"(%83) <{shape = [12 : i32, 128 : i32, 1024 : i32]}> : (tensor<1x12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.236")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%83) <{force = false}> : (tensor<1x12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.236")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %85 = "ttnn.matmul"(%63, %84) <{transpose_a = false, transpose_b = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.269")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%84) <{force = false}> : (tensor<12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.269")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%63) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.269")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %86 = "ttnn.typecast"(%85) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.271")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%85) <{force = false}> : (tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.271")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %87 = "ttnn.multiply"(%86, %4) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.273")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%86) <{force = false}> : (tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.273")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%4) <{force = false}> : (tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.273")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %88 = "ttnn.typecast"(%87) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.274")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%87) <{force = false}> : (tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.274")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %89 = "ttnn.reshape"(%88) <{shape = [1 : i32, 12 : i32, 1 : i32, 1024 : i32]}> : (tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.274")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%88) <{force = false}> : (tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.274")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %90 = "ttnn.reshape"(%20) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.278_tm0_tm1_tm0_tm1"("broadcast.278_tm0_tm1_tm0"("broadcast.278_tm0_tm1"("broadcast.278_tm0"("broadcast.278")))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%20) <{force = false}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.278_tm0_tm1_tm0_tm1"("broadcast.278_tm0_tm1_tm0"("broadcast.278_tm0_tm1"("broadcast.278_tm0"("broadcast.278")))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %91 = "ttnn.typecast"(%90) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("compare.129_workaround"("compare.129"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%90) <{force = false}> : (tensor<1x1x1x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("compare.129_workaround"("compare.129"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %92 = "ttnn.gt"(%13, %91) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("compare.129")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%91) <{force = false}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("compare.129")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("compare.129")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %93 = "ttnn.typecast"(%92) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("compare.129_workaround"("compare.129"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%92) <{force = false}> : (tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("compare.129_workaround"("compare.129"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %94 = "ttnn.typecast"(%93) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.130")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%93) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.130")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %95 = "ttnn.multiply"(%2, %94) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.137")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%94) <{force = false}> : (tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.137")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.137")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %96 = "ttnn.typecast"(%95) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.138")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%95) <{force = false}> : (tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.138")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %97 = "ttnn.add"(%89, %96) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.279")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%96) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.279")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%89) <{force = false}> : (tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.279")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %98 = "ttnn.typecast"(%97) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.280")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%97) <{force = false}> : (tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.280")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %99 = "ttnn.softmax"(%98) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("divide.297")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%98) <{force = false}> : (tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("divide.297")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %100 = "ttnn.typecast"(%99) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.298")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%99) <{force = false}> : (tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.298")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %101 = "ttnn.reshape"(%100) <{shape = [12 : i32, 1 : i32, 1024 : i32]}> : (tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.298")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%100) <{force = false}> : (tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.298")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %102 = "ttnn.matmul"(%39, %6) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.86")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%39) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.86")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%6) <{force = false}> : (tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.86")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %103 = "ttnn.reshape"(%102) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32]}> : (tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("transpose.89")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%102) <{force = false}> : (tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("transpose.89")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %104 = "ttnn.typecast"(%arg9) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("scatter.110_workaround"("scatter.110"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg9) <{force = false}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("scatter.110_workaround"("scatter.110"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.update_cache"(%21, %103, %104) <{batch_offset = 0 : i32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("scatter.110")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%104) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("scatter.110")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%103) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("scatter.110")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %105 = "ttnn.reshape"(%21) <{shape = [1 : i32, 4 : i32, 1 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.119")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%21) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.119")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %106 = "ttnn.repeat"(%105) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.119")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%105) <{force = false}> : (tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.119")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %107 = "ttnn.reshape"(%106) <{shape = [12 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 1024 + d1, d2), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.122")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%106) <{force = false}> : (tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.122")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %108 = "ttnn.matmul"(%101, %107) <{transpose_a = false, transpose_b = false}> : (tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 1024 + d1, d2), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.301")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%107) <{force = false}> : (tensor<12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 1024 + d1, d2), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.301")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%101) <{force = false}> : (tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.301")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %109 = "ttnn.reshape"(%108) <{shape = [1 : i32, 1536 : i32]}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.305")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%108) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.305")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %110 = "ttnn.matmul"(%109, %10) <{transpose_a = false, transpose_b = true}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.306")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%109) <{force = false}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.306")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%10) <{force = false}> : (tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.306")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %111 = "ttnn.reshape"(%110) <{shape = [1 : i32, 1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.306_reduceScatter_reshape_to_4d"("dot.306_reduceScatter"("dot.306")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%110) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.306_reduceScatter_reshape_to_4d"("dot.306_reduceScatter"("dot.306")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %112 = "ttnn.reduce_scatter"(%111, %17) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 3 : si32}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.306_reduceScatter_reduce_scatter_4d"("dot.306_reduceScatter"("dot.306")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%111) <{force = false}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.306_reduceScatter_reduce_scatter_4d"("dot.306_reduceScatter"("dot.306")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %113 = "ttnn.all_gather"(%112, %17) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.306_all_gather_4d"("dot.306"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%112) <{force = false}> : (tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.306_all_gather_4d"("dot.306"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %114 = "ttnn.reshape"(%113) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.306")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%113) <{force = false}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.306")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %115 = "ttnn.add"(%29, %114) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.310")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%114) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.310")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%29) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.310")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %116 = "ttnn.typecast"(%115) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.311")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %117 = "ttnn.reshape"(%116) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.311")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %118 = "ttnn.pow"(%117, %15) : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("power.313")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%117) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("power.313")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %119 = "ttnn.sum"(%118) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reduce.320")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%118) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reduce.320")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %120 = "ttnn.multiply"(%119, %18) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.329")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%119) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.329")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %121 = "ttnn.add"(%120, %11#1) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.334")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%120) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.334")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%11#1) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.334")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %122 = "ttnn.rsqrt"(%121) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("rsqrt.335")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%121) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("rsqrt.335")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %123 = "ttnn.multiply"(%116, %122) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.338")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%122) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.338")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%116) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.338")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %124 = "ttnn.multiply"(%7, %123) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.347")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%123) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.347")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.347")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %125 = "ttnn.typecast"(%124) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.348")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%124) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.348")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %126 = "ttnn.matmul"(%125, %5) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.359")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%5) <{force = false}> : (tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.359")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %127 = "ttnn.typecast"(%126) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.363")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %128 = "ttnn.sigmoid"(%126) : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("logistic.361")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%126) <{force = false}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("logistic.361")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %129 = "ttnn.typecast"(%128) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.362")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%128) <{force = false}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.362")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %130 = "ttnn.multiply"(%127, %129) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.364")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%129) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.364")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%127) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.364")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %131 = "ttnn.matmul"(%125, %16) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.350")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%125) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.350")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%16) <{force = false}> : (tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.350")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %132 = "ttnn.typecast"(%131) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.352")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%131) <{force = false}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.352")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %133 = "ttnn.multiply"(%130, %132) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.367")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%132) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.367")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%130) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.367")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %134 = "ttnn.typecast"(%133) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.368")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%133) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.368")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %135 = "ttnn.matmul"(%134, %9) <{transpose_a = false, transpose_b = true}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.370")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%134) <{force = false}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.370")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%9) <{force = false}> : (tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.370")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %136 = "ttnn.reshape"(%135) <{shape = [1 : i32, 1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.370_reduceScatter_reshape_to_4d"("dot.370_reduceScatter"("dot.370")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%135) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.370_reduceScatter_reshape_to_4d"("dot.370_reduceScatter"("dot.370")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %137 = "ttnn.reduce_scatter"(%136, %17) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 3 : si32}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.370_reduceScatter_reduce_scatter_4d"("dot.370_reduceScatter"("dot.370")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%136) <{force = false}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.370_reduceScatter_reduce_scatter_4d"("dot.370_reduceScatter"("dot.370")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %138 = "ttnn.all_gather"(%137, %17) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.370_all_gather_4d"("dot.370"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%137) <{force = false}> : (tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.370_all_gather_4d"("dot.370"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %139 = "ttnn.reshape"(%138) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.370")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%138) <{force = false}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.370")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %140 = "ttnn.add"(%115, %139) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.374")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%139) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.374")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%115) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.374")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %141 = "ttnn.typecast"(%140) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.375")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%140) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.375")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %142 = "ttnn.reshape"(%141) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.375")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %143 = "ttnn.pow"(%142, %15) : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("power.377")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%142) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("power.377")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("power.377")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %144 = "ttnn.sum"(%143) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reduce.384")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%143) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reduce.384")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %145 = "ttnn.multiply"(%144, %18) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.393")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%144) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.393")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%18) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.393")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %146 = "ttnn.add"(%145, %11#2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.398")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%145) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.398")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%11#2) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.398")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %147 = "ttnn.rsqrt"(%146) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("rsqrt.399")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%146) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("rsqrt.399")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %148 = "ttnn.multiply"(%141, %147) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.402")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%147) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.402")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%141) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.402")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %149 = "ttnn.multiply"(%12, %148) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.411")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%148) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.411")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.411")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %150 = "ttnn.typecast"(%149) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.412")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%149) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.412")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %151 = "ttnn.matmul"(%150, %0) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.417")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%150) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.417")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%0) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.417")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %152 = "ttnn.reshape"(%151) <{shape = [1 : i32, 1 : i32, 128256 : i32]}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.418")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %153 = "ttnn.to_layout"(%151) <{layout = #ttnn.layout<row_major>}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%151) <{force = false}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %154 = "ttnn.from_device"(%153) : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%153) <{force = false}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %155 = "ttnn.mesh_shard"(%154, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%154) <{force = false}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %156 = "ttnn.to_layout"(%152) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%152) <{force = false}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %157 = "ttnn.from_device"(%156) : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%156) <{force = false}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %158 = "ttnn.mesh_shard"(%157, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%157) <{force = false}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main
2025-09-15 15:42:11.942 (  80.268s) [        F2FFD640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:11.942 (  80.268s) [        F2FFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:11.942 (  80.268s) [        F2FFD640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:11.942 (  80.268s) [        F2FFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:11.942 (  80.268s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:11.942 (  80.268s) [        F2FFD640]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:11.942 (  80.268s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:11.942 (  80.268s) [        F2FFD640]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:11.942 (  80.268s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:11.942 (  80.268s) [        F2FFD640]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:11.942 (  80.268s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:11.942 (  80.268s) [        F2FFD640]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:11.942 (  80.268s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:11.942 (  80.268s) [        F2FFD640]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:11.942 (  80.268s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:11.942 (  80.268s) [        F2FFD640]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:11.942 (  80.268s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:11.942 (  80.268s) [        F2FFD640]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:11.942 (  80.268s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:11.942 (  80.268s) [        F2FFD640]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:11.942 (  80.268s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:11.942 (  80.268s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:11.942 (  80.268s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:42:11.942 (  80.268s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:11.942 (  80.269s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:11.942 (  80.269s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:42:11.942 (  80.269s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User is requesting to copy the data from a runtime tensor with data type: Int32 into buffer with expected data type: Int64, the values will be casted, this may impact the throughput and the integrity of the data.
2025-09-15 15:42:11.943 (  80.269s) [        12FFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:11.943 (  80.270s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:11.944 (  80.270s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:11.944 (  80.270s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:42:11.944 (  80.270s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:11.944 (  80.270s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:11.944 (  80.270s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:42:11.944 (  80.270s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User is requesting to copy the data from a runtime tensor with data type: Int32 into buffer with expected data type: Int64, the values will be casted, this may impact the throughput and the integrity of the data.
2025-09-15 15:42:11.944 (  80.270s) [        12FFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:11.944 (  80.270s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:11.944 (  80.271s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:11.945 (  80.271s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:11.950 (  80.276s) [        F073E000]     client_instance.cc:428      1| ClientInstance::PJRT_Client_Compile
2025-09-15 15:42:11.950 (  80.276s) [        F073E000]      module_builder.cc:103      1| ModuleBuilder::buildModule
2025-09-15 15:42:11.950 (  80.277s) [        F073E000]      module_builder.cc:165      1| VHLO Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) {
    %0 = "vhlo.custom_call_v1"(%arg0) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"xla.sdy.FuncResultSharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<true>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {}, {}, {}]>]>">}>} : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>
    "vhlo.return_v1"(%0) : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>]>, res_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, sym_visibility = #vhlo.string_v1<"">}
}
2025-09-15 15:42:11.952 (  80.278s) [        F073E000]      module_builder.cc:185      1| Is using shardy? true
2025-09-15 15:42:11.954 (  80.280s) [        F073E000]      module_builder.cc:203      1| SHLO Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}) -> (tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>}) {
    return %arg0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:11.954 (  80.281s) [        F073E000]      module_builder.cc:212      1| SHLO Module after frontend StableHLO pipeline:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}) -> (tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>}) {
    return %arg0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:11.958 (  80.285s) [        F073E000]      module_builder.cc:486      1| SHLO Module after compiler StableHLO pipeline:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = sdy.manual_computation(%arg0) in_shardings=[<@mesh, [{}, {"_axis_0"}, {}, {}]>] out_shardings=[<@mesh, [{}, {}, {}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg1: tensor<1x4x1024x128xbf16>) {
      %1 = "stablehlo.all_gather"(%arg1) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> : (tensor<1x4x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
      sdy.return %1 : tensor<1x8x1024x128xbf16>
    } : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    return %0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:11.961 (  80.287s) [        F073E000]      module_builder.cc:510      1| TTIR Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = ttir.empty() : tensor<1x4x1024x128xbf16>
    %1 = "ttir.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16>, tensor<1x4x1024x128xbf16>) -> tensor<1x4x1024x128xbf16>
    %2 = ttir.empty() : tensor<1x8x1024x128xbf16>
    %3 = "ttir.all_gather"(%1, %2) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<1x4x1024x128xbf16>, tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %4 = ttir.empty() : tensor<1x8x1024x128xbf16>
    %5 = "ttir.mesh_shard"(%3, %4) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16>, tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    return %5 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:11.961 (  80.287s) [        F073E000]      module_builder.cc:561   WARN| `mhlo.num_partitions` attribute not found, assuming default number of partitions: 1
2025-09-15 15:42:11.961 (  80.287s) [        F073E000]      module_builder.cc:575   WARN| `mhlo.num_replicas` attribute not found, assuming default number of replicas: 1
2025-09-15 15:42:11.961 (  80.287s) [        F073E000]      module_builder.cc:585   WARN| Num replicas and num partitions are not set, inferring the number of devices from mesh shape
2025-09-15 15:42:11.991 (  80.317s) [        F073E000]      module_builder.cc:630      1| TTNN Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073185088, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073193408, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x16, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 2)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 2, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %2 = "ttnn.all_gather"(%1, %0) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.mesh_shard"(%2, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %3 : tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
    }
  }
}
2025-09-15 15:42:12.002 (  80.328s) [        F073E000]loaded_executable_insta:441      1| LoadedExecutableInstance::PJRT_LoadedExecutable_GetExecutable
2025-09-15 15:42:12.002 (  80.328s) [        F073E000]loaded_executable_insta:460      1| LoadedExecutableInstance::PJRT_LoadedExecutable_AddressableDevices
2025-09-15 15:42:12.002 (  80.329s) [        F073E000]              stubs.inc:76    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2025-09-15 15:42:12.002 (  80.329s) [        F073E000]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-09-15 15:42:12.002 (  80.329s) [        F073E000]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-09-15 15:42:12.002 (  80.329s) [        F073E000]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-09-15 15:42:12.002 (  80.329s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:12.002 (  80.329s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:12.005 (  80.332s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:12.006 (  80.332s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:12.007 (  80.334s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:12.007 (  80.334s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:12.007 (  80.334s) [        F073E000]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.007 (  80.334s) [        F073E000]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.007 (  80.334s) [        F073E000]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.007 (  80.334s) [        F073E000]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.007 (  80.334s) [        F073E000] executable_instance.cc:138      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-09-15 15:42:12.007 (  80.334s) [        F073E000]loaded_executable_insta:497      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-09-15 15:42:12.007 (  80.334s) [        F073E000]loaded_executable_insta:81       1| LoadedExecutableInstance::Execute
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Device grid size = { 8, 8 }
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.all_gather"(%1, %0) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("p0.1")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("p0.1")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.mesh_shard"(%2, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main
2025-09-15 15:42:12.289 (  80.615s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:12.289 (  80.615s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:12.289 (  80.615s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:12.289 (  80.615s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:12.289 (  80.615s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:12.289 (  80.615s) [        F073E000]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:12.289 (  80.615s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:12.289 (  80.615s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:12.289 (  80.615s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:12.289 (  80.615s) [        F073E000]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:12.289 (  80.615s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:12.289 (  80.615s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:12.289 (  80.615s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:12.289 (  80.615s) [        F073E000] executable_instance.cc:58       1| ExecutableInstance::PJRT_Executable_Destroy
2025-09-15 15:42:12.289 (  80.615s) [        F073E000]loaded_executable_insta:432      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Destroy
2025-09-15 15:42:12.289 (  80.615s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:42:12.289 (  80.615s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:12.289 (  80.615s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:12.289 (  80.616s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:42:12.290 (  80.616s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:12.290 (  80.616s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:12.290 (  80.616s) [         AFFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:12.293 (  80.620s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.293 (  80.620s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.294 (  80.620s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:12.299 (  80.626s) [        F073E000]     client_instance.cc:428      1| ClientInstance::PJRT_Client_Compile
2025-09-15 15:42:12.299 (  80.626s) [        F073E000]      module_builder.cc:103      1| ModuleBuilder::buildModule
2025-09-15 15:42:12.300 (  80.626s) [        F073E000]      module_builder.cc:165      1| VHLO Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) {
    %0 = "vhlo.custom_call_v1"(%arg0) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"xla.sdy.FuncResultSharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<true>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {}, {}, {}]>]>">}>} : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>
    "vhlo.return_v1"(%0) : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>]>, res_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, sym_visibility = #vhlo.string_v1<"">}
}
2025-09-15 15:42:12.301 (  80.628s) [        F073E000]      module_builder.cc:185      1| Is using shardy? true
2025-09-15 15:42:12.303 (  80.630s) [        F073E000]      module_builder.cc:203      1| SHLO Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}) -> (tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>}) {
    return %arg0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:12.304 (  80.630s) [        F073E000]      module_builder.cc:212      1| SHLO Module after frontend StableHLO pipeline:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}) -> (tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>}) {
    return %arg0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:12.308 (  80.634s) [        F073E000]      module_builder.cc:486      1| SHLO Module after compiler StableHLO pipeline:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = sdy.manual_computation(%arg0) in_shardings=[<@mesh, [{}, {"_axis_0"}, {}, {}]>] out_shardings=[<@mesh, [{}, {}, {}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg1: tensor<1x4x1024x128xbf16>) {
      %1 = "stablehlo.all_gather"(%arg1) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> : (tensor<1x4x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
      sdy.return %1 : tensor<1x8x1024x128xbf16>
    } : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    return %0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:12.310 (  80.636s) [        F073E000]      module_builder.cc:510      1| TTIR Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = ttir.empty() : tensor<1x4x1024x128xbf16>
    %1 = "ttir.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16>, tensor<1x4x1024x128xbf16>) -> tensor<1x4x1024x128xbf16>
    %2 = ttir.empty() : tensor<1x8x1024x128xbf16>
    %3 = "ttir.all_gather"(%1, %2) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<1x4x1024x128xbf16>, tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %4 = ttir.empty() : tensor<1x8x1024x128xbf16>
    %5 = "ttir.mesh_shard"(%3, %4) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16>, tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    return %5 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:12.310 (  80.637s) [        F073E000]      module_builder.cc:561   WARN| `mhlo.num_partitions` attribute not found, assuming default number of partitions: 1
2025-09-15 15:42:12.310 (  80.637s) [        F073E000]      module_builder.cc:575   WARN| `mhlo.num_replicas` attribute not found, assuming default number of replicas: 1
2025-09-15 15:42:12.310 (  80.637s) [        F073E000]      module_builder.cc:585   WARN| Num replicas and num partitions are not set, inferring the number of devices from mesh shape
2025-09-15 15:42:12.341 (  80.667s) [        F073E000]      module_builder.cc:630      1| TTNN Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073185088, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073193408, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x16, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 2)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 2, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %2 = "ttnn.all_gather"(%1, %0) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.mesh_shard"(%2, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %3 : tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
    }
  }
}
2025-09-15 15:42:12.352 (  80.679s) [        F073E000]loaded_executable_insta:441      1| LoadedExecutableInstance::PJRT_LoadedExecutable_GetExecutable
2025-09-15 15:42:12.352 (  80.679s) [        F073E000]loaded_executable_insta:460      1| LoadedExecutableInstance::PJRT_LoadedExecutable_AddressableDevices
2025-09-15 15:42:12.353 (  80.679s) [        F073E000]              stubs.inc:76    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2025-09-15 15:42:12.353 (  80.679s) [        F073E000]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-09-15 15:42:12.353 (  80.679s) [        F073E000]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-09-15 15:42:12.353 (  80.679s) [        F073E000]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-09-15 15:42:12.353 (  80.679s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:12.353 (  80.679s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:12.356 (  80.682s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:12.356 (  80.683s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:12.358 (  80.685s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:12.358 (  80.685s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:12.358 (  80.685s) [        F073E000]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.358 (  80.685s) [        F073E000]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.358 (  80.685s) [        F073E000]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.358 (  80.685s) [        F073E000]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.358 (  80.685s) [        F073E000] executable_instance.cc:138      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-09-15 15:42:12.358 (  80.685s) [        F073E000]loaded_executable_insta:497      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-09-15 15:42:12.358 (  80.685s) [        F073E000]loaded_executable_insta:81       1| LoadedExecutableInstance::Execute
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Device grid size = { 8, 8 }
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.all_gather"(%1, %0) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("p0.1")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("p0.1")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.mesh_shard"(%2, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main
2025-09-15 15:42:12.726 (  81.052s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:12.726 (  81.052s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:12.726 (  81.052s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:12.726 (  81.052s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:12.726 (  81.052s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:12.726 (  81.052s) [        F073E000]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:12.726 (  81.052s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:12.726 (  81.052s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:12.726 (  81.052s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:12.726 (  81.052s) [        F073E000]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:12.726 (  81.052s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:12.726 (  81.052s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:12.726 (  81.052s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:12.726 (  81.052s) [        F073E000] executable_instance.cc:58       1| ExecutableInstance::PJRT_Executable_Destroy
2025-09-15 15:42:12.726 (  81.052s) [        F073E000]loaded_executable_insta:432      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Destroy
2025-09-15 15:42:12.726 (  81.053s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:42:12.726 (  81.053s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:12.726 (  81.053s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:12.727 (  81.053s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:42:12.727 (  81.053s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:12.727 (  81.053s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:12.727 (  81.054s) [         BFFF640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:12.731 (  81.057s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:12.731 (  81.058s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:12.731 (  81.058s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:12.732 (  81.058s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:12.732 (  81.058s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.732 (  81.058s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.732 (  81.058s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:42:12.732 (  81.058s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:12.732 (  81.058s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:12.732 (  81.058s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:42:12.732 (  81.059s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:12.732 (  81.059s) [        10FF9640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:12.733 (  81.059s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.733 (  81.060s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.733 (  81.060s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:42:12.733 (  81.060s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:12.733 (  81.060s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:42:12.733 (  81.060s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:12.733 (  81.060s) [         BFFF640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:12.735 (  81.062s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:12.735 (  81.062s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:12.735 (  81.062s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.735 (  81.062s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.735 (  81.062s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:42:12.735 (  81.062s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:12.735 (  81.062s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:42:12.736 (  81.062s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User is requesting to copy the data from a runtime tensor with data type: Int32 into buffer with expected data type: Int64, the values will be casted, this may impact the throughput and the integrity of the data.
2025-09-15 15:42:12.736 (  81.062s) [        127FC640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:12.736 (  81.062s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:12.736 (  81.063s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first

Args after sync (tensor([[279]], device='xla:0'), tensor([8], device='xla:0'), tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='xla:0',
       dtype=torch.bfloat16), tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='xla:0',
       dtype=torch.bfloat16))
CausalLMOutputWithPast(loss=None, logits=tensor([[[ 2.5781,  1.0938, -0.8008,  ..., -3.5312, -3.5469, -3.5469]]],
       device='xla:0', dtype=torch.bfloat16), past_key_values=<transformers.cache_utils.StaticCache object at 0x7f24ac6c3e50>, hidden_states=None, attentions=None)
Generated token:  the
readable graph module
class GraphModule(torch.nn.Module):
    def forward(self, args_0, args_1, args_2, args_3):
        args_0: "i64[1, 1]"; args_1: "i64[1]"; args_2: "bf16[1, 8, 1024, 128]"; args_3: "bf16[1, 8, 1024, 128]"; 
    
        args_0, args_1, args_2, args_3, = fx_pytree.tree_flatten_spec(([args_0, args_1, args_2, args_3], {}), self._in_spec)
        # No stacktrace found for following nodes
        l__self___model_layers__modules__0___input_layernorm_weight: "bf16[3072]" = self.L__self___model_layers__modules__0___input_layernorm_weight
        mark_argument_attributes = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___input_layernorm_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___input_layernorm_weight');  l__self___model_layers__modules__0___input_layernorm_weight = None
        l__self___model_layers__modules__0___post_attention_layernorm_weight: "bf16[3072]" = self.L__self___model_layers__modules__0___post_attention_layernorm_weight
        mark_argument_attributes_1 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___post_attention_layernorm_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___post_attention_layernorm_weight');  l__self___model_layers__modules__0___post_attention_layernorm_weight = None
        l__self___model_norm_weight: "bf16[3072]" = self.L__self___model_norm_weight
        mark_argument_attributes_2 = torch.ops.tt.mark_argument_attributes(l__self___model_norm_weight, argument_type = 'parameter', name = 'l__self___model_norm_weight');  l__self___model_norm_weight = None
        l__self___model_embed_tokens_weight: "bf16[128256, 3072]" = self.L__self___model_embed_tokens.weight
        mark_argument_attributes_3 = torch.ops.tt.mark_argument_attributes(l__self___model_embed_tokens_weight, argument_type = 'parameter', name = 'l__self___model_embed_tokens_weight');  l__self___model_embed_tokens_weight = None
        l__self___model_layers__modules__0___self_attn_q_proj_weight: "bf16[3072, 3072]" = self.L__self___model_layers__modules__0___self_attn_q_proj.weight
        mark_argument_attributes_4 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___self_attn_q_proj_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___self_attn_q_proj_weight');  l__self___model_layers__modules__0___self_attn_q_proj_weight = None
        l__self___model_layers__modules__0___self_attn_k_proj_weight: "bf16[1024, 3072]" = self.L__self___model_layers__modules__0___self_attn_k_proj.weight
        mark_argument_attributes_5 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___self_attn_k_proj_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___self_attn_k_proj_weight');  l__self___model_layers__modules__0___self_attn_k_proj_weight = None
        l__self___model_layers__modules__0___self_attn_v_proj_weight: "bf16[1024, 3072]" = self.L__self___model_layers__modules__0___self_attn_v_proj.weight
        mark_argument_attributes_6 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___self_attn_v_proj_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___self_attn_v_proj_weight');  l__self___model_layers__modules__0___self_attn_v_proj_weight = None
        l__self___model_layers__modules__0___self_attn_o_proj_weight: "bf16[3072, 3072]" = self.L__self___model_layers__modules__0___self_attn_o_proj.weight
        mark_argument_attributes_7 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___self_attn_o_proj_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___self_attn_o_proj_weight');  l__self___model_layers__modules__0___self_attn_o_proj_weight = None
        l__self___model_layers__modules__0___mlp_gate_proj_weight: "bf16[8192, 3072]" = self.L__self___model_layers__modules__0___mlp_gate_proj.weight
        mark_argument_attributes_8 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___mlp_gate_proj_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___mlp_gate_proj_weight');  l__self___model_layers__modules__0___mlp_gate_proj_weight = None
        l__self___model_layers__modules__0___mlp_up_proj_weight: "bf16[8192, 3072]" = self.L__self___model_layers__modules__0___mlp_up_proj.weight
        mark_argument_attributes_9 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___mlp_up_proj_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___mlp_up_proj_weight');  l__self___model_layers__modules__0___mlp_up_proj_weight = None
        l__self___model_layers__modules__0___mlp_down_proj_weight: "bf16[3072, 8192]" = self.L__self___model_layers__modules__0___mlp_down_proj.weight
        mark_argument_attributes_10 = torch.ops.tt.mark_argument_attributes(l__self___model_layers__modules__0___mlp_down_proj_weight, argument_type = 'parameter', name = 'l__self___model_layers__modules__0___mlp_down_proj_weight');  l__self___model_layers__modules__0___mlp_down_proj_weight = None
        l__self___lm_head_weight: "bf16[128256, 3072]" = self.L__self___lm_head.weight
        mark_argument_attributes_11 = torch.ops.tt.mark_argument_attributes(l__self___lm_head_weight, argument_type = 'parameter', name = 'l__self___lm_head_weight');  l__self___lm_head_weight = None
        l__self___model_rotary_emb_inv_freq: "f32[64]" = self.L__self___model_rotary_emb_inv_freq
        mark_argument_attributes_12 = torch.ops.tt.mark_argument_attributes(l__self___model_rotary_emb_inv_freq, argument_type = 'input', name = 'l__self___model_rotary_emb_inv_freq');  l__self___model_rotary_emb_inv_freq = None
        mark_argument_attributes_13 = torch.ops.tt.mark_argument_attributes(args_0, argument_type = 'input', name = 'args_0');  args_0 = None
        mark_argument_attributes_14 = torch.ops.tt.mark_argument_attributes(args_1, argument_type = 'input', name = 'args_1');  args_1 = None
        mark_argument_attributes_15 = torch.ops.tt.mark_argument_attributes(args_2, argument_type = 'input', name = 'args_2');  args_2 = None
        mark_argument_attributes_16 = torch.ops.tt.mark_argument_attributes(args_3, argument_type = 'input', name = 'args_3');  args_3 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:422 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        embedding: "bf16[1, 1, 3072]" = torch.ops.aten.embedding.default(mark_argument_attributes_3, mark_argument_attributes_13);  mark_argument_attributes_3 = mark_argument_attributes_13 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:434 in forward, code: position_ids = cache_position.unsqueeze(0)
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(mark_argument_attributes_14, 0)
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:586 in _prepare_4d_causal_attention_mask_with_cache_position, code: causal_mask = torch.full(
        empty_strided: "bf16[1, 1024]" = torch.ops.aten.empty_strided.default([1, 1024], [1024, 1], dtype = torch.bfloat16, layout = torch.strided, device = device(type='xla', index=0), pin_memory = False)
        full_like: "bf16[1, 1024]" = torch.ops.aten.full_like.default(empty_strided, -3.3895313892515355e+38, pin_memory = False);  empty_strided = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:591 in _prepare_4d_causal_attention_mask_with_cache_position, code: causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)
        arange: "i64[1024]" = torch.ops.aten.arange.start_step(0, 1024, layout = torch.strided, device = device(type='xla', index=0), pin_memory = False)
        view: "i64[1, 1]" = torch.ops.aten.view.default(mark_argument_attributes_14, [-1, 1])
        gt: "b8[1, 1024]" = torch.ops.aten.gt.Tensor(arange, view);  arange = view = None
        mul: "bf16[1, 1024]" = torch.ops.aten.mul.Tensor(full_like, gt);  full_like = gt = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:103 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        unsqueeze_3: "f32[1, 64]" = torch.ops.aten.unsqueeze.default(mark_argument_attributes_12, 0);  mark_argument_attributes_12 = None
        slice_3: "f32[1, 64]" = torch.ops.aten.slice.Tensor(unsqueeze_3, 1, 0, 9223372036854775807);  unsqueeze_3 = None
        unsqueeze_4: "f32[1, 64, 1]" = torch.ops.aten.unsqueeze.default(slice_3, 2);  slice_3 = None
        convert_element_type: "f32[1, 64, 1]" = torch.ops.prims.convert_element_type.default(unsqueeze_4, torch.float32);  convert_element_type = None
        expand_1: "f32[1, 64, 1]" = torch.ops.aten.expand.default(unsqueeze_4, [1, -1, 1]);  unsqueeze_4 = None
        convert_element_type_1: "f32[1, 64, 1]" = torch.ops.prims.convert_element_type.default(expand_1, torch.float32);  convert_element_type_1 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:104 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        slice_4: "i64[1, 1]" = torch.ops.aten.slice.Tensor(unsqueeze, 0, 0, 9223372036854775807);  unsqueeze = None
        unsqueeze_5: "i64[1, 1, 1]" = torch.ops.aten.unsqueeze.default(slice_4, 1);  slice_4 = None
        slice_5: "i64[1, 1, 1]" = torch.ops.aten.slice.Tensor(unsqueeze_5, 2, 0, 9223372036854775807);  unsqueeze_5 = None
        convert_element_type_2: "f32[1, 1, 1]" = torch.ops.prims.convert_element_type.default(slice_5, torch.float32);  slice_5 = None
        
         # File: <eval_with_key>.32:5 in forward, code: to_3 = torch.ops.aten.to.dtype(to_1, torch.float32);  to_1 = None
        convert_element_type_3: "f32[1, 64, 1]" = torch.ops.prims.convert_element_type.default(expand_1, torch.float32);  convert_element_type_3 = None
        
         # File: <eval_with_key>.32:6 in forward, code: to_4 = torch.ops.aten.to.dtype(to_2, torch.float32);  to_2 = None
        convert_element_type_4: "f32[1, 1, 1]" = torch.ops.prims.convert_element_type.default(convert_element_type_2, torch.float32);  convert_element_type_4 = None
        
         # File: <eval_with_key>.32:7 in forward, code: matmul = torch.ops.aten.matmul.default(to_3, to_4);  to_3 = to_4 = None
        expand_2: "f32[1, 64, 1]" = torch.ops.aten.expand.default(expand_1, [1, 64, 1]);  expand_1 = None
        view_1: "f32[1, 64, 1]" = torch.ops.aten.view.default(expand_2, [1, 64, 1]);  expand_2 = None
        expand_3: "f32[1, 1, 1]" = torch.ops.aten.expand.default(convert_element_type_2, [1, 1, 1]);  convert_element_type_2 = None
        view_2: "f32[1, 1, 1]" = torch.ops.aten.view.default(expand_3, [1, 1, 1]);  expand_3 = None
        bmm: "f32[1, 64, 1]" = torch.ops.aten.bmm.default(view_1, view_2);  view_1 = view_2 = None
        view_3: "f32[1, 64, 1]" = torch.ops.aten.view.default(bmm, [1, 64, 1]);  bmm = None
        
         # File: <eval_with_key>.32:8 in forward, code: transpose = torch.ops.aten.transpose.int(matmul, 1, 2);  matmul = None
        permute: "f32[1, 1, 64]" = torch.ops.aten.permute.default(view_3, [0, 2, 1]);  view_3 = None
        
         # File: <eval_with_key>.32:9 in forward, code: cat = torch.ops.aten.cat.default([transpose, transpose], -1);  transpose = None
        cat: "f32[1, 1, 128]" = torch.ops.aten.cat.default([permute, permute], -1);  permute = None
        
         # File: <eval_with_key>.32:10 in forward, code: cos = torch.ops.aten.cos.default(cat)
        cos: "f32[1, 1, 128]" = torch.ops.aten.cos.default(cat)
        
         # File: <eval_with_key>.32:11 in forward, code: mul = torch.ops.aten.mul.Tensor(cos, 1.0);  cos = None
        mul_1: "f32[1, 1, 128]" = torch.ops.aten.mul.Tensor(cos, 1.0);  cos = None
        
         # File: <eval_with_key>.32:12 in forward, code: sin = torch.ops.aten.sin.default(cat);  cat = None
        sin: "f32[1, 1, 128]" = torch.ops.aten.sin.default(cat);  cat = None
        
         # File: <eval_with_key>.32:13 in forward, code: mul_1 = torch.ops.aten.mul.Tensor(sin, 1.0);  sin = None
        mul_2: "f32[1, 1, 128]" = torch.ops.aten.mul.Tensor(sin, 1.0);  sin = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        convert_element_type_5: "bf16[1, 1, 128]" = torch.ops.prims.convert_element_type.default(mul_1, torch.bfloat16);  mul_1 = None
        convert_element_type_6: "bf16[1, 1, 128]" = torch.ops.prims.convert_element_type.default(mul_2, torch.bfloat16);  mul_2 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:70 in forward, code: hidden_states = hidden_states.to(torch.float32)
        convert_element_type_7: "f32[1, 1, 3072]" = torch.ops.prims.convert_element_type.default(embedding, torch.float32)
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:71 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 1, 3072]" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_7, 2)
        mean: "f32[1, 1, 1]" = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:72 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 1, 1]" = torch.ops.aten.add.Tensor(mean, 1e-05);  mean = None
        rsqrt: "f32[1, 1, 1]" = torch.ops.aten.rsqrt.default(add);  add = None
        mul_3: "f32[1, 1, 3072]" = torch.ops.aten.mul.Tensor(convert_element_type_7, rsqrt);  convert_element_type_7 = rsqrt = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:73 in forward, code: return self.weight * hidden_states.to(input_dtype)
        convert_element_type_8: "bf16[1, 1, 3072]" = torch.ops.prims.convert_element_type.default(mul_3, torch.bfloat16);  mul_3 = None
        mul_4: "bf16[1, 1, 3072]" = torch.ops.aten.mul.Tensor(mark_argument_attributes, convert_element_type_8);  mark_argument_attributes = convert_element_type_8 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:242 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_1: "bf16[3072, 3072]" = torch.ops.aten.permute.default(mark_argument_attributes_4, [1, 0]);  mark_argument_attributes_4 = None
        view_4: "bf16[1, 3072]" = torch.ops.aten.view.default(mul_4, [1, 3072])
        mm: "bf16[1, 3072]" = torch.ops.aten.mm.default(view_4, permute_1);  view_4 = permute_1 = None
        view_5: "bf16[1, 1, 3072]" = torch.ops.aten.view.default(mm, [1, 1, 3072]);  mm = None
        view_6: "bf16[1, 1, 24, 128]" = torch.ops.aten.view.default(view_5, [1, 1, -1, 128]);  view_5 = None
        permute_2: "bf16[1, 24, 1, 128]" = torch.ops.aten.permute.default(view_6, [0, 2, 1, 3]);  view_6 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:243 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_3: "bf16[3072, 1024]" = torch.ops.aten.permute.default(mark_argument_attributes_5, [1, 0]);  mark_argument_attributes_5 = None
        view_7: "bf16[1, 3072]" = torch.ops.aten.view.default(mul_4, [1, 3072])
        mm_1: "bf16[1, 1024]" = torch.ops.aten.mm.default(view_7, permute_3);  view_7 = permute_3 = None
        view_8: "bf16[1, 1, 1024]" = torch.ops.aten.view.default(mm_1, [1, 1, 1024]);  mm_1 = None
        view_9: "bf16[1, 1, 8, 128]" = torch.ops.aten.view.default(view_8, [1, 1, -1, 128]);  view_8 = None
        permute_4: "bf16[1, 8, 1, 128]" = torch.ops.aten.permute.default(view_9, [0, 2, 1, 3]);  view_9 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:244 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_5: "bf16[3072, 1024]" = torch.ops.aten.permute.default(mark_argument_attributes_6, [1, 0]);  mark_argument_attributes_6 = None
        view_10: "bf16[1, 3072]" = torch.ops.aten.view.default(mul_4, [1, 3072]);  mul_4 = None
        mm_2: "bf16[1, 1024]" = torch.ops.aten.mm.default(view_10, permute_5);  view_10 = permute_5 = None
        view_11: "bf16[1, 1, 1024]" = torch.ops.aten.view.default(mm_2, [1, 1, 1024]);  mm_2 = None
        view_12: "bf16[1, 1, 8, 128]" = torch.ops.aten.view.default(view_11, [1, 1, -1, 128]);  view_11 = None
        permute_6: "bf16[1, 8, 1, 128]" = torch.ops.aten.permute.default(view_12, [0, 2, 1, 3]);  view_12 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:143 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_6: "bf16[1, 1, 1, 128]" = torch.ops.aten.unsqueeze.default(convert_element_type_5, 1);  convert_element_type_5 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:144 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_7: "bf16[1, 1, 1, 128]" = torch.ops.aten.unsqueeze.default(convert_element_type_6, 1);  convert_element_type_6 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:145 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 24, 1, 128]" = torch.ops.aten.mul.Tensor(permute_2, unsqueeze_6)
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_6: "bf16[1, 24, 1, 64]" = torch.ops.aten.slice.Tensor(permute_2, 3, 0, 64)
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_7: "bf16[1, 24, 1, 64]" = torch.ops.aten.slice.Tensor(permute_2, 3, 64, 9223372036854775807);  permute_2 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 24, 1, 64]" = torch.ops.aten.neg.default(slice_7);  slice_7 = None
        cat_1: "bf16[1, 24, 1, 128]" = torch.ops.aten.cat.default([neg, slice_6], -1);  neg = slice_6 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:145 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_6: "bf16[1, 24, 1, 128]" = torch.ops.aten.mul.Tensor(cat_1, unsqueeze_7);  cat_1 = None
        add_1: "bf16[1, 24, 1, 128]" = torch.ops.aten.add.Tensor(mul_5, mul_6);  mul_5 = mul_6 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:146 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 1, 128]" = torch.ops.aten.mul.Tensor(permute_4, unsqueeze_6);  unsqueeze_6 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_8: "bf16[1, 8, 1, 64]" = torch.ops.aten.slice.Tensor(permute_4, 3, 0, 64)
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_9: "bf16[1, 8, 1, 64]" = torch.ops.aten.slice.Tensor(permute_4, 3, 64, 9223372036854775807);  permute_4 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 1, 64]" = torch.ops.aten.neg.default(slice_9);  slice_9 = None
        cat_2: "bf16[1, 8, 1, 128]" = torch.ops.aten.cat.default([neg_1, slice_8], -1);  neg_1 = slice_8 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:146 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_8: "bf16[1, 8, 1, 128]" = torch.ops.aten.mul.Tensor(cat_2, unsqueeze_7);  cat_2 = unsqueeze_7 = None
        add_2: "bf16[1, 8, 1, 128]" = torch.ops.aten.add.Tensor(mul_7, mul_8);  mul_7 = mul_8 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/cache_utils.py:1368 in update, code: key_states = key_states.to(self.key_cache[layer_idx].dtype)
        convert_element_type_9: "bf16[1, 8, 1, 128]" = torch.ops.prims.convert_element_type.default(add_2, torch.bfloat16);  convert_element_type_9 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/cache_utils.py:1369 in update, code: value_states = value_states.to(self.value_cache[layer_idx].dtype)
        convert_element_type_10: "bf16[1, 8, 1, 128]" = torch.ops.prims.convert_element_type.default(permute_6, torch.bfloat16);  convert_element_type_10 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/cache_utils.py:54 in _static_cache_update, code: k_cache.index_copy_(2, cache_position, key_states)
        index_put: "bf16[1, 8, 1024, 128]" = torch.ops.aten.index_put.default(mark_argument_attributes_15, [None, None, mark_argument_attributes_14], add_2);  add_2 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/cache_utils.py:55 in _static_cache_update, code: v_cache.index_copy_(2, cache_position, value_states)
        index_put_1: "bf16[1, 8, 1024, 128]" = torch.ops.aten.index_put.default(mark_argument_attributes_16, [None, None, mark_argument_attributes_14], permute_6);  mark_argument_attributes_14 = permute_6 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        slice_14: "bf16[1, 8, 1024, 128]" = torch.ops.aten.slice.Tensor(index_put, 0, 0, 9223372036854775807)
        slice_15: "bf16[1, 8, 1024, 128]" = torch.ops.aten.slice.Tensor(slice_14, 1, 0, 9223372036854775807);  slice_14 = None
        unsqueeze_9: "bf16[1, 8, 1, 1024, 128]" = torch.ops.aten.unsqueeze.default(slice_15, 2);  slice_15 = None
        slice_16: "bf16[1, 8, 1, 1024, 128]" = torch.ops.aten.slice.Tensor(unsqueeze_9, 3, 0, 9223372036854775807);  unsqueeze_9 = None
        slice_17: "bf16[1, 8, 1, 1024, 128]" = torch.ops.aten.slice.Tensor(slice_16, 4, 0, 9223372036854775807);  slice_16 = None
        expand_5: "bf16[1, 8, 3, 1024, 128]" = torch.ops.aten.expand.default(slice_17, [1, 8, 3, 1024, 128]);  slice_17 = None
        clone: "bf16[1, 8, 3, 1024, 128]" = torch.ops.aten.clone.default(expand_5, memory_format = torch.contiguous_format);  expand_5 = None
        view_13: "bf16[1, 24, 1024, 128]" = torch.ops.aten.view.default(clone, [1, 24, 1024, 128]);  clone = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:175 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        slice_22: "bf16[1, 8, 1024, 128]" = torch.ops.aten.slice.Tensor(index_put_1, 0, 0, 9223372036854775807)
        slice_23: "bf16[1, 8, 1024, 128]" = torch.ops.aten.slice.Tensor(slice_22, 1, 0, 9223372036854775807);  slice_22 = None
        unsqueeze_11: "bf16[1, 8, 1, 1024, 128]" = torch.ops.aten.unsqueeze.default(slice_23, 2);  slice_23 = None
        slice_24: "bf16[1, 8, 1, 1024, 128]" = torch.ops.aten.slice.Tensor(unsqueeze_11, 3, 0, 9223372036854775807);  unsqueeze_11 = None
        slice_25: "bf16[1, 8, 1, 1024, 128]" = torch.ops.aten.slice.Tensor(slice_24, 4, 0, 9223372036854775807);  slice_24 = None
        expand_7: "bf16[1, 8, 3, 1024, 128]" = torch.ops.aten.expand.default(slice_25, [1, 8, 3, 1024, 128]);  slice_25 = None
        clone_1: "bf16[1, 8, 3, 1024, 128]" = torch.ops.aten.clone.default(expand_7, memory_format = torch.contiguous_format);  expand_7 = None
        view_14: "bf16[1, 24, 1024, 128]" = torch.ops.aten.view.default(clone_1, [1, 24, 1024, 128]);  clone_1 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:191 in eager_attention_forward, code: attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling
        permute_7: "bf16[1, 24, 128, 1024]" = torch.ops.aten.permute.default(view_13, [0, 1, 3, 2]);  view_13 = None
        expand_8: "bf16[1, 24, 1, 128]" = torch.ops.aten.expand.default(add_1, [1, 24, 1, 128]);  add_1 = None
        view_15: "bf16[24, 1, 128]" = torch.ops.aten.view.default(expand_8, [24, 1, 128]);  expand_8 = None
        expand_9: "bf16[1, 24, 128, 1024]" = torch.ops.aten.expand.default(permute_7, [1, 24, 128, 1024]);  permute_7 = None
        view_16: "bf16[24, 128, 1024]" = torch.ops.aten.view.default(expand_9, [24, 128, 1024]);  expand_9 = None
        bmm_1: "bf16[24, 1, 1024]" = torch.ops.aten.bmm.default(view_15, view_16);  view_15 = view_16 = None
        view_17: "bf16[1, 24, 1, 1024]" = torch.ops.aten.view.default(bmm_1, [1, 24, 1, 1024]);  bmm_1 = None
        mul_9: "bf16[1, 24, 1, 1024]" = torch.ops.aten.mul.Tensor(view_17, 0.08838834764831845);  view_17 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:194 in eager_attention_forward, code: attn_weights = attn_weights + causal_mask
        unsqueeze_12: "bf16[1, 1, 1024]" = torch.ops.aten.unsqueeze.default(mul, 0);  mul = None
        unsqueeze_13: "bf16[1, 1, 1, 1024]" = torch.ops.aten.unsqueeze.default(unsqueeze_12, 1);  unsqueeze_12 = None
        slice_29: "bf16[1, 1, 1, 1024]" = torch.ops.aten.slice.Tensor(unsqueeze_13, 2, 0, 9223372036854775807);  unsqueeze_13 = None
        slice_30: "bf16[1, 1, 1, 1024]" = torch.ops.aten.slice.Tensor(slice_29, 3, 0, 9223372036854775807);  slice_29 = None
        expand_10: "bf16[1, 1, 1, 1024]" = torch.ops.aten.expand.default(slice_30, [1, 1, -1, -1]);  slice_30 = None
        slice_31: "bf16[1, 1, 1, 1024]" = torch.ops.aten.slice.Tensor(expand_10, 0, 0, 9223372036854775807);  expand_10 = None
        slice_32: "bf16[1, 1, 1, 1024]" = torch.ops.aten.slice.Tensor(slice_31, 1, 0, 9223372036854775807);  slice_31 = None
        slice_33: "bf16[1, 1, 1, 1024]" = torch.ops.aten.slice.Tensor(slice_32, 2, 0, 9223372036854775807);  slice_32 = None
        add_3: "bf16[1, 24, 1, 1024]" = torch.ops.aten.add.Tensor(mul_9, slice_33);  mul_9 = slice_33 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:196 in eager_attention_forward, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
        convert_element_type_11: "f32[1, 24, 1, 1024]" = torch.ops.prims.convert_element_type.default(add_3, torch.float32);  add_3 = None
        _softmax: "f32[1, 24, 1, 1024]" = torch.ops.aten._softmax.default(convert_element_type_11, -1, False);  convert_element_type_11 = None
        convert_element_type_12: "bf16[1, 24, 1, 1024]" = torch.ops.prims.convert_element_type.default(_softmax, torch.bfloat16);  _softmax = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:197 in eager_attention_forward, code: attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
        clone_2: "bf16[1, 24, 1, 1024]" = torch.ops.aten.clone.default(convert_element_type_12);  convert_element_type_12 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:198 in eager_attention_forward, code: attn_output = torch.matmul(attn_weights, value_states)
        expand_11: "bf16[1, 24, 1, 1024]" = torch.ops.aten.expand.default(clone_2, [1, 24, 1, 1024]);  clone_2 = None
        view_18: "bf16[24, 1, 1024]" = torch.ops.aten.view.default(expand_11, [24, 1, 1024]);  expand_11 = None
        expand_12: "bf16[1, 24, 1024, 128]" = torch.ops.aten.expand.default(view_14, [1, 24, 1024, 128]);  view_14 = None
        view_19: "bf16[24, 1024, 128]" = torch.ops.aten.view.default(expand_12, [24, 1024, 128]);  expand_12 = None
        bmm_2: "bf16[24, 1, 128]" = torch.ops.aten.bmm.default(view_18, view_19);  view_18 = view_19 = None
        view_20: "bf16[1, 24, 1, 128]" = torch.ops.aten.view.default(bmm_2, [1, 24, 1, 128]);  bmm_2 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:199 in eager_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_8: "bf16[1, 1, 24, 128]" = torch.ops.aten.permute.default(view_20, [0, 2, 1, 3]);  view_20 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:276 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_21: "bf16[1, 1, 3072]" = torch.ops.aten.view.default(permute_8, [1, 1, -1]);  permute_8 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:277 in forward, code: attn_output = self.o_proj(attn_output)
        permute_9: "bf16[3072, 3072]" = torch.ops.aten.permute.default(mark_argument_attributes_7, [1, 0]);  mark_argument_attributes_7 = None
        view_22: "bf16[1, 3072]" = torch.ops.aten.view.default(view_21, [1, 3072]);  view_21 = None
        mm_3: "bf16[1, 3072]" = torch.ops.aten.mm.default(view_22, permute_9);  view_22 = permute_9 = None
        view_23: "bf16[1, 1, 3072]" = torch.ops.aten.view.default(mm_3, [1, 1, 3072]);  mm_3 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:319 in forward, code: hidden_states = residual + hidden_states
        add_4: "bf16[1, 1, 3072]" = torch.ops.aten.add.Tensor(embedding, view_23);  embedding = view_23 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:70 in forward, code: hidden_states = hidden_states.to(torch.float32)
        convert_element_type_13: "f32[1, 1, 3072]" = torch.ops.prims.convert_element_type.default(add_4, torch.float32)
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:71 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_2: "f32[1, 1, 3072]" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_13, 2)
        mean_1: "f32[1, 1, 1]" = torch.ops.aten.mean.dim(pow_2, [-1], True);  pow_2 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:72 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add_5: "f32[1, 1, 1]" = torch.ops.aten.add.Tensor(mean_1, 1e-05);  mean_1 = None
        rsqrt_1: "f32[1, 1, 1]" = torch.ops.aten.rsqrt.default(add_5);  add_5 = None
        mul_10: "f32[1, 1, 3072]" = torch.ops.aten.mul.Tensor(convert_element_type_13, rsqrt_1);  convert_element_type_13 = rsqrt_1 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:73 in forward, code: return self.weight * hidden_states.to(input_dtype)
        convert_element_type_14: "bf16[1, 1, 3072]" = torch.ops.prims.convert_element_type.default(mul_10, torch.bfloat16);  mul_10 = None
        mul_11: "bf16[1, 1, 3072]" = torch.ops.aten.mul.Tensor(mark_argument_attributes_1, convert_element_type_14);  mark_argument_attributes_1 = convert_element_type_14 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:162 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_10: "bf16[3072, 8192]" = torch.ops.aten.permute.default(mark_argument_attributes_8, [1, 0]);  mark_argument_attributes_8 = None
        view_24: "bf16[1, 3072]" = torch.ops.aten.view.default(mul_11, [1, 3072])
        mm_4: "bf16[1, 8192]" = torch.ops.aten.mm.default(view_24, permute_10);  view_24 = permute_10 = None
        view_25: "bf16[1, 1, 8192]" = torch.ops.aten.view.default(mm_4, [1, 1, 8192]);  mm_4 = None
        convert_element_type_15: "f32[1, 1, 8192]" = torch.ops.prims.convert_element_type.default(view_25, torch.float32);  convert_element_type_15 = None
        sigmoid: "f32[1, 1, 8192]" = torch.ops.aten.sigmoid.default(view_25)
        mul_12: "f32[1, 1, 8192]" = torch.ops.aten.mul.Tensor(view_25, sigmoid);  view_25 = sigmoid = None
        convert_element_type_16: "bf16[1, 1, 8192]" = torch.ops.prims.convert_element_type.default(mul_12, torch.bfloat16);  mul_12 = None
        permute_11: "bf16[3072, 8192]" = torch.ops.aten.permute.default(mark_argument_attributes_9, [1, 0]);  mark_argument_attributes_9 = None
        view_26: "bf16[1, 3072]" = torch.ops.aten.view.default(mul_11, [1, 3072]);  mul_11 = None
        mm_5: "bf16[1, 8192]" = torch.ops.aten.mm.default(view_26, permute_11);  view_26 = permute_11 = None
        view_27: "bf16[1, 1, 8192]" = torch.ops.aten.view.default(mm_5, [1, 1, 8192]);  mm_5 = None
        mul_13: "bf16[1, 1, 8192]" = torch.ops.aten.mul.Tensor(convert_element_type_16, view_27);  convert_element_type_16 = view_27 = None
        permute_12: "bf16[8192, 3072]" = torch.ops.aten.permute.default(mark_argument_attributes_10, [1, 0]);  mark_argument_attributes_10 = None
        view_28: "bf16[1, 8192]" = torch.ops.aten.view.default(mul_13, [1, 8192]);  mul_13 = None
        mm_6: "bf16[1, 3072]" = torch.ops.aten.mm.default(view_28, permute_12);  view_28 = permute_12 = None
        view_29: "bf16[1, 1, 3072]" = torch.ops.aten.view.default(mm_6, [1, 1, 3072]);  mm_6 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:325 in forward, code: hidden_states = residual + hidden_states
        add_6: "bf16[1, 1, 3072]" = torch.ops.aten.add.Tensor(add_4, view_29);  add_4 = view_29 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:70 in forward, code: hidden_states = hidden_states.to(torch.float32)
        convert_element_type_17: "f32[1, 1, 3072]" = torch.ops.prims.convert_element_type.default(add_6, torch.float32);  add_6 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:71 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_3: "f32[1, 1, 3072]" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_17, 2)
        mean_2: "f32[1, 1, 1]" = torch.ops.aten.mean.dim(pow_3, [-1], True);  pow_3 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:72 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add_7: "f32[1, 1, 1]" = torch.ops.aten.add.Tensor(mean_2, 1e-05);  mean_2 = None
        rsqrt_2: "f32[1, 1, 1]" = torch.ops.aten.rsqrt.default(add_7);  add_7 = None
        mul_14: "f32[1, 1, 3072]" = torch.ops.aten.mul.Tensor(convert_element_type_17, rsqrt_2);  convert_element_type_17 = rsqrt_2 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:73 in forward, code: return self.weight * hidden_states.to(input_dtype)
        convert_element_type_18: "bf16[1, 1, 3072]" = torch.ops.prims.convert_element_type.default(mul_14, torch.bfloat16);  mul_14 = None
        mul_15: "bf16[1, 1, 3072]" = torch.ops.aten.mul.Tensor(mark_argument_attributes_2, convert_element_type_18);  mark_argument_attributes_2 = convert_element_type_18 = None
        
         # File: /localdev/jameszianxu/tt-xla/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:704 in forward, code: logits = self.lm_head(hidden_states[:, slice_indices, :])
        slice_34: "bf16[1, 1, 3072]" = torch.ops.aten.slice.Tensor(mul_15, 0, 0, 9223372036854775807);  mul_15 = None
        slice_35: "bf16[1, 1, 3072]" = torch.ops.aten.slice.Tensor(slice_34, 1, 0, 9223372036854775807);  slice_34 = None
        slice_36: "bf16[1, 1, 3072]" = torch.ops.aten.slice.Tensor(slice_35, 2, 0, 9223372036854775807);  slice_35 = None
        permute_13: "bf16[3072, 128256]" = torch.ops.aten.permute.default(mark_argument_attributes_11, [1, 0]);  mark_argument_attributes_11 = None
        view_30: "bf16[1, 3072]" = torch.ops.aten.view.default(slice_36, [1, 3072]);  slice_36 = None
        mm_7: "bf16[1, 128256]" = torch.ops.aten.mm.default(view_30, permute_13);  view_30 = permute_13 = None
        view_31: "bf16[1, 1, 128256]" = torch.ops.aten.view.default(mm_7, [1, 1, 128256]);  mm_7 = None
        
        # No stacktrace found for following nodes
        copy__default = torch.ops.aten.copy_.default(mark_argument_attributes_15, index_put);  mark_argument_attributes_15 = index_put = copy__default = None
        copy__default_1 = torch.ops.aten.copy_.default(mark_argument_attributes_16, index_put_1);  mark_argument_attributes_16 = index_put_1 = copy__default_1 = None
        return pytree.tree_unflatten((view_31,), self._out_spec)
        2025-09-15 15:42:12.744 (  81.071s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:12.744 (  81.071s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:12.744 (  81.071s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:42:12.744 (  81.071s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:42:12.744 (  81.071s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-09-15 15:42:12.745 (  81.071s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:12.745 (  81.071s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:12.745 (  81.071s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:42:12.745 (  81.071s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:42:12.745 (  81.071s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-09-15 15:42:12.745 (  81.071s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:12.745 (  81.071s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:12.745 (  81.071s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:12.745 (  81.071s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:12.745 (  81.071s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:42:12.745 (  81.071s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:42:12.745 (  81.071s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-09-15 15:42:12.745 (  81.072s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:12.745 (  81.072s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:12.745 (  81.072s) [        F073E000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-09-15 15:42:12.745 (  81.072s) [        F073E000]     client_instance.cc:466      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-09-15 15:42:12.745 (  81.072s) [        F073E000]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-09-15 15:42:12.745 (  81.072s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:12.745 (  81.072s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.078s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.079s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.079s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.079s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.079s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.079s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.079s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.079s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.079s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.079s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.079s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.079s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.079s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.079s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.079s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.079s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:12.752 (  81.079s) [        F2FFD640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:12.752 (  81.079s) [        F2FFD640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:12.752 (  81.079s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.752 (  81.079s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.752 (  81.079s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.752 (  81.079s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.752 (  81.079s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.752 (  81.079s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.752 (  81.079s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.752 (  81.079s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.752 (  81.079s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.752 (  81.079s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.752 (  81.079s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.752 (  81.079s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.752 (  81.079s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.752 (  81.079s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.752 (  81.079s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.752 (  81.079s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.752 (  81.079s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.752 (  81.079s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.752 (  81.079s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.752 (  81.079s) [        6FFFF640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        6FFFF640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        6FFFF640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        6FFFF640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        6EFFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        6EFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        A8FF9640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        A8FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        F2FFD640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        F2FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        6F7FE640]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:12.753 (  81.079s) [        6F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:12.753 (  81.079s) [        F2FFD640] executable_instance.cc:138      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-09-15 15:42:12.753 (  81.079s) [        F2FFD640]loaded_executable_insta:497      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-09-15 15:42:12.753 (  81.079s) [        F2FFD640]loaded_executable_insta:81       1| LoadedExecutableInstance::Execute
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Device grid size = { 8, 8 }
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = ttcore.load_cached(@main_const_eval_0, [%arg0]) : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_0
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_0
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_0
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_0
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg10) <{force = false}> : (tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = ttcore.load_cached(@main_const_eval_1, [%arg7]) : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_1
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_1
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_1
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_1
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg7) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = ttcore.load_cached(@main_const_eval_2, [%arg12]) : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_2
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_2
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.278_tm0_tm0_tm0"("broadcast.278_tm0_tm0"("broadcast.278_tm0"("broadcast.278"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.278_tm0_tm0_tm0"("broadcast.278_tm0_tm0"("broadcast.278_tm0"("broadcast.278"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.136")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.136")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_2
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_2
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg12) <{force = false}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = ttcore.load_cached(@main_const_eval_3, [%arg17]) : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_3
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_3
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_3
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_3
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg17) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %4 = ttcore.load_cached(@main_const_eval_4, [%arg13]) : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_4
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_4
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.272")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.272")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.repeat"(%2) <{repeat_dims = #ttnn.shape<1x12x1x1024>}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.272")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.272")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %4 = "ttnn.reshape"(%3) <{shape = [12 : i32, 1 : i32, 1024 : i32]}> : (tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.271")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.271")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_4
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_4
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg13) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %5 = ttcore.load_cached(@main_const_eval_5, [%arg19]) : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_5
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_5
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_5
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_5
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg19) <{force = false}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %6 = ttcore.load_cached(@main_const_eval_6, [%arg5]) : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_6
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_6
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_6
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_6
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg5) <{force = false}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %7 = ttcore.load_cached(@main_const_eval_7, [%arg18]) : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_7
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_7
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.358_tm0_tm0_tm0"("reshape.358_tm0_tm0"("reshape.358_tm0"("reshape.358"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.358_tm0_tm0_tm0"("reshape.358_tm0_tm0"("reshape.358_tm0"("reshape.358"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.345")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.345")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_7
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_7
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg18) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %8 = ttcore.load_cached(@main_const_eval_8, [%arg8]) : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_8
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_8
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.244_tm0_tm0_tm0"("reshape.244_tm0_tm0"("reshape.244_tm0"("reshape.244"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.244_tm0_tm0_tm0"("reshape.244_tm0_tm0"("reshape.244_tm0"("reshape.244"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.81")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.81")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_8
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_8
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg8) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %9 = ttcore.load_cached(@main_const_eval_9, [%arg2]) : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_9
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_9
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_9
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_9
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %10 = ttcore.load_cached(@main_const_eval_10, [%arg4]) : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_10
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_10
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_10
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_10
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %11:3 = ttcore.load_cached(@main_const_eval_11, [%arg1]) : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_11
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_11
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.244_tm0_tm1_tm1_tm0_tm1"("reshape.244_tm0_tm1_tm1_tm0"("reshape.244_tm0_tm1_tm1"("reshape.244_tm0_tm1"("reshape.244_tm0"("reshape.244"))))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.358_tm0_tm1_tm1_tm0_tm1"("reshape.358_tm0_tm1_tm1_tm0"("reshape.358_tm0_tm1_tm1"("reshape.358_tm0_tm1"("reshape.358_tm0"("reshape.358"))))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %4 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.416_tm0_tm1_tm1_tm0_tm1"("reshape.416_tm0_tm1_tm1_tm0"("reshape.416_tm0_tm1_tm1"("reshape.416_tm0_tm1"("reshape.416_tm0"("reshape.416"))))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.416_tm0_tm1_tm1_tm0_tm1"("reshape.416_tm0_tm1_tm1_tm0"("reshape.416_tm0_tm1_tm1"("reshape.416_tm0_tm1"("reshape.416_tm0"("reshape.416"))))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_11
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_11
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %12 = ttcore.load_cached(@main_const_eval_12, [%arg20]) : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_12
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_12
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.416_tm0_tm0_tm0"("reshape.416_tm0_tm0"("reshape.416_tm0"("reshape.416"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.416_tm0_tm0_tm0"("reshape.416_tm0_tm0"("reshape.416_tm0"("reshape.416"))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.409")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.409")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_12
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_12
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg20) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %13 = ttcore.load_cached(@main_const_eval_13, []) : () -> tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_13
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_13
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.constant"(%0) <{dtype = #ttcore.supportedDataTypes<si32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <interleaved>>, value = dense_resource<__elided__> : tensor<1x1024xsi32>}> : (!ttnn.device) -> tensor<1x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1024xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.278_tm0_tm1_tm0_tm0"("broadcast.278_tm0_tm1_tm0"("broadcast.278_tm0_tm1"("broadcast.278_tm0"("broadcast.278")))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x1024xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.278_tm0_tm1_tm0_tm0"("broadcast.278_tm0_tm1_tm0"("broadcast.278_tm0_tm1"("broadcast.278_tm0"("broadcast.278")))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x1024xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("compare.129_workaround"("compare.129"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1x1x1024xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("compare.129_workaround"("compare.129"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_13
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_13
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %14 = ttcore.load_cached(@main_const_eval_14, [%arg15]) : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_14
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_14
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_14
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_14
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg15) <{force = false}> : (tensor<1024x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %15 = ttcore.load_cached(@main_const_eval_15, []) : () -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_15
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_15
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_15
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_15
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %16 = ttcore.load_cached(@main_const_eval_16, [%arg3]) : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Cache miss or invalid cache for function: main_const_eval_16
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main_const_eval_16
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main_const_eval_16
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | executed sub-func: main_const_eval_16
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %17 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %18 = "ttnn.full"(%17) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 3.25520843E-4 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %19 = "ttnn.mesh_shard"(%arg6, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg6) <{force = false}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %20 = "ttnn.mesh_shard"(%arg9, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %21 = "ttnn.mesh_shard"(%arg11, %17) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg11) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %22 = "ttnn.mesh_shard"(%arg14, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg14) <{force = false}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %23 = "ttnn.mesh_shard"(%arg16, %17) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg16) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %24 = "ttnn.typecast"(%19) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.44")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.44")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %25 = "ttnn.reshape"(%24) <{shape = [1 : i32]}> : (tensor<1x1xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.44")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%24) <{force = false}> : (tensor<1x1xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.44")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %26 = "ttnn.from_device"(%25) : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>> loc("gather.45_workaround"("gather.45"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%25) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("gather.45_workaround"("gather.45"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %27 = "ttnn.to_layout"(%26) <{layout = #ttnn.layout<row_major>}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<system_memory>>>> loc("gather.45_workaround"("gather.45"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%26) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> () loc("gather.45_workaround"("gather.45"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %28 = "ttnn.to_device"(%27, %17) <{memory_config = #ttnn.memory_config<<dram>, <interleaved>>}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<dram>>, <interleaved>>> loc("gather.45_workaround"("gather.45"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%27) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<system_memory>>>>) -> () loc("gather.45_workaround"("gather.45"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %29 = "ttnn.embedding"(%28, %1) : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("gather.45")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%28) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("gather.45")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("gather.45")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %30 = "ttnn.typecast"(%29) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.47")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %31 = "ttnn.reshape"(%30) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.47")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %32 = "ttnn.pow"(%31, %15) : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("power.49")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%31) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("power.49")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %33 = "ttnn.sum"(%32) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reduce.56")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%32) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reduce.56")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %34 = "ttnn.multiply"(%33, %18) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.65")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%33) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.65")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %35 = "ttnn.add"(%34, %11#0) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.70")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%34) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.70")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%11#0) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.70")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %36 = "ttnn.rsqrt"(%35) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("rsqrt.71")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%35) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("rsqrt.71")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %37 = "ttnn.multiply"(%30, %36) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.74")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%36) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.74")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%30) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.74")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %38 = "ttnn.multiply"(%8, %37) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.83")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%37) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.83")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.83")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %39 = "ttnn.typecast"(%38) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.84")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%38) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.84")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %40 = "ttnn.matmul"(%39, %3) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.245")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%3) <{force = false}> : (tensor<1536x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<48x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.245")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %41 = "ttnn.reshape"(%40) <{shape = [1 : i32, 12 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("transpose.248")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %42 = "ttnn.typecast"(%40) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1536xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.259")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%40) <{force = false}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.259")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %43 = "ttnn.reshape"(%42) <{shape = [12 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1536xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.259")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%42) <{force = false}> : (tensor<1x1536xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.259")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %44 = "ttnn.reshape"(%22) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.162")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%22) <{force = false}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.162")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %45 = "ttnn.typecast"(%20) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.154")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %46 = "ttnn.reshape"(%45) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.154")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%45) <{force = false}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.154")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %47 = "ttnn.matmul"(%44, %46) <{transpose_a = false, transpose_b = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.165")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%46) <{force = false}> : (tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.165")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%44) <{force = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.165")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %48 = "ttnn.reshape"(%47) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("transpose.166")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%47) <{force = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("transpose.166")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %49 = "ttnn.concat"(%48, %48) <{dim = 2 : si32}> : (tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("concatenate.167")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%48) <{force = false}> : (tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("concatenate.167")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %50 = "ttnn.cos"(%49) : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("cosine.194")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %51 = "ttnn.multiply"(%43, %50) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.262")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%43) <{force = false}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.262")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %52 = "ttnn.typecast"(%51) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.263")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%51) <{force = false}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.263")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %53 = "ttnn.slice_static"(%41) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 12 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("slice.250")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %54 = "ttnn.neg"(%53) : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("negate.251")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%53) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("negate.251")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %55 = "ttnn.reshape"(%54) <{shape = [12 : i32, 1 : i32, 64 : i32]}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("negate.251")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%54) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("negate.251")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %56 = "ttnn.slice_static"(%41) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 12 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("slice.249")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%41) <{force = false}> : (tensor<1x12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("slice.249")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %57 = "ttnn.reshape"(%56) <{shape = [12 : i32, 1 : i32, 64 : i32]}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("concatenate.252")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%56) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("concatenate.252")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %58 = "ttnn.concat"(%55, %57) <{dim = 2 : si32}> : (tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("concatenate.252")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%57) <{force = false}> : (tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("concatenate.252")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%55) <{force = false}> : (tensor<12x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("concatenate.252")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %59 = "ttnn.typecast"(%58) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.253")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%58) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.253")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %60 = "ttnn.sin"(%49) : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("sine.168")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%49) <{force = false}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("sine.168")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %61 = "ttnn.multiply"(%59, %60) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.256")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%59) <{force = false}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.256")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %62 = "ttnn.typecast"(%61) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.257")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%61) <{force = false}> : (tensor<12x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.257")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %63 = "ttnn.add"(%52, %62) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.266")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%62) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.266")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%52) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.266")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %64 = "ttnn.matmul"(%39, %14) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.180")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%14) <{force = false}> : (tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.180")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %65 = "ttnn.reshape"(%64) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32]}> : (tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("transpose.183")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%64) <{force = false}> : (tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("transpose.183")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %66 = "ttnn.typecast"(%65) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.200")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %67 = "ttnn.reshape"(%50) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.202")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%50) <{force = false}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.202")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %68 = "ttnn.multiply"(%66, %67) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.203")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%67) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.203")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%66) <{force = false}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.203")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %69 = "ttnn.typecast"(%68) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.204")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%68) <{force = false}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.204")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %70 = "ttnn.slice_static"(%65) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("slice.185")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %71 = "ttnn.neg"(%70) : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("negate.186")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%70) <{force = false}> : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("negate.186")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %72 = "ttnn.slice_static"(%65) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("slice.184")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%65) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("slice.184")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %73 = "ttnn.concat"(%71, %72) <{dim = 3 : si32}> : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("concatenate.187")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%72) <{force = false}> : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("concatenate.187")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%71) <{force = false}> : (tensor<1x4x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("concatenate.187")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %74 = "ttnn.typecast"(%73) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.188")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%73) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.188")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %75 = "ttnn.reshape"(%60) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.190")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%60) <{force = false}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.190")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %76 = "ttnn.multiply"(%74, %75) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.191")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%75) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.191")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%74) <{force = false}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.191")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %77 = "ttnn.typecast"(%76) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.192")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%76) <{force = false}> : (tensor<1x4x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.192")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %78 = "ttnn.add"(%69, %77) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.207")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%77) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.207")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%69) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.207")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %79 = "ttnn.typecast"(%arg9) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("scatter.223_workaround"("scatter.223"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.update_cache"(%23, %78, %79) <{batch_offset = 0 : i32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("scatter.223")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%79) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("scatter.223")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%78) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("scatter.223")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %80 = "ttnn.reshape"(%23) <{shape = [1 : i32, 4 : i32, 1 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.232")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%23) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.232")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %81 = "ttnn.repeat"(%80) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.232")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%80) <{force = false}> : (tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.232")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %82 = "ttnn.reshape"(%81) <{shape = [1 : i32, 12 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12288 + d1 * 1024 + d2, d3), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.233")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%81) <{force = false}> : (tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.233")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %83 = "ttnn.permute"(%82) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12288 + d1 * 1024 + d2, d3), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("transpose.234")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%82) <{force = false}> : (tensor<1x12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12288 + d1 * 1024 + d2, d3), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("transpose.234")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %84 = "ttnn.reshape"(%83) <{shape = [12 : i32, 128 : i32, 1024 : i32]}> : (tensor<1x12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.236")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%83) <{force = false}> : (tensor<1x12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 128 + d2, d3), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.236")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %85 = "ttnn.matmul"(%63, %84) <{transpose_a = false, transpose_b = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.269")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%84) <{force = false}> : (tensor<12x128x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<48x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.269")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%63) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.269")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %86 = "ttnn.typecast"(%85) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.271")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%85) <{force = false}> : (tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.271")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %87 = "ttnn.multiply"(%86, %4) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.273")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%86) <{force = false}> : (tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.273")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%4) <{force = false}> : (tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.273")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %88 = "ttnn.typecast"(%87) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.274")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%87) <{force = false}> : (tensor<12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.274")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %89 = "ttnn.reshape"(%88) <{shape = [1 : i32, 12 : i32, 1 : i32, 1024 : i32]}> : (tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.274")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%88) <{force = false}> : (tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.274")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %90 = "ttnn.reshape"(%20) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.278_tm0_tm1_tm0_tm1"("broadcast.278_tm0_tm1_tm0"("broadcast.278_tm0_tm1"("broadcast.278_tm0"("broadcast.278")))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%20) <{force = false}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.278_tm0_tm1_tm0_tm1"("broadcast.278_tm0_tm1_tm0"("broadcast.278_tm0_tm1"("broadcast.278_tm0"("broadcast.278")))))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %91 = "ttnn.typecast"(%90) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("compare.129_workaround"("compare.129"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%90) <{force = false}> : (tensor<1x1x1x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("compare.129_workaround"("compare.129"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %92 = "ttnn.gt"(%13, %91) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("compare.129")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%91) <{force = false}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("compare.129")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("compare.129")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %93 = "ttnn.typecast"(%92) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("compare.129_workaround"("compare.129"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%92) <{force = false}> : (tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("compare.129_workaround"("compare.129"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %94 = "ttnn.typecast"(%93) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.130")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%93) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.130")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %95 = "ttnn.multiply"(%2, %94) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.137")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%94) <{force = false}> : (tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.137")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.137")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %96 = "ttnn.typecast"(%95) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.138")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%95) <{force = false}> : (tensor<1x1x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.138")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %97 = "ttnn.add"(%89, %96) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.279")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%96) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.279")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%89) <{force = false}> : (tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.279")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %98 = "ttnn.typecast"(%97) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.280")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%97) <{force = false}> : (tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.280")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %99 = "ttnn.softmax"(%98) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("divide.297")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%98) <{force = false}> : (tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("divide.297")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %100 = "ttnn.typecast"(%99) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.298")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%99) <{force = false}> : (tensor<1x12x1x1024xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.298")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %101 = "ttnn.reshape"(%100) <{shape = [12 : i32, 1 : i32, 1024 : i32]}> : (tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.298")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%100) <{force = false}> : (tensor<1x12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.298")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %102 = "ttnn.matmul"(%39, %6) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.86")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%39) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.86")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%6) <{force = false}> : (tensor<512x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.86")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %103 = "ttnn.reshape"(%102) <{shape = [1 : i32, 4 : i32, 1 : i32, 128 : i32]}> : (tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("transpose.89")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%102) <{force = false}> : (tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("transpose.89")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %104 = "ttnn.typecast"(%arg9) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("scatter.110_workaround"("scatter.110"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg9) <{force = false}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("scatter.110_workaround"("scatter.110"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.update_cache"(%21, %103, %104) <{batch_offset = 0 : i32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("scatter.110")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%104) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("scatter.110")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%103) <{force = false}> : (tensor<1x4x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("scatter.110")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %105 = "ttnn.reshape"(%21) <{shape = [1 : i32, 4 : i32, 1 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.119")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%21) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.119")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %106 = "ttnn.repeat"(%105) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.119")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%105) <{force = false}> : (tensor<1x4x1x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 1024 + d2 * 1024 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.119")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %107 = "ttnn.reshape"(%106) <{shape = [12 : i32, 1024 : i32, 128 : i32]}> : (tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 1024 + d1, d2), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.122")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%106) <{force = false}> : (tensor<1x4x3x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 12288 + d1 * 3072 + d2 * 1024 + d3, d4), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.122")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %108 = "ttnn.matmul"(%101, %107) <{transpose_a = false, transpose_b = false}> : (tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 1024 + d1, d2), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.301")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%107) <{force = false}> : (tensor<12x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 1024 + d1, d2), <1x1>, memref<384x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.301")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%101) <{force = false}> : (tensor<12x1x1024xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.301")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %109 = "ttnn.reshape"(%108) <{shape = [1 : i32, 1536 : i32]}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.305")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%108) <{force = false}> : (tensor<12x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<12x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.305")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %110 = "ttnn.matmul"(%109, %10) <{transpose_a = false, transpose_b = true}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.306")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%109) <{force = false}> : (tensor<1x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.306")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%10) <{force = false}> : (tensor<3072x1536xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.306")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %111 = "ttnn.reshape"(%110) <{shape = [1 : i32, 1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.306_reduceScatter_reshape_to_4d"("dot.306_reduceScatter"("dot.306")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%110) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.306_reduceScatter_reshape_to_4d"("dot.306_reduceScatter"("dot.306")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %112 = "ttnn.reduce_scatter"(%111, %17) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 3 : si32}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.306_reduceScatter_reduce_scatter_4d"("dot.306_reduceScatter"("dot.306")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%111) <{force = false}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.306_reduceScatter_reduce_scatter_4d"("dot.306_reduceScatter"("dot.306")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %113 = "ttnn.all_gather"(%112, %17) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.306_all_gather_4d"("dot.306"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%112) <{force = false}> : (tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.306_all_gather_4d"("dot.306"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %114 = "ttnn.reshape"(%113) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.306")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%113) <{force = false}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.306")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %115 = "ttnn.add"(%29, %114) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.310")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%114) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.310")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%29) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.310")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %116 = "ttnn.typecast"(%115) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.311")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %117 = "ttnn.reshape"(%116) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.311")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %118 = "ttnn.pow"(%117, %15) : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("power.313")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%117) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("power.313")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %119 = "ttnn.sum"(%118) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reduce.320")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%118) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reduce.320")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %120 = "ttnn.multiply"(%119, %18) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.329")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%119) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.329")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %121 = "ttnn.add"(%120, %11#1) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.334")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%120) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.334")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%11#1) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.334")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %122 = "ttnn.rsqrt"(%121) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("rsqrt.335")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%121) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("rsqrt.335")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %123 = "ttnn.multiply"(%116, %122) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.338")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%122) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.338")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%116) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.338")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %124 = "ttnn.multiply"(%7, %123) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.347")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%123) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.347")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.347")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %125 = "ttnn.typecast"(%124) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.348")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%124) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.348")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %126 = "ttnn.matmul"(%125, %5) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.359")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%5) <{force = false}> : (tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.359")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %127 = "ttnn.typecast"(%126) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.363")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %128 = "ttnn.sigmoid"(%126) : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("logistic.361")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%126) <{force = false}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("logistic.361")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %129 = "ttnn.typecast"(%128) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.362")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%128) <{force = false}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.362")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %130 = "ttnn.multiply"(%127, %129) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.364")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%129) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.364")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%127) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.364")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %131 = "ttnn.matmul"(%125, %16) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.350")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%125) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.350")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%16) <{force = false}> : (tensor<4096x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.350")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %132 = "ttnn.typecast"(%131) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.352")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%131) <{force = false}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.352")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %133 = "ttnn.multiply"(%130, %132) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.367")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%132) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.367")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%130) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.367")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %134 = "ttnn.typecast"(%133) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.368")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%133) <{force = false}> : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.368")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %135 = "ttnn.matmul"(%134, %9) <{transpose_a = false, transpose_b = true}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.370")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%134) <{force = false}> : (tensor<1x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.370")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%9) <{force = false}> : (tensor<3072x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.370")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %136 = "ttnn.reshape"(%135) <{shape = [1 : i32, 1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.370_reduceScatter_reshape_to_4d"("dot.370_reduceScatter"("dot.370")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%135) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.370_reduceScatter_reshape_to_4d"("dot.370_reduceScatter"("dot.370")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %137 = "ttnn.reduce_scatter"(%136, %17) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 3 : si32}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.370_reduceScatter_reduce_scatter_4d"("dot.370_reduceScatter"("dot.370")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%136) <{force = false}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.370_reduceScatter_reduce_scatter_4d"("dot.370_reduceScatter"("dot.370")))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %138 = "ttnn.all_gather"(%137, %17) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.370_all_gather_4d"("dot.370"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%137) <{force = false}> : (tensor<1x1x1x1536xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x48x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.370_all_gather_4d"("dot.370"))
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %139 = "ttnn.reshape"(%138) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.370")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%138) <{force = false}> : (tensor<1x1x1x3072xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.370")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %140 = "ttnn.add"(%115, %139) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.374")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%139) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.374")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%115) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.374")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %141 = "ttnn.typecast"(%140) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.375")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%140) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.375")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %142 = "ttnn.reshape"(%141) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.375")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %143 = "ttnn.pow"(%142, %15) : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("power.377")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%142) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("power.377")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("power.377")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %144 = "ttnn.sum"(%143) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reduce.384")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%143) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reduce.384")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %145 = "ttnn.multiply"(%144, %18) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.393")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%144) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.393")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%18) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.393")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %146 = "ttnn.add"(%145, %11#2) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.398")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%145) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.398")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%11#2) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.398")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %147 = "ttnn.rsqrt"(%146) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("rsqrt.399")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%146) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("rsqrt.399")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %148 = "ttnn.multiply"(%141, %147) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.402")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%147) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.402")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%141) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.402")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %149 = "ttnn.multiply"(%12, %148) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.411")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%148) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.411")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.411")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %150 = "ttnn.typecast"(%149) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.412")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%149) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.412")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %151 = "ttnn.matmul"(%150, %0) <{transpose_a = false, transpose_b = true}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.417")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%150) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.417")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%0) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.417")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %152 = "ttnn.reshape"(%151) <{shape = [1 : i32, 1 : i32, 128256 : i32]}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.418")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %153 = "ttnn.to_layout"(%151) <{layout = #ttnn.layout<row_major>}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%151) <{force = false}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %154 = "ttnn.from_device"(%153) : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%153) <{force = false}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %155 = "ttnn.mesh_shard"(%154, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%154) <{force = false}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %156 = "ttnn.to_layout"(%152) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%152) <{force = false}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %157 = "ttnn.from_device"(%156) : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%156) <{force = false}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %158 = "ttnn.mesh_shard"(%157, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%157) <{force = false}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main
2025-09-15 15:42:13.977 (  82.303s) [        F2FFD640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:13.977 (  82.303s) [        F2FFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:13.977 (  82.303s) [        F2FFD640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:13.977 (  82.303s) [        F2FFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:13.977 (  82.303s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:13.977 (  82.304s) [        F2FFD640]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:13.977 (  82.304s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:13.977 (  82.304s) [        F2FFD640]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:13.977 (  82.304s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:13.977 (  82.304s) [        F2FFD640]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:13.977 (  82.304s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:13.977 (  82.304s) [        F2FFD640]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:13.977 (  82.304s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:13.977 (  82.304s) [        F2FFD640]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:13.977 (  82.304s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:13.977 (  82.304s) [        F2FFD640]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:13.977 (  82.304s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:13.977 (  82.304s) [        F2FFD640]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:13.977 (  82.304s) [        F2FFD640]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:13.977 (  82.304s) [        F2FFD640]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:13.977 (  82.304s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:13.977 (  82.304s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:13.977 (  82.304s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:42:13.977 (  82.304s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:13.977 (  82.304s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:13.977 (  82.304s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:42:13.977 (  82.304s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User is requesting to copy the data from a runtime tensor with data type: Int32 into buffer with expected data type: Int64, the values will be casted, this may impact the throughput and the integrity of the data.
2025-09-15 15:42:13.978 (  82.304s) [        12FFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:13.978 (  82.304s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:13.978 (  82.304s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:13.978 (  82.305s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:42:13.978 (  82.305s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:13.978 (  82.305s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:13.978 (  82.305s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:42:13.978 (  82.305s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User is requesting to copy the data from a runtime tensor with data type: Int32 into buffer with expected data type: Int64, the values will be casted, this may impact the throughput and the integrity of the data.
2025-09-15 15:42:13.978 (  82.305s) [        12FFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:13.979 (  82.305s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:13.979 (  82.305s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:13.979 (  82.306s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:13.985 (  82.311s) [        F073E000]     client_instance.cc:428      1| ClientInstance::PJRT_Client_Compile
2025-09-15 15:42:13.985 (  82.311s) [        F073E000]      module_builder.cc:103      1| ModuleBuilder::buildModule
2025-09-15 15:42:13.985 (  82.311s) [        F073E000]      module_builder.cc:165      1| VHLO Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) {
    %0 = "vhlo.custom_call_v1"(%arg0) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"xla.sdy.FuncResultSharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<true>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {}, {}, {}]>]>">}>} : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>
    "vhlo.return_v1"(%0) : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>]>, res_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, sym_visibility = #vhlo.string_v1<"">}
}
2025-09-15 15:42:13.986 (  82.313s) [        F073E000]      module_builder.cc:185      1| Is using shardy? true
2025-09-15 15:42:13.988 (  82.315s) [        F073E000]      module_builder.cc:203      1| SHLO Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}) -> (tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>}) {
    return %arg0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:13.989 (  82.315s) [        F073E000]      module_builder.cc:212      1| SHLO Module after frontend StableHLO pipeline:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}) -> (tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>}) {
    return %arg0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:13.993 (  82.319s) [        F073E000]      module_builder.cc:486      1| SHLO Module after compiler StableHLO pipeline:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = sdy.manual_computation(%arg0) in_shardings=[<@mesh, [{}, {"_axis_0"}, {}, {}]>] out_shardings=[<@mesh, [{}, {}, {}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg1: tensor<1x4x1024x128xbf16>) {
      %1 = "stablehlo.all_gather"(%arg1) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> : (tensor<1x4x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
      sdy.return %1 : tensor<1x8x1024x128xbf16>
    } : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    return %0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:13.995 (  82.322s) [        F073E000]      module_builder.cc:510      1| TTIR Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = ttir.empty() : tensor<1x4x1024x128xbf16>
    %1 = "ttir.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16>, tensor<1x4x1024x128xbf16>) -> tensor<1x4x1024x128xbf16>
    %2 = ttir.empty() : tensor<1x8x1024x128xbf16>
    %3 = "ttir.all_gather"(%1, %2) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<1x4x1024x128xbf16>, tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %4 = ttir.empty() : tensor<1x8x1024x128xbf16>
    %5 = "ttir.mesh_shard"(%3, %4) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16>, tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    return %5 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:13.996 (  82.322s) [        F073E000]      module_builder.cc:561   WARN| `mhlo.num_partitions` attribute not found, assuming default number of partitions: 1
2025-09-15 15:42:13.996 (  82.322s) [        F073E000]      module_builder.cc:575   WARN| `mhlo.num_replicas` attribute not found, assuming default number of replicas: 1
2025-09-15 15:42:13.996 (  82.322s) [        F073E000]      module_builder.cc:585   WARN| Num replicas and num partitions are not set, inferring the number of devices from mesh shape
2025-09-15 15:42:14.026 (  82.353s) [        F073E000]      module_builder.cc:630      1| TTNN Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073185088, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073193408, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x16, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 2)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 2, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %2 = "ttnn.all_gather"(%1, %0) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.mesh_shard"(%2, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %3 : tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
    }
  }
}
2025-09-15 15:42:14.037 (  82.364s) [        F073E000]loaded_executable_insta:441      1| LoadedExecutableInstance::PJRT_LoadedExecutable_GetExecutable
2025-09-15 15:42:14.037 (  82.364s) [        F073E000]loaded_executable_insta:460      1| LoadedExecutableInstance::PJRT_LoadedExecutable_AddressableDevices
2025-09-15 15:42:14.038 (  82.364s) [        F073E000]              stubs.inc:76    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2025-09-15 15:42:14.038 (  82.364s) [        F073E000]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-09-15 15:42:14.038 (  82.364s) [        F073E000]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-09-15 15:42:14.038 (  82.364s) [        F073E000]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-09-15 15:42:14.038 (  82.364s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:14.038 (  82.364s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:14.041 (  82.367s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:14.041 (  82.367s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:14.043 (  82.369s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:14.043 (  82.369s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:14.043 (  82.369s) [        F073E000]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:14.043 (  82.369s) [        F073E000]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:14.043 (  82.369s) [        F073E000]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:14.043 (  82.369s) [        F073E000]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:14.043 (  82.370s) [        F073E000] executable_instance.cc:138      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-09-15 15:42:14.043 (  82.370s) [        F073E000]loaded_executable_insta:497      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-09-15 15:42:14.043 (  82.370s) [        F073E000]loaded_executable_insta:81       1| LoadedExecutableInstance::Execute
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Device grid size = { 8, 8 }
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.all_gather"(%1, %0) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("p0.1")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("p0.1")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.mesh_shard"(%2, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main
2025-09-15 15:42:14.412 (  82.739s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:14.412 (  82.739s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:14.413 (  82.739s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:14.413 (  82.739s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:14.413 (  82.739s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:14.413 (  82.739s) [        F073E000]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:14.413 (  82.739s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:14.413 (  82.739s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:14.413 (  82.739s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:14.413 (  82.739s) [        F073E000]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:14.413 (  82.739s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:14.413 (  82.739s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:14.413 (  82.739s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:14.413 (  82.739s) [        F073E000] executable_instance.cc:58       1| ExecutableInstance::PJRT_Executable_Destroy
2025-09-15 15:42:14.413 (  82.739s) [        F073E000]loaded_executable_insta:432      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Destroy
2025-09-15 15:42:14.413 (  82.739s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:42:14.413 (  82.739s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:14.413 (  82.739s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:14.413 (  82.739s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:42:14.413 (  82.740s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:14.413 (  82.740s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:14.414 (  82.740s) [         AFFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:14.417 (  82.743s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:14.417 (  82.743s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:14.417 (  82.743s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:14.423 (  82.749s) [        F073E000]     client_instance.cc:428      1| ClientInstance::PJRT_Client_Compile
2025-09-15 15:42:14.423 (  82.749s) [        F073E000]      module_builder.cc:103      1| ModuleBuilder::buildModule
2025-09-15 15:42:14.423 (  82.750s) [        F073E000]      module_builder.cc:165      1| VHLO Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) {
    %0 = "vhlo.custom_call_v1"(%arg0) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"xla.sdy.FuncResultSharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<true>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {}, {}, {}]>]>">}>} : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>
    "vhlo.return_v1"(%0) : (!vhlo.tensor_v1<1x8x1024x128x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2,1,1]<=[2]}">}>]>, res_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, sym_visibility = #vhlo.string_v1<"">}
}
2025-09-15 15:42:14.425 (  82.751s) [        F073E000]      module_builder.cc:185      1| Is using shardy? true
2025-09-15 15:42:14.427 (  82.753s) [        F073E000]      module_builder.cc:203      1| SHLO Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}) -> (tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>}) {
    return %arg0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:14.427 (  82.754s) [        F073E000]      module_builder.cc:212      1| SHLO Module after frontend StableHLO pipeline:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}, {}, {}]>}) -> (tensor<1x8x1024x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>}) {
    return %arg0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:14.431 (  82.758s) [        F073E000]      module_builder.cc:486      1| SHLO Module after compiler StableHLO pipeline:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = sdy.manual_computation(%arg0) in_shardings=[<@mesh, [{}, {"_axis_0"}, {}, {}]>] out_shardings=[<@mesh, [{}, {}, {}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg1: tensor<1x4x1024x128xbf16>) {
      %1 = "stablehlo.all_gather"(%arg1) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> : (tensor<1x4x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
      sdy.return %1 : tensor<1x8x1024x128xbf16>
    } : (tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    return %0 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:14.434 (  82.760s) [        F073E000]      module_builder.cc:510      1| TTIR Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  func.func @main(%arg0: tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = ttir.empty() : tensor<1x4x1024x128xbf16>
    %1 = "ttir.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16>, tensor<1x4x1024x128xbf16>) -> tensor<1x4x1024x128xbf16>
    %2 = ttir.empty() : tensor<1x8x1024x128xbf16>
    %3 = "ttir.all_gather"(%1, %2) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<1x4x1024x128xbf16>, tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    %4 = ttir.empty() : tensor<1x8x1024x128xbf16>
    %5 = "ttir.mesh_shard"(%3, %4) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16>, tensor<1x8x1024x128xbf16>) -> tensor<1x8x1024x128xbf16>
    return %5 : tensor<1x8x1024x128xbf16>
  }
}
2025-09-15 15:42:14.434 (  82.760s) [        F073E000]      module_builder.cc:561   WARN| `mhlo.num_partitions` attribute not found, assuming default number of partitions: 1
2025-09-15 15:42:14.434 (  82.760s) [        F073E000]      module_builder.cc:575   WARN| `mhlo.num_replicas` attribute not found, assuming default number of replicas: 1
2025-09-15 15:42:14.434 (  82.760s) [        F073E000]      module_builder.cc:585   WARN| Num replicas and num partitions are not set, inferring the number of devices from mesh shape
2025-09-15 15:42:14.464 (  82.790s) [        F073E000]      module_builder.cc:630      1| TTNN Module:
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073185088, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073193408, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x16, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 2)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 2, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %2 = "ttnn.all_gather"(%1, %0) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.mesh_shard"(%2, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %3 : tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
    }
  }
}
2025-09-15 15:42:14.475 (  82.801s) [        F073E000]loaded_executable_insta:441      1| LoadedExecutableInstance::PJRT_LoadedExecutable_GetExecutable
2025-09-15 15:42:14.475 (  82.801s) [        F073E000]loaded_executable_insta:460      1| LoadedExecutableInstance::PJRT_LoadedExecutable_AddressableDevices
2025-09-15 15:42:14.476 (  82.802s) [        F073E000]              stubs.inc:76    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2025-09-15 15:42:14.476 (  82.802s) [        F073E000]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-09-15 15:42:14.476 (  82.802s) [        F073E000]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-09-15 15:42:14.476 (  82.802s) [        F073E000]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-09-15 15:42:14.476 (  82.802s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:14.476 (  82.802s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:14.479 (  82.805s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:14.479 (  82.805s) [        F073E000] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-09-15 15:42:14.481 (  82.807s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:14.481 (  82.807s) [        F073E000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-09-15 15:42:14.481 (  82.807s) [        F073E000]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:14.481 (  82.807s) [        F073E000]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:14.481 (  82.807s) [        F073E000]     buffer_instance.cc:532      1| BufferInstance::PJRT_Buffer_Device
2025-09-15 15:42:14.481 (  82.807s) [        F073E000]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-09-15 15:42:14.481 (  82.807s) [        F073E000] executable_instance.cc:138      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-09-15 15:42:14.481 (  82.807s) [        F073E000]loaded_executable_insta:497      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-09-15 15:42:14.481 (  82.807s) [        F073E000]loaded_executable_insta:81       1| LoadedExecutableInstance::Execute
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Device grid size = { 8, 8 }
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Starting execution of program: main
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2, 1, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.all_gather"(%1, %0) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("p0.1")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x4x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 1024 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("p0.1")
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.mesh_shard"(%2, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x8x1024x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 1024 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Finished execution of program: main
2025-09-15 15:42:14.759 (  83.085s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:14.759 (  83.085s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:14.759 (  83.085s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:14.759 (  83.086s) [        F073E000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:14.759 (  83.086s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:14.759 (  83.086s) [        F073E000]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:14.759 (  83.086s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:14.759 (  83.086s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:14.759 (  83.086s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:14.759 (  83.086s) [        F073E000]     buffer_instance.cc:437      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-09-15 15:42:14.759 (  83.086s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:14.759 (  83.086s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:14.759 (  83.086s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:14.759 (  83.086s) [        F073E000] executable_instance.cc:58       1| ExecutableInstance::PJRT_Executable_Destroy
2025-09-15 15:42:14.759 (  83.086s) [        F073E000]loaded_executable_insta:432      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Destroy
2025-09-15 15:42:14.759 (  83.086s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:42:14.759 (  83.086s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:14.759 (  83.086s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:14.760 (  83.086s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:42:14.760 (  83.086s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:14.760 (  83.086s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:14.761 (  83.087s) [         BFFF640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:14.764 (  83.090s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:14.764 (  83.090s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:14.764 (  83.090s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:14.764 (  83.090s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:14.764 (  83.091s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:14.764 (  83.091s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:14.764 (  83.091s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:42:14.764 (  83.091s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:14.764 (  83.091s) [        F073E000]     buffer_instance.cc:414      1| BufferInstance::PJRT_Buffer_Dimensions
2025-09-15 15:42:14.764 (  83.091s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:42:14.764 (  83.091s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:14.768 (  83.094s) [        10FF9640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:14.768 (  83.095s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:14.768 (  83.095s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:14.768 (  83.095s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:42:14.768 (  83.095s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:14.768 (  83.095s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:42:14.769 (  83.095s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-09-15 15:42:14.769 (  83.095s) [         BFFF640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:14.770 (  83.097s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:14.770 (  83.097s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:14.770 (  83.097s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:14.770 (  83.097s) [        F073E000]     buffer_instance.cc:480      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-09-15 15:42:14.770 (  83.097s) [        F073E000]     buffer_instance.cc:425      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-09-15 15:42:14.770 (  83.097s) [        F073E000]     buffer_instance.cc:406      1| BufferInstance::PJRT_Buffer_ElementType
2025-09-15 15:42:14.770 (  83.097s) [        F073E000]     buffer_instance.cc:447      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-09-15 15:42:14.771 (  83.097s) [        F073E000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User is requesting to copy the data from a runtime tensor with data type: Int32 into buffer with expected data type: Int64, the values will be casted, this may impact the throughput and the integrity of the data.
2025-09-15 15:42:14.771 (  83.097s) [        11FFB640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-09-15 15:42:14.771 (  83.097s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:14.771 (  83.097s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:14.800 (  83.126s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:14.800 (  83.126s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:14.800 (  83.126s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:14.800 (  83.126s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:14.800 (  83.126s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:14.800 (  83.127s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:14.800 (  83.127s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:14.800 (  83.127s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first

Args after sync (tensor([[279]], device='xla:0'), tensor([9], device='xla:0'), tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='xla:0',
       dtype=torch.bfloat16), tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='xla:0',
       dtype=torch.bfloat16))
CausalLMOutputWithPast(loss=None, logits=tensor([[[ 2.6250,  1.1406, -0.7148,  ..., -3.5156, -3.5156, -3.5156]]],
       device='xla:0', dtype=torch.bfloat16), past_key_values=<transformers.cache_utils.StaticCache object at 0x7f24ac6c3e50>, hidden_states=None, attentions=None)
Generated token:  the
output tokens: [' the', ' the', ' the', ' the']
2025-09-15 15:42:15.845 (  84.171s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.845 (  84.172s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.845 (  84.172s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.846 (  84.172s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.846 (  84.172s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.846 (  84.172s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.846 (  84.172s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.846 (  84.173s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.846 (  84.173s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.846 (  84.173s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.846 (  84.173s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.846 (  84.173s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.847 (  84.173s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.847 (  84.173s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.847 (  84.173s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.847 (  84.173s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.847 (  84.174s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.847 (  84.174s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.847 (  84.174s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.848 (  84.174s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.848 (  84.174s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.848 (  84.174s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.848 (  84.174s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.848 (  84.174s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.848 (  84.174s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.848 (  84.174s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.848 (  84.175s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.848 (  84.175s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.848 (  84.175s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.848 (  84.175s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.848 (  84.175s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.849 (  84.175s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.849 (  84.175s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.849 (  84.175s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.849 (  84.175s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.849 (  84.175s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.849 (  84.175s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.849 (  84.176s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.849 (  84.176s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.849 (  84.176s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.849 (  84.176s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.849 (  84.176s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.850 (  84.176s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.850 (  84.176s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.850 (  84.176s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.850 (  84.176s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.850 (  84.176s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.850 (  84.176s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.850 (  84.176s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.850 (  84.177s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.850 (  84.177s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.850 (  84.177s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.851 (  84.177s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.851 (  84.177s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.851 (  84.177s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.851 (  84.177s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.851 (  84.177s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.851 (  84.178s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.851 (  84.178s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.851 (  84.178s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.851 (  84.178s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.851 (  84.178s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.851 (  84.178s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.852 (  84.178s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.852 (  84.178s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.852 (  84.178s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.852 (  84.178s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.852 (  84.178s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.852 (  84.178s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.852 (  84.178s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.852 (  84.179s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.852 (  84.179s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.852 (  84.179s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.852 (  84.179s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.853 (  84.179s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.853 (  84.179s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.853 (  84.179s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.853 (  84.179s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.853 (  84.179s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.853 (  84.179s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.853 (  84.179s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.853 (  84.179s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.853 (  84.180s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.853 (  84.180s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.853 (  84.180s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.853 (  84.180s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.853 (  84.180s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.854 (  84.180s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.854 (  84.180s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.854 (  84.180s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.854 (  84.180s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.854 (  84.180s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.854 (  84.180s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.854 (  84.181s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.854 (  84.181s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.854 (  84.181s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.854 (  84.181s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.854 (  84.181s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.855 (  84.181s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.855 (  84.181s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.855 (  84.181s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.855 (  84.181s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.855 (  84.181s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.855 (  84.181s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.855 (  84.181s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.855 (  84.182s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.855 (  84.182s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.855 (  84.182s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.856 (  84.182s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.856 (  84.182s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.856 (  84.182s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.856 (  84.182s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.856 (  84.182s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.856 (  84.182s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.856 (  84.183s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.856 (  84.183s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.856 (  84.183s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.856 (  84.183s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.856 (  84.183s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.857 (  84.183s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.857 (  84.183s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.857 (  84.183s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.857 (  84.183s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.857 (  84.183s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.857 (  84.183s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.857 (  84.183s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.857 (  84.184s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.857 (  84.184s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.857 (  84.184s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.857 (  84.184s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.858 (  84.184s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.858 (  84.184s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.858 (  84.184s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.858 (  84.184s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.858 (  84.184s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.858 (  84.184s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.858 (  84.184s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.858 (  84.185s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.858 (  84.185s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.858 (  84.185s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.858 (  84.185s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.858 (  84.185s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.859 (  84.185s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.859 (  84.185s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.859 (  84.185s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.859 (  84.185s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.859 (  84.185s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.859 (  84.185s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.859 (  84.186s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.859 (  84.186s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.859 (  84.186s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.859 (  84.186s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.859 (  84.186s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.860 (  84.186s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.860 (  84.186s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.860 (  84.186s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.860 (  84.186s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.860 (  84.186s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.860 (  84.186s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.860 (  84.187s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.860 (  84.187s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.861 (  84.187s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.861 (  84.187s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.861 (  84.187s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.861 (  84.187s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.861 (  84.187s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.861 (  84.188s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.861 (  84.188s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.861 (  84.188s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.861 (  84.188s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.862 (  84.188s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.862 (  84.188s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.862 (  84.188s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.862 (  84.188s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.862 (  84.188s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.862 (  84.188s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.862 (  84.188s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.862 (  84.189s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.862 (  84.189s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.862 (  84.189s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.862 (  84.189s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.863 (  84.189s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.863 (  84.189s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.863 (  84.189s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.863 (  84.189s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.863 (  84.189s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.863 (  84.189s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.863 (  84.189s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.863 (  84.190s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.863 (  84.190s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.863 (  84.190s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.863 (  84.190s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.863 (  84.190s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.864 (  84.190s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.864 (  84.190s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.864 (  84.190s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.864 (  84.190s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.864 (  84.190s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.864 (  84.190s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.864 (  84.191s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.864 (  84.191s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.865 (  84.191s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.865 (  84.191s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.865 (  84.191s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.865 (  84.191s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.865 (  84.191s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.865 (  84.191s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.865 (  84.191s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.865 (  84.192s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.865 (  84.192s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.865 (  84.192s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.865 (  84.192s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.866 (  84.192s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.866 (  84.192s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.866 (  84.192s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.866 (  84.192s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.866 (  84.192s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.866 (  84.192s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.866 (  84.193s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.866 (  84.193s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.866 (  84.193s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.866 (  84.193s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.866 (  84.193s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.867 (  84.193s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.867 (  84.193s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.867 (  84.193s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.867 (  84.193s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.867 (  84.193s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.867 (  84.193s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.867 (  84.193s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.867 (  84.194s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.867 (  84.194s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.867 (  84.194s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.867 (  84.194s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.868 (  84.194s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.868 (  84.194s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.868 (  84.194s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.868 (  84.194s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.868 (  84.194s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.868 (  84.194s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.868 (  84.194s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.868 (  84.195s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.868 (  84.195s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.868 (  84.195s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.868 (  84.195s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.868 (  84.195s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.869 (  84.195s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.869 (  84.195s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.869 (  84.195s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.869 (  84.195s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.869 (  84.195s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.869 (  84.195s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.869 (  84.196s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.869 (  84.196s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.869 (  84.196s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.870 (  84.196s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.870 (  84.196s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.870 (  84.196s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.870 (  84.196s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.870 (  84.196s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.870 (  84.196s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.870 (  84.196s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.870 (  84.197s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.870 (  84.197s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.870 (  84.197s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.870 (  84.197s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.870 (  84.197s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.871 (  84.197s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.871 (  84.197s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.871 (  84.197s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.871 (  84.197s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.871 (  84.197s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.871 (  84.197s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.871 (  84.197s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.871 (  84.198s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.871 (  84.198s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.871 (  84.198s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.871 (  84.198s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.872 (  84.198s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.872 (  84.198s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.872 (  84.198s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.872 (  84.198s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.872 (  84.198s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.872 (  84.198s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.872 (  84.198s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.872 (  84.199s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.872 (  84.199s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.872 (  84.199s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.872 (  84.199s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.873 (  84.199s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.873 (  84.199s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.873 (  84.199s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.873 (  84.199s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.873 (  84.199s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.873 (  84.200s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.873 (  84.200s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.873 (  84.200s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.874 (  84.200s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.874 (  84.200s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.874 (  84.200s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.874 (  84.200s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.874 (  84.200s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.874 (  84.200s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.874 (  84.201s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.874 (  84.201s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.874 (  84.201s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.874 (  84.201s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.875 (  84.201s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.875 (  84.201s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.875 (  84.201s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.875 (  84.201s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.875 (  84.201s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.875 (  84.201s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.875 (  84.201s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.875 (  84.202s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.875 (  84.202s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.875 (  84.202s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.875 (  84.202s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.875 (  84.202s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.875 (  84.202s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.876 (  84.202s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.876 (  84.202s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.876 (  84.202s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.876 (  84.202s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.876 (  84.202s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.876 (  84.203s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.876 (  84.203s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.876 (  84.203s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.876 (  84.203s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.877 (  84.203s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.877 (  84.203s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.877 (  84.203s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.877 (  84.203s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.877 (  84.203s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.877 (  84.203s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.877 (  84.203s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.877 (  84.203s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.877 (  84.204s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.877 (  84.204s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.877 (  84.204s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.877 (  84.204s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.878 (  84.204s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.878 (  84.204s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.878 (  84.204s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.878 (  84.204s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.878 (  84.204s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.878 (  84.204s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.878 (  84.204s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.878 (  84.204s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.878 (  84.205s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.878 (  84.205s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.878 (  84.205s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.878 (  84.205s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.879 (  84.205s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.879 (  84.205s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.879 (  84.205s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.879 (  84.205s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.879 (  84.205s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.879 (  84.205s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.879 (  84.205s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.879 (  84.206s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.879 (  84.206s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.879 (  84.206s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.879 (  84.206s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.879 (  84.206s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.880 (  84.206s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.880 (  84.206s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.880 (  84.206s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.880 (  84.206s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.880 (  84.206s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.880 (  84.206s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.880 (  84.206s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.880 (  84.207s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.880 (  84.207s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.880 (  84.207s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.880 (  84.207s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.881 (  84.207s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.881 (  84.207s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.881 (  84.208s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.881 (  84.208s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.881 (  84.208s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.881 (  84.208s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.882 (  84.208s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.882 (  84.208s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.882 (  84.208s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.882 (  84.208s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.882 (  84.208s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.882 (  84.208s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.882 (  84.208s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.882 (  84.209s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.882 (  84.209s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.882 (  84.209s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.882 (  84.209s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.883 (  84.209s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.883 (  84.209s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.883 (  84.209s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.883 (  84.209s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.883 (  84.209s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.883 (  84.209s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.883 (  84.210s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.883 (  84.210s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.883 (  84.210s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.884 (  84.210s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.884 (  84.210s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.884 (  84.210s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.884 (  84.210s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.884 (  84.210s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.884 (  84.210s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.884 (  84.210s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.884 (  84.211s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.884 (  84.211s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.885 (  84.211s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.885 (  84.211s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.885 (  84.212s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.885 (  84.212s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.885 (  84.212s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.886 (  84.212s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.886 (  84.212s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.886 (  84.212s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.886 (  84.212s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.886 (  84.212s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.886 (  84.212s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.886 (  84.212s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.886 (  84.213s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.886 (  84.213s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.886 (  84.213s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.887 (  84.213s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.887 (  84.213s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.887 (  84.213s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.887 (  84.213s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.887 (  84.213s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.887 (  84.213s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.887 (  84.214s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.887 (  84.214s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.887 (  84.214s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.887 (  84.214s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.888 (  84.214s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.888 (  84.214s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.888 (  84.214s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.888 (  84.214s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.888 (  84.214s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.888 (  84.214s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.889 (  84.215s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.889 (  84.215s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.889 (  84.215s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.889 (  84.215s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.889 (  84.215s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.889 (  84.215s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.889 (  84.216s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.889 (  84.216s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.889 (  84.216s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.889 (  84.216s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.890 (  84.216s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.890 (  84.216s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.890 (  84.216s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.890 (  84.216s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.890 (  84.216s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.890 (  84.216s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.890 (  84.217s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.890 (  84.217s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.890 (  84.217s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.890 (  84.217s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.890 (  84.217s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.891 (  84.217s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.891 (  84.217s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.891 (  84.217s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.891 (  84.217s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.891 (  84.217s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.891 (  84.218s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.891 (  84.218s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.891 (  84.218s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.892 (  84.218s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.892 (  84.218s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.892 (  84.218s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.892 (  84.218s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.892 (  84.218s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.892 (  84.218s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.892 (  84.219s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.892 (  84.219s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.892 (  84.219s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.892 (  84.219s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.901 (  84.227s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.901 (  84.227s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.901 (  84.227s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.901 (  84.228s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.901 (  84.228s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.901 (  84.228s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:15.957 (  84.284s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:16.004 (  84.330s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:16.004 (  84.330s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:16.004 (  84.331s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:16.005 (  84.331s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:16.005 (  84.331s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:16.005 (  84.331s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:16.005 (  84.331s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:16.005 (  84.332s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:16.005 (  84.332s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:16.061 (  84.387s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:16.118 (  84.444s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:16.118 (  84.445s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:16.118 (  84.445s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:16.119 (  84.445s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:16.119 (  84.445s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:16.119 (  84.445s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:16.119 (  84.445s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:16.119 (  84.445s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:16.119 (  84.445s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:16.119 (  84.446s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:16.119 (  84.446s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:16.119 (  84.446s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:16.120 (  84.446s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
2025-09-15 15:42:16.120 (  84.446s) [        F073E000]     buffer_instance.cc:398      1| BufferInstance::PJRT_Buffer_Destroy
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Tensor is retained thus not deallocating. To deallocate, set retain to false first
Merging ToLayoutOp into ttnn.constant
Merging ToLayoutOp into ttnn.constant
Merging ToLayoutOp into ttnn.constant
