WARNING:root:Defaulting to PJRT_DEVICE=CPU
2025-11-26 19:57:55.838 (   0.000s) [        E0290480]   plugin_attributes.cc:58       1| PluginAttributes::PJRT_Plugin_Initialize
2025-11-26 19:57:55.838 (   0.000s) [        E0290480]     client_instance.cc:613      1| ClientInstance::PJRT_Client_Create
2025-11-26 19:57:55.846 (   0.007s) [        E0290480]     client_instance.cc:152      1| ClientInstance::ClientInstance
2025-11-26 19:57:55.846 (   0.008s) [        E0290480]     client_instance.cc:173      1| ClientInstance::Initialize
2025-11-26 19:57:57.826 (   1.987s) [        E0290480]              stubs.inc:103   WARN| STUB: PJRT_Client_TopologyDescription
2025-11-26 19:57:57.826 (   1.987s) [        E0290480]      error_instance.cc:52       1| ErrorInstance::PJRT_Error_Message
2025-11-26 19:57:57.826 (   1.987s) [        E0290480]      error_instance.cc:61       1| ErrorInstance::PJRT_Error_GetCode
2025-11-26 19:57:57.826 (   1.987s) [        E0290480]      error_instance.cc:46       1| ErrorInstance::PJRT_Error_Destroy
2025-11-26 19:57:57.826 (   1.987s) [        E0290480]     client_instance.cc:667      1| ClientInstance::PJRT_Client_PlatformVersion
2025-11-26 19:57:57.826 (   1.987s) [        E0290480]     client_instance.cc:648      1| ClientInstance::PJRT_Client_PlatformName
2025-11-26 19:57:57.826 (   1.987s) [        E0290480]     client_instance.cc:678      1| ClientInstance::PJRT_Client_Devices
2025-11-26 19:57:57.826 (   1.987s) [        E0290480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-11-26 19:57:57.826 (   1.987s) [        E0290480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]     client_instance.cc:691      1| ClientInstance::PJRT_Client_AddressableDevices
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]     client_instance.cc:741      1| ClientInstance::PJRT_Client_AddressableMemories
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]   plugin_attributes.cc:64       1| PluginAttributes::PJRT_Plugin_Attributes
2025-11-26 19:57:57.826937: W torch_xla/csrc/runtime/profiler.cpp:88] Profiler API not found for PJRT plugin
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.826 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.827 (   1.988s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.835 (   1.996s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.835 (   1.996s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.835 (   1.996s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.835 (   1.996s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.835 (   1.996s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.835 (   1.996s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.835 (   1.996s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.835 (   1.996s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.835 (   1.997s) [        E0290480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-11-26 19:57:57.835 (   1.997s) [        E0290480]     client_instance.cc:797      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-11-26 19:57:57.835 (   1.997s) [        E0290480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]     client_instance.cc:797      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]     client_instance.cc:797      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]     client_instance.cc:797      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]     client_instance.cc:797      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]     client_instance.cc:797      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]     client_instance.cc:797      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]     client_instance.cc:797      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-11-26 19:57:57.836 (   1.997s) [        E0290480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-11-26 19:57:57.842 (   2.003s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.842 (   2.003s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.842 (   2.003s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.842 (   2.003s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.842 (   2.003s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.842 (   2.003s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.842 (   2.003s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.842 (   2.003s) [        E0290480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:57.842 (   2.003s) [        E0290480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-11-26 19:57:57.842 (   2.003s) [        E0290480]     client_instance.cc:797      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-11-26 19:57:57.842 (   2.003s) [        E0290480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-11-26 19:57:57.842 (   2.004s) [        E0290480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-11-26 19:57:57.842 (   2.004s) [        E0290480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-11-26 19:57:57.842 (   2.004s) [        E0290480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-11-26 19:57:57.842 (   2.004s) [        E0290480]     client_instance.cc:797      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-11-26 19:57:57.842 (   2.004s) [        E0290480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-11-26 19:57:57.842 (   2.004s) [        E0290480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-11-26 19:57:57.842 (   2.004s) [        E0290480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-11-26 19:57:57.842 (   2.004s) [        E0290480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-11-26 19:57:57.842 (   2.004s) [        E0290480]     client_instance.cc:797      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-11-26 19:57:57.842 (   2.004s) [        E0290480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-11-26 19:57:57.842 (   2.004s) [        E0290480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-11-26 19:57:57.842 (   2.004s) [        E0290480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-11-26 19:57:57.842 (   2.004s) [        E0290480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-11-26 19:57:57.842 (   2.004s) [        E0290480]     client_instance.cc:797      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-11-26 19:57:57.842 (   2.004s) [        E0290480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-11-26 19:57:57.842 (   2.004s) [        E0290480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-11-26 19:57:57.842 (   2.004s) [        E0290480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-11-26 19:57:57.843 (   2.004s) [        E0290480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-11-26 19:57:57.843 (   2.004s) [        E0290480]     client_instance.cc:797      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-11-26 19:57:57.843 (   2.004s) [        E0290480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-11-26 19:57:57.843 (   2.004s) [        E0290480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-11-26 19:57:57.843 (   2.004s) [        E0290480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-11-26 19:57:57.843 (   2.004s) [        E0290480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-11-26 19:57:57.843 (   2.004s) [        E0290480]     client_instance.cc:797      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-11-26 19:57:57.843 (   2.004s) [        E0290480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-11-26 19:57:57.843 (   2.004s) [        E0290480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-11-26 19:57:57.843 (   2.004s) [        E0290480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-11-26 19:57:57.843 (   2.004s) [        E0290480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-11-26 19:57:57.843 (   2.004s) [        E0290480]     client_instance.cc:797      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-11-26 19:57:57.843 (   2.004s) [        E0290480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-11-26 19:57:57.843 (   2.004s) [        E0290480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-11-26 19:57:57.843 (   2.004s) [        E0290480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-11-26 19:57:57.843 (   2.004s) [        E0290480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-11-26 19:57:57.843 (   2.004s) [        E0290480]     client_instance.cc:797      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-11-26 19:57:57.843 (   2.004s) [        E0290480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-11-26 19:57:57.843 (   2.004s) [        E0290480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-11-26 19:57:57.843 (   2.004s) [        E0290480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-11-26 19:57:58.030 (   2.191s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.030 (   2.191s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.030 (   2.191s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.030 (   2.191s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.030 (   2.191s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.030 (   2.191s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.030 (   2.191s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.030 (   2.191s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.030 (   2.191s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.030 (   2.191s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.030 (   2.191s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.030 (   2.191s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.030 (   2.191s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.030 (   2.191s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.030 (   2.191s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.030 (   2.191s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.030 (   2.191s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.030 (   2.192s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.030 (   2.192s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.030 (   2.192s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.036 (   2.198s) [        E0290480]     client_instance.cc:754      1| ClientInstance::PJRT_Client_Compile
2025-11-26 19:57:58.037 (   2.198s) [        E0290480]      module_builder.cc:229      1| ModuleBuilder::buildModule
2025-11-26 19:57:58.038 (   2.199s) [        E0290480]      module_builder.cc:983      1| MLIR Module vhlo:
#loc1 = loc("p0.2")
#loc2 = loc("p1.8")
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<32x16x!vhlo.f32_v1> loc("p0.2"), %arg1: !vhlo.tensor_v1<32x32x!vhlo.f32_v1> loc("p1.8")) -> (!vhlo.tensor_v1<32x16x!vhlo.f32_v1>, !vhlo.tensor_v1<32x16x!vhlo.f32_v1>, !vhlo.tensor_v1<32x16x!vhlo.f32_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.broadcast_in_dim_v1"(%0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x16x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<32x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x16x!vhlo.f32_v1> loc(#loc3)
    %3 = "vhlo.custom_call_v1"(%2) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_0">}>} : (!vhlo.tensor_v1<1x32x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x16x!vhlo.f32_v1> loc(#loc4)
    %4 = "vhlo.reshape_v1"(%3) : (!vhlo.tensor_v1<1x32x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x16x!vhlo.f32_v1> loc(#loc5)
    %5 = "vhlo.multiply_v1"(%4, %1) {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>">}>, mhlo.sharding = #vhlo.string_v1<"{devices=[1,8]<=[8]}">} : (!vhlo.tensor_v1<32x16x!vhlo.f32_v1>, !vhlo.tensor_v1<32x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x16x!vhlo.f32_v1> loc(#loc6)
    %6 = "vhlo.reshape_v1"(%arg1) : (!vhlo.tensor_v1<32x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x32x!vhlo.f32_v1> loc(#loc7)
    %7 = "vhlo.custom_call_v1"(%6) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___x1">}>} : (!vhlo.tensor_v1<1x32x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x32x!vhlo.f32_v1> loc(#loc8)
    %8 = "vhlo.reshape_v1"(%7) : (!vhlo.tensor_v1<1x32x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.f32_v1> loc(#loc9)
    %9 = "vhlo.dot_general_v2"(%8, %5) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x32x!vhlo.f32_v1>, !vhlo.tensor_v1<32x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x16x!vhlo.f32_v1> loc(#loc10)
    "vhlo.return_v1"(%5, %5, %9) : (!vhlo.tensor_v1<32x16x!vhlo.f32_v1>, !vhlo.tensor_v1<32x16x!vhlo.f32_v1>, !vhlo.tensor_v1<32x16x!vhlo.f32_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">} loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc3 = loc("reshape.3")
#loc4 = loc("custom-call.4")
#loc5 = loc("reshape.5")
#loc6 = loc("multiply.7")
#loc7 = loc("reshape.9")
#loc8 = loc("custom-call.10")
#loc9 = loc("reshape.11")
#loc10 = loc("dot.12")
------------------ END OF MLIR MODULE ------------------
// -----// IR Dump Before VhloToVersionPass (vhlo-to-version) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<32x16x!vhlo.f32_v1>, %arg1: !vhlo.tensor_v1<32x32x!vhlo.f32_v1>) -> (!vhlo.tensor_v1<32x16x!vhlo.f32_v1>, !vhlo.tensor_v1<32x16x!vhlo.f32_v1>, !vhlo.tensor_v1<32x16x!vhlo.f32_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %1 = "vhlo.broadcast_in_dim_v1"(%0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x16x!vhlo.f32_v1>
    %2 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<32x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x16x!vhlo.f32_v1>
    %3 = "vhlo.custom_call_v1"(%2) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_0">}>} : (!vhlo.tensor_v1<1x32x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x16x!vhlo.f32_v1>
    %4 = "vhlo.reshape_v1"(%3) : (!vhlo.tensor_v1<1x32x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x16x!vhlo.f32_v1>
    %5 = "vhlo.multiply_v1"(%4, %1) {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>">}>, mhlo.sharding = #vhlo.string_v1<"{devices=[1,8]<=[8]}">} : (!vhlo.tensor_v1<32x16x!vhlo.f32_v1>, !vhlo.tensor_v1<32x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x16x!vhlo.f32_v1>
    %6 = "vhlo.reshape_v1"(%arg1) : (!vhlo.tensor_v1<32x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x32x!vhlo.f32_v1>
    %7 = "vhlo.custom_call_v1"(%6) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___x1">}>} : (!vhlo.tensor_v1<1x32x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x32x!vhlo.f32_v1>
    %8 = "vhlo.reshape_v1"(%7) : (!vhlo.tensor_v1<1x32x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.f32_v1>
    %9 = "vhlo.dot_general_v2"(%8, %5) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x32x!vhlo.f32_v1>, !vhlo.tensor_v1<32x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x16x!vhlo.f32_v1>
    "vhlo.return_v1"(%5, %5, %9) : (!vhlo.tensor_v1<32x16x!vhlo.f32_v1>, !vhlo.tensor_v1<32x16x!vhlo.f32_v1>, !vhlo.tensor_v1<32x16x!vhlo.f32_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}


// -----// IR Dump Before VhloLegalizeToStablehloPass (vhlo-legalize-to-stablehlo) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<32x16x!vhlo.f32_v1>, %arg1: !vhlo.tensor_v1<32x32x!vhlo.f32_v1>) -> (!vhlo.tensor_v1<32x16x!vhlo.f32_v1>, !vhlo.tensor_v1<32x16x!vhlo.f32_v1>, !vhlo.tensor_v1<32x16x!vhlo.f32_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %1 = "vhlo.broadcast_in_dim_v1"(%0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x16x!vhlo.f32_v1>
    %2 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<32x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x16x!vhlo.f32_v1>
    %3 = "vhlo.custom_call_v1"(%2) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_0">}>} : (!vhlo.tensor_v1<1x32x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x16x!vhlo.f32_v1>
    %4 = "vhlo.reshape_v1"(%3) : (!vhlo.tensor_v1<1x32x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x16x!vhlo.f32_v1>
    %5 = "vhlo.multiply_v1"(%4, %1) {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>">}>, mhlo.sharding = #vhlo.string_v1<"{devices=[1,8]<=[8]}">} : (!vhlo.tensor_v1<32x16x!vhlo.f32_v1>, !vhlo.tensor_v1<32x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x16x!vhlo.f32_v1>
    %6 = "vhlo.reshape_v1"(%arg1) : (!vhlo.tensor_v1<32x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x32x!vhlo.f32_v1>
    %7 = "vhlo.custom_call_v1"(%6) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___x1">}>} : (!vhlo.tensor_v1<1x32x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x32x!vhlo.f32_v1>
    %8 = "vhlo.reshape_v1"(%7) : (!vhlo.tensor_v1<1x32x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.f32_v1>
    %9 = "vhlo.dot_general_v2"(%8, %5) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x32x!vhlo.f32_v1>, !vhlo.tensor_v1<32x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x16x!vhlo.f32_v1>
    "vhlo.return_v1"(%5, %5, %9) : (!vhlo.tensor_v1<32x16x!vhlo.f32_v1>, !vhlo.tensor_v1<32x16x!vhlo.f32_v1>, !vhlo.tensor_v1<32x16x!vhlo.f32_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}


// -----// IR Dump After VhloLegalizeToStablehloPass (vhlo-legalize-to-stablehlo) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<32x16xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"}, %arg1: tensor<32x32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}) -> (tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>) {
    %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x16xf32>
    %1 = stablehlo.reshape %arg0 : (tensor<32x16xf32>) -> tensor<1x32x16xf32>
    %2 = stablehlo.custom_call @tt.mark_argument(%1) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_0"}} : (tensor<1x32x16xf32>) -> tensor<1x32x16xf32>
    %3 = stablehlo.reshape %2 : (tensor<1x32x16xf32>) -> tensor<32x16xf32>
    %4 = stablehlo.multiply %3, %0 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"} : tensor<32x16xf32>
    %5 = stablehlo.reshape %arg1 : (tensor<32x32xf32>) -> tensor<1x32x32xf32>
    %6 = stablehlo.custom_call @tt.mark_argument(%5) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___x1"}} : (tensor<1x32x32xf32>) -> tensor<1x32x32xf32>
    %7 = stablehlo.reshape %6 : (tensor<1x32x32xf32>) -> tensor<32x32xf32>
    %8 = stablehlo.dot_general %7, %4, contracting_dims = [1] x [0] : (tensor<32x32xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
    return %4, %4, %8 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


2025-11-26 19:57:58.043 (   2.204s) [        E0290480]      module_builder.cc:983      1| MLIR Module shlo:
#loc1 = loc("p0.2")
#loc2 = loc("p1.8")
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<32x16xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"} loc("p0.2"), %arg1: tensor<32x32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"} loc("p1.8")) -> (tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>) {
    %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x16xf32> loc(#loc)
    %1 = stablehlo.reshape %arg0 : (tensor<32x16xf32>) -> tensor<1x32x16xf32> loc(#loc3)
    %2 = stablehlo.custom_call @tt.mark_argument(%1) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_0"}} : (tensor<1x32x16xf32>) -> tensor<1x32x16xf32> loc(#loc4)
    %3 = stablehlo.reshape %2 : (tensor<1x32x16xf32>) -> tensor<32x16xf32> loc(#loc5)
    %4 = stablehlo.multiply %3, %0 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"} : tensor<32x16xf32> loc(#loc6)
    %5 = stablehlo.reshape %arg1 : (tensor<32x32xf32>) -> tensor<1x32x32xf32> loc(#loc7)
    %6 = stablehlo.custom_call @tt.mark_argument(%5) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___x1"}} : (tensor<1x32x32xf32>) -> tensor<1x32x32xf32> loc(#loc8)
    %7 = stablehlo.reshape %6 : (tensor<1x32x32xf32>) -> tensor<32x32xf32> loc(#loc9)
    %8 = stablehlo.dot_general %7, %4, contracting_dims = [1] x [0] : (tensor<32x32xf32>, tensor<32x16xf32>) -> tensor<32x16xf32> loc(#loc10)
    return %4, %4, %8 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc3 = loc("reshape.3")
#loc4 = loc("custom-call.4")
#loc5 = loc("reshape.5")
#loc6 = loc("multiply.7")
#loc7 = loc("reshape.9")
#loc8 = loc("custom-call.10")
#loc9 = loc("reshape.11")
#loc10 = loc("dot.12")
------------------ END OF MLIR MODULE ------------------
2025-11-26 19:57:58.044 (   2.206s) [        E0290480]      module_builder.cc:983      1| MLIR Module shlo_frontend:
#loc1 = loc("p0.2")
#loc2 = loc("p1.8")
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<32x16xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"} loc("p0.2"), %arg1: tensor<32x32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___x1"} loc("p1.8")) -> (tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>) {
    %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x16xf32> loc(#loc)
    %1 = stablehlo.reshape %arg0 : (tensor<32x16xf32>) -> tensor<1x32x16xf32> loc(#loc3)
    %2 = stablehlo.reshape %1 : (tensor<1x32x16xf32>) -> tensor<32x16xf32> loc(#loc4)
    %3 = stablehlo.multiply %2, %0 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"} : tensor<32x16xf32> loc(#loc5)
    %4 = stablehlo.reshape %arg1 : (tensor<32x32xf32>) -> tensor<1x32x32xf32> loc(#loc6)
    %5 = stablehlo.reshape %4 : (tensor<1x32x32xf32>) -> tensor<32x32xf32> loc(#loc7)
    %6 = stablehlo.dot_general %5, %3, contracting_dims = [1] x [0] : (tensor<32x32xf32>, tensor<32x16xf32>) -> tensor<32x16xf32> loc(#loc8)
    return %3, %3, %6 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc3 = loc("reshape.3")
#loc4 = loc("reshape.5")
#loc5 = loc("multiply.7")
#loc6 = loc("reshape.9")
#loc7 = loc("reshape.11")
#loc8 = loc("dot.12")
------------------ END OF MLIR MODULE ------------------
// -----// IR Dump Before Inliner (inline) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<32x16xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>) {
    %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x16xf32>
    %1 = stablehlo.reshape %arg0 : (tensor<32x16xf32>) -> tensor<1x32x16xf32>
    %2 = stablehlo.reshape %1 : (tensor<1x32x16xf32>) -> tensor<32x16xf32>
    %3 = stablehlo.multiply %2, %0 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"} : tensor<32x16xf32>
    %4 = stablehlo.reshape %arg1 : (tensor<32x32xf32>) -> tensor<1x32x32xf32>
    %5 = stablehlo.reshape %4 : (tensor<1x32x32xf32>) -> tensor<32x32xf32>
    %6 = stablehlo.dot_general %5, %3, contracting_dims = [1] x [0] : (tensor<32x32xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
    return %3, %3, %6 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<32x16xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>) {
    %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x16xf32>
    %1 = stablehlo.reshape %arg0 : (tensor<32x16xf32>) -> tensor<1x32x16xf32>
    %2 = stablehlo.reshape %1 : (tensor<1x32x16xf32>) -> tensor<32x16xf32>
    %3 = stablehlo.multiply %2, %0 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"} : tensor<32x16xf32>
    %4 = stablehlo.reshape %arg1 : (tensor<32x32xf32>) -> tensor<1x32x32xf32>
    %5 = stablehlo.reshape %4 : (tensor<1x32x32xf32>) -> tensor<32x32xf32>
    %6 = stablehlo.dot_general %5, %3, contracting_dims = [1] x [0] : (tensor<32x32xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
    return %3, %3, %6 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump Before TTPopulateArgumentTypes (tt-populate-argument-types) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<32x16xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>) {
    %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x16xf32>
    %1 = stablehlo.reshape %arg0 : (tensor<32x16xf32>) -> tensor<1x32x16xf32>
    %2 = stablehlo.reshape %1 : (tensor<1x32x16xf32>) -> tensor<32x16xf32>
    %3 = stablehlo.multiply %2, %0 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"} : tensor<32x16xf32>
    %4 = stablehlo.reshape %arg1 : (tensor<32x32xf32>) -> tensor<1x32x32xf32>
    %5 = stablehlo.reshape %4 : (tensor<1x32x32xf32>) -> tensor<32x32xf32>
    %6 = stablehlo.dot_general %5, %3, contracting_dims = [1] x [0] : (tensor<32x32xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
    return %3, %3, %6 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump Before ApplyArgumentShardStatusPass (apply-argument-shard-status) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<32x16xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>) {
    %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x16xf32>
    %1 = stablehlo.reshape %arg0 : (tensor<32x16xf32>) -> tensor<1x32x16xf32>
    %2 = stablehlo.reshape %1 : (tensor<1x32x16xf32>) -> tensor<32x16xf32>
    %3 = stablehlo.multiply %2, %0 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"} : tensor<32x16xf32>
    %4 = stablehlo.reshape %arg1 : (tensor<32x32xf32>) -> tensor<1x32x32xf32>
    %5 = stablehlo.reshape %4 : (tensor<1x32x32xf32>) -> tensor<32x32xf32>
    %6 = stablehlo.dot_general %5, %3, contracting_dims = [1] x [0] : (tensor<32x32xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
    return %3, %3, %6 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump After ApplyArgumentShardStatusPass (apply-argument-shard-status) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<32x16xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x16xf32>
    %1 = stablehlo.reshape %arg0 : (tensor<32x16xf32>) -> tensor<1x32x16xf32>
    %2 = stablehlo.reshape %1 : (tensor<1x32x16xf32>) -> tensor<32x16xf32>
    %3 = stablehlo.multiply %2, %0 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"} : tensor<32x16xf32>
    %4 = stablehlo.reshape %arg1 : (tensor<32x32xf32>) -> tensor<1x32x32xf32>
    %5 = stablehlo.reshape %4 : (tensor<1x32x32xf32>) -> tensor<32x32xf32>
    %6 = stablehlo.dot_general %5, %3, contracting_dims = [1] x [0] : (tensor<32x32xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
    return %3, %3, %6 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump Before ConvertXlaSdyToSdyPass (convert-xla-sdy-to-sdy) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<32x16xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x16xf32>
    %1 = stablehlo.reshape %arg0 : (tensor<32x16xf32>) -> tensor<1x32x16xf32>
    %2 = stablehlo.reshape %1 : (tensor<1x32x16xf32>) -> tensor<32x16xf32>
    %3 = stablehlo.multiply %2, %0 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"} : tensor<32x16xf32>
    %4 = stablehlo.reshape %arg1 : (tensor<32x32xf32>) -> tensor<1x32x32xf32>
    %5 = stablehlo.reshape %4 : (tensor<1x32x32xf32>) -> tensor<32x32xf32>
    %6 = stablehlo.dot_general %5, %3, contracting_dims = [1] x [0] : (tensor<32x32xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
    return %3, %3, %6 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump After ConvertXlaSdyToSdyPass (convert-xla-sdy-to-sdy) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x16xf32>
    %1 = stablehlo.reshape %arg0 : (tensor<32x16xf32>) -> tensor<1x32x16xf32>
    %2 = stablehlo.reshape %1 : (tensor<1x32x16xf32>) -> tensor<32x16xf32>
    %3 = stablehlo.multiply %2, %0 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"} : tensor<32x16xf32>
    %4 = stablehlo.reshape %arg1 : (tensor<32x32xf32>) -> tensor<1x32x32xf32>
    %5 = stablehlo.reshape %4 : (tensor<1x32x32xf32>) -> tensor<32x32xf32>
    %6 = stablehlo.dot_general %5, %3, contracting_dims = [1] x [0] : (tensor<32x32xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
    return %3, %3, %6 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump Before AnalyzeMeshPass (analyze-mesh) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x16xf32>
    %1 = stablehlo.reshape %arg0 : (tensor<32x16xf32>) -> tensor<1x32x16xf32>
    %2 = stablehlo.reshape %1 : (tensor<1x32x16xf32>) -> tensor<32x16xf32>
    %3 = stablehlo.multiply %2, %0 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"} : tensor<32x16xf32>
    %4 = stablehlo.reshape %arg1 : (tensor<32x32xf32>) -> tensor<1x32x32xf32>
    %5 = stablehlo.reshape %4 : (tensor<1x32x32xf32>) -> tensor<32x32xf32>
    %6 = stablehlo.dot_general %5, %3, contracting_dims = [1] x [0] : (tensor<32x32xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
    return %3, %3, %6 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump Before DecoupleConstFanoutPass (decouple-const-fanout) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x16xf32>
    %1 = stablehlo.reshape %arg0 : (tensor<32x16xf32>) -> tensor<1x32x16xf32>
    %2 = stablehlo.reshape %1 : (tensor<1x32x16xf32>) -> tensor<32x16xf32>
    %3 = stablehlo.multiply %2, %0 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"} : tensor<32x16xf32>
    %4 = stablehlo.reshape %arg1 : (tensor<32x32xf32>) -> tensor<1x32x32xf32>
    %5 = stablehlo.reshape %4 : (tensor<1x32x32xf32>) -> tensor<32x32xf32>
    %6 = stablehlo.dot_general %5, %3, contracting_dims = [1] x [0] : (tensor<32x32xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
    return %3, %3, %6 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump Before FlattenCompositePass (flatten-composite) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x16xf32>
    %1 = stablehlo.reshape %arg0 : (tensor<32x16xf32>) -> tensor<1x32x16xf32>
    %2 = stablehlo.reshape %1 : (tensor<1x32x16xf32>) -> tensor<32x16xf32>
    %3 = stablehlo.multiply %2, %0 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"} : tensor<32x16xf32>
    %4 = stablehlo.reshape %arg1 : (tensor<32x32xf32>) -> tensor<1x32x32xf32>
    %5 = stablehlo.reshape %4 : (tensor<1x32x32xf32>) -> tensor<32x32xf32>
    %6 = stablehlo.dot_general %5, %3, contracting_dims = [1] x [0] : (tensor<32x32xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
    return %3, %3, %6 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump Before RegisterCustomShardingRulePass (register-custom-sharding-rule) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x16xf32>
    %1 = stablehlo.reshape %arg0 : (tensor<32x16xf32>) -> tensor<1x32x16xf32>
    %2 = stablehlo.reshape %1 : (tensor<1x32x16xf32>) -> tensor<32x16xf32>
    %3 = stablehlo.multiply %2, %0 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"} : tensor<32x16xf32>
    %4 = stablehlo.reshape %arg1 : (tensor<32x32xf32>) -> tensor<1x32x32xf32>
    %5 = stablehlo.reshape %4 : (tensor<1x32x32xf32>) -> tensor<32x32xf32>
    %6 = stablehlo.dot_general %5, %3, contracting_dims = [1] x [0] : (tensor<32x32xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
    return %3, %3, %6 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump Before ApplyShardingConstraintsPass (sdy-apply-sharding-constraints) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x16xf32>
    %1 = stablehlo.reshape %arg0 : (tensor<32x16xf32>) -> tensor<1x32x16xf32>
    %2 = stablehlo.reshape %1 : (tensor<1x32x16xf32>) -> tensor<32x16xf32>
    %3 = stablehlo.multiply %2, %0 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"} : tensor<32x16xf32>
    %4 = stablehlo.reshape %arg1 : (tensor<32x32xf32>) -> tensor<1x32x32xf32>
    %5 = stablehlo.reshape %4 : (tensor<1x32x32xf32>) -> tensor<32x32xf32>
    %6 = stablehlo.dot_general %5, %3, contracting_dims = [1] x [0] : (tensor<32x32xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
    return %3, %3, %6 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump Before AggressivePropagationPass (sdy-aggressive-propagate) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x16xf32>
    %1 = stablehlo.reshape %arg0 : (tensor<32x16xf32>) -> tensor<1x32x16xf32>
    %2 = stablehlo.reshape %1 : (tensor<1x32x16xf32>) -> tensor<32x16xf32>
    %3 = stablehlo.multiply %2, %0 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"} : tensor<32x16xf32>
    %4 = stablehlo.reshape %arg1 : (tensor<32x32xf32>) -> tensor<1x32x32xf32>
    %5 = stablehlo.reshape %4 : (tensor<1x32x32xf32>) -> tensor<32x32xf32>
    %6 = stablehlo.dot_general %5, %3, contracting_dims = [1] x [0] : (tensor<32x32xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
    return %3, %3, %6 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump After AggressivePropagationPass (sdy-aggressive-propagate) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<f32>) -> tensor<32x16xf32>
    %1 = stablehlo.reshape %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<32x16xf32>) -> tensor<1x32x16xf32>
    %2 = stablehlo.reshape %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x32x16xf32>) -> tensor<32x16xf32>
    %3 = stablehlo.multiply %2, %0 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x16xf32>
    %4 = stablehlo.reshape %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<32x32xf32>) -> tensor<1x32x32xf32>
    %5 = stablehlo.reshape %4 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x32x32xf32>) -> tensor<32x32xf32>
    %6 = stablehlo.dot_general %5, %3, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<32x32xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
    return %3, %3, %6 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump Before ShardingConstraintToReshardPass (sdy-sharding-constraint-to-reshard) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<f32>) -> tensor<32x16xf32>
    %1 = stablehlo.reshape %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<32x16xf32>) -> tensor<1x32x16xf32>
    %2 = stablehlo.reshape %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x32x16xf32>) -> tensor<32x16xf32>
    %3 = stablehlo.multiply %2, %0 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x16xf32>
    %4 = stablehlo.reshape %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<32x32xf32>) -> tensor<1x32x32xf32>
    %5 = stablehlo.reshape %4 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x32x32xf32>) -> tensor<32x32xf32>
    %6 = stablehlo.dot_general %5, %3, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<32x32xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
    return %3, %3, %6 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump Before InsertExplicitReshardsPass (insert-explicit-reshards) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<f32>) -> tensor<32x16xf32>
    %1 = stablehlo.reshape %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<32x16xf32>) -> tensor<1x32x16xf32>
    %2 = stablehlo.reshape %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x32x16xf32>) -> tensor<32x16xf32>
    %3 = stablehlo.multiply %2, %0 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x16xf32>
    %4 = stablehlo.reshape %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<32x32xf32>) -> tensor<1x32x32xf32>
    %5 = stablehlo.reshape %4 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x32x32xf32>) -> tensor<32x32xf32>
    %6 = stablehlo.dot_general %5, %3, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<32x32xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
    return %3, %3, %6 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump After InsertExplicitReshardsPass (insert-explicit-reshards) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<f32>) -> tensor<32x16xf32>
    %1 = stablehlo.reshape %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<32x16xf32>) -> tensor<1x32x16xf32>
    %2 = stablehlo.reshape %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x32x16xf32>) -> tensor<32x16xf32>
    %3 = stablehlo.multiply %2, %0 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x16xf32>
    %4 = stablehlo.reshape %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<32x32xf32>) -> tensor<1x32x32xf32>
    %5 = stablehlo.reshape %4 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x32x32xf32>) -> tensor<32x32xf32>
    %6 = sdy.reshard %3 <@mesh, [{?}, {?}]> : tensor<32x16xf32>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<32x32xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
    return %3, %3, %7 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump Before WrapUnderManualComputationPass (wrap-under-manual-computation) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<f32>) -> tensor<32x16xf32>
    %1 = stablehlo.reshape %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<32x16xf32>) -> tensor<1x32x16xf32>
    %2 = stablehlo.reshape %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x32x16xf32>) -> tensor<32x16xf32>
    %3 = stablehlo.multiply %2, %0 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x16xf32>
    %4 = stablehlo.reshape %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<32x32xf32>) -> tensor<1x32x32xf32>
    %5 = stablehlo.reshape %4 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x32x32xf32>) -> tensor<32x32xf32>
    %6 = sdy.reshard %3 <@mesh, [{?}, {?}]> : tensor<32x16xf32>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<32x32xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
    return %3, %3, %7 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump After WrapUnderManualComputationPass (wrap-under-manual-computation) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:3 = sdy.manual_computation(%arg0, %arg1) in_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{"_axis_0", ?}, {?}]>] manual_axes={} (%arg2: tensor<32x16xf32>, %arg3: tensor<32x32xf32>) {
      %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<f32>) -> tensor<32x16xf32>
      %2 = stablehlo.reshape %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<32x16xf32>) -> tensor<1x32x16xf32>
      %3 = stablehlo.reshape %2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x32x16xf32>) -> tensor<32x16xf32>
      %4 = stablehlo.multiply %3, %1 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x16xf32>
      %5 = stablehlo.reshape %arg3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<32x32xf32>) -> tensor<1x32x32xf32>
      %6 = stablehlo.reshape %5 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x32x32xf32>) -> tensor<32x32xf32>
      %7 = sdy.reshard %4 <@mesh, [{?}, {?}]> : tensor<32x16xf32>
      %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<32x32xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
      sdy.return %4, %4, %8 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
    } : (tensor<32x16xf32>, tensor<32x32xf32>) -> (tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>)
    return %0#0, %0#1, %0#2 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump Before ReshardToCollectivesPass (sdy-reshard-to-collectives) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:3 = sdy.manual_computation(%arg0, %arg1) in_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{"_axis_0", ?}, {?}]>] manual_axes={} (%arg2: tensor<32x16xf32>, %arg3: tensor<32x32xf32>) {
      %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<f32>) -> tensor<32x16xf32>
      %2 = stablehlo.reshape %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<32x16xf32>) -> tensor<1x32x16xf32>
      %3 = stablehlo.reshape %2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x32x16xf32>) -> tensor<32x16xf32>
      %4 = stablehlo.multiply %3, %1 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x16xf32>
      %5 = stablehlo.reshape %arg3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<32x32xf32>) -> tensor<1x32x32xf32>
      %6 = stablehlo.reshape %5 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x32x32xf32>) -> tensor<32x32xf32>
      %7 = sdy.reshard %4 <@mesh, [{?}, {?}]> : tensor<32x16xf32>
      %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<32x32xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
      sdy.return %4, %4, %8 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
    } : (tensor<32x16xf32>, tensor<32x32xf32>) -> (tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>)
    return %0#0, %0#1, %0#2 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump After ReshardToCollectivesPass (sdy-reshard-to-collectives) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:3 = sdy.manual_computation(%arg0, %arg1) in_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{"_axis_0", ?}, {?}]>] manual_axes={} (%arg2: tensor<32x16xf32>, %arg3: tensor<32x32xf32>) {
      %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<f32>) -> tensor<32x16xf32>
      %2 = stablehlo.reshape %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<32x16xf32>) -> tensor<1x32x16xf32>
      %3 = stablehlo.reshape %2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x32x16xf32>) -> tensor<32x16xf32>
      %4 = stablehlo.multiply %3, %1 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x16xf32>
      %5 = stablehlo.reshape %arg3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<32x32xf32>) -> tensor<1x32x32xf32>
      %6 = stablehlo.reshape %5 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x32x32xf32>) -> tensor<32x32xf32>
      %7 = sdy.all_gather [{}, {"_axis_0"}] %4 out_sharding=<@mesh, [{}, {}]> : tensor<32x16xf32>
      %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<32x32xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
      sdy.return %4, %4, %8 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
    } : (tensor<32x16xf32>, tensor<32x32xf32>) -> (tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>)
    return %0#0, %0#1, %0#2 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump Before UpdateGlobalToLocalShapesPass (update-global-to-local-shapes) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:3 = sdy.manual_computation(%arg0, %arg1) in_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{"_axis_0", ?}, {?}]>] manual_axes={} (%arg2: tensor<32x16xf32>, %arg3: tensor<32x32xf32>) {
      %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<f32>) -> tensor<32x16xf32>
      %2 = stablehlo.reshape %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<32x16xf32>) -> tensor<1x32x16xf32>
      %3 = stablehlo.reshape %2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x32x16xf32>) -> tensor<32x16xf32>
      %4 = stablehlo.multiply %3, %1 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x16xf32>
      %5 = stablehlo.reshape %arg3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<32x32xf32>) -> tensor<1x32x32xf32>
      %6 = stablehlo.reshape %5 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x32x32xf32>) -> tensor<32x32xf32>
      %7 = sdy.all_gather [{}, {"_axis_0"}] %4 out_sharding=<@mesh, [{}, {}]> : tensor<32x16xf32>
      %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<32x32xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
      sdy.return %4, %4, %8 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
    } : (tensor<32x16xf32>, tensor<32x32xf32>) -> (tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>)
    return %0#0, %0#1, %0#2 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump After UpdateGlobalToLocalShapesPass (update-global-to-local-shapes) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:3 = sdy.manual_computation(%arg0, %arg1) in_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{"_axis_0", ?}, {?}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg2: tensor<32x2xf32>, %arg3: tensor<4x32xf32>) {
      %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x2xf32>
      %2 = stablehlo.reshape %arg2 : (tensor<32x2xf32>) -> tensor<1x32x2xf32>
      %3 = stablehlo.reshape %2 : (tensor<1x32x2xf32>) -> tensor<32x2xf32>
      %4 = stablehlo.multiply %3, %1 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"} : tensor<32x2xf32>
      %5 = stablehlo.reshape %arg3 : (tensor<4x32xf32>) -> tensor<1x4x32xf32>
      %6 = stablehlo.reshape %5 : (tensor<1x4x32xf32>) -> tensor<4x32xf32>
      %7 = "stablehlo.all_gather"(%4) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
      %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] : (tensor<4x32xf32>, tensor<32x16xf32>) -> tensor<4x16xf32>
      sdy.return %4, %4, %8 : tensor<32x2xf32>, tensor<32x2xf32>, tensor<4x16xf32>
    } : (tensor<32x16xf32>, tensor<32x32xf32>) -> (tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>)
    return %0#0, %0#1, %0#2 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump Before ReoutlineCompositePass (reoutline-composite) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:3 = sdy.manual_computation(%arg0, %arg1) in_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{"_axis_0", ?}, {?}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg2: tensor<32x2xf32>, %arg3: tensor<4x32xf32>) {
      %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x2xf32>
      %2 = stablehlo.reshape %arg2 : (tensor<32x2xf32>) -> tensor<1x32x2xf32>
      %3 = stablehlo.reshape %2 : (tensor<1x32x2xf32>) -> tensor<32x2xf32>
      %4 = stablehlo.multiply %3, %1 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"} : tensor<32x2xf32>
      %5 = stablehlo.reshape %arg3 : (tensor<4x32xf32>) -> tensor<1x4x32xf32>
      %6 = stablehlo.reshape %5 : (tensor<1x4x32xf32>) -> tensor<4x32xf32>
      %7 = "stablehlo.all_gather"(%4) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
      %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] : (tensor<4x32xf32>, tensor<32x16xf32>) -> tensor<4x16xf32>
      sdy.return %4, %4, %8 : tensor<32x2xf32>, tensor<32x2xf32>, tensor<4x16xf32>
    } : (tensor<32x16xf32>, tensor<32x32xf32>) -> (tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>)
    return %0#0, %0#1, %0#2 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump Before CloseShardingsPass (sdy-close-shardings) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:3 = sdy.manual_computation(%arg0, %arg1) in_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{"_axis_0", ?}, {?}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg2: tensor<32x2xf32>, %arg3: tensor<4x32xf32>) {
      %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x2xf32>
      %2 = stablehlo.reshape %arg2 : (tensor<32x2xf32>) -> tensor<1x32x2xf32>
      %3 = stablehlo.reshape %2 : (tensor<1x32x2xf32>) -> tensor<32x2xf32>
      %4 = stablehlo.multiply %3, %1 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"} : tensor<32x2xf32>
      %5 = stablehlo.reshape %arg3 : (tensor<4x32xf32>) -> tensor<1x4x32xf32>
      %6 = stablehlo.reshape %5 : (tensor<1x4x32xf32>) -> tensor<4x32xf32>
      %7 = "stablehlo.all_gather"(%4) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
      %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] : (tensor<4x32xf32>, tensor<32x16xf32>) -> tensor<4x16xf32>
      sdy.return %4, %4, %8 : tensor<32x2xf32>, tensor<32x2xf32>, tensor<4x16xf32>
    } : (tensor<32x16xf32>, tensor<32x32xf32>) -> (tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>)
    return %0#0, %0#1, %0#2 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump After CloseShardingsPass (sdy-close-shardings) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:3 = sdy.manual_computation(%arg0, %arg1) in_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg2: tensor<32x2xf32>, %arg3: tensor<4x32xf32>) {
      %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x2xf32>
      %2 = stablehlo.reshape %arg2 : (tensor<32x2xf32>) -> tensor<1x32x2xf32>
      %3 = stablehlo.reshape %2 : (tensor<1x32x2xf32>) -> tensor<32x2xf32>
      %4 = stablehlo.multiply %3, %1 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"} : tensor<32x2xf32>
      %5 = stablehlo.reshape %arg3 : (tensor<4x32xf32>) -> tensor<1x4x32xf32>
      %6 = stablehlo.reshape %5 : (tensor<1x4x32xf32>) -> tensor<4x32xf32>
      %7 = "stablehlo.all_gather"(%4) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
      %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] : (tensor<4x32xf32>, tensor<32x16xf32>) -> tensor<4x16xf32>
      sdy.return %4, %4, %8 : tensor<32x2xf32>, tensor<32x2xf32>, tensor<4x16xf32>
    } : (tensor<32x16xf32>, tensor<32x32xf32>) -> (tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>)
    return %0#0, %0#1, %0#2 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:3 = sdy.manual_computation(%arg0, %arg1) in_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg2: tensor<32x2xf32>, %arg3: tensor<4x32xf32>) {
      %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x2xf32>
      %2 = stablehlo.reshape %arg2 : (tensor<32x2xf32>) -> tensor<1x32x2xf32>
      %3 = stablehlo.reshape %2 : (tensor<1x32x2xf32>) -> tensor<32x2xf32>
      %4 = stablehlo.multiply %3, %1 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"} : tensor<32x2xf32>
      %5 = stablehlo.reshape %arg3 : (tensor<4x32xf32>) -> tensor<1x4x32xf32>
      %6 = stablehlo.reshape %5 : (tensor<1x4x32xf32>) -> tensor<4x32xf32>
      %7 = "stablehlo.all_gather"(%4) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
      %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] : (tensor<4x32xf32>, tensor<32x16xf32>) -> tensor<4x16xf32>
      sdy.return %4, %4, %8 : tensor<32x2xf32>, tensor<32x2xf32>, tensor<4x16xf32>
    } : (tensor<32x16xf32>, tensor<32x32xf32>) -> (tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>)
    return %0#0, %0#1, %0#2 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


2025-11-26 19:57:58.059 (   2.221s) [        E0290480]      module_builder.cc:983      1| MLIR Module shlo_compiler:
#loc1 = loc("p0.2")
#loc2 = loc("p1.8")
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]> loc(#loc)
  func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"} loc("p0.2"), %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"} loc("p1.8")) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:3 = sdy.manual_computation(%arg0, %arg1) in_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg2: tensor<32x2xf32> loc("p0.2"), %arg3: tensor<4x32xf32> loc("p1.8")) {
      %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32> loc(#loc)
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x2xf32> loc(#loc)
      %2 = stablehlo.reshape %arg2 : (tensor<32x2xf32>) -> tensor<1x32x2xf32> loc(#loc3)
      %3 = stablehlo.reshape %2 : (tensor<1x32x2xf32>) -> tensor<32x2xf32> loc(#loc4)
      %4 = stablehlo.multiply %3, %1 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"} : tensor<32x2xf32> loc(#loc5)
      %5 = stablehlo.reshape %arg3 : (tensor<4x32xf32>) -> tensor<1x4x32xf32> loc(#loc6)
      %6 = stablehlo.reshape %5 : (tensor<1x4x32xf32>) -> tensor<4x32xf32> loc(#loc7)
      %7 = "stablehlo.all_gather"(%4) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> : (tensor<32x2xf32>) -> tensor<32x16xf32> loc(#loc5)
      %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] : (tensor<4x32xf32>, tensor<32x16xf32>) -> tensor<4x16xf32> loc(#loc8)
      sdy.return %4, %4, %8 : tensor<32x2xf32>, tensor<32x2xf32>, tensor<4x16xf32> loc(#loc)
    } : (tensor<32x16xf32>, tensor<32x32xf32>) -> (tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>) loc(#loc)
    return %0#0, %0#1, %0#2 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc3 = loc("reshape.3")
#loc4 = loc("reshape.5")
#loc5 = loc("multiply.7")
#loc6 = loc("reshape.9")
#loc7 = loc("reshape.11")
#loc8 = loc("dot.12")
------------------ END OF MLIR MODULE ------------------
// -----// IR Dump Before ConvertArithToStableHLO (convert-arith-to-stablehlo) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:3 = sdy.manual_computation(%arg0, %arg1) in_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg2: tensor<32x2xf32>, %arg3: tensor<4x32xf32>) {
      %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x2xf32>
      %2 = stablehlo.reshape %arg2 : (tensor<32x2xf32>) -> tensor<1x32x2xf32>
      %3 = stablehlo.reshape %2 : (tensor<1x32x2xf32>) -> tensor<32x2xf32>
      %4 = stablehlo.multiply %3, %1 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"} : tensor<32x2xf32>
      %5 = stablehlo.reshape %arg3 : (tensor<4x32xf32>) -> tensor<1x4x32xf32>
      %6 = stablehlo.reshape %5 : (tensor<1x4x32xf32>) -> tensor<4x32xf32>
      %7 = "stablehlo.all_gather"(%4) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
      %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] : (tensor<4x32xf32>, tensor<32x16xf32>) -> tensor<4x16xf32>
      sdy.return %4, %4, %8 : tensor<32x2xf32>, tensor<32x2xf32>, tensor<4x16xf32>
    } : (tensor<32x16xf32>, tensor<32x32xf32>) -> (tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>)
    return %0#0, %0#1, %0#2 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump Before LegalizeStableHLOCompositeToTTIR (legalize-stablehlo-composite-to-ttir) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:3 = sdy.manual_computation(%arg0, %arg1) in_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg2: tensor<32x2xf32>, %arg3: tensor<4x32xf32>) {
      %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x2xf32>
      %2 = stablehlo.reshape %arg2 : (tensor<32x2xf32>) -> tensor<1x32x2xf32>
      %3 = stablehlo.reshape %2 : (tensor<1x32x2xf32>) -> tensor<32x2xf32>
      %4 = stablehlo.multiply %3, %1 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"} : tensor<32x2xf32>
      %5 = stablehlo.reshape %arg3 : (tensor<4x32xf32>) -> tensor<1x4x32xf32>
      %6 = stablehlo.reshape %5 : (tensor<1x4x32xf32>) -> tensor<4x32xf32>
      %7 = "stablehlo.all_gather"(%4) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
      %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] : (tensor<4x32xf32>, tensor<32x16xf32>) -> tensor<4x16xf32>
      sdy.return %4, %4, %8 : tensor<32x2xf32>, tensor<32x2xf32>, tensor<4x16xf32>
    } : (tensor<32x16xf32>, tensor<32x32xf32>) -> (tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>)
    return %0#0, %0#1, %0#2 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump Before StablehloLegalizeCompositeToCallPass (stablehlo-legalize-composite-to-call) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:3 = sdy.manual_computation(%arg0, %arg1) in_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg2: tensor<32x2xf32>, %arg3: tensor<4x32xf32>) {
      %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x2xf32>
      %2 = stablehlo.reshape %arg2 : (tensor<32x2xf32>) -> tensor<1x32x2xf32>
      %3 = stablehlo.reshape %2 : (tensor<1x32x2xf32>) -> tensor<32x2xf32>
      %4 = stablehlo.multiply %3, %1 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"} : tensor<32x2xf32>
      %5 = stablehlo.reshape %arg3 : (tensor<4x32xf32>) -> tensor<1x4x32xf32>
      %6 = stablehlo.reshape %5 : (tensor<1x4x32xf32>) -> tensor<4x32xf32>
      %7 = "stablehlo.all_gather"(%4) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
      %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] : (tensor<4x32xf32>, tensor<32x16xf32>) -> tensor<4x16xf32>
      sdy.return %4, %4, %8 : tensor<32x2xf32>, tensor<32x2xf32>, tensor<4x16xf32>
    } : (tensor<32x16xf32>, tensor<32x32xf32>) -> (tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>)
    return %0#0, %0#1, %0#2 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump Before Inliner (inline) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:3 = sdy.manual_computation(%arg0, %arg1) in_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg2: tensor<32x2xf32>, %arg3: tensor<4x32xf32>) {
      %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x2xf32>
      %2 = stablehlo.reshape %arg2 : (tensor<32x2xf32>) -> tensor<1x32x2xf32>
      %3 = stablehlo.reshape %2 : (tensor<1x32x2xf32>) -> tensor<32x2xf32>
      %4 = stablehlo.multiply %3, %1 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"} : tensor<32x2xf32>
      %5 = stablehlo.reshape %arg3 : (tensor<4x32xf32>) -> tensor<1x4x32xf32>
      %6 = stablehlo.reshape %5 : (tensor<1x4x32xf32>) -> tensor<4x32xf32>
      %7 = "stablehlo.all_gather"(%4) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
      %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] : (tensor<4x32xf32>, tensor<32x16xf32>) -> tensor<4x16xf32>
      sdy.return %4, %4, %8 : tensor<32x2xf32>, tensor<32x2xf32>, tensor<4x16xf32>
    } : (tensor<32x16xf32>, tensor<32x32xf32>) -> (tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>)
    return %0#0, %0#1, %0#2 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:3 = sdy.manual_computation(%arg0, %arg1) in_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg2: tensor<32x2xf32>, %arg3: tensor<4x32xf32>) {
      %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x2xf32>
      %2 = stablehlo.reshape %arg2 : (tensor<32x2xf32>) -> tensor<1x32x2xf32>
      %3 = stablehlo.reshape %2 : (tensor<1x32x2xf32>) -> tensor<32x2xf32>
      %4 = stablehlo.multiply %3, %1 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"} : tensor<32x2xf32>
      %5 = stablehlo.reshape %arg3 : (tensor<4x32xf32>) -> tensor<1x4x32xf32>
      %6 = stablehlo.reshape %5 : (tensor<1x4x32xf32>) -> tensor<4x32xf32>
      %7 = "stablehlo.all_gather"(%4) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
      %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] : (tensor<4x32xf32>, tensor<32x16xf32>) -> tensor<4x16xf32>
      sdy.return %4, %4, %8 : tensor<32x2xf32>, tensor<32x2xf32>, tensor<4x16xf32>
    } : (tensor<32x16xf32>, tensor<32x32xf32>) -> (tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>)
    return %0#0, %0#1, %0#2 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump Before ConvertStableHLOToTTIR (convert-stablehlo-to-ttir) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:3 = sdy.manual_computation(%arg0, %arg1) in_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg2: tensor<32x2xf32>, %arg3: tensor<4x32xf32>) {
      %cst = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x2xf32>
      %2 = stablehlo.reshape %arg2 : (tensor<32x2xf32>) -> tensor<1x32x2xf32>
      %3 = stablehlo.reshape %2 : (tensor<1x32x2xf32>) -> tensor<32x2xf32>
      %4 = stablehlo.multiply %3, %1 {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {\22_axis_0\22}]>]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"} : tensor<32x2xf32>
      %5 = stablehlo.reshape %arg3 : (tensor<4x32xf32>) -> tensor<1x4x32xf32>
      %6 = stablehlo.reshape %5 : (tensor<1x4x32xf32>) -> tensor<4x32xf32>
      %7 = "stablehlo.all_gather"(%4) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
      %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] : (tensor<4x32xf32>, tensor<32x16xf32>) -> tensor<4x16xf32>
      sdy.return %4, %4, %8 : tensor<32x2xf32>, tensor<32x2xf32>, tensor<4x16xf32>
    } : (tensor<32x16xf32>, tensor<32x32xf32>) -> (tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>)
    return %0#0, %0#1, %0#2 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump After ConvertStableHLOToTTIR (convert-stablehlo-to-ttir) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
    %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
    %2 = "ttir.constant"() <{value = dense<2.000000e+00> : tensor<f32>}> : () -> tensor<f32>
    %3 = ttir.empty() : tensor<1x1xf32>
    %4 = "ttir.reshape"(%2, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %5 = ttir.empty() : tensor<32x2xf32>
    %6 = "ttir.broadcast"(%4, %5) <{broadcast_dimensions = array<i64: 32, 2>}> : (tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
    %7 = ttir.empty() : tensor<1x32x2xf32>
    %8 = "ttir.reshape"(%0, %7) <{shape = [1 : i32, 32 : i32, 2 : i32]}> : (tensor<32x2xf32>, tensor<1x32x2xf32>) -> tensor<1x32x2xf32>
    %9 = ttir.empty() : tensor<32x2xf32>
    %10 = "ttir.reshape"(%8, %9) <{shape = [32 : i32, 2 : i32]}> : (tensor<1x32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
    %11 = ttir.empty() : tensor<32x2xf32>
    %12 = "ttir.multiply"(%10, %6, %11) : (tensor<32x2xf32>, tensor<32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
    %13 = ttir.empty() : tensor<1x4x32xf32>
    %14 = "ttir.reshape"(%1, %13) <{shape = [1 : i32, 4 : i32, 32 : i32]}> : (tensor<4x32xf32>, tensor<1x4x32xf32>) -> tensor<1x4x32xf32>
    %15 = ttir.empty() : tensor<4x32xf32>
    %16 = "ttir.reshape"(%14, %15) <{shape = [4 : i32, 32 : i32]}> : (tensor<1x4x32xf32>, tensor<4x32xf32>) -> tensor<4x32xf32>
    %17 = ttir.empty() : tensor<32x16xf32>
    %18 = "ttir.all_gather"(%12, %17) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
    %19 = "ttir.dot_general"(%16, %18) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<4x32xf32>, tensor<32x16xf32>) -> tensor<4x16xf32>
    %20 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
    %21 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
    %22 = "ttir.mesh_shard"(%19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
    return %20, %21, %22 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump Before TTCoreWrapDeviceModulePass (ttcore-wrap-device-module) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
    %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
    %2 = "ttir.constant"() <{value = dense<2.000000e+00> : tensor<f32>}> : () -> tensor<f32>
    %3 = ttir.empty() : tensor<1x1xf32>
    %4 = "ttir.reshape"(%2, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %5 = ttir.empty() : tensor<32x2xf32>
    %6 = "ttir.broadcast"(%4, %5) <{broadcast_dimensions = array<i64: 32, 2>}> : (tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
    %7 = ttir.empty() : tensor<1x32x2xf32>
    %8 = "ttir.reshape"(%0, %7) <{shape = [1 : i32, 32 : i32, 2 : i32]}> : (tensor<32x2xf32>, tensor<1x32x2xf32>) -> tensor<1x32x2xf32>
    %9 = ttir.empty() : tensor<32x2xf32>
    %10 = "ttir.reshape"(%8, %9) <{shape = [32 : i32, 2 : i32]}> : (tensor<1x32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
    %11 = ttir.empty() : tensor<32x2xf32>
    %12 = "ttir.multiply"(%10, %6, %11) : (tensor<32x2xf32>, tensor<32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
    %13 = ttir.empty() : tensor<1x4x32xf32>
    %14 = "ttir.reshape"(%1, %13) <{shape = [1 : i32, 4 : i32, 32 : i32]}> : (tensor<4x32xf32>, tensor<1x4x32xf32>) -> tensor<1x4x32xf32>
    %15 = ttir.empty() : tensor<4x32xf32>
    %16 = "ttir.reshape"(%14, %15) <{shape = [4 : i32, 32 : i32]}> : (tensor<1x4x32xf32>, tensor<4x32xf32>) -> tensor<4x32xf32>
    %17 = ttir.empty() : tensor<32x16xf32>
    %18 = "ttir.all_gather"(%12, %17) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
    %19 = "ttir.dot_general"(%16, %18) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<4x32xf32>, tensor<32x16xf32>) -> tensor<4x16xf32>
    %20 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
    %21 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
    %22 = "ttir.mesh_shard"(%19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
    return %20, %21, %22 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
  }
}


// -----// IR Dump After TTCoreWrapDeviceModulePass (ttcore-wrap-device-module) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
      func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
        %2 = "ttir.constant"() <{value = dense<2.000000e+00> : tensor<f32>}> : () -> tensor<f32>
        %3 = ttir.empty() : tensor<1x1xf32>
        %4 = "ttir.reshape"(%2, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %5 = ttir.empty() : tensor<32x2xf32>
        %6 = "ttir.broadcast"(%4, %5) <{broadcast_dimensions = array<i64: 32, 2>}> : (tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %7 = ttir.empty() : tensor<1x32x2xf32>
        %8 = "ttir.reshape"(%0, %7) <{shape = [1 : i32, 32 : i32, 2 : i32]}> : (tensor<32x2xf32>, tensor<1x32x2xf32>) -> tensor<1x32x2xf32>
        %9 = ttir.empty() : tensor<32x2xf32>
        %10 = "ttir.reshape"(%8, %9) <{shape = [32 : i32, 2 : i32]}> : (tensor<1x32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %11 = ttir.empty() : tensor<32x2xf32>
        %12 = "ttir.multiply"(%10, %6, %11) : (tensor<32x2xf32>, tensor<32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %13 = ttir.empty() : tensor<1x4x32xf32>
        %14 = "ttir.reshape"(%1, %13) <{shape = [1 : i32, 4 : i32, 32 : i32]}> : (tensor<4x32xf32>, tensor<1x4x32xf32>) -> tensor<1x4x32xf32>
        %15 = ttir.empty() : tensor<4x32xf32>
        %16 = "ttir.reshape"(%14, %15) <{shape = [4 : i32, 32 : i32]}> : (tensor<1x4x32xf32>, tensor<4x32xf32>) -> tensor<4x32xf32>
        %17 = ttir.empty() : tensor<32x16xf32>
        %18 = "ttir.all_gather"(%12, %17) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %19 = "ttir.dot_general"(%16, %18) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<4x32xf32>, tensor<32x16xf32>) -> tensor<4x16xf32>
        %20 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %21 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %22 = "ttir.mesh_shard"(%19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
        return %20, %21, %22 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
      }
    }
  }
}


// -----// IR Dump Before TTIRHoistTransform (ttir-cpu-hoist-transform) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
      func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
        %2 = "ttir.constant"() <{value = dense<2.000000e+00> : tensor<f32>}> : () -> tensor<f32>
        %3 = ttir.empty() : tensor<1x1xf32>
        %4 = "ttir.reshape"(%2, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %5 = ttir.empty() : tensor<32x2xf32>
        %6 = "ttir.broadcast"(%4, %5) <{broadcast_dimensions = array<i64: 32, 2>}> : (tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %7 = ttir.empty() : tensor<1x32x2xf32>
        %8 = "ttir.reshape"(%0, %7) <{shape = [1 : i32, 32 : i32, 2 : i32]}> : (tensor<32x2xf32>, tensor<1x32x2xf32>) -> tensor<1x32x2xf32>
        %9 = ttir.empty() : tensor<32x2xf32>
        %10 = "ttir.reshape"(%8, %9) <{shape = [32 : i32, 2 : i32]}> : (tensor<1x32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %11 = ttir.empty() : tensor<32x2xf32>
        %12 = "ttir.multiply"(%10, %6, %11) : (tensor<32x2xf32>, tensor<32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %13 = ttir.empty() : tensor<1x4x32xf32>
        %14 = "ttir.reshape"(%1, %13) <{shape = [1 : i32, 4 : i32, 32 : i32]}> : (tensor<4x32xf32>, tensor<1x4x32xf32>) -> tensor<1x4x32xf32>
        %15 = ttir.empty() : tensor<4x32xf32>
        %16 = "ttir.reshape"(%14, %15) <{shape = [4 : i32, 32 : i32]}> : (tensor<1x4x32xf32>, tensor<4x32xf32>) -> tensor<4x32xf32>
        %17 = ttir.empty() : tensor<32x16xf32>
        %18 = "ttir.all_gather"(%12, %17) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %19 = "ttir.dot_general"(%16, %18) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<4x32xf32>, tensor<32x16xf32>) -> tensor<4x16xf32>
        %20 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %21 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %22 = "ttir.mesh_shard"(%19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
        return %20, %21, %22 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
      }
    }
  }
}


// -----// IR Dump Before ConvertStableHLOToTTIR (convert-stablehlo-to-ttir) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
      func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
        %2 = "ttir.constant"() <{value = dense<2.000000e+00> : tensor<f32>}> : () -> tensor<f32>
        %3 = ttir.empty() : tensor<1x1xf32>
        %4 = "ttir.reshape"(%2, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %5 = ttir.empty() : tensor<32x2xf32>
        %6 = "ttir.broadcast"(%4, %5) <{broadcast_dimensions = array<i64: 32, 2>}> : (tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %7 = ttir.empty() : tensor<1x32x2xf32>
        %8 = "ttir.reshape"(%0, %7) <{shape = [1 : i32, 32 : i32, 2 : i32]}> : (tensor<32x2xf32>, tensor<1x32x2xf32>) -> tensor<1x32x2xf32>
        %9 = ttir.empty() : tensor<32x2xf32>
        %10 = "ttir.reshape"(%8, %9) <{shape = [32 : i32, 2 : i32]}> : (tensor<1x32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %11 = ttir.empty() : tensor<32x2xf32>
        %12 = "ttir.multiply"(%10, %6, %11) : (tensor<32x2xf32>, tensor<32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %13 = ttir.empty() : tensor<1x4x32xf32>
        %14 = "ttir.reshape"(%1, %13) <{shape = [1 : i32, 4 : i32, 32 : i32]}> : (tensor<4x32xf32>, tensor<1x4x32xf32>) -> tensor<1x4x32xf32>
        %15 = ttir.empty() : tensor<4x32xf32>
        %16 = "ttir.reshape"(%14, %15) <{shape = [4 : i32, 32 : i32]}> : (tensor<1x4x32xf32>, tensor<4x32xf32>) -> tensor<4x32xf32>
        %17 = ttir.empty() : tensor<32x16xf32>
        %18 = "ttir.all_gather"(%12, %17) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %19 = "ttir.dot_general"(%16, %18) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<4x32xf32>, tensor<32x16xf32>) -> tensor<4x16xf32>
        %20 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %21 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %22 = "ttir.mesh_shard"(%19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
        return %20, %21, %22 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
      }
    }
  }
}


2025-11-26 19:57:58.069 (   2.230s) [        E0290480]      module_builder.cc:983      1| MLIR Module ttir:
#loc1 = loc("p0.2")
#loc2 = loc("p1.8")
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
      func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"} loc("p0.2"), %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"} loc("p1.8")) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32> loc(#loc)
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32> loc(#loc)
        %2 = "ttir.constant"() <{value = dense<2.000000e+00> : tensor<f32>}> : () -> tensor<f32> loc(#loc)
        %3 = ttir.empty() : tensor<1x1xf32> loc(#loc)
        %4 = "ttir.reshape"(%2, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc)
        %5 = ttir.empty() : tensor<32x2xf32> loc(#loc)
        %6 = "ttir.broadcast"(%4, %5) <{broadcast_dimensions = array<i64: 32, 2>}> : (tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32> loc(#loc)
        %7 = ttir.empty() : tensor<1x32x2xf32> loc(#loc3)
        %8 = "ttir.reshape"(%0, %7) <{shape = [1 : i32, 32 : i32, 2 : i32]}> : (tensor<32x2xf32>, tensor<1x32x2xf32>) -> tensor<1x32x2xf32> loc(#loc3)
        %9 = ttir.empty() : tensor<32x2xf32> loc(#loc4)
        %10 = "ttir.reshape"(%8, %9) <{shape = [32 : i32, 2 : i32]}> : (tensor<1x32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32> loc(#loc4)
        %11 = ttir.empty() : tensor<32x2xf32> loc(#loc5)
        %12 = "ttir.multiply"(%10, %6, %11) : (tensor<32x2xf32>, tensor<32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32> loc(#loc5)
        %13 = ttir.empty() : tensor<1x4x32xf32> loc(#loc6)
        %14 = "ttir.reshape"(%1, %13) <{shape = [1 : i32, 4 : i32, 32 : i32]}> : (tensor<4x32xf32>, tensor<1x4x32xf32>) -> tensor<1x4x32xf32> loc(#loc6)
        %15 = ttir.empty() : tensor<4x32xf32> loc(#loc7)
        %16 = "ttir.reshape"(%14, %15) <{shape = [4 : i32, 32 : i32]}> : (tensor<1x4x32xf32>, tensor<4x32xf32>) -> tensor<4x32xf32> loc(#loc7)
        %17 = ttir.empty() : tensor<32x16xf32> loc(#loc5)
        %18 = "ttir.all_gather"(%12, %17) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32> loc(#loc5)
        %19 = "ttir.dot_general"(%16, %18) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<4x32xf32>, tensor<32x16xf32>) -> tensor<4x16xf32> loc(#loc8)
        %20 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32> loc(#loc)
        %21 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32> loc(#loc)
        %22 = "ttir.mesh_shard"(%19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32> loc(#loc)
        return %20, %21, %22 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32> loc(#loc)
      } loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc3 = loc("reshape.3")
#loc4 = loc("reshape.5")
#loc5 = loc("multiply.7")
#loc6 = loc("reshape.9")
#loc7 = loc("reshape.11")
#loc8 = loc("dot.12")
------------------ END OF MLIR MODULE ------------------
2025-11-26 19:57:58.069 (   2.231s) [        E0290480]      module_builder.cc:784   WARN| `mhlo.num_partitions` attribute not found, assuming default number of partitions: 1
2025-11-26 19:57:58.069 (   2.231s) [        E0290480]      module_builder.cc:798   WARN| `mhlo.num_replicas` attribute not found, assuming default number of replicas: 1
2025-11-26 19:57:58.069 (   2.231s) [        E0290480]      module_builder.cc:808   WARN| Num replicas and num partitions are not set, inferring the number of devices from mesh shape
// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
      func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
        %2 = "ttir.constant"() <{value = dense<2.000000e+00> : tensor<f32>}> : () -> tensor<f32>
        %3 = ttir.empty() : tensor<1x1xf32>
        %4 = "ttir.reshape"(%2, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %5 = ttir.empty() : tensor<32x2xf32>
        %6 = "ttir.broadcast"(%4, %5) <{broadcast_dimensions = array<i64: 32, 2>}> : (tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %7 = ttir.empty() : tensor<1x32x2xf32>
        %8 = "ttir.reshape"(%0, %7) <{shape = [1 : i32, 32 : i32, 2 : i32]}> : (tensor<32x2xf32>, tensor<1x32x2xf32>) -> tensor<1x32x2xf32>
        %9 = ttir.empty() : tensor<32x2xf32>
        %10 = "ttir.reshape"(%8, %9) <{shape = [32 : i32, 2 : i32]}> : (tensor<1x32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %11 = ttir.empty() : tensor<32x2xf32>
        %12 = "ttir.multiply"(%10, %6, %11) : (tensor<32x2xf32>, tensor<32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %13 = ttir.empty() : tensor<1x4x32xf32>
        %14 = "ttir.reshape"(%1, %13) <{shape = [1 : i32, 4 : i32, 32 : i32]}> : (tensor<4x32xf32>, tensor<1x4x32xf32>) -> tensor<1x4x32xf32>
        %15 = ttir.empty() : tensor<4x32xf32>
        %16 = "ttir.reshape"(%14, %15) <{shape = [4 : i32, 32 : i32]}> : (tensor<1x4x32xf32>, tensor<4x32xf32>) -> tensor<4x32xf32>
        %17 = ttir.empty() : tensor<32x16xf32>
        %18 = "ttir.all_gather"(%12, %17) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %19 = "ttir.dot_general"(%16, %18) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<4x32xf32>, tensor<32x16xf32>) -> tensor<4x16xf32>
        %20 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %21 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %22 = "ttir.mesh_shard"(%19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
        return %20, %21, %22 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
      }
    }
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
      func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
        %3 = ttir.empty() : tensor<1x1xf32>
        %4 = "ttir.reshape"(%0, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %5 = ttir.empty() : tensor<32x2xf32>
        %6 = "ttir.broadcast"(%4, %5) <{broadcast_dimensions = array<i64: 32, 2>}> : (tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %7 = ttir.empty() : tensor<32x2xf32>
        %8 = "ttir.multiply"(%1, %6, %7) : (tensor<32x2xf32>, tensor<32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %9 = ttir.empty() : tensor<32x16xf32>
        %10 = "ttir.all_gather"(%8, %9) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %11 = "ttir.dot_general"(%2, %10) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<4x32xf32>, tensor<32x16xf32>) -> tensor<4x16xf32>
        %12 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %13 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %14 = "ttir.mesh_shard"(%11) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
        return %12, %13, %14 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
      }
    }
  }
}


// -----// IR Dump Before TTIRToTTIRDecomposition (ttir-to-ttir-decomposition) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
      func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
        %3 = ttir.empty() : tensor<1x1xf32>
        %4 = "ttir.reshape"(%0, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %5 = ttir.empty() : tensor<32x2xf32>
        %6 = "ttir.broadcast"(%4, %5) <{broadcast_dimensions = array<i64: 32, 2>}> : (tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %7 = ttir.empty() : tensor<32x2xf32>
        %8 = "ttir.multiply"(%1, %6, %7) : (tensor<32x2xf32>, tensor<32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %9 = ttir.empty() : tensor<32x16xf32>
        %10 = "ttir.all_gather"(%8, %9) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %11 = "ttir.dot_general"(%2, %10) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<4x32xf32>, tensor<32x16xf32>) -> tensor<4x16xf32>
        %12 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %13 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %14 = "ttir.mesh_shard"(%11) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
        return %12, %13, %14 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
      }
    }
  }
}


// -----// IR Dump After TTIRToTTIRDecomposition (ttir-to-ttir-decomposition) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
      func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
        %3 = ttir.empty() : tensor<1x1xf32>
        %4 = "ttir.reshape"(%0, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %5 = ttir.empty() : tensor<32x2xf32>
        %6 = "ttir.broadcast"(%4, %5) <{broadcast_dimensions = array<i64: 32, 2>}> : (tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %7 = ttir.empty() : tensor<32x2xf32>
        %8 = "ttir.multiply"(%1, %6, %7) : (tensor<32x2xf32>, tensor<32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %9 = ttir.empty() : tensor<32x16xf32>
        %10 = "ttir.all_gather"(%8, %9) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %11 = ttir.empty() : tensor<4x32xf32>
        %12 = "ttir.permute"(%2, %11) <{permutation = array<i64: 0, 1>}> : (tensor<4x32xf32>, tensor<4x32xf32>) -> tensor<4x32xf32>
        %13 = ttir.empty() : tensor<32x16xf32>
        %14 = "ttir.permute"(%10, %13) <{permutation = array<i64: 0, 1>}> : (tensor<32x16xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %15 = ttir.empty() : tensor<4x32xf32>
        %16 = "ttir.reshape"(%12, %15) <{shape = [4 : i32, 32 : i32]}> : (tensor<4x32xf32>, tensor<4x32xf32>) -> tensor<4x32xf32>
        %17 = ttir.empty() : tensor<32x16xf32>
        %18 = "ttir.reshape"(%14, %17) <{shape = [32 : i32, 16 : i32]}> : (tensor<32x16xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %19 = ttir.empty() : tensor<4x16xf32>
        %20 = "ttir.matmul"(%16, %18, %19) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32>, tensor<32x16xf32>, tensor<4x16xf32>) -> tensor<4x16xf32>
        %21 = ttir.empty() : tensor<4x16xf32>
        %22 = "ttir.reshape"(%20, %21) <{shape = [4 : i32, 16 : i32]}> : (tensor<4x16xf32>, tensor<4x16xf32>) -> tensor<4x16xf32>
        %23 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %24 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %25 = "ttir.mesh_shard"(%22) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
        return %23, %24, %25 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
      }
    }
  }
}


// -----// IR Dump Before TTCoreWrapDeviceModulePass (ttcore-wrap-device-module) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
      func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
        %3 = ttir.empty() : tensor<1x1xf32>
        %4 = "ttir.reshape"(%0, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %5 = ttir.empty() : tensor<32x2xf32>
        %6 = "ttir.broadcast"(%4, %5) <{broadcast_dimensions = array<i64: 32, 2>}> : (tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %7 = ttir.empty() : tensor<32x2xf32>
        %8 = "ttir.multiply"(%1, %6, %7) : (tensor<32x2xf32>, tensor<32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %9 = ttir.empty() : tensor<32x16xf32>
        %10 = "ttir.all_gather"(%8, %9) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %11 = ttir.empty() : tensor<4x32xf32>
        %12 = "ttir.permute"(%2, %11) <{permutation = array<i64: 0, 1>}> : (tensor<4x32xf32>, tensor<4x32xf32>) -> tensor<4x32xf32>
        %13 = ttir.empty() : tensor<32x16xf32>
        %14 = "ttir.permute"(%10, %13) <{permutation = array<i64: 0, 1>}> : (tensor<32x16xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %15 = ttir.empty() : tensor<4x32xf32>
        %16 = "ttir.reshape"(%12, %15) <{shape = [4 : i32, 32 : i32]}> : (tensor<4x32xf32>, tensor<4x32xf32>) -> tensor<4x32xf32>
        %17 = ttir.empty() : tensor<32x16xf32>
        %18 = "ttir.reshape"(%14, %17) <{shape = [32 : i32, 16 : i32]}> : (tensor<32x16xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %19 = ttir.empty() : tensor<4x16xf32>
        %20 = "ttir.matmul"(%16, %18, %19) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32>, tensor<32x16xf32>, tensor<4x16xf32>) -> tensor<4x16xf32>
        %21 = ttir.empty() : tensor<4x16xf32>
        %22 = "ttir.reshape"(%20, %21) <{shape = [4 : i32, 16 : i32]}> : (tensor<4x16xf32>, tensor<4x16xf32>) -> tensor<4x16xf32>
        %23 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %24 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %25 = "ttir.mesh_shard"(%22) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
        return %23, %24, %25 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
      }
    }
  }
}


// -----// IR Dump Before TTIRHoistTransform (ttir-cpu-hoist-transform) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
      func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
        %3 = ttir.empty() : tensor<1x1xf32>
        %4 = "ttir.reshape"(%0, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %5 = ttir.empty() : tensor<32x2xf32>
        %6 = "ttir.broadcast"(%4, %5) <{broadcast_dimensions = array<i64: 32, 2>}> : (tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %7 = ttir.empty() : tensor<32x2xf32>
        %8 = "ttir.multiply"(%1, %6, %7) : (tensor<32x2xf32>, tensor<32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %9 = ttir.empty() : tensor<32x16xf32>
        %10 = "ttir.all_gather"(%8, %9) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %11 = ttir.empty() : tensor<4x32xf32>
        %12 = "ttir.permute"(%2, %11) <{permutation = array<i64: 0, 1>}> : (tensor<4x32xf32>, tensor<4x32xf32>) -> tensor<4x32xf32>
        %13 = ttir.empty() : tensor<32x16xf32>
        %14 = "ttir.permute"(%10, %13) <{permutation = array<i64: 0, 1>}> : (tensor<32x16xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %15 = ttir.empty() : tensor<4x32xf32>
        %16 = "ttir.reshape"(%12, %15) <{shape = [4 : i32, 32 : i32]}> : (tensor<4x32xf32>, tensor<4x32xf32>) -> tensor<4x32xf32>
        %17 = ttir.empty() : tensor<32x16xf32>
        %18 = "ttir.reshape"(%14, %17) <{shape = [32 : i32, 16 : i32]}> : (tensor<32x16xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %19 = ttir.empty() : tensor<4x16xf32>
        %20 = "ttir.matmul"(%16, %18, %19) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32>, tensor<32x16xf32>, tensor<4x16xf32>) -> tensor<4x16xf32>
        %21 = ttir.empty() : tensor<4x16xf32>
        %22 = "ttir.reshape"(%20, %21) <{shape = [4 : i32, 16 : i32]}> : (tensor<4x16xf32>, tensor<4x16xf32>) -> tensor<4x16xf32>
        %23 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %24 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %25 = "ttir.mesh_shard"(%22) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
        return %23, %24, %25 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
      }
    }
  }
}


// -----// IR Dump Before ElementTypeNormalization (ttir-element-type-normalization) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
      func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
        %3 = ttir.empty() : tensor<1x1xf32>
        %4 = "ttir.reshape"(%0, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %5 = ttir.empty() : tensor<32x2xf32>
        %6 = "ttir.broadcast"(%4, %5) <{broadcast_dimensions = array<i64: 32, 2>}> : (tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %7 = ttir.empty() : tensor<32x2xf32>
        %8 = "ttir.multiply"(%1, %6, %7) : (tensor<32x2xf32>, tensor<32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %9 = ttir.empty() : tensor<32x16xf32>
        %10 = "ttir.all_gather"(%8, %9) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %11 = ttir.empty() : tensor<4x32xf32>
        %12 = "ttir.permute"(%2, %11) <{permutation = array<i64: 0, 1>}> : (tensor<4x32xf32>, tensor<4x32xf32>) -> tensor<4x32xf32>
        %13 = ttir.empty() : tensor<32x16xf32>
        %14 = "ttir.permute"(%10, %13) <{permutation = array<i64: 0, 1>}> : (tensor<32x16xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %15 = ttir.empty() : tensor<4x32xf32>
        %16 = "ttir.reshape"(%12, %15) <{shape = [4 : i32, 32 : i32]}> : (tensor<4x32xf32>, tensor<4x32xf32>) -> tensor<4x32xf32>
        %17 = ttir.empty() : tensor<32x16xf32>
        %18 = "ttir.reshape"(%14, %17) <{shape = [32 : i32, 16 : i32]}> : (tensor<32x16xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %19 = ttir.empty() : tensor<4x16xf32>
        %20 = "ttir.matmul"(%16, %18, %19) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32>, tensor<32x16xf32>, tensor<4x16xf32>) -> tensor<4x16xf32>
        %21 = ttir.empty() : tensor<4x16xf32>
        %22 = "ttir.reshape"(%20, %21) <{shape = [4 : i32, 16 : i32]}> : (tensor<4x16xf32>, tensor<4x16xf32>) -> tensor<4x16xf32>
        %23 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %24 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %25 = "ttir.mesh_shard"(%22) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
        return %23, %24, %25 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
      }
    }
  }
}


// -----// IR Dump After ElementTypeNormalization (ttir-element-type-normalization) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
      func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
        %3 = ttir.empty() : tensor<1x1xf32>
        %4 = "ttir.reshape"(%0, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %5 = ttir.empty() : tensor<32x2xf32>
        %6 = "ttir.broadcast"(%4, %5) <{broadcast_dimensions = array<i64: 32, 2>}> : (tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %7 = ttir.empty() : tensor<32x2xf32>
        %8 = "ttir.multiply"(%1, %6, %7) : (tensor<32x2xf32>, tensor<32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %9 = ttir.empty() : tensor<32x16xf32>
        %10 = "ttir.all_gather"(%8, %9) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %11 = ttir.empty() : tensor<4x16xf32>
        %12 = "ttir.matmul"(%2, %10, %11) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32>, tensor<32x16xf32>, tensor<4x16xf32>) -> tensor<4x16xf32>
        %13 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %14 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %15 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
        return %13, %14, %15 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
      }
    }
  }
}


// -----// IR Dump Before TTCoreRegisterDevicePass (ttcore-register-device) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
      func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
        %3 = ttir.empty() : tensor<1x1xf32>
        %4 = "ttir.reshape"(%0, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %5 = ttir.empty() : tensor<32x2xf32>
        %6 = "ttir.broadcast"(%4, %5) <{broadcast_dimensions = array<i64: 32, 2>}> : (tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %7 = ttir.empty() : tensor<32x2xf32>
        %8 = "ttir.multiply"(%1, %6, %7) : (tensor<32x2xf32>, tensor<32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %9 = ttir.empty() : tensor<32x16xf32>
        %10 = "ttir.all_gather"(%8, %9) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %11 = ttir.empty() : tensor<4x16xf32>
        %12 = "ttir.matmul"(%2, %10, %11) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32>, tensor<32x16xf32>, tensor<4x16xf32>) -> tensor<4x16xf32>
        %13 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %14 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %15 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
        return %13, %14, %15 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
      }
    }
  }
}


// -----// IR Dump After TTCoreRegisterDevicePass (ttcore-register-device) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
        %3 = ttir.empty() : tensor<1x1xf32>
        %4 = "ttir.reshape"(%0, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %5 = ttir.empty() : tensor<32x2xf32>
        %6 = "ttir.broadcast"(%4, %5) <{broadcast_dimensions = array<i64: 32, 2>}> : (tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %7 = ttir.empty() : tensor<32x2xf32>
        %8 = "ttir.multiply"(%1, %6, %7) : (tensor<32x2xf32>, tensor<32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %9 = ttir.empty() : tensor<32x16xf32>
        %10 = "ttir.all_gather"(%8, %9) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %11 = ttir.empty() : tensor<4x16xf32>
        %12 = "ttir.matmul"(%2, %10, %11) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32>, tensor<32x16xf32>, tensor<4x16xf32>) -> tensor<4x16xf32>
        %13 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %14 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %15 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
        return %13, %14, %15 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
      }
    }
  }
}


// -----// IR Dump Before TTPopulateArgumentTypes (tt-populate-argument-types) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
        %3 = ttir.empty() : tensor<1x1xf32>
        %4 = "ttir.reshape"(%0, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %5 = ttir.empty() : tensor<32x2xf32>
        %6 = "ttir.broadcast"(%4, %5) <{broadcast_dimensions = array<i64: 32, 2>}> : (tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %7 = ttir.empty() : tensor<32x2xf32>
        %8 = "ttir.multiply"(%1, %6, %7) : (tensor<32x2xf32>, tensor<32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %9 = ttir.empty() : tensor<32x16xf32>
        %10 = "ttir.all_gather"(%8, %9) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %11 = ttir.empty() : tensor<4x16xf32>
        %12 = "ttir.matmul"(%2, %10, %11) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32>, tensor<32x16xf32>, tensor<4x16xf32>) -> tensor<4x16xf32>
        %13 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %14 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %15 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
        return %13, %14, %15 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
      }
    }
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
        %3 = ttir.empty() : tensor<1x1xf32>
        %4 = "ttir.reshape"(%0, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %5 = ttir.empty() : tensor<32x2xf32>
        %6 = "ttir.broadcast"(%4, %5) <{broadcast_dimensions = array<i64: 32, 2>}> : (tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %7 = ttir.empty() : tensor<32x2xf32>
        %8 = "ttir.multiply"(%1, %6, %7) : (tensor<32x2xf32>, tensor<32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %9 = ttir.empty() : tensor<32x16xf32>
        %10 = "ttir.all_gather"(%8, %9) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %11 = ttir.empty() : tensor<4x16xf32>
        %12 = "ttir.matmul"(%2, %10, %11) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32>, tensor<32x16xf32>, tensor<4x16xf32>) -> tensor<4x16xf32>
        %13 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %14 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %15 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
        return %13, %14, %15 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
      }
    }
  }
}


// -----// IR Dump Before TTIRFusing (ttir-fusing) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
        %3 = ttir.empty() : tensor<1x1xf32>
        %4 = "ttir.reshape"(%0, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %5 = ttir.empty() : tensor<32x2xf32>
        %6 = "ttir.broadcast"(%4, %5) <{broadcast_dimensions = array<i64: 32, 2>}> : (tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %7 = ttir.empty() : tensor<32x2xf32>
        %8 = "ttir.multiply"(%1, %6, %7) : (tensor<32x2xf32>, tensor<32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %9 = ttir.empty() : tensor<32x16xf32>
        %10 = "ttir.all_gather"(%8, %9) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %11 = ttir.empty() : tensor<4x16xf32>
        %12 = "ttir.matmul"(%2, %10, %11) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32>, tensor<32x16xf32>, tensor<4x16xf32>) -> tensor<4x16xf32>
        %13 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %14 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %15 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
        return %13, %14, %15 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
      }
    }
  }
}


// -----// IR Dump Before TTIRQuantDequantConversion (ttir-quant-dequant-conversion) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
        %3 = ttir.empty() : tensor<1x1xf32>
        %4 = "ttir.reshape"(%0, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %5 = ttir.empty() : tensor<32x2xf32>
        %6 = "ttir.broadcast"(%4, %5) <{broadcast_dimensions = array<i64: 32, 2>}> : (tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %7 = ttir.empty() : tensor<32x2xf32>
        %8 = "ttir.multiply"(%1, %6, %7) : (tensor<32x2xf32>, tensor<32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %9 = ttir.empty() : tensor<32x16xf32>
        %10 = "ttir.all_gather"(%8, %9) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %11 = ttir.empty() : tensor<4x16xf32>
        %12 = "ttir.matmul"(%2, %10, %11) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32>, tensor<32x16xf32>, tensor<4x16xf32>) -> tensor<4x16xf32>
        %13 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %14 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %15 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
        return %13, %14, %15 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
      }
    }
  }
}


// -----// IR Dump Before TTIRToTTIRDecomposition (ttir-to-ttir-decomposition) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
        %3 = ttir.empty() : tensor<1x1xf32>
        %4 = "ttir.reshape"(%0, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %5 = ttir.empty() : tensor<32x2xf32>
        %6 = "ttir.broadcast"(%4, %5) <{broadcast_dimensions = array<i64: 32, 2>}> : (tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %7 = ttir.empty() : tensor<32x2xf32>
        %8 = "ttir.multiply"(%1, %6, %7) : (tensor<32x2xf32>, tensor<32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %9 = ttir.empty() : tensor<32x16xf32>
        %10 = "ttir.all_gather"(%8, %9) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %11 = ttir.empty() : tensor<4x16xf32>
        %12 = "ttir.matmul"(%2, %10, %11) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32>, tensor<32x16xf32>, tensor<4x16xf32>) -> tensor<4x16xf32>
        %13 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %14 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %15 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
        return %13, %14, %15 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
      }
    }
  }
}


// -----// IR Dump Before TTIRFusing (ttir-fusing) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
        %3 = ttir.empty() : tensor<1x1xf32>
        %4 = "ttir.reshape"(%0, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %5 = ttir.empty() : tensor<32x2xf32>
        %6 = "ttir.broadcast"(%4, %5) <{broadcast_dimensions = array<i64: 32, 2>}> : (tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %7 = ttir.empty() : tensor<32x2xf32>
        %8 = "ttir.multiply"(%1, %6, %7) : (tensor<32x2xf32>, tensor<32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %9 = ttir.empty() : tensor<32x16xf32>
        %10 = "ttir.all_gather"(%8, %9) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %11 = ttir.empty() : tensor<4x16xf32>
        %12 = "ttir.matmul"(%2, %10, %11) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32>, tensor<32x16xf32>, tensor<4x16xf32>) -> tensor<4x16xf32>
        %13 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %14 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %15 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
        return %13, %14, %15 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
      }
    }
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
        %3 = ttir.empty() : tensor<1x1xf32>
        %4 = "ttir.reshape"(%0, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %5 = ttir.empty() : tensor<32x2xf32>
        %6 = "ttir.broadcast"(%4, %5) <{broadcast_dimensions = array<i64: 32, 2>}> : (tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %7 = ttir.empty() : tensor<32x2xf32>
        %8 = "ttir.multiply"(%1, %6, %7) : (tensor<32x2xf32>, tensor<32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %9 = ttir.empty() : tensor<32x16xf32>
        %10 = "ttir.all_gather"(%8, %9) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %11 = ttir.empty() : tensor<4x16xf32>
        %12 = "ttir.matmul"(%2, %10, %11) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32>, tensor<32x16xf32>, tensor<4x16xf32>) -> tensor<4x16xf32>
        %13 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %14 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %15 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
        return %13, %14, %15 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
      }
    }
  }
}


// -----// IR Dump Before Inliner (inline) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
        %3 = ttir.empty() : tensor<1x1xf32>
        %4 = "ttir.reshape"(%0, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %5 = ttir.empty() : tensor<32x2xf32>
        %6 = "ttir.broadcast"(%4, %5) <{broadcast_dimensions = array<i64: 32, 2>}> : (tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %7 = ttir.empty() : tensor<32x2xf32>
        %8 = "ttir.multiply"(%1, %6, %7) : (tensor<32x2xf32>, tensor<32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %9 = ttir.empty() : tensor<32x16xf32>
        %10 = "ttir.all_gather"(%8, %9) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %11 = ttir.empty() : tensor<4x16xf32>
        %12 = "ttir.matmul"(%2, %10, %11) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32>, tensor<32x16xf32>, tensor<4x16xf32>) -> tensor<4x16xf32>
        %13 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %14 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %15 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
        return %13, %14, %15 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
      }
    }
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
        %3 = ttir.empty() : tensor<1x1xf32>
        %4 = "ttir.reshape"(%0, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %5 = ttir.empty() : tensor<32x2xf32>
        %6 = "ttir.broadcast"(%4, %5) <{broadcast_dimensions = array<i64: 32, 2>}> : (tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %7 = ttir.empty() : tensor<32x2xf32>
        %8 = "ttir.multiply"(%1, %6, %7) : (tensor<32x2xf32>, tensor<32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %9 = ttir.empty() : tensor<32x16xf32>
        %10 = "ttir.all_gather"(%8, %9) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %11 = ttir.empty() : tensor<4x16xf32>
        %12 = "ttir.matmul"(%2, %10, %11) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32>, tensor<32x16xf32>, tensor<4x16xf32>) -> tensor<4x16xf32>
        %13 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %14 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %15 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
        return %13, %14, %15 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
      }
    }
  }
}


// -----// IR Dump Before TTIRFlattenSlidingWindow (ttir-flatten-sliding-window) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
        %3 = ttir.empty() : tensor<1x1xf32>
        %4 = "ttir.reshape"(%0, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %5 = ttir.empty() : tensor<32x2xf32>
        %6 = "ttir.broadcast"(%4, %5) <{broadcast_dimensions = array<i64: 32, 2>}> : (tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %7 = ttir.empty() : tensor<32x2xf32>
        %8 = "ttir.multiply"(%1, %6, %7) : (tensor<32x2xf32>, tensor<32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %9 = ttir.empty() : tensor<32x16xf32>
        %10 = "ttir.all_gather"(%8, %9) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %11 = ttir.empty() : tensor<4x16xf32>
        %12 = "ttir.matmul"(%2, %10, %11) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32>, tensor<32x16xf32>, tensor<4x16xf32>) -> tensor<4x16xf32>
        %13 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %14 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %15 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
        return %13, %14, %15 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
      }
    }
  }
}


// -----// IR Dump Before TTIRExplicateTMs (ttir-explicate-tms) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
        %3 = ttir.empty() : tensor<1x1xf32>
        %4 = "ttir.reshape"(%0, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %5 = ttir.empty() : tensor<32x2xf32>
        %6 = "ttir.broadcast"(%4, %5) <{broadcast_dimensions = array<i64: 32, 2>}> : (tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %7 = ttir.empty() : tensor<32x2xf32>
        %8 = "ttir.multiply"(%1, %6, %7) : (tensor<32x2xf32>, tensor<32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %9 = ttir.empty() : tensor<32x16xf32>
        %10 = "ttir.all_gather"(%8, %9) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %11 = ttir.empty() : tensor<4x16xf32>
        %12 = "ttir.matmul"(%2, %10, %11) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32>, tensor<32x16xf32>, tensor<4x16xf32>) -> tensor<4x16xf32>
        %13 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %14 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %15 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
        return %13, %14, %15 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
      }
    }
  }
}


// -----// IR Dump Before TTIREraseInverseOps (ttir-erase-inverse-ops) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
        %3 = ttir.empty() : tensor<1x1xf32>
        %4 = "ttir.reshape"(%0, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %5 = ttir.empty() : tensor<32x2xf32>
        %6 = "ttir.broadcast"(%4, %5) <{broadcast_dimensions = array<i64: 32, 2>}> : (tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %7 = ttir.empty() : tensor<32x2xf32>
        %8 = "ttir.multiply"(%1, %6, %7) : (tensor<32x2xf32>, tensor<32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %9 = ttir.empty() : tensor<32x16xf32>
        %10 = "ttir.all_gather"(%8, %9) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %11 = ttir.empty() : tensor<4x16xf32>
        %12 = "ttir.matmul"(%2, %10, %11) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32>, tensor<32x16xf32>, tensor<4x16xf32>) -> tensor<4x16xf32>
        %13 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %14 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %15 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
        return %13, %14, %15 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
      }
    }
  }
}


// -----// IR Dump Before TTIRImplicitBroadcastFold (ttir-implicit-broadcast-fold) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
        %3 = ttir.empty() : tensor<1x1xf32>
        %4 = "ttir.reshape"(%0, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %5 = ttir.empty() : tensor<32x2xf32>
        %6 = "ttir.broadcast"(%4, %5) <{broadcast_dimensions = array<i64: 32, 2>}> : (tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %7 = ttir.empty() : tensor<32x2xf32>
        %8 = "ttir.multiply"(%1, %6, %7) : (tensor<32x2xf32>, tensor<32x2xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %9 = ttir.empty() : tensor<32x16xf32>
        %10 = "ttir.all_gather"(%8, %9) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %11 = ttir.empty() : tensor<4x16xf32>
        %12 = "ttir.matmul"(%2, %10, %11) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32>, tensor<32x16xf32>, tensor<4x16xf32>) -> tensor<4x16xf32>
        %13 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %14 = "ttir.mesh_shard"(%8) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %15 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
        return %13, %14, %15 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
      }
    }
  }
}


// -----// IR Dump After TTIRImplicitBroadcastFold (ttir-implicit-broadcast-fold) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
        %3 = ttir.empty() : tensor<1x1xf32>
        %4 = "ttir.reshape"(%0, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %5 = ttir.empty() : tensor<32x2xf32>
        %6 = "ttir.multiply"(%1, %4, %5) : (tensor<32x2xf32>, tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %7 = ttir.empty() : tensor<32x16xf32>
        %8 = "ttir.all_gather"(%6, %7) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %9 = ttir.empty() : tensor<4x16xf32>
        %10 = "ttir.matmul"(%2, %8, %9) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32>, tensor<32x16xf32>, tensor<4x16xf32>) -> tensor<4x16xf32>
        %11 = "ttir.mesh_shard"(%6) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %12 = "ttir.mesh_shard"(%6) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %13 = "ttir.mesh_shard"(%10) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
        return %11, %12, %13 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
      }
    }
  }
}


// -----// IR Dump Before TTIRFusing (ttir-fusing) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
        %3 = ttir.empty() : tensor<1x1xf32>
        %4 = "ttir.reshape"(%0, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %5 = ttir.empty() : tensor<32x2xf32>
        %6 = "ttir.multiply"(%1, %4, %5) : (tensor<32x2xf32>, tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %7 = ttir.empty() : tensor<32x16xf32>
        %8 = "ttir.all_gather"(%6, %7) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %9 = ttir.empty() : tensor<4x16xf32>
        %10 = "ttir.matmul"(%2, %8, %9) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32>, tensor<32x16xf32>, tensor<4x16xf32>) -> tensor<4x16xf32>
        %11 = "ttir.mesh_shard"(%6) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %12 = "ttir.mesh_shard"(%6) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %13 = "ttir.mesh_shard"(%10) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
        return %11, %12, %13 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
      }
    }
  }
}


// -----// IR Dump Before TTIRQuantDataTypeConversionPass (ttir-quant-data-type-conversion) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
        %3 = ttir.empty() : tensor<1x1xf32>
        %4 = "ttir.reshape"(%0, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %5 = ttir.empty() : tensor<32x2xf32>
        %6 = "ttir.multiply"(%1, %4, %5) : (tensor<32x2xf32>, tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %7 = ttir.empty() : tensor<32x16xf32>
        %8 = "ttir.all_gather"(%6, %7) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %9 = ttir.empty() : tensor<4x16xf32>
        %10 = "ttir.matmul"(%2, %8, %9) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32>, tensor<32x16xf32>, tensor<4x16xf32>) -> tensor<4x16xf32>
        %11 = "ttir.mesh_shard"(%6) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %12 = "ttir.mesh_shard"(%6) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %13 = "ttir.mesh_shard"(%10) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
        return %11, %12, %13 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
      }
    }
  }
}


// -----// IR Dump Before TTNNLayout (ttnn-layout) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<32x16xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32>) -> tensor<32x2xf32>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32>) -> tensor<4x32xf32>
        %3 = ttir.empty() : tensor<1x1xf32>
        %4 = "ttir.reshape"(%0, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %5 = ttir.empty() : tensor<32x2xf32>
        %6 = "ttir.multiply"(%1, %4, %5) : (tensor<32x2xf32>, tensor<1x1xf32>, tensor<32x2xf32>) -> tensor<32x2xf32>
        %7 = ttir.empty() : tensor<32x16xf32>
        %8 = "ttir.all_gather"(%6, %7) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32>, tensor<32x16xf32>) -> tensor<32x16xf32>
        %9 = ttir.empty() : tensor<4x16xf32>
        %10 = "ttir.matmul"(%2, %8, %9) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32>, tensor<32x16xf32>, tensor<4x16xf32>) -> tensor<4x16xf32>
        %11 = "ttir.mesh_shard"(%6) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %12 = "ttir.mesh_shard"(%6) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32>) -> tensor<32x16xf32>
        %13 = "ttir.mesh_shard"(%10) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32>) -> tensor<32x16xf32>
        return %11, %12, %13 : tensor<32x16xf32>, tensor<32x16xf32>, tensor<32x16xf32>
      }
    }
  }
}


// -----// IR Dump After TTNNLayout (ttnn-layout) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x16xf32, #system_memory>>
#ttnn_layout2 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x2xf32, #system_memory>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x16xf32, #system_memory>>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<32x16xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32, #ttnn_layout2>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32, #ttnn_layout>) -> tensor<4x32xf32, #ttnn_layout>
        %3 = ttir.empty() : tensor<1x1xf32, #ttnn_layout>
        %4 = "ttir.reshape"(%0, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout2>, tensor<1x1xf32, #ttnn_layout>) -> tensor<1x1xf32, #ttnn_layout>
        %5 = ttir.empty() : tensor<32x2xf32, #ttnn_layout>
        %6 = "ttir.multiply"(%1, %4, %5) : (tensor<32x2xf32, #ttnn_layout>, tensor<1x1xf32, #ttnn_layout>, tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout>
        %7 = ttir.empty() : tensor<32x16xf32, #ttnn_layout>
        %8 = "ttir.all_gather"(%6, %7) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32, #ttnn_layout>, tensor<32x16xf32, #ttnn_layout>) -> tensor<32x16xf32, #ttnn_layout>
        %9 = ttir.empty() : tensor<4x16xf32, #ttnn_layout>
        %10 = "ttir.matmul"(%2, %8, %9) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32, #ttnn_layout>, tensor<32x16xf32, #ttnn_layout>, tensor<4x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout>
        %11 = ttir.empty() : tensor<32x2xf32, #ttnn_layout3>
        %12 = ttir.to_layout %6, %11 : tensor<32x2xf32, #ttnn_layout> into tensor<32x2xf32, #ttnn_layout3> -> tensor<32x2xf32, #ttnn_layout3>
        %13 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout3>) -> tensor<32x16xf32, #ttnn_layout1>
        %14 = ttir.empty() : tensor<32x2xf32, #ttnn_layout3>
        %15 = ttir.to_layout %6, %14 : tensor<32x2xf32, #ttnn_layout> into tensor<32x2xf32, #ttnn_layout3> -> tensor<32x2xf32, #ttnn_layout3>
        %16 = "ttir.mesh_shard"(%15) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout3>) -> tensor<32x16xf32, #ttnn_layout1>
        %17 = ttir.empty() : tensor<4x16xf32, #ttnn_layout4>
        %18 = ttir.to_layout %10, %17 : tensor<4x16xf32, #ttnn_layout> into tensor<4x16xf32, #ttnn_layout4> -> tensor<4x16xf32, #ttnn_layout4>
        %19 = "ttir.mesh_shard"(%18) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32, #ttnn_layout4>) -> tensor<32x16xf32, #ttnn_layout1>
        return %13, %16, %19 : tensor<32x16xf32, #ttnn_layout1>, tensor<32x16xf32, #ttnn_layout1>, tensor<32x16xf32, #ttnn_layout1>
      }
    }
  }
}


// -----// IR Dump Before ConvertTTIRToTTNN (convert-ttir-to-ttnn) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x16xf32, #system_memory>>
#ttnn_layout2 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x2xf32, #system_memory>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x16xf32, #system_memory>>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<32x16xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32, #ttnn_layout2>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32, #ttnn_layout>) -> tensor<4x32xf32, #ttnn_layout>
        %3 = ttir.empty() : tensor<1x1xf32, #ttnn_layout>
        %4 = "ttir.reshape"(%0, %3) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout2>, tensor<1x1xf32, #ttnn_layout>) -> tensor<1x1xf32, #ttnn_layout>
        %5 = ttir.empty() : tensor<32x2xf32, #ttnn_layout>
        %6 = "ttir.multiply"(%1, %4, %5) : (tensor<32x2xf32, #ttnn_layout>, tensor<1x1xf32, #ttnn_layout>, tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout>
        %7 = ttir.empty() : tensor<32x16xf32, #ttnn_layout>
        %8 = "ttir.all_gather"(%6, %7) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32, #ttnn_layout>, tensor<32x16xf32, #ttnn_layout>) -> tensor<32x16xf32, #ttnn_layout>
        %9 = ttir.empty() : tensor<4x16xf32, #ttnn_layout>
        %10 = "ttir.matmul"(%2, %8, %9) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32, #ttnn_layout>, tensor<32x16xf32, #ttnn_layout>, tensor<4x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout>
        %11 = ttir.empty() : tensor<32x2xf32, #ttnn_layout3>
        %12 = ttir.to_layout %6, %11 : tensor<32x2xf32, #ttnn_layout> into tensor<32x2xf32, #ttnn_layout3> -> tensor<32x2xf32, #ttnn_layout3>
        %13 = "ttir.mesh_shard"(%12) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout3>) -> tensor<32x16xf32, #ttnn_layout1>
        %14 = ttir.empty() : tensor<32x2xf32, #ttnn_layout3>
        %15 = ttir.to_layout %6, %14 : tensor<32x2xf32, #ttnn_layout> into tensor<32x2xf32, #ttnn_layout3> -> tensor<32x2xf32, #ttnn_layout3>
        %16 = "ttir.mesh_shard"(%15) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout3>) -> tensor<32x16xf32, #ttnn_layout1>
        %17 = ttir.empty() : tensor<4x16xf32, #ttnn_layout4>
        %18 = ttir.to_layout %10, %17 : tensor<4x16xf32, #ttnn_layout> into tensor<4x16xf32, #ttnn_layout4> -> tensor<4x16xf32, #ttnn_layout4>
        %19 = "ttir.mesh_shard"(%18) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32, #ttnn_layout4>) -> tensor<32x16xf32, #ttnn_layout1>
        return %13, %16, %19 : tensor<32x16xf32, #ttnn_layout1>, tensor<32x16xf32, #ttnn_layout1>, tensor<32x16xf32, #ttnn_layout1>
      }
    }
  }
}


// -----// IR Dump After ConvertTTIRToTTNN (convert-ttir-to-ttnn) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x16xf32, #system_memory>>
#ttnn_layout2 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x2xf32, #system_memory>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x16xf32, #system_memory>>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<32x16xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<f32, #ttnn_layout2>
        %2 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32, #ttnn_layout>, !ttnn.device) -> tensor<32x2xf32, #ttnn_layout>
        %3 = "ttnn.mesh_shard"(%arg1, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32, #ttnn_layout>, !ttnn.device) -> tensor<4x32xf32, #ttnn_layout>
        %4 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xf32, #ttnn_layout>
        %5 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout2>) -> tensor<1x1xf32, #ttnn_layout>
        %6 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<32x2>}> : (!ttnn.device) -> tensor<32x2xf32, #ttnn_layout>
        %7 = "ttnn.multiply"(%2, %5) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<32x2xf32, #ttnn_layout>, tensor<1x1xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout>
        %8 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<32x16>}> : (!ttnn.device) -> tensor<32x16xf32, #ttnn_layout>
        %9 = "ttnn.all_gather"(%7) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x16xf32, #ttnn_layout>
        %10 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<4x16>}> : (!ttnn.device) -> tensor<4x16xf32, #ttnn_layout>
        %11 = "ttnn.matmul"(%3, %9) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32, #ttnn_layout>, tensor<32x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout>
        %12 = "ttnn.zeros"() <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, shape = #ttnn.shape<32x2>}> : () -> tensor<32x2xf32, #ttnn_layout3>
        %13 = "ttnn.to_layout"(%7) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout3>
        %14 = "ttnn.mesh_shard"(%13, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout3>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout1>
        %15 = "ttnn.zeros"() <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, shape = #ttnn.shape<32x2>}> : () -> tensor<32x2xf32, #ttnn_layout3>
        %16 = "ttnn.to_layout"(%7) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout3>
        %17 = "ttnn.mesh_shard"(%16, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout3>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout1>
        %18 = "ttnn.zeros"() <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, shape = #ttnn.shape<4x16>}> : () -> tensor<4x16xf32, #ttnn_layout4>
        %19 = "ttnn.to_layout"(%11) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<4x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout4>
        %20 = "ttnn.mesh_shard"(%19, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32, #ttnn_layout4>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout1>
        return %14, %17, %20 : tensor<32x16xf32, #ttnn_layout1>, tensor<32x16xf32, #ttnn_layout1>, tensor<32x16xf32, #ttnn_layout1>
      }
    }
  }
}


// -----// IR Dump Before TTNNFusing (ttnn-fusing) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x16xf32, #system_memory>>
#ttnn_layout2 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x2xf32, #system_memory>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x16xf32, #system_memory>>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<32x16xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<f32, #ttnn_layout2>
        %2 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32, #ttnn_layout>, !ttnn.device) -> tensor<32x2xf32, #ttnn_layout>
        %3 = "ttnn.mesh_shard"(%arg1, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32, #ttnn_layout>, !ttnn.device) -> tensor<4x32xf32, #ttnn_layout>
        %4 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xf32, #ttnn_layout>
        %5 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout2>) -> tensor<1x1xf32, #ttnn_layout>
        %6 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<32x2>}> : (!ttnn.device) -> tensor<32x2xf32, #ttnn_layout>
        %7 = "ttnn.multiply"(%2, %5) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<32x2xf32, #ttnn_layout>, tensor<1x1xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout>
        %8 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<32x16>}> : (!ttnn.device) -> tensor<32x16xf32, #ttnn_layout>
        %9 = "ttnn.all_gather"(%7) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x16xf32, #ttnn_layout>
        %10 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<4x16>}> : (!ttnn.device) -> tensor<4x16xf32, #ttnn_layout>
        %11 = "ttnn.matmul"(%3, %9) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32, #ttnn_layout>, tensor<32x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout>
        %12 = "ttnn.zeros"() <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, shape = #ttnn.shape<32x2>}> : () -> tensor<32x2xf32, #ttnn_layout3>
        %13 = "ttnn.to_layout"(%7) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout3>
        %14 = "ttnn.mesh_shard"(%13, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout3>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout1>
        %15 = "ttnn.zeros"() <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, shape = #ttnn.shape<32x2>}> : () -> tensor<32x2xf32, #ttnn_layout3>
        %16 = "ttnn.to_layout"(%7) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout3>
        %17 = "ttnn.mesh_shard"(%16, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout3>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout1>
        %18 = "ttnn.zeros"() <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, shape = #ttnn.shape<4x16>}> : () -> tensor<4x16xf32, #ttnn_layout4>
        %19 = "ttnn.to_layout"(%11) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<4x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout4>
        %20 = "ttnn.mesh_shard"(%19, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32, #ttnn_layout4>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout1>
        return %14, %17, %20 : tensor<32x16xf32, #ttnn_layout1>, tensor<32x16xf32, #ttnn_layout1>, tensor<32x16xf32, #ttnn_layout1>
      }
    }
  }
}


// -----// IR Dump After TTNNFusing (ttnn-fusing) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x16xf32, #system_memory>>
#ttnn_layout2 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x2xf32, #system_memory>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x16xf32, #system_memory>>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<32x16xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<f32, #ttnn_layout2>
        %2 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32, #ttnn_layout>, !ttnn.device) -> tensor<32x2xf32, #ttnn_layout>
        %3 = "ttnn.mesh_shard"(%arg1, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32, #ttnn_layout>, !ttnn.device) -> tensor<4x32xf32, #ttnn_layout>
        %4 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout2>) -> tensor<1x1xf32, #ttnn_layout>
        %5 = "ttnn.multiply"(%2, %4) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<32x2xf32, #ttnn_layout>, tensor<1x1xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout>
        %6 = "ttnn.all_gather"(%5) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x16xf32, #ttnn_layout>
        %7 = "ttnn.matmul"(%3, %6) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32, #ttnn_layout>, tensor<32x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout>
        %8 = "ttnn.to_layout"(%5) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout3>
        %9 = "ttnn.mesh_shard"(%8, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout3>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout1>
        %10 = "ttnn.to_layout"(%5) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout3>
        %11 = "ttnn.mesh_shard"(%10, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout3>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout1>
        %12 = "ttnn.to_layout"(%7) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<4x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout4>
        %13 = "ttnn.mesh_shard"(%12, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32, #ttnn_layout4>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout1>
        return %9, %11, %13 : tensor<32x16xf32, #ttnn_layout1>, tensor<32x16xf32, #ttnn_layout1>, tensor<32x16xf32, #ttnn_layout1>
      }
    }
  }
}


// -----// IR Dump Before TTNNWorkarounds (ttnn-workaround) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x16xf32, #system_memory>>
#ttnn_layout2 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x2xf32, #system_memory>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x16xf32, #system_memory>>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<32x16xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<f32, #ttnn_layout2>
        %2 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32, #ttnn_layout>, !ttnn.device) -> tensor<32x2xf32, #ttnn_layout>
        %3 = "ttnn.mesh_shard"(%arg1, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32, #ttnn_layout>, !ttnn.device) -> tensor<4x32xf32, #ttnn_layout>
        %4 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout2>) -> tensor<1x1xf32, #ttnn_layout>
        %5 = "ttnn.multiply"(%2, %4) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<32x2xf32, #ttnn_layout>, tensor<1x1xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout>
        %6 = "ttnn.all_gather"(%5) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x16xf32, #ttnn_layout>
        %7 = "ttnn.matmul"(%3, %6) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32, #ttnn_layout>, tensor<32x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout>
        %8 = "ttnn.to_layout"(%5) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout3>
        %9 = "ttnn.mesh_shard"(%8, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout3>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout1>
        %10 = "ttnn.to_layout"(%5) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout3>
        %11 = "ttnn.mesh_shard"(%10, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout3>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout1>
        %12 = "ttnn.to_layout"(%7) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<4x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout4>
        %13 = "ttnn.mesh_shard"(%12, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32, #ttnn_layout4>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout1>
        return %9, %11, %13 : tensor<32x16xf32, #ttnn_layout1>, tensor<32x16xf32, #ttnn_layout1>, tensor<32x16xf32, #ttnn_layout1>
      }
    }
  }
}


// -----// IR Dump After TTNNWorkarounds (ttnn-workaround) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x16xf32, #system_memory>>
#ttnn_layout2 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x2xf32, #system_memory>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x16xf32, #system_memory>>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<32x16xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<f32, #ttnn_layout2>
        %2 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32, #ttnn_layout>, !ttnn.device) -> tensor<32x2xf32, #ttnn_layout>
        %3 = "ttnn.mesh_shard"(%arg1, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32, #ttnn_layout>, !ttnn.device) -> tensor<4x32xf32, #ttnn_layout>
        %4 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout2>) -> tensor<1x1xf32, #ttnn_layout>
        %5 = "ttnn.multiply"(%2, %4) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<32x2xf32, #ttnn_layout>, tensor<1x1xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout>
        %6 = "ttnn.reshape"(%5) <{shape = [1 : i32, 1 : i32, 32 : i32, 2 : i32]}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<1x1x32x2xf32, #ttnn_layout3>
        %7 = "ttnn.all_gather"(%6) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32}> : (tensor<1x1x32x2xf32, #ttnn_layout3>) -> tensor<1x1x32x16xf32, #ttnn_layout3>
        %8 = "ttnn.reshape"(%7) <{shape = [32 : i32, 16 : i32]}> : (tensor<1x1x32x16xf32, #ttnn_layout3>) -> tensor<32x16xf32, #ttnn_layout>
        %9 = "ttnn.matmul"(%3, %8) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32, #ttnn_layout>, tensor<32x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout>
        %10 = "ttnn.to_layout"(%5) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout4>
        %11 = "ttnn.mesh_shard"(%10, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout4>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout1>
        %12 = "ttnn.to_layout"(%5) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout4>
        %13 = "ttnn.mesh_shard"(%12, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout4>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout1>
        %14 = "ttnn.to_layout"(%9) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<4x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout5>
        %15 = "ttnn.mesh_shard"(%14, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32, #ttnn_layout5>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout1>
        return %11, %13, %15 : tensor<32x16xf32, #ttnn_layout1>, tensor<32x16xf32, #ttnn_layout1>, tensor<32x16xf32, #ttnn_layout1>
      }
    }
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x16xf32, #system_memory>>
#ttnn_layout2 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x2xf32, #system_memory>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x16xf32, #system_memory>>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<32x16xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<f32, #ttnn_layout2>
        %2 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32, #ttnn_layout>, !ttnn.device) -> tensor<32x2xf32, #ttnn_layout>
        %3 = "ttnn.mesh_shard"(%arg1, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32, #ttnn_layout>, !ttnn.device) -> tensor<4x32xf32, #ttnn_layout>
        %4 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout2>) -> tensor<1x1xf32, #ttnn_layout>
        %5 = "ttnn.multiply"(%2, %4) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<32x2xf32, #ttnn_layout>, tensor<1x1xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout>
        %6 = "ttnn.reshape"(%5) <{shape = [1 : i32, 1 : i32, 32 : i32, 2 : i32]}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<1x1x32x2xf32, #ttnn_layout3>
        %7 = "ttnn.all_gather"(%6) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32}> : (tensor<1x1x32x2xf32, #ttnn_layout3>) -> tensor<1x1x32x16xf32, #ttnn_layout3>
        %8 = "ttnn.reshape"(%7) <{shape = [32 : i32, 16 : i32]}> : (tensor<1x1x32x16xf32, #ttnn_layout3>) -> tensor<32x16xf32, #ttnn_layout>
        %9 = "ttnn.matmul"(%3, %8) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32, #ttnn_layout>, tensor<32x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout>
        %10 = "ttnn.to_layout"(%5) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout4>
        %11 = "ttnn.mesh_shard"(%10, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout4>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout1>
        %12 = "ttnn.to_layout"(%5) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout4>
        %13 = "ttnn.mesh_shard"(%12, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout4>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout1>
        %14 = "ttnn.to_layout"(%9) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<4x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout5>
        %15 = "ttnn.mesh_shard"(%14, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32, #ttnn_layout5>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout1>
        return %11, %13, %15 : tensor<32x16xf32, #ttnn_layout1>, tensor<32x16xf32, #ttnn_layout1>, tensor<32x16xf32, #ttnn_layout1>
      }
    }
  }
}


// -----// IR Dump Before ConstEvalHoistTransform (const-eval-hoist-transform) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x16xf32, #system_memory>>
#ttnn_layout2 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x2xf32, #system_memory>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x16xf32, #system_memory>>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<32x16xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<f32, #ttnn_layout2>
        %2 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32, #ttnn_layout>, !ttnn.device) -> tensor<32x2xf32, #ttnn_layout>
        %3 = "ttnn.mesh_shard"(%arg1, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32, #ttnn_layout>, !ttnn.device) -> tensor<4x32xf32, #ttnn_layout>
        %4 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout2>) -> tensor<1x1xf32, #ttnn_layout>
        %5 = "ttnn.multiply"(%2, %4) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<32x2xf32, #ttnn_layout>, tensor<1x1xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout>
        %6 = "ttnn.reshape"(%5) <{shape = [1 : i32, 1 : i32, 32 : i32, 2 : i32]}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<1x1x32x2xf32, #ttnn_layout3>
        %7 = "ttnn.all_gather"(%6) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32}> : (tensor<1x1x32x2xf32, #ttnn_layout3>) -> tensor<1x1x32x16xf32, #ttnn_layout3>
        %8 = "ttnn.reshape"(%7) <{shape = [32 : i32, 16 : i32]}> : (tensor<1x1x32x16xf32, #ttnn_layout3>) -> tensor<32x16xf32, #ttnn_layout>
        %9 = "ttnn.matmul"(%3, %8) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32, #ttnn_layout>, tensor<32x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout>
        %10 = "ttnn.to_layout"(%5) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout4>
        %11 = "ttnn.mesh_shard"(%10, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout4>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout1>
        %12 = "ttnn.to_layout"(%5) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout4>
        %13 = "ttnn.mesh_shard"(%12, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout4>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout1>
        %14 = "ttnn.to_layout"(%9) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<4x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout5>
        %15 = "ttnn.mesh_shard"(%14, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32, #ttnn_layout5>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout1>
        return %11, %13, %15 : tensor<32x16xf32, #ttnn_layout1>, tensor<32x16xf32, #ttnn_layout1>, tensor<32x16xf32, #ttnn_layout1>
      }
    }
  }
}


// -----// IR Dump After ConstEvalHoistTransform (const-eval-hoist-transform) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x16xf32, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x2xf32, #system_memory>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x16xf32, #system_memory>>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main_const_eval_0() -> tensor<1x1xf32, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<f32, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout1>) -> tensor<1x1xf32, #ttnn_layout>
        return %2 : tensor<1x1xf32, #ttnn_layout>
      }
      func.func @main_const_eval_1(%arg0: tensor<32x32xf32, #ttnn_layout>) -> tensor<4x32xf32, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32, #ttnn_layout>, !ttnn.device) -> tensor<4x32xf32, #ttnn_layout>
        return %1 : tensor<4x32xf32, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<32x16xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xf32, #ttnn_layout>
        %1 = ttcore.load_cached(@main_const_eval_1, [%arg1]) : (tensor<32x32xf32, #ttnn_layout>) -> tensor<4x32xf32, #ttnn_layout>
        %2 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %3 = "ttnn.mesh_shard"(%arg0, %2) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32, #ttnn_layout>, !ttnn.device) -> tensor<32x2xf32, #ttnn_layout>
        %4 = "ttnn.multiply"(%3, %0) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<32x2xf32, #ttnn_layout>, tensor<1x1xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout>
        %5 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 32 : i32, 2 : i32]}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<1x1x32x2xf32, #ttnn_layout3>
        %6 = "ttnn.all_gather"(%5) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32}> : (tensor<1x1x32x2xf32, #ttnn_layout3>) -> tensor<1x1x32x16xf32, #ttnn_layout3>
        %7 = "ttnn.reshape"(%6) <{shape = [32 : i32, 16 : i32]}> : (tensor<1x1x32x16xf32, #ttnn_layout3>) -> tensor<32x16xf32, #ttnn_layout>
        %8 = "ttnn.matmul"(%1, %7) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32, #ttnn_layout>, tensor<32x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout>
        %9 = "ttnn.to_layout"(%4) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout4>
        %10 = "ttnn.mesh_shard"(%9, %2) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout4>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout2>
        %11 = "ttnn.to_layout"(%4) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout4>
        %12 = "ttnn.mesh_shard"(%11, %2) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout4>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout2>
        %13 = "ttnn.to_layout"(%8) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<4x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout5>
        %14 = "ttnn.mesh_shard"(%13, %2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32, #ttnn_layout5>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout2>
        return %10, %12, %14 : tensor<32x16xf32, #ttnn_layout2>, tensor<32x16xf32, #ttnn_layout2>, tensor<32x16xf32, #ttnn_layout2>
      }
    }
  }
}


// -----// IR Dump Before ConstEvalHoistTransform (const-eval-hoist-transform) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x16xf32, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x2xf32, #system_memory>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x16xf32, #system_memory>>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main_const_eval_0() -> tensor<1x1xf32, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<f32, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout1>) -> tensor<1x1xf32, #ttnn_layout>
        return %2 : tensor<1x1xf32, #ttnn_layout>
      }
      func.func @main_const_eval_1(%arg0: tensor<32x32xf32, #ttnn_layout>) -> tensor<4x32xf32, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32, #ttnn_layout>, !ttnn.device) -> tensor<4x32xf32, #ttnn_layout>
        return %1 : tensor<4x32xf32, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<32x16xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xf32, #ttnn_layout>
        %1 = ttcore.load_cached(@main_const_eval_1, [%arg1]) : (tensor<32x32xf32, #ttnn_layout>) -> tensor<4x32xf32, #ttnn_layout>
        %2 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %3 = "ttnn.mesh_shard"(%arg0, %2) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32, #ttnn_layout>, !ttnn.device) -> tensor<32x2xf32, #ttnn_layout>
        %4 = "ttnn.multiply"(%3, %0) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<32x2xf32, #ttnn_layout>, tensor<1x1xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout>
        %5 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 32 : i32, 2 : i32]}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<1x1x32x2xf32, #ttnn_layout3>
        %6 = "ttnn.all_gather"(%5) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32}> : (tensor<1x1x32x2xf32, #ttnn_layout3>) -> tensor<1x1x32x16xf32, #ttnn_layout3>
        %7 = "ttnn.reshape"(%6) <{shape = [32 : i32, 16 : i32]}> : (tensor<1x1x32x16xf32, #ttnn_layout3>) -> tensor<32x16xf32, #ttnn_layout>
        %8 = "ttnn.matmul"(%1, %7) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32, #ttnn_layout>, tensor<32x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout>
        %9 = "ttnn.to_layout"(%4) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout4>
        %10 = "ttnn.mesh_shard"(%9, %2) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout4>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout2>
        %11 = "ttnn.to_layout"(%4) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout4>
        %12 = "ttnn.mesh_shard"(%11, %2) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout4>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout2>
        %13 = "ttnn.to_layout"(%8) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<4x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout5>
        %14 = "ttnn.mesh_shard"(%13, %2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32, #ttnn_layout5>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout2>
        return %10, %12, %14 : tensor<32x16xf32, #ttnn_layout2>, tensor<32x16xf32, #ttnn_layout2>, tensor<32x16xf32, #ttnn_layout2>
      }
    }
  }
}


// -----// IR Dump After ConstEvalHoistTransform (const-eval-hoist-transform) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x16xf32, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x2xf32, #system_memory>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x16xf32, #system_memory>>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main_const_eval_0() -> tensor<1x1xf32, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<f32, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout1>) -> tensor<1x1xf32, #ttnn_layout>
        return %2 : tensor<1x1xf32, #ttnn_layout>
      }
      func.func @main_const_eval_1(%arg0: tensor<32x32xf32, #ttnn_layout>) -> tensor<4x32xf32, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32, #ttnn_layout>, !ttnn.device) -> tensor<4x32xf32, #ttnn_layout>
        return %1 : tensor<4x32xf32, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<32x16xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xf32, #ttnn_layout>
        %1 = ttcore.load_cached(@main_const_eval_1, [%arg1]) : (tensor<32x32xf32, #ttnn_layout>) -> tensor<4x32xf32, #ttnn_layout>
        %2 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %3 = "ttnn.mesh_shard"(%arg0, %2) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32, #ttnn_layout>, !ttnn.device) -> tensor<32x2xf32, #ttnn_layout>
        %4 = "ttnn.multiply"(%3, %0) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<32x2xf32, #ttnn_layout>, tensor<1x1xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout>
        %5 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 32 : i32, 2 : i32]}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<1x1x32x2xf32, #ttnn_layout3>
        %6 = "ttnn.all_gather"(%5) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32}> : (tensor<1x1x32x2xf32, #ttnn_layout3>) -> tensor<1x1x32x16xf32, #ttnn_layout3>
        %7 = "ttnn.reshape"(%6) <{shape = [32 : i32, 16 : i32]}> : (tensor<1x1x32x16xf32, #ttnn_layout3>) -> tensor<32x16xf32, #ttnn_layout>
        %8 = "ttnn.matmul"(%1, %7) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32, #ttnn_layout>, tensor<32x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout>
        %9 = "ttnn.to_layout"(%4) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout4>
        %10 = "ttnn.mesh_shard"(%9, %2) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout4>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout2>
        %11 = "ttnn.to_layout"(%4) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout4>
        %12 = "ttnn.mesh_shard"(%11, %2) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout4>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout2>
        %13 = "ttnn.to_layout"(%8) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<4x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout5>
        %14 = "ttnn.mesh_shard"(%13, %2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32, #ttnn_layout5>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout2>
        return %10, %12, %14 : tensor<32x16xf32, #ttnn_layout2>, tensor<32x16xf32, #ttnn_layout2>, tensor<32x16xf32, #ttnn_layout2>
      }
    }
  }
}


// -----// IR Dump Before TTNNDecomposeLayouts (ttnn-decompose-layouts) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x16xf32, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x2xf32, #system_memory>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x16xf32, #system_memory>>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main_const_eval_0() -> tensor<1x1xf32, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<f32, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout1>) -> tensor<1x1xf32, #ttnn_layout>
        return %2 : tensor<1x1xf32, #ttnn_layout>
      }
      func.func @main_const_eval_1(%arg0: tensor<32x32xf32, #ttnn_layout>) -> tensor<4x32xf32, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32, #ttnn_layout>, !ttnn.device) -> tensor<4x32xf32, #ttnn_layout>
        return %1 : tensor<4x32xf32, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<32x16xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xf32, #ttnn_layout>
        %1 = ttcore.load_cached(@main_const_eval_1, [%arg1]) : (tensor<32x32xf32, #ttnn_layout>) -> tensor<4x32xf32, #ttnn_layout>
        %2 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %3 = "ttnn.mesh_shard"(%arg0, %2) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32, #ttnn_layout>, !ttnn.device) -> tensor<32x2xf32, #ttnn_layout>
        %4 = "ttnn.multiply"(%3, %0) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<32x2xf32, #ttnn_layout>, tensor<1x1xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout>
        %5 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 32 : i32, 2 : i32]}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<1x1x32x2xf32, #ttnn_layout3>
        %6 = "ttnn.all_gather"(%5) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32}> : (tensor<1x1x32x2xf32, #ttnn_layout3>) -> tensor<1x1x32x16xf32, #ttnn_layout3>
        %7 = "ttnn.reshape"(%6) <{shape = [32 : i32, 16 : i32]}> : (tensor<1x1x32x16xf32, #ttnn_layout3>) -> tensor<32x16xf32, #ttnn_layout>
        %8 = "ttnn.matmul"(%1, %7) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32, #ttnn_layout>, tensor<32x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout>
        %9 = "ttnn.to_layout"(%4) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout4>
        %10 = "ttnn.mesh_shard"(%9, %2) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout4>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout2>
        %11 = "ttnn.to_layout"(%4) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout4>
        %12 = "ttnn.mesh_shard"(%11, %2) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout4>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout2>
        %13 = "ttnn.to_layout"(%8) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<4x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout5>
        %14 = "ttnn.mesh_shard"(%13, %2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32, #ttnn_layout5>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout2>
        return %10, %12, %14 : tensor<32x16xf32, #ttnn_layout2>, tensor<32x16xf32, #ttnn_layout2>, tensor<32x16xf32, #ttnn_layout2>
      }
    }
  }
}


// -----// IR Dump After TTNNDecomposeLayouts (ttnn-decompose-layouts) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x16xf32, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #system_memory>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x2xf32, #system_memory>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x16xf32, #system_memory>>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main_const_eval_0() -> tensor<1x1xf32, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<f32, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout1>) -> tensor<1x1xf32, #ttnn_layout>
        return %2 : tensor<1x1xf32, #ttnn_layout>
      }
      func.func @main_const_eval_1(%arg0: tensor<32x32xf32, #ttnn_layout>) -> tensor<4x32xf32, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32, #ttnn_layout>, !ttnn.device) -> tensor<4x32xf32, #ttnn_layout>
        return %1 : tensor<4x32xf32, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<32x16xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xf32, #ttnn_layout>
        %1 = ttcore.load_cached(@main_const_eval_1, [%arg1]) : (tensor<32x32xf32, #ttnn_layout>) -> tensor<4x32xf32, #ttnn_layout>
        %2 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %3 = "ttnn.mesh_shard"(%arg0, %2) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32, #ttnn_layout>, !ttnn.device) -> tensor<32x2xf32, #ttnn_layout>
        %4 = "ttnn.multiply"(%3, %0) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<32x2xf32, #ttnn_layout>, tensor<1x1xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout>
        %5 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 32 : i32, 2 : i32]}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<1x1x32x2xf32, #ttnn_layout3>
        %6 = "ttnn.all_gather"(%5) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32}> : (tensor<1x1x32x2xf32, #ttnn_layout3>) -> tensor<1x1x32x16xf32, #ttnn_layout3>
        %7 = "ttnn.reshape"(%6) <{shape = [32 : i32, 16 : i32]}> : (tensor<1x1x32x16xf32, #ttnn_layout3>) -> tensor<32x16xf32, #ttnn_layout>
        %8 = "ttnn.matmul"(%1, %7) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32, #ttnn_layout>, tensor<32x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout>
        %9 = "ttnn.from_device"(%4) : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout4>
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<row_major>}> : (tensor<32x2xf32, #ttnn_layout4>) -> tensor<32x2xf32, #ttnn_layout5>
        %11 = "ttnn.mesh_shard"(%10, %2) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout5>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout2>
        %12 = "ttnn.from_device"(%4) : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout4>
        %13 = "ttnn.to_layout"(%12) <{layout = #ttnn.layout<row_major>}> : (tensor<32x2xf32, #ttnn_layout4>) -> tensor<32x2xf32, #ttnn_layout5>
        %14 = "ttnn.mesh_shard"(%13, %2) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout5>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout2>
        %15 = "ttnn.from_device"(%8) : (tensor<4x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout4>
        %16 = "ttnn.to_layout"(%15) <{layout = #ttnn.layout<row_major>}> : (tensor<4x16xf32, #ttnn_layout4>) -> tensor<4x16xf32, #ttnn_layout6>
        %17 = "ttnn.mesh_shard"(%16, %2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32, #ttnn_layout6>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout2>
        return %11, %14, %17 : tensor<32x16xf32, #ttnn_layout2>, tensor<32x16xf32, #ttnn_layout2>, tensor<32x16xf32, #ttnn_layout2>
      }
    }
  }
}


// -----// IR Dump Before TTCoreOptimizationBarrierFold (ttcore-optimization-barrier-fold) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x16xf32, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #system_memory>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x2xf32, #system_memory>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x16xf32, #system_memory>>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main_const_eval_0() -> tensor<1x1xf32, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<f32, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout1>) -> tensor<1x1xf32, #ttnn_layout>
        return %2 : tensor<1x1xf32, #ttnn_layout>
      }
      func.func @main_const_eval_1(%arg0: tensor<32x32xf32, #ttnn_layout>) -> tensor<4x32xf32, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32, #ttnn_layout>, !ttnn.device) -> tensor<4x32xf32, #ttnn_layout>
        return %1 : tensor<4x32xf32, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<32x16xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xf32, #ttnn_layout>
        %1 = ttcore.load_cached(@main_const_eval_1, [%arg1]) : (tensor<32x32xf32, #ttnn_layout>) -> tensor<4x32xf32, #ttnn_layout>
        %2 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %3 = "ttnn.mesh_shard"(%arg0, %2) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32, #ttnn_layout>, !ttnn.device) -> tensor<32x2xf32, #ttnn_layout>
        %4 = "ttnn.multiply"(%3, %0) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<32x2xf32, #ttnn_layout>, tensor<1x1xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout>
        %5 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 32 : i32, 2 : i32]}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<1x1x32x2xf32, #ttnn_layout3>
        %6 = "ttnn.all_gather"(%5) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32}> : (tensor<1x1x32x2xf32, #ttnn_layout3>) -> tensor<1x1x32x16xf32, #ttnn_layout3>
        %7 = "ttnn.reshape"(%6) <{shape = [32 : i32, 16 : i32]}> : (tensor<1x1x32x16xf32, #ttnn_layout3>) -> tensor<32x16xf32, #ttnn_layout>
        %8 = "ttnn.matmul"(%1, %7) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32, #ttnn_layout>, tensor<32x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout>
        %9 = "ttnn.from_device"(%4) : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout4>
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<row_major>}> : (tensor<32x2xf32, #ttnn_layout4>) -> tensor<32x2xf32, #ttnn_layout5>
        %11 = "ttnn.mesh_shard"(%10, %2) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout5>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout2>
        %12 = "ttnn.from_device"(%4) : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout4>
        %13 = "ttnn.to_layout"(%12) <{layout = #ttnn.layout<row_major>}> : (tensor<32x2xf32, #ttnn_layout4>) -> tensor<32x2xf32, #ttnn_layout5>
        %14 = "ttnn.mesh_shard"(%13, %2) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout5>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout2>
        %15 = "ttnn.from_device"(%8) : (tensor<4x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout4>
        %16 = "ttnn.to_layout"(%15) <{layout = #ttnn.layout<row_major>}> : (tensor<4x16xf32, #ttnn_layout4>) -> tensor<4x16xf32, #ttnn_layout6>
        %17 = "ttnn.mesh_shard"(%16, %2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32, #ttnn_layout6>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout2>
        return %11, %14, %17 : tensor<32x16xf32, #ttnn_layout2>, tensor<32x16xf32, #ttnn_layout2>, tensor<32x16xf32, #ttnn_layout2>
      }
    }
  }
}


// -----// IR Dump Before TTNNDeallocate (ttnn-deallocate) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x16xf32, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #system_memory>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x2xf32, #system_memory>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x16xf32, #system_memory>>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main_const_eval_0() -> tensor<1x1xf32, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<f32, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout1>) -> tensor<1x1xf32, #ttnn_layout>
        return %2 : tensor<1x1xf32, #ttnn_layout>
      }
      func.func @main_const_eval_1(%arg0: tensor<32x32xf32, #ttnn_layout>) -> tensor<4x32xf32, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32, #ttnn_layout>, !ttnn.device) -> tensor<4x32xf32, #ttnn_layout>
        return %1 : tensor<4x32xf32, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<32x16xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xf32, #ttnn_layout>
        %1 = ttcore.load_cached(@main_const_eval_1, [%arg1]) : (tensor<32x32xf32, #ttnn_layout>) -> tensor<4x32xf32, #ttnn_layout>
        %2 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %3 = "ttnn.mesh_shard"(%arg0, %2) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32, #ttnn_layout>, !ttnn.device) -> tensor<32x2xf32, #ttnn_layout>
        %4 = "ttnn.multiply"(%3, %0) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<32x2xf32, #ttnn_layout>, tensor<1x1xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout>
        %5 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 32 : i32, 2 : i32]}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<1x1x32x2xf32, #ttnn_layout3>
        %6 = "ttnn.all_gather"(%5) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32}> : (tensor<1x1x32x2xf32, #ttnn_layout3>) -> tensor<1x1x32x16xf32, #ttnn_layout3>
        %7 = "ttnn.reshape"(%6) <{shape = [32 : i32, 16 : i32]}> : (tensor<1x1x32x16xf32, #ttnn_layout3>) -> tensor<32x16xf32, #ttnn_layout>
        %8 = "ttnn.matmul"(%1, %7) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32, #ttnn_layout>, tensor<32x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout>
        %9 = "ttnn.from_device"(%4) : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout4>
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<row_major>}> : (tensor<32x2xf32, #ttnn_layout4>) -> tensor<32x2xf32, #ttnn_layout5>
        %11 = "ttnn.mesh_shard"(%10, %2) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout5>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout2>
        %12 = "ttnn.from_device"(%4) : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout4>
        %13 = "ttnn.to_layout"(%12) <{layout = #ttnn.layout<row_major>}> : (tensor<32x2xf32, #ttnn_layout4>) -> tensor<32x2xf32, #ttnn_layout5>
        %14 = "ttnn.mesh_shard"(%13, %2) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout5>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout2>
        %15 = "ttnn.from_device"(%8) : (tensor<4x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout4>
        %16 = "ttnn.to_layout"(%15) <{layout = #ttnn.layout<row_major>}> : (tensor<4x16xf32, #ttnn_layout4>) -> tensor<4x16xf32, #ttnn_layout6>
        %17 = "ttnn.mesh_shard"(%16, %2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32, #ttnn_layout6>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout2>
        return %11, %14, %17 : tensor<32x16xf32, #ttnn_layout2>, tensor<32x16xf32, #ttnn_layout2>, tensor<32x16xf32, #ttnn_layout2>
      }
    }
  }
}


// -----// IR Dump After TTNNDeallocate (ttnn-deallocate) ('builtin.module' operation: @SyncTensorsGraph.14) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x16xf32, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #system_memory>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x2xf32, #system_memory>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x16xf32, #system_memory>>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main_const_eval_0() -> tensor<1x1xf32, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<f32, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout1>) -> tensor<1x1xf32, #ttnn_layout>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<f32, #ttnn_layout1>) -> ()
        return %2 : tensor<1x1xf32, #ttnn_layout>
      }
      func.func @main_const_eval_1(%arg0: tensor<32x32xf32, #ttnn_layout>) -> tensor<4x32xf32, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32, #ttnn_layout>, !ttnn.device) -> tensor<4x32xf32, #ttnn_layout>
        return %1 : tensor<4x32xf32, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<32x16xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<32x32xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"}) -> (tensor<32x16xf32, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xf32, #ttnn_layout>
        %1 = ttcore.load_cached(@main_const_eval_1, [%arg1]) : (tensor<32x32xf32, #ttnn_layout>) -> tensor<4x32xf32, #ttnn_layout>
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<32x32xf32, #ttnn_layout>) -> ()
        %2 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %3 = "ttnn.mesh_shard"(%arg0, %2) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32, #ttnn_layout>, !ttnn.device) -> tensor<32x2xf32, #ttnn_layout>
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<32x16xf32, #ttnn_layout>) -> ()
        %4 = "ttnn.multiply"(%3, %0) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<32x2xf32, #ttnn_layout>, tensor<1x1xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout>
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<32x2xf32, #ttnn_layout>) -> ()
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1xf32, #ttnn_layout>) -> ()
        %5 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 32 : i32, 2 : i32]}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<1x1x32x2xf32, #ttnn_layout3>
        %6 = "ttnn.all_gather"(%5) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32}> : (tensor<1x1x32x2xf32, #ttnn_layout3>) -> tensor<1x1x32x16xf32, #ttnn_layout3>
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x1x32x2xf32, #ttnn_layout3>) -> ()
        %7 = "ttnn.reshape"(%6) <{shape = [32 : i32, 16 : i32]}> : (tensor<1x1x32x16xf32, #ttnn_layout3>) -> tensor<32x16xf32, #ttnn_layout>
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x1x32x16xf32, #ttnn_layout3>) -> ()
        %8 = "ttnn.matmul"(%1, %7) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32, #ttnn_layout>, tensor<32x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout>
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<32x16xf32, #ttnn_layout>) -> ()
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<4x32xf32, #ttnn_layout>) -> ()
        %9 = "ttnn.from_device"(%4) : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout4>
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<row_major>}> : (tensor<32x2xf32, #ttnn_layout4>) -> tensor<32x2xf32, #ttnn_layout5>
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<32x2xf32, #ttnn_layout4>) -> ()
        %11 = "ttnn.mesh_shard"(%10, %2) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout5>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout2>
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<32x2xf32, #ttnn_layout5>) -> ()
        %12 = "ttnn.from_device"(%4) : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout4>
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<32x2xf32, #ttnn_layout>) -> ()
        %13 = "ttnn.to_layout"(%12) <{layout = #ttnn.layout<row_major>}> : (tensor<32x2xf32, #ttnn_layout4>) -> tensor<32x2xf32, #ttnn_layout5>
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<32x2xf32, #ttnn_layout4>) -> ()
        %14 = "ttnn.mesh_shard"(%13, %2) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout5>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout2>
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<32x2xf32, #ttnn_layout5>) -> ()
        %15 = "ttnn.from_device"(%8) : (tensor<4x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout4>
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<4x16xf32, #ttnn_layout>) -> ()
        %16 = "ttnn.to_layout"(%15) <{layout = #ttnn.layout<row_major>}> : (tensor<4x16xf32, #ttnn_layout4>) -> tensor<4x16xf32, #ttnn_layout6>
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<4x16xf32, #ttnn_layout4>) -> ()
        %17 = "ttnn.mesh_shard"(%16, %2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32, #ttnn_layout6>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout2>
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<4x16xf32, #ttnn_layout6>) -> ()
        return %11, %14, %17 : tensor<32x16xf32, #ttnn_layout2>, tensor<32x16xf32, #ttnn_layout2>, tensor<32x16xf32, #ttnn_layout2>
      }
    }
  }
}


2025-11-26 19:57:58.323 (   2.484s) [        E0290480]      module_builder.cc:983      1| MLIR Module ttnn:
#dram = #ttnn.buffer_type<dram>
#loc = loc(unknown)
#loc1 = loc("p0.2")
#loc2 = loc("p1.8")
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073145600, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073162624, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x16xf32, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #system_memory>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x2xf32, #system_memory>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x16xf32, #system_memory>>
module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.14 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]> loc(#loc)
      func.func @main_const_eval_0() -> tensor<1x1xf32, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<f32, #ttnn_layout1> loc(#loc)
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout1>) -> tensor<1x1xf32, #ttnn_layout> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<f32, #ttnn_layout1>) -> () loc(#loc)
        return %2 : tensor<1x1xf32, #ttnn_layout> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_1(%arg0: tensor<32x32xf32, #ttnn_layout> loc(unknown)) -> tensor<4x32xf32, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x32xf32, #ttnn_layout>, !ttnn.device) -> tensor<4x32xf32, #ttnn_layout> loc(#loc)
        return %1 : tensor<4x32xf32, #ttnn_layout> loc(#loc)
      } loc(#loc)
      func.func @main(%arg0: tensor<32x16xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"} loc("p0.2"), %arg1: tensor<32x32xf32, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___x1"} loc("p1.8")) -> (tensor<32x16xf32, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x16xf32, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xf32, #ttnn_layout> loc(#loc)
        %1 = ttcore.load_cached(@main_const_eval_1, [%arg1]) : (tensor<32x32xf32, #ttnn_layout>) -> tensor<4x32xf32, #ttnn_layout> loc(#loc)
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<32x32xf32, #ttnn_layout>) -> () loc(#loc)
        %2 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device loc(#loc)
        %3 = "ttnn.mesh_shard"(%arg0, %2) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x16xf32, #ttnn_layout>, !ttnn.device) -> tensor<32x2xf32, #ttnn_layout> loc(#loc)
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<32x16xf32, #ttnn_layout>) -> () loc(#loc)
        %4 = "ttnn.multiply"(%3, %0) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<32x2xf32, #ttnn_layout>, tensor<1x1xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout> loc(#loc3)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<32x2xf32, #ttnn_layout>) -> () loc(#loc3)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1xf32, #ttnn_layout>) -> () loc(#loc3)
        %5 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 32 : i32, 2 : i32]}> : (tensor<32x2xf32, #ttnn_layout>) -> tensor<1x1x32x2xf32, #ttnn_layout3> loc(#loc5)
        %6 = "ttnn.all_gather"(%5) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32}> : (tensor<1x1x32x2xf32, #ttnn_layout3>) -> tensor<1x1x32x16xf32, #ttnn_layout3> loc(#loc6)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x1x32x2xf32, #ttnn_layout3>) -> () loc(#loc6)
        %7 = "ttnn.reshape"(%6) <{shape = [32 : i32, 16 : i32]}> : (tensor<1x1x32x16xf32, #ttnn_layout3>) -> tensor<32x16xf32, #ttnn_layout> loc(#loc3)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x1x32x16xf32, #ttnn_layout3>) -> () loc(#loc3)
        %8 = "ttnn.matmul"(%1, %7) <{transpose_a = false, transpose_b = false}> : (tensor<4x32xf32, #ttnn_layout>, tensor<32x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout> loc(#loc4)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<32x16xf32, #ttnn_layout>) -> () loc(#loc4)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<4x32xf32, #ttnn_layout>) -> () loc(#loc4)
        %9 = "ttnn.from_device"(%4) : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout4> loc(#loc)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<row_major>}> : (tensor<32x2xf32, #ttnn_layout4>) -> tensor<32x2xf32, #ttnn_layout5> loc(#loc)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<32x2xf32, #ttnn_layout4>) -> () loc(#loc)
        %11 = "ttnn.mesh_shard"(%10, %2) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout5>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<32x2xf32, #ttnn_layout5>) -> () loc(#loc)
        %12 = "ttnn.from_device"(%4) : (tensor<32x2xf32, #ttnn_layout>) -> tensor<32x2xf32, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<32x2xf32, #ttnn_layout>) -> () loc(#loc)
        %13 = "ttnn.to_layout"(%12) <{layout = #ttnn.layout<row_major>}> : (tensor<32x2xf32, #ttnn_layout4>) -> tensor<32x2xf32, #ttnn_layout5> loc(#loc)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<32x2xf32, #ttnn_layout4>) -> () loc(#loc)
        %14 = "ttnn.mesh_shard"(%13, %2) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x2xf32, #ttnn_layout5>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<32x2xf32, #ttnn_layout5>) -> () loc(#loc)
        %15 = "ttnn.from_device"(%8) : (tensor<4x16xf32, #ttnn_layout>) -> tensor<4x16xf32, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<4x16xf32, #ttnn_layout>) -> () loc(#loc)
        %16 = "ttnn.to_layout"(%15) <{layout = #ttnn.layout<row_major>}> : (tensor<4x16xf32, #ttnn_layout4>) -> tensor<4x16xf32, #ttnn_layout6> loc(#loc)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<4x16xf32, #ttnn_layout4>) -> () loc(#loc)
        %17 = "ttnn.mesh_shard"(%16, %2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<4x16xf32, #ttnn_layout6>, !ttnn.device) -> tensor<32x16xf32, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<4x16xf32, #ttnn_layout6>) -> () loc(#loc)
        return %11, %14, %17 : tensor<32x16xf32, #ttnn_layout2>, tensor<32x16xf32, #ttnn_layout2>, tensor<32x16xf32, #ttnn_layout2> loc(#loc)
      } loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc3 = loc("multiply.7")
#loc4 = loc("dot.12")
#loc5 = loc("multiply.7_reshape_to_4d"(#loc3))
#loc6 = loc("multiply.7_all_gather_4d"(#loc3))
------------------ END OF MLIR MODULE ------------------
2025-11-26 19:57:58.326 (   2.487s) [        E0290480]loaded_executable_insta:276      1| LoadedExecutableInstance::PJRT_LoadedExecutable_GetExecutable
2025-11-26 19:57:58.326 (   2.487s) [        E0290480]loaded_executable_insta:295      1| LoadedExecutableInstance::PJRT_LoadedExecutable_AddressableDevices
2025-11-26 19:57:58.326 (   2.488s) [        E0290480]              stubs.inc:70    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2025-11-26 19:57:58.326 (   2.488s) [        E0290480]      error_instance.cc:52       1| ErrorInstance::PJRT_Error_Message
2025-11-26 19:57:58.326 (   2.488s) [        E0290480]      error_instance.cc:61       1| ErrorInstance::PJRT_Error_GetCode
2025-11-26 19:57:58.326 (   2.488s) [        E0290480]      error_instance.cc:46       1| ErrorInstance::PJRT_Error_Destroy
2025-11-26 19:57:58.326 (   2.488s) [        E0290480] executable_instance.cc:108      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-11-26 19:57:58.327 (   2.488s) [        E0290480] executable_instance.cc:108      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-11-26 19:57:58.330 (   2.491s) [        E0290480] executable_instance.cc:108      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-11-26 19:57:58.330 (   2.491s) [        E0290480] executable_instance.cc:108      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     buffer_instance.cc:662      1| BufferInstance::PJRT_Buffer_Device
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     buffer_instance.cc:662      1| BufferInstance::PJRT_Buffer_Device
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     buffer_instance.cc:662      1| BufferInstance::PJRT_Buffer_Device
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     buffer_instance.cc:662      1| BufferInstance::PJRT_Buffer_Device
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     buffer_instance.cc:662      1| BufferInstance::PJRT_Buffer_Device
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     buffer_instance.cc:662      1| BufferInstance::PJRT_Buffer_Device
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     buffer_instance.cc:662      1| BufferInstance::PJRT_Buffer_Device
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     buffer_instance.cc:662      1| BufferInstance::PJRT_Buffer_Device
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     buffer_instance.cc:662      1| BufferInstance::PJRT_Buffer_Device
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     buffer_instance.cc:662      1| BufferInstance::PJRT_Buffer_Device
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     buffer_instance.cc:662      1| BufferInstance::PJRT_Buffer_Device
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     buffer_instance.cc:662      1| BufferInstance::PJRT_Buffer_Device
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     buffer_instance.cc:662      1| BufferInstance::PJRT_Buffer_Device
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     buffer_instance.cc:662      1| BufferInstance::PJRT_Buffer_Device
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     buffer_instance.cc:662      1| BufferInstance::PJRT_Buffer_Device
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     buffer_instance.cc:662      1| BufferInstance::PJRT_Buffer_Device
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640] executable_instance.cc:140      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]loaded_executable_insta:331      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]flatbuffer_loaded_execu:270      1| FlatbufferLoadedExecutableInstance::Execute
2025-11-26 19:57:58.334 (   2.495s) [        10FF9640]     client_instance.cc:489      1| ClientInstance::getOrCreateMeshDevice - reusing already opened mesh device [1, 8]
2025-11-26 19:57:58.403 (   2.564s) [        10FF9640]flatbuffer_loaded_execu:205      1| Filled output at output_index 0 device_index 0 with shape [32, 16] and UID 16
2025-11-26 19:57:58.403 (   2.564s) [        10FF9640]flatbuffer_loaded_execu:205      1| Filled output at output_index 0 device_index 1 with shape [32, 16] and UID 17
2025-11-26 19:57:58.403 (   2.564s) [        10FF9640]flatbuffer_loaded_execu:205      1| Filled output at output_index 0 device_index 2 with shape [32, 16] and UID 18
2025-11-26 19:57:58.403 (   2.564s) [        10FF9640]flatbuffer_loaded_execu:205      1| Filled output at output_index 0 device_index 3 with shape [32, 16] and UID 19
2025-11-26 19:57:58.403 (   2.564s) [        10FF9640]flatbuffer_loaded_execu:205      1| Filled output at output_index 0 device_index 4 with shape [32, 16] and UID 20
2025-11-26 19:57:58.403 (   2.564s) [        10FF9640]flatbuffer_loaded_execu:205      1| Filled output at output_index 0 device_index 5 with shape [32, 16] and UID 21
2025-11-26 19:57:58.403 (   2.564s) [        10FF9640]flatbuffer_loaded_execu:205      1| Filled output at output_index 0 device_index 6 with shape [32, 16] and UID 22
2025-11-26 19:57:58.403 (   2.564s) [        10FF9640]flatbuffer_loaded_execu:205      1| Filled output at output_index 0 device_index 7 with shape [32, 16] and UID 23
2025-11-26 19:57:58.403 (   2.564s) [        10FF9640]flatbuffer_loaded_execu:205      1| Filled output at output_index 1 device_index 0 with shape [32, 16] and UID 24
2025-11-26 19:57:58.403 (   2.564s) [        10FF9640]flatbuffer_loaded_execu:205      1| Filled output at output_index 1 device_index 1 with shape [32, 16] and UID 25
2025-11-26 19:57:58.403 (   2.564s) [        10FF9640]flatbuffer_loaded_execu:205      1| Filled output at output_index 1 device_index 2 with shape [32, 16] and UID 26
2025-11-26 19:57:58.403 (   2.564s) [        10FF9640]flatbuffer_loaded_execu:205      1| Filled output at output_index 1 device_index 3 with shape [32, 16] and UID 27
2025-11-26 19:57:58.403 (   2.564s) [        10FF9640]flatbuffer_loaded_execu:205      1| Filled output at output_index 1 device_index 4 with shape [32, 16] and UID 28
2025-11-26 19:57:58.403 (   2.564s) [        10FF9640]flatbuffer_loaded_execu:205      1| Filled output at output_index 1 device_index 5 with shape [32, 16] and UID 29
2025-11-26 19:57:58.403 (   2.564s) [        10FF9640]flatbuffer_loaded_execu:205      1| Filled output at output_index 1 device_index 6 with shape [32, 16] and UID 30
2025-11-26 19:57:58.403 (   2.564s) [        10FF9640]flatbuffer_loaded_execu:205      1| Filled output at output_index 1 device_index 7 with shape [32, 16] and UID 31
2025-11-26 19:57:58.403 (   2.564s) [        10FF9640]flatbuffer_loaded_execu:205      1| Filled output at output_index 2 device_index 0 with shape [32, 16] and UID 32
2025-11-26 19:57:58.403 (   2.564s) [        10FF9640]flatbuffer_loaded_execu:205      1| Filled output at output_index 2 device_index 1 with shape [32, 16] and UID 33
2025-11-26 19:57:58.403 (   2.564s) [        10FF9640]flatbuffer_loaded_execu:205      1| Filled output at output_index 2 device_index 2 with shape [32, 16] and UID 34
2025-11-26 19:57:58.403 (   2.564s) [        10FF9640]flatbuffer_loaded_execu:205      1| Filled output at output_index 2 device_index 3 with shape [32, 16] and UID 35
2025-11-26 19:57:58.403 (   2.564s) [        10FF9640]flatbuffer_loaded_execu:205      1| Filled output at output_index 2 device_index 4 with shape [32, 16] and UID 36
2025-11-26 19:57:58.403 (   2.564s) [        10FF9640]flatbuffer_loaded_execu:205      1| Filled output at output_index 2 device_index 5 with shape [32, 16] and UID 37
2025-11-26 19:57:58.403 (   2.564s) [        10FF9640]flatbuffer_loaded_execu:205      1| Filled output at output_index 2 device_index 6 with shape [32, 16] and UID 38
2025-11-26 19:57:58.403 (   2.564s) [        10FF9640]flatbuffer_loaded_execu:205      1| Filled output at output_index 2 device_index 7 with shape [32, 16] and UID 39
2025-11-26 19:57:58.403 (   2.565s) [        10FF9640]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-11-26 19:57:58.403 (   2.565s) [        10FF9640]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-11-26 19:57:58.403 (   2.565s) [        10FF9640]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-11-26 19:57:58.403 (   2.565s) [        10FF9640]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-11-26 19:57:58.403 (   2.565s) [        10FF9640]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-11-26 19:57:58.403 (   2.565s) [        10FF9640]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-11-26 19:57:58.403 (   2.565s) [        10FF9640]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-11-26 19:57:58.403 (   2.565s) [        10FF9640]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-11-26 19:57:58.403 (   2.565s) [        10FF9640]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-11-26 19:57:58.403 (   2.565s) [        10FF9640]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:548      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:517      1| BufferInstance::PJRT_Buffer_ElementType
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:548      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:517      1| BufferInstance::PJRT_Buffer_ElementType
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:548      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:517      1| BufferInstance::PJRT_Buffer_ElementType
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:548      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:517      1| BufferInstance::PJRT_Buffer_ElementType
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:548      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:517      1| BufferInstance::PJRT_Buffer_ElementType
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:548      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:517      1| BufferInstance::PJRT_Buffer_ElementType
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:548      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:517      1| BufferInstance::PJRT_Buffer_ElementType
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:548      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:517      1| BufferInstance::PJRT_Buffer_ElementType
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:548      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:517      1| BufferInstance::PJRT_Buffer_ElementType
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:548      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:517      1| BufferInstance::PJRT_Buffer_ElementType
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:548      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:517      1| BufferInstance::PJRT_Buffer_ElementType
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:548      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:517      1| BufferInstance::PJRT_Buffer_ElementType
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:548      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:517      1| BufferInstance::PJRT_Buffer_ElementType
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:548      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:517      1| BufferInstance::PJRT_Buffer_ElementType
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:548      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:517      1| BufferInstance::PJRT_Buffer_ElementType
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:548      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:517      1| BufferInstance::PJRT_Buffer_ElementType
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:548      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:517      1| BufferInstance::PJRT_Buffer_ElementType
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:548      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:517      1| BufferInstance::PJRT_Buffer_ElementType
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:548      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:517      1| BufferInstance::PJRT_Buffer_ElementType
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:548      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:517      1| BufferInstance::PJRT_Buffer_ElementType
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:548      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:517      1| BufferInstance::PJRT_Buffer_ElementType
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:548      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:517      1| BufferInstance::PJRT_Buffer_ElementType
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:548      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:517      1| BufferInstance::PJRT_Buffer_ElementType
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:548      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:517      1| BufferInstance::PJRT_Buffer_ElementType
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:58.404 (   2.565s) [        10FF9640]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:58.404 (   2.566s) [        10FF9640]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:58.404 (   2.566s) [        10FF9640]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:58.404 (   2.566s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.404 (   2.566s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.404 (   2.566s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.404 (   2.566s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.404 (   2.566s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.404 (   2.566s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.404 (   2.566s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.404 (   2.566s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.404 (   2.566s) [        E0290480]     buffer_instance.cc:536      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-11-26 19:57:58.404 (   2.566s) [        E0290480]     buffer_instance.cc:517      1| BufferInstance::PJRT_Buffer_ElementType
2025-11-26 19:57:58.405 (   2.566s) [        E0290480]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.405 (   2.566s) [        E0290480]     buffer_instance.cc:558      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-11-26 19:57:58.405 (   2.566s) [        E0290480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-11-26 19:57:58.405 (   2.566s) [        B57FA640]     buffer_instance.cc:402      1| Returning tensor to host with host_runtime_tensors ct = 1 from device 0 with buffer UID 32
2025-11-26 19:57:58.405 (   2.566s) [        B4FF9640]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-11-26 19:57:58.405 (   2.566s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.405 (   2.566s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.405 (   2.566s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.405 (   2.566s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.405 (   2.566s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.405 (   2.566s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.405 (   2.566s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.405 (   2.566s) [        E0290480]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-11-26 19:57:58.405 (   2.566s) [        E0290480]     buffer_instance.cc:536      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-11-26 19:57:58.405 (   2.566s) [        E0290480]     buffer_instance.cc:517      1| BufferInstance::PJRT_Buffer_ElementType
2025-11-26 19:57:58.405 (   2.566s) [        E0290480]     buffer_instance.cc:525      1| BufferInstance::PJRT_Buffer_Dimensions
2025-11-26 19:57:58.405 (   2.566s) [        E0290480]     buffer_instance.cc:558      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-11-26 19:57:58.405 (   2.566s) [        E0290480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-11-26 19:57:58.405 (   2.566s) [        B67FC640]     buffer_instance.cc:402      1| Returning tensor to host with host_runtime_tensors ct = 1 from device 0 with buffer UID 16
2025-11-26 19:57:58.405 (   2.566s) [        B4FF9640]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-11-26 19:57:58.415 (   2.576s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:58.415 (   2.577s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:58.415 (   2.577s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:58.415 (   2.577s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:58.415 (   2.577s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:58.415 (   2.577s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:58.415 (   2.577s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:58.415 (   2.577s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:58.415 (   2.577s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:58.415 (   2.577s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:58.415 (   2.577s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:58.415 (   2.577s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:58.415 (   2.577s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:58.415 (   2.577s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:58.415 (   2.577s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:58.415 (   2.577s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:58.415 (   2.577s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:58.415 (   2.577s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:58.415 (   2.577s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:58.415 (   2.577s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:58.415 (   2.577s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:58.415 (   2.577s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:58.415 (   2.577s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:58.415 (   2.577s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
Using PJRT plugin directory: /localdev/jameszianxu/tt-xla/python_package/pjrt_plugin_tt
Using TT-Metal from the source tree: /localdev/jameszianxu/tt-xla/third_party/tt-mlir/src/tt-mlir/third_party/tt-metal/src/tt-metal
WARNING: TT plugin is setting XLA_STABLEHLO_COMPILE to 1. This is required for TT PJRT plugin to work correctly.
Created device mesh: (1, 8) with 8 devices
[tensor([[64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.],
        [64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.],
        [64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.],
        [64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.],
        [64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.],
        [64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.],
        [64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.],
        [64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.],
        [64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.],
        [64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.],
        [64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.],
        [64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.],
        [64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.],
        [64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.],
        [64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.],
        [64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.],
        [64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.],
        [64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.],
        [64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.],
        [64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.],
        [64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.],
        [64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.],
        [64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.],
        [64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.],
        [64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.],
        [64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.],
        [64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.],
        [64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.],
        [64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.],
        [64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.],
        [64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.],
        [64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,
         64., 64.]], grad_fn=<ToCopyBackward0>), tensor([[2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]])]
2025-11-26 19:57:59.095 (   3.256s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:59.095 (   3.256s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:59.095 (   3.256s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:59.095 (   3.256s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:59.095 (   3.256s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:59.095 (   3.256s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:59.095 (   3.256s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:59.095 (   3.256s) [        E0290480]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-11-26 19:57:59.737 (   3.898s) [        E0290480]     client_instance.cc:167      1| ClientInstance::~ClientInstance
2025-11-26 19:57:59.737 (   3.898s) [        E0290480]     client_instance.cc:566      1| Closing parent mesh.
